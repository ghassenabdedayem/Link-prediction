{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dF0MJHXZVRpt"
   },
   "source": [
    "# Packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5GfuP1hlTmg",
    "outputId": "8d120d7a-c5e7-40bb-f961-42578c1c6f82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\r\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from random import choice\n",
    "from scipy.sparse import identity, diags\n",
    "from unidecode import unidecode\n",
    "from urllib.request import urlopen\n",
    "import gzip\n",
    "import pickle\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "import io\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "id": "BJSlSwejljfp"
   },
   "outputs": [],
   "source": [
    "def save_subgraph_in_file(nbr_nodes, source_path='../input_data/edgelist.txt', destination_path='../input_data/small_edgelist.txt'):\n",
    "    G = nx.read_edgelist(source_path, delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "    G = G.subgraph(range(nbr_nodes))\n",
    "    nx.write_edgelist(G, path=destination_path, delimiter=',')\n",
    "    print(G.number_of_nodes(), 'nodes,', G.number_of_edges(), 'edges Graph extracted from', source_path[source_path.rfind('/')+1:])\n",
    "    G = nx.read_edgelist(destination_path, delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "    print(G.number_of_nodes(), 'nodes,', G.number_of_edges(), 'edges Graph saved in', destination_path[destination_path.rfind('/')+1:])\n",
    "    print(max(G.nodes))\n",
    "    return\n",
    "\n",
    "\n",
    "def read_train_val_graph(path='../input_data/edgelist.txt', val_ratio=0.1):\n",
    "    #gets the data from the file on the distant server\n",
    "    G = nx.read_edgelist(urlopen('https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/edgelist.txt'), delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "    nodes = list(G.nodes())\n",
    "    edges = list(G.edges())\n",
    "    n = G.number_of_nodes()\n",
    "    m = G.number_of_edges()\n",
    "    \n",
    "    random.shuffle(edges)\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(edges)\n",
    "    permutation = np.random.permutation(n)\n",
    "    # create a mapping from old nodes labels to new nodes labels\n",
    "    mapping_permutation = dict(zip(range(n), permutation))\n",
    "\n",
    "    G = nx.relabel_nodes(G, mapping_permutation)    \n",
    "    \n",
    "    edges = list(G.edges())\n",
    "    nodes = list(G.nodes())\n",
    "\n",
    "\n",
    "    print('Number of nodes:', n, 'number of edges:', m,'in the Complete set')\n",
    "\n",
    "    node_to_idx = dict()\n",
    "    for i, node in enumerate(nodes):\n",
    "        node_to_idx[node] = i\n",
    "\n",
    "    val_edges = list()\n",
    "    G_train = G.copy()\n",
    "\n",
    "    for edge in edges:\n",
    "        if random.random() < val_ratio and edge[0] < n and edge[1] < n:\n",
    "            val_edges.append(edge)\n",
    "            G_train.remove_edge(edge[0], edge[1]) # We remove the val edges from the graph G\n",
    "\n",
    "   \n",
    "    #for edge in val_edges:\n",
    "        \n",
    "\n",
    "    n = G_train.number_of_nodes()\n",
    "    m = G_train.number_of_edges()\n",
    "    train_edges = list(G_train.edges())\n",
    "\n",
    "    print('Number of nodes:', n, 'number of edges:', m, 'in the Training set')\n",
    "    print('len(nodes)', len(nodes))\n",
    "\n",
    "    y_val = [1]*len(val_edges)\n",
    "\n",
    "    n_val_edges = len(val_edges)\n",
    "    \n",
    "    print('Creating random val_edges...')\n",
    "    for i in range(n_val_edges):\n",
    "        n1 = nodes[randint(0, n-1)]\n",
    "        n2 = nodes[randint(0, n-1)]\n",
    "        (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "        while n2 >= n: #or (n1, n2) in train_edges:\n",
    "            if (n1, n2) in train_edges:\n",
    "                print((n1, n2), 'in train_edges:')\n",
    "            n1 = nodes[randint(0, n-1)]\n",
    "            n2 = nodes[randint(0, n-1)]\n",
    "            (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "        val_edges.append((n1, n2))\n",
    "\n",
    "    y_val.extend([0]*(n_val_edges))\n",
    "    \n",
    "    ### From Giannis /!\\\n",
    "    val_indices = np.zeros((2,len(val_edges)))\n",
    "    for i,edge in enumerate(val_edges):\n",
    "        val_indices[0,i] = node_to_idx[edge[0]]\n",
    "        val_indices[1,i] = node_to_idx[edge[1]]\n",
    "    \n",
    "    print('Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects')\n",
    "    print('Loaded from', path[path.rfind('/')+1:], 'and with a training validation split ratio =', val_ratio)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx, mapping_permutation\n",
    "\n",
    "def random_walk(G, node, walk_length):\n",
    "    walk = [node]\n",
    "  \n",
    "    for i in range(walk_length-1):\n",
    "        neibor_nodes = list(G.neighbors(walk[-1]))\n",
    "        if len(neibor_nodes) > 0:\n",
    "            next_node = choice(neibor_nodes)\n",
    "            walk.append(next_node)\n",
    "    walk = [node for node in walk] # in case the nodes are in string format, we don't need to cast into string, but if the nodes are in numeric or integer, we need this line to cast into string\n",
    "    return walk\n",
    "\n",
    "\n",
    "def generate_walks(G, num_walks, walk_length):\n",
    "  # Runs \"num_walks\" random walks from each node, and returns a list of all random walk\n",
    "    t = time()\n",
    "    print('Start generating walks....')\n",
    "    walks = list()  \n",
    "    for i in range(num_walks):\n",
    "        for node in G.nodes():\n",
    "            walk = random_walk(G, node, walk_length)\n",
    "            walks.append(walk)\n",
    "        #print('walks : ', walks)\n",
    "    print('Random walks generated in in {}s!'.format(round(time()-t)))\n",
    "    return walks\n",
    "\n",
    "def apply_word2vec_on_features(features, nodes, vector_size=128, window=5, min_count=0, sg=1, workers=8):\n",
    "    t = time()\n",
    "    print('Start applying Word2Vec...')\n",
    "    wv_model = Word2Vec(vector_size=vector_size, window=window, min_count=min_count, sg=sg, workers=workers)\n",
    "    wv_model.build_vocab(features)\n",
    "    wv_model.train(features, total_examples=wv_model.corpus_count, epochs=5) \n",
    "    print('Word2vec model trained on features in {} min!'.format(round((time()-t)/60)))\n",
    "    features_np = []\n",
    "    for node in nodes:\n",
    "        features_np.append(wv_model.wv[node])\n",
    "\n",
    "    features_np = np.array(features_np)\n",
    "    print(features_np.shape, 'features numpy array created in {} min!'.format(round((time()-t)/60)))\n",
    "    return features_np\n",
    "\n",
    "\n",
    "\n",
    "def normalize_adjacency(A):\n",
    "    n = A.shape[0]\n",
    "    A = A + identity(n)\n",
    "    degs = A.dot(np.ones(n))\n",
    "    inv_degs = np.power(degs, -1)\n",
    "    D_inv = diags(inv_degs)\n",
    "    A_hat = D_inv.dot(A)\n",
    "    return A_hat\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "def create_and_normalize_adjacency(G):\n",
    "    adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix of the training graph\n",
    "    #adj = normalize_adjacency(adj)\n",
    "    adj = normalize_adj(adj)\n",
    "    print('Created a normalized adjancency matrix of shape', adj.shape)\n",
    "    indices = np.array(adj.nonzero()) # Gets the positions of non zeros of adj into indices\n",
    "    print('Created indices', indices.shape, 'with the positions of non zeros in adj matrix')\n",
    "    return adj, indices\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "def text_to_list(text):\n",
    "    return unidecode(text).split(',')\n",
    "\n",
    "def intersection(lst1, lst2): # a function that returns the number of common items of two lists and 1 or 0 if there are common. This function will be used in add_authors_to_pairs to add this features to the pairs.\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    is_common = 1 if len(lst3)>0 else 0\n",
    "    return len(lst3)+1, is_common+1\n",
    "\n",
    "\n",
    "def add_authors_to_pairs (pairs, authors):\n",
    "    authors = pd.DataFrame(authors)\n",
    "    try: \n",
    "        pairs = pairs.detach().cpu().numpy()\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "\n",
    "    pairs_df = pd.DataFrame(np.transpose(pairs)).rename(columns={0: \"paper_1\", 1: \"paper_2\"})\n",
    "    pairs_df = pairs_df.merge(authors, left_on='paper_1', right_on='paper_permut', how='left').rename(columns={'authors': \"authors_1\"})\n",
    "    pairs_df = pairs_df.merge(authors, left_on='paper_2', right_on='paper_permut', how='left').rename(columns={'authors': \"authors_2\"})\n",
    "    pairs_df.drop(['paper_id_x', 'paper_id_y'], axis=1, inplace=True)\n",
    "\n",
    "    pairs_df['nb_common_author'] = pairs_df.apply(lambda row: intersection(row['authors_1'], row['authors_2'])[0], axis=1)\n",
    "    pairs_df['is_common_author'] = pairs_df.apply(lambda row: intersection(row['authors_1'], row['authors_2'])[1], axis=1)\n",
    "\n",
    "    pairs_tensor = torch.LongTensor(np.transpose(pairs_df[[\"paper_1\", \"paper_2\", 'is_common_author', 'nb_common_author']].values.tolist())).to(device)\n",
    "    \n",
    "    return pairs_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "HLahMzTSg4Xz"
   },
   "outputs": [],
   "source": [
    "def read_and_clean_abstracts (nodes, sample_length=-1, abstracts_path = 'https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/abstracts.txt'):\n",
    "    t = time()\n",
    "    abstracts = dict()\n",
    "    abstracts_list = list()\n",
    "    f = urlopen(abstracts_path)\n",
    "    \n",
    "    for i, line in tqdm(enumerate(f)):\n",
    "        if i == sample_length:\n",
    "            break\n",
    "        if i in nodes:\n",
    "            node, abstract = str(line).lower().split('|--|')\n",
    "            abstract = remove_stopwords(abstract)\n",
    "            #abstract = re.sub(r\"[,.;@#?!&$()-]\", \" \", abstract)\n",
    "            abstract = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", abstract)\n",
    "            #abstract = re.sub(r\"\\\\\", \" \", abstract)\n",
    "            abstract = remove_stopwords(abstract)\n",
    "\n",
    "            for word in abstract.split()[:-1]:\n",
    "                #abstract = abstract.replace(word, stemmer.stem(word))\n",
    "                abstract = abstract.replace(word, lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word), pos='s'), pos='a'), pos='n'), pos='v'), pos='r'))\n",
    "            \n",
    "            node = re.sub(\"[^0-9]\", \"\", node)\n",
    "            if i != int(node):\n",
    "                print('i and node not the same', i, node)\n",
    "            abstracts[int(node)] = abstract\n",
    "            abstracts_list.append(abstract)\n",
    "        \n",
    "    print('Text loaded and cleaned in {:.0f} min'.format((time()-t)/60))\n",
    "    return abstracts\n",
    "\n",
    "def doc_counter (documents, word): #a function that return the number of documents containing a word\n",
    "    counter = 0\n",
    "    for i in documents:\n",
    "        if word in documents[i]:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "8HP9n7D5g4Xz"
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        self.word_occurrence = {}\n",
    "        self.word2node = {}\n",
    "        self.words_list = []\n",
    "        self.sentences_list = []\n",
    "        self.sentences_list_words = []\n",
    "        self.num_words = 0\n",
    "        self.num_sentences = 0\n",
    "        self.longest_sentence = 0\n",
    "\n",
    "    def add_word(self, word, node):\n",
    "        if word not in self.word2index:\n",
    "            # First entry of word into vocabulary\n",
    "            self.words_list.append(word)\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "            self.word_occurrence[word] = 1\n",
    "            self.word2node[word] = [node]\n",
    "        else:\n",
    "            # Word exists; increase word count\n",
    "            self.word2count[word] += 1\n",
    "            self.word_occurrence[word] += 1\n",
    "            if node not in self.word2node[word]:\n",
    "                self.word2node[word].append(node)\n",
    "            # self.num_words += 1\n",
    "            \n",
    "    def add_sentence(self, sentence, node):\n",
    "        sentence_len = 0\n",
    "        for word in sentence.split()[:-1]:\n",
    "            sentence_len += 1\n",
    "            self.add_word(word, node)\n",
    "        if sentence_len > self.longest_sentence:\n",
    "            # This is the longest sentence\n",
    "            self.longest_sentence = sentence_len\n",
    "        # Count the number of sentences\n",
    "        self.num_sentences += 1\n",
    "        self.sentences_list.append(sentence)\n",
    "        self.sentences_list_words.append(sentence.split()[:-1])\n",
    "\n",
    "    def to_word(self, index):\n",
    "        return self.index2word[index]\n",
    "\n",
    "    def to_index(self, word):\n",
    "        return self.word2index[word]\n",
    "\n",
    "    def words(self):\n",
    "        return self.words_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_to_train (features, authors, adj, indices, val_indices, y_val):\n",
    "    \n",
    "    print('Preparing the data for training...')\n",
    "    \n",
    "    t = time()\n",
    "    \n",
    "    y_val = torch.LongTensor(y_val).to(device)\n",
    "\n",
    "    # Create class labels\n",
    "    y = np.zeros(2*indices.shape[1])\n",
    "    y[:indices.shape[1]] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
    "    y = torch.LongTensor(y).to(device)\n",
    "    \n",
    "    features = torch.FloatTensor(features).to(device)\n",
    "    \n",
    "    indices = torch.LongTensor(indices).to(device)\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "    #tfidf_matrix = sparse_mx_to_torch_sparse_tensor(tfidf_matrix).to(device)\n",
    "    \n",
    "    # the function add_authors_to_pairs converts into torch tensors and sends to Device    \n",
    "    val_indices = add_authors_to_pairs(val_indices, authors) #we add the authors to val_pairs\n",
    "    indices = add_authors_to_pairs(indices, authors) #we add the authors to indices    \n",
    "    rand_indices = np.random.randint(0, features.shape[0], (indices.shape[0],indices.shape[1]))# We take random indices each time we run an epoch\n",
    "    rand_indices = add_authors_to_pairs(rand_indices, authors)\n",
    "\n",
    "    pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices. \n",
    "    del(authors, indices, rand_indices)\n",
    "    \n",
    "    print('Data converted into torch tensors and authors added to indices in {:.0f} min'.format((time()-t)/60))\n",
    "\n",
    "    return features, adj, pairs, y, val_indices, y_val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_features_with_permutation(features, permutation):\n",
    "    new_features = np.zeros((len(features), len(features[0])))\n",
    "    for i in range(len(features)):\n",
    "        new_features[i] = features[permutation[i]]\n",
    "    return new_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ubeigqung4X0"
   },
   "source": [
    "# Load graph and authors data from sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oe6DhotaSuqg",
    "outputId": "12afce01-9a25-4855-fcc6-4563d9e7b524"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499 number of edges: 1091955 in the Complete set\n",
      "Number of nodes: 138499 number of edges: 982730 in the Training set\n",
      "len(nodes) 138499\n",
      "Creating random val_edges...\n",
      "Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects\n",
      "Loaded from edgelist.txt and with a training validation split ratio = 0.1\n"
     ]
    }
   ],
   "source": [
    "G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx, mapping_permutation = read_train_val_graph(val_ratio=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EyBibxd6zQhV",
    "outputId": "d296b9fc-3300-4e1c-b6fb-050415b901e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pd/glcnl2497w5b6xn3p94tnwlr0000gn/T/ipykernel_91389/2709409960.py:154: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix of the training graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a normalized adjancency matrix of shape (138499, 138499)\n",
      "Created indices (2, 1965460) with the positions of non zeros in adj matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pd/glcnl2497w5b6xn3p94tnwlr0000gn/T/ipykernel_91389/2709409960.py:148: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n"
     ]
    }
   ],
   "source": [
    "adj, indices = create_and_normalize_adjacency(G_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "dWAzHqmxmYff",
    "outputId": "74e1ad99-4e78-45ed-d85b-cf18c1ee295c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>authors</th>\n",
       "      <th>paper_permut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[James H. Niblock, Jian-Xun Peng, Karen R. McM...</td>\n",
       "      <td>12798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Jian-Xun Peng, Kang Li, De-Shuang Huang]</td>\n",
       "      <td>11956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[J. Heikkila]</td>\n",
       "      <td>110297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]</td>\n",
       "      <td>8849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[Long Zhang, Kang Li, Er-Wei Bai, George W. Ir...</td>\n",
       "      <td>47674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id                                            authors  paper_permut\n",
       "0         0  [James H. Niblock, Jian-Xun Peng, Karen R. McM...         12798\n",
       "1         1          [Jian-Xun Peng, Kang Li, De-Shuang Huang]         11956\n",
       "2         2                                      [J. Heikkila]        110297\n",
       "3         3    [L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]          8849\n",
       "4         4  [Long Zhang, Kang Li, Er-Wei Bai, George W. Ir...         47674"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors = pd.read_csv(urlopen('https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/authors.txt'), sep = '|', header=None)\n",
    "authors = authors.rename(columns={0: \"paper_id\", 2: \"authors\"})\n",
    "authors['authors'] = authors['authors'].apply(text_to_list)\n",
    "authors = authors[[\"paper_id\", \"authors\"]]\n",
    "authors = authors[authors['paper_id'] <= max(G.nodes())]\n",
    "authors['paper_permut'] = permutation[authors['paper_id']]\n",
    "authors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2czqslmiVfcQ"
   },
   "source": [
    "# Read processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf-_fYkF5xiO",
    "outputId": "86eaab25-a6af-4b7f-8bce-300126c5a4c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wv walks loaded from GCP in 73 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(138499, 64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time()\n",
    "from io import BytesIO\n",
    "\n",
    "walks_url = 'https://storage.googleapis.com/link_prediction_processed_data/walks_wv.npy'\n",
    "with urlopen(walks_url) as url:\n",
    "    data = url.read()\n",
    "\n",
    "# Create a seekable file-like object from the data\n",
    "fileobj = BytesIO(data)\n",
    "\n",
    "# Load the data from the file object\n",
    "walks_wv = np.load(fileobj)\n",
    "print('wv walks loaded from GCP in {:.0f} sec'.format(time()-t))\n",
    "walks_wv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IrvRUVduhoAI"
   },
   "outputs": [],
   "source": [
    "# Load the BART embedding torch tensor\n",
    "\n",
    "# url = \"https://storage.googleapis.com/link_prediction_processed_data/bart_embeddings.pt\"\n",
    "# response = requests.get(url)\n",
    "\n",
    "# with open(\"bart_embeddings.pt\", \"wb\") as f:\n",
    "#     f.write(response.content)\n",
    "\n",
    "# abstracts_bart_embeddings = torch.load('bart_embeddings.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LlQCihvfmBQP",
    "outputId": "a0ffca9d-02c2-47c3-bbda-f20779c638e7"
   },
   "outputs": [],
   "source": [
    "# # Load the wv300 mean embedding\n",
    "# url = 'https://storage.googleapis.com/link_prediction_processed_data/embedded_abstracts_local_wv300.npy'\n",
    "\n",
    "# response = requests.get(url)\n",
    "\n",
    "# with open('embedded_abstracts_local_wv300.npy', 'wb') as f:\n",
    "#     f.write(response.content)\n",
    "\n",
    "# # Load the numpy array from the saved file\n",
    "# local_wv300_abstracts = np.load('embedded_abstracts_local_wv300.npy')\n",
    "\n",
    "# local_wv300_abstracts.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F4jEFSE6in_w",
    "outputId": "64bb4a2e-b523-4f97-c6b7-2705ab9957f4"
   },
   "outputs": [],
   "source": [
    "# # Load the goog300 mean embedding\n",
    "# url = 'https://storage.googleapis.com/link_prediction_processed_data/embedded_abstracts_goog300.npy'\n",
    "\n",
    "# response = requests.get(url)\n",
    "\n",
    "# with open('embedded_abstracts_goog300.npy', 'wb') as f:\n",
    "#     f.write(response.content)\n",
    "\n",
    "# # Load the numpy array from the saved file\n",
    "# goog300_abstracts = np.load('embedded_abstracts_goog300.npy')\n",
    "\n",
    "# goog300_abstracts.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_xIGkD_wuUn",
    "outputId": "86dcd569-c4a2-42c2-bf85-59decdc831b6"
   },
   "outputs": [],
   "source": [
    "# # Load TF-IDF matrix\n",
    "\n",
    "# url = \"https://storage.googleapis.com/link_prediction_processed_data/tfidf_matrix.npz\"\n",
    "\n",
    "# response = requests.get(url)\n",
    "\n",
    "# with open(\"tfidf_matrix.npz\", \"wb\") as f:\n",
    "#     f.write(response.content)\n",
    "\n",
    "# tfidf_matrix = sparse.load_npz(\"tfidf_matrix.npz\")\n",
    "\n",
    "# tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mJjyDTiVPblx",
    "outputId": "4cad6b8b-5787-4a54-d8b2-ec4017275a78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138499"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load words 192 embedding\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "with gzip.open('embedded_abstracts_dict_192array.pkl.gz', 'rb') as f:\n",
    "    words_embedding_192 = pickle.load(f)\n",
    "\n",
    "len(words_embedding_192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138499"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_embedding_192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "9dfbd4b78ff24c8d96e6d5d9473ed2e0",
      "ce6b224b0b904cf6b7d0031401d3d8b4",
      "73d746420b814d2a932f5f27dfce9deb",
      "f160946f509943a0b36ab628373c1c1b",
      "6fbf8bf5039e438e91820792b9d38a1c",
      "f75064ca348f4de9b99b6301edc088be",
      "58e47c2ee268430aaa7f2d4bcd403174",
      "e6dd31ba6317434fbb78869513d18727",
      "d0d0137c8095472e8de432293708c8cc",
      "ae332fd482c64880a6a204ccb5708705",
      "6d0392accb0540a18da20bca3739e343"
     ]
    },
    "id": "20sWKHiSsgKV",
    "outputId": "adfa48d1-a786-425a-def4-82100cd7db11"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e8dc03772b4cafa6bdc94c6640767e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "words_embedding_192_trunc128 = dict ()\n",
    "for i in tqdm(range(len(words_embedding_192))):\n",
    "    if len(words_embedding_192[i])>0:\n",
    "        arr = np.zeros((128, 192))\n",
    "        vec = words_embedding_192[i][:128, :]\n",
    "        arr[:vec.shape[0],:] = vec\n",
    "        words_embedding_192_trunc128[i] = torch.tensor(arr).to(device)\n",
    "    else:\n",
    "        words_embedding_192_trunc128[i] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fkzNXQF1taxU"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3e4a48e7a54319afbd9d27d188aced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensor_list = []\n",
    "i = 0\n",
    "for i in tqdm(range(len(words_embedding_192_trunc128))):\n",
    "    if len(words_embedding_192_trunc128[i])>0:\n",
    "        tensor_list.append(words_embedding_192_trunc128[i])\n",
    "    else:\n",
    "        tensor_list.append(torch.zeros((128, 192)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(words_embedding_192_trunc128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tAo6RBG-xy47",
    "outputId": "9b25d916-f17a-47b7-95a6-5f4308a250a5"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "filename = 'words_embedding_192_trunc128.pkl.gz'\n",
    "\n",
    "# open the file in binary mode and write the dictionary to it, compressing the data with gzip\n",
    "with gzip.open(filename, 'wb') as f:\n",
    "    pickle.dump(words_embedding_192_trunc128, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65cec14ff040477da50689c7125cc557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensor_list_float = []\n",
    "for i, tensor in tqdm(enumerate(tensor_list)):\n",
    "    tensor_list_float.append(tensor_list[i].float())\n",
    "del(tensor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words_embedding_192_trunc128' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[394], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwords_embedding_192_trunc128\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'words_embedding_192_trunc128' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxvIuCP_bFrN"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc1 = nn.Linear(1536, 128)\n",
    "        self.fc2 = nn.Linear(128*2, 64)\n",
    "        self.fc5 = nn.Linear(64, 2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.conv1(x1)\n",
    "        x2 = self.conv1(x2)\n",
    "\n",
    "        x1 = self.relu(x1)\n",
    "        x2 = self.relu(x2)\n",
    "\n",
    "        x1 = self.pool(x1)\n",
    "        x2 = self.pool(x2)\n",
    "\n",
    "        x1 = self.conv2(x1)\n",
    "        x2 = self.conv2(x2)\n",
    "\n",
    "        x1 = self.relu(x1)\n",
    "        x2 = self.relu(x2)\n",
    "\n",
    "        x1 = self.pool(x1)\n",
    "        x2 = self.pool(x2)\n",
    "\n",
    "        x1 = x1.view(-1, 1536)\n",
    "        x2 = x2.view(-1, 1536)\n",
    "\n",
    "        x1 = self.fc1(x1)\n",
    "        x2 = self.fc1(x2)\n",
    "\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        del(x2)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1), x1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CNN(model, learning_rate, x, pairs, y, val_indices, y_val, epochs, batch_size, run_number, window = 10):\n",
    "    # Train model\n",
    "    start_time = time()\n",
    "    \n",
    "    print('Initializing the optimizer with learning rate:', learning_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) #optimizer with halving learning rate in training\n",
    "\n",
    "    today = datetime.today().strftime('%Y-%m-%d-%H:%M')\n",
    "    list_loss_val = []\n",
    "    list_loss_train = []\n",
    "    \n",
    "    \n",
    "    halving_lr = 0 # counter of the number of halving lr\n",
    "    patience = 16\n",
    "    early_stopping = EarlyStopping(model, patience=patience, delta=0.1, path='checkpoint.pt')\n",
    "    print('Start training...')\n",
    "\n",
    "    num_batches = (len(pairs) + batch_size - 1) // batch_size\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    num_pairs = pairs.shape[1]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model_CNN.train()\n",
    "\n",
    "        perm = np.random.permutation(num_pairs)\n",
    "\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            pairs1 = pairs[0][perm[i: i+batch_size]].numpy()\n",
    "            pairs2 = pairs[1][perm[i: i+batch_size]].numpy()\n",
    "            print(type(pairs1), pairs1)\n",
    "            y = y[perm[i: i+batch_size]]\n",
    "            x1_batch = (x[pairs1])\n",
    "            print(x1_batch)\n",
    "            x2_batch = (x[pairs2])\n",
    "            output, _ = model_CNN(x1_batch, x2_batch) # we run the model that gives the output.\n",
    "            output.to(device)\n",
    "            loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "            acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "            loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "            optimizer.step() # Performs a single optimization step (parameter update).\n",
    "            total_loss += loss_train.item()\n",
    "            total_acc += acc_train\n",
    "        loss_train = total_loss / num_batches\n",
    "        acc_train = total_acc / num_batches\n",
    "        \n",
    "        model.eval()\n",
    "        val_indices_torch = torch.Tensor(val_indices).long()\n",
    "        \n",
    "        x1 = x[val_indices[0].tolist()]\n",
    "        x2 = x[val_indices[1].tolist()]\n",
    "        output, _ = model_CNN(x1, x2)\n",
    "        output.to(device)\n",
    "        #y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        list_loss_val.append(loss_val.item())\n",
    "        list_loss_train.append(loss_train.item())\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "        \n",
    "\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {} s'.format(int(round(time()) - round(t))),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "            break # <----- break to remove\n",
    "            \n",
    "            if epoch % 50 == 0:\n",
    "                model_path = \"outputs/{}-model-{}epochs-{}.pt\".format(today, epoch, run_number)\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "\n",
    "        \n",
    "        early_stopping(loss_val)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            halving_lr += 1\n",
    "            if halving_lr > 4:\n",
    "                break\n",
    "            learning_rate = learning_rate/10\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            early_stopping = EarlyStopping(model, patience=patience, delta=0.01, path='checkpoint.pt')\n",
    "            print('Deviding the learning rate by 10. New learning rate: {}'.format(learning_rate))\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for key, tensor in words_embedding_dict.items():\n",
    "#     output_dict[key] = model(tensor.unsqueeze(0), pairs).squeeze()\n",
    "\n",
    "# # Print the output shape of each tensor\n",
    "# for key, tensor in output_dict.items():\n",
    "#     print(key, tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "id": "ROBXxQaZnfQ3"
   },
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_text, n_feat, n_hidden, n_class, sub_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        #self.input_txt = nn.Linear(n_text, int(n_hidden))\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden)        \n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(2*n_hidden, n_hidden)\n",
    "        self.fc4 = nn.Linear(n_hidden, sub_class)\n",
    "        self.fc5 = nn.Linear(sub_class, n_class)        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x_in, abstract, adj, pairs):\n",
    "\n",
    "        # text shape: [batch_size, max_sent_len]\n",
    "\n",
    "        # Embedding layer\n",
    "        embedded = self.embedding(abstract) # shape: [batch_size, max_sent_len, embedding_dim]\n",
    "\n",
    "        # Add a channel dimension for convolutional layers\n",
    "        embedded = embedded.unsqueeze(1) # shape: [batch_size, 1, max_sent_len, embedding_dim]\n",
    "\n",
    "        # Convolutional layers\n",
    "        conved = [nn.functional.relu(conv(embedded)).squeeze(3) for conv in self.convs] # shape: [batch_size, num_filters, max_sent_len - filter_size + 1]\n",
    "\n",
    "        # Max pooling\n",
    "        pooled = [nn.functional.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved] # shape: [batch_size, num_filters]\n",
    "\n",
    "        # Concatenate pooled outputs from different filter sizes\n",
    "        cat = torch.cat(pooled, dim=1) # shape: [batch_size, num_filters * len(filter_sizes)]\n",
    "\n",
    "        # Fully connected layer\n",
    "        output = self.fc(cat) # shape: [batch_size, output_dim]\n",
    "\n",
    "        # Sigmoid activation\n",
    "        output = self.sigmoid(output) # shape: [batch_size, output_dim]\n",
    "        \n",
    "        \n",
    "        \n",
    "        h1 = self.fc1(x_in)\n",
    "        z1 = self.relu(torch.spmm(adj, h1))\n",
    "        z1 = self.dropout(z1)\n",
    "        del(x_in, h1)\n",
    "\n",
    "        h2 = self.fc2(z1)\n",
    "        z2 = self.relu(torch.spmm(adj, h2))\n",
    "        z2 = self.dropout(z2)\n",
    "        del(h2, z1, adj)\n",
    "        \n",
    "        x = torch.cat((z2[pairs[0]] , z2[pairs[1]]), dim=1)\n",
    "        del(z2)\n",
    "        x = (pairs[2][:, None])*x       \n",
    "        \n",
    "        #x = torch.cat((x, y), dim=1)        \n",
    "        del(pairs)\n",
    "\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONGuhqK5OEB_"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_text, n_feat, n_hidden, n_class, sub_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        self.input_txt = nn.Linear(n_text, int(n_hidden))\n",
    "        self.conv1 = nn.Conv1d(in_channels=n_hidden, out_channels=n_hidden, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(4*n_hidden, n_hidden)\n",
    "        self.fc4 = nn.Linear(n_hidden, sub_class)\n",
    "        self.fc5 = nn.Linear(sub_class, n_class)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x_in, tf_idf, adj, pairs):\n",
    "        tf_idf_sparse = torch.sparse_coo_tensor(tf_idf._indices(), tf_idf._values(), tf_idf.size())\n",
    "        y = nn.functional.relu(self.input_txt(tf_idf_sparse.to(device)))\n",
    "        y = torch.transpose(y, 0, 1)\n",
    "        y = nn.functional.relu(self.conv1(y))\n",
    "        y = y.unsqueeze(dim=0)  # add a new dimension with size 1 instead of batch size\n",
    "        y = torch.max_pool1d(y, kernel_size=y.shape[2])\n",
    "        y = y.squeeze(dim=2)\n",
    "\n",
    "        y = self.dropout(y)\n",
    "        del(tf_idf)\n",
    "\n",
    "        y = torch.cat((y[pairs[0]], y[pairs[1]]), dim=1)\n",
    "\n",
    "        y = torch.cat((y[pairs[0]], y[pairs[1]]), dim=1)\n",
    "\n",
    "        h1 = self.fc1(x_in)\n",
    "        z1 = self.relu(torch.spmm(adj, h1)) # sparce matrix multiplication\n",
    "        z1 = self.dropout(z1)\n",
    "        del(x_in, h1)\n",
    "\n",
    "        h2 = self.fc2(z1)\n",
    "        z2 = self.relu(torch.spmm(adj, h2))\n",
    "        z2 = self.dropout(z2)\n",
    "        del(h2, z1, adj)\n",
    "\n",
    "        x = torch.cat((z2[pairs[0]], z2[pairs[1]]), dim=1)\n",
    "        del(z2)\n",
    "        x = (pairs[2][:, None])*x\n",
    "\n",
    "        x = torch.cat((x, y), dim=1)\n",
    "\n",
    "        del(pairs)\n",
    "\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc5(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "id": "bnRNz5z4nh6x"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, model, patience, delta, path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.model = model\n",
    "        self.val_loss_min = np.Inf\n",
    "        \n",
    "    def __call__(self, val_loss, path='checkpoint.pt'):\n",
    "        score = val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(path)\n",
    "        elif score > self.best_score + 0:\n",
    "            self.counter += 1\n",
    "            #print(self.counter)\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def load_checkpoint(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, learning_rate, bart_abstract, features, adj, pairs, y, val_indices, y_val, epochs, run_number, window = 10):\n",
    "    # Train model\n",
    "    start_time = time()\n",
    "    \n",
    "    print('Initializing the optimizer with learning rate:', learning_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) #optimizer with halving learning rate in training\n",
    "    try: os.mkdir('./outputs')\n",
    "    except: pass\n",
    "\n",
    "    today = datetime.today().strftime('%Y-%m-%d-%H:%M')\n",
    "    list_loss_val = []\n",
    "    list_loss_train = []\n",
    "    list_epochs = []\n",
    "    \n",
    "    halving_lr = 0 # counter of the number of halving lr\n",
    "    patience = 16\n",
    "    early_stopping = EarlyStopping(model, patience=patience, delta=0.1, path='checkpoint.pt')\n",
    "    print('Start training...')\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        output = model(features, bart_abstract, adj, pairs).to(device) # we run the model that gives the output.\n",
    "        loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "        acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "        loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "        optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, bart_abstract, adj, val_indices).to(device)\n",
    "        #y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        list_loss_val.append(loss_val.item())\n",
    "        list_loss_train.append(loss_train.item())\n",
    "        list_epochs.append(epoch)\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "        \n",
    "\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {} s'.format(int(round(time()) - round(t))),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "            \n",
    "            if epoch % 50 == 0:\n",
    "                model_path = \"outputs/{}-model-{}epochs-{}.pt\".format(today, epoch, run_number)\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "\n",
    "        \n",
    "        early_stopping(loss_val)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            halving_lr += 1\n",
    "            if halving_lr > 4:\n",
    "                break\n",
    "            learning_rate = learning_rate/10\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            early_stopping = EarlyStopping(model, patience=patience, delta=0.01, path='checkpoint.pt')\n",
    "            print('Deviding the learning rate by 10. New learning rate: {}'.format(learning_rate))\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model, list_loss_val, list_loss_train, list_epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJqiI7fxbVhl"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle features the same way as the Graph\n",
    "walks_wv = map_features_with_permutation(walks_wv, permutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZqvqcdPg4X6",
    "outputId": "e4e5da2b-a35d-4851-c963-2e1b09d0c396"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the data for training...\n",
      "Data converted into torch tensors and authors added to indices in 2 min\n"
     ]
    }
   ],
   "source": [
    "features_torch, adj_torch, pairs_torch, y_torch, val_indices_torch, y_val_torch = prepare_data_to_train(walks_wv, authors, adj, indices, val_indices, y_val)\n",
    "\n",
    "#abstracts_bart_embeddings = abstracts_bart_embeddings.to(device)\n",
    "#goog300_abstracts_torch = torch.FloatTensor(goog300_abstracts).to(device)\n",
    "#local_wv300_abstracts_torch = torch.FloatTensor(local_wv300_abstracts).to(device)\n",
    "#tfidf_matrix_torch = sparse_mx_to_torch_sparse_tensor(tfidf_matrix).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "id": "7i8UpGHuNhQp"
   },
   "outputs": [],
   "source": [
    "n_hidden = 64\n",
    "dropout_rate = 0.2\n",
    "sub_class = 8\n",
    "n_class = 2\n",
    "text_embedding = 0 #tfidf_matrix_torch\n",
    "n_text = 1 #text_embedding.shape[1]\n",
    "n_features = features_torch.shape[1]\n",
    "\n",
    "\n",
    "# Creates the model\n",
    "model = GNN(n_text, n_features, n_hidden, n_class, sub_class, dropout_rate).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the optimizer with learning rate: 0.01\n",
      "Start training...\n",
      "Epoch: 001 loss_train: 0.7009 loss_val: 0.6841 acc_train: 0.5000 acc_val: 0.5293 time: 11 s total_time: 0 min\n",
      "Epoch: 006 loss_train: 0.6495 loss_val: 0.6490 acc_train: 0.6827 acc_val: 0.7336 time: 8 s total_time: 1 min\n",
      "Epoch: 011 loss_train: 0.6215 loss_val: 0.6100 acc_train: 0.6269 acc_val: 0.7275 time: 6 s total_time: 1 min\n",
      "Epoch: 016 loss_train: 0.6153 loss_val: 0.5919 acc_train: 0.6681 acc_val: 0.6335 time: 7 s total_time: 2 min\n",
      "Epoch: 021 loss_train: 0.5680 loss_val: 0.5529 acc_train: 0.7346 acc_val: 0.7575 time: 9 s total_time: 3 min\n",
      "Epoch: 026 loss_train: 0.5314 loss_val: 0.5209 acc_train: 0.7645 acc_val: 0.7716 time: 7 s total_time: 3 min\n",
      "Epoch: 031 loss_train: 0.5204 loss_val: 0.5235 acc_train: 0.7538 acc_val: 0.7536 time: 7 s total_time: 4 min\n",
      "Epoch: 036 loss_train: 0.5043 loss_val: 0.4955 acc_train: 0.7643 acc_val: 0.7762 time: 7 s total_time: 4 min\n",
      "Epoch: 041 loss_train: 0.5014 loss_val: 0.4945 acc_train: 0.7648 acc_val: 0.7679 time: 7 s total_time: 5 min\n",
      "Epoch: 046 loss_train: 0.4876 loss_val: 0.4864 acc_train: 0.7819 acc_val: 0.7767 time: 7 s total_time: 6 min\n",
      "Epoch: 051 loss_train: 0.4803 loss_val: 0.4784 acc_train: 0.7846 acc_val: 0.7827 time: 8 s total_time: 6 min\n",
      "Epoch: 056 loss_train: 0.4739 loss_val: 0.4705 acc_train: 0.7857 acc_val: 0.7845 time: 8 s total_time: 7 min\n",
      "Epoch: 061 loss_train: 0.4656 loss_val: 0.4634 acc_train: 0.7873 acc_val: 0.7841 time: 8 s total_time: 7 min\n",
      "Epoch: 066 loss_train: 0.4580 loss_val: 0.4599 acc_train: 0.7886 acc_val: 0.7845 time: 8 s total_time: 8 min\n",
      "Epoch: 071 loss_train: 0.4510 loss_val: 0.4533 acc_train: 0.7914 acc_val: 0.7886 time: 8 s total_time: 9 min\n",
      "Epoch: 076 loss_train: 0.4480 loss_val: 0.4568 acc_train: 0.7892 acc_val: 0.7866 time: 8 s total_time: 10 min\n",
      "Epoch: 081 loss_train: 0.4433 loss_val: 0.4412 acc_train: 0.7974 acc_val: 0.8015 time: 8 s total_time: 10 min\n",
      "Epoch: 086 loss_train: 0.4325 loss_val: 0.4413 acc_train: 0.7984 acc_val: 0.8017 time: 7 s total_time: 11 min\n",
      "Epoch: 091 loss_train: 0.4254 loss_val: 0.4386 acc_train: 0.8078 acc_val: 0.8047 time: 8 s total_time: 12 min\n",
      "Epoch: 096 loss_train: 0.4207 loss_val: 0.4349 acc_train: 0.8078 acc_val: 0.8067 time: 8 s total_time: 12 min\n",
      "Epoch: 101 loss_train: 0.4179 loss_val: 0.4367 acc_train: 0.8114 acc_val: 0.8040 time: 7 s total_time: 13 min\n",
      "Epoch: 106 loss_train: 0.4223 loss_val: 0.4345 acc_train: 0.8062 acc_val: 0.8100 time: 7 s total_time: 14 min\n",
      "Epoch: 111 loss_train: 0.4191 loss_val: 0.4314 acc_train: 0.8116 acc_val: 0.8088 time: 8 s total_time: 14 min\n",
      "Epoch: 116 loss_train: 0.4143 loss_val: 0.4304 acc_train: 0.8115 acc_val: 0.8085 time: 8 s total_time: 15 min\n",
      "Epoch: 121 loss_train: 0.4117 loss_val: 0.4311 acc_train: 0.8142 acc_val: 0.8102 time: 9 s total_time: 16 min\n",
      "Epoch: 126 loss_train: 0.4081 loss_val: 0.4278 acc_train: 0.8162 acc_val: 0.8084 time: 8 s total_time: 16 min\n",
      "Epoch: 131 loss_train: 0.4073 loss_val: 0.4271 acc_train: 0.8163 acc_val: 0.8101 time: 9 s total_time: 17 min\n",
      "Epoch: 136 loss_train: 0.4053 loss_val: 0.4257 acc_train: 0.8171 acc_val: 0.8121 time: 9 s total_time: 18 min\n",
      "Epoch: 141 loss_train: 0.4077 loss_val: 0.4323 acc_train: 0.8171 acc_val: 0.8114 time: 8 s total_time: 19 min\n",
      "Epoch: 146 loss_train: 0.4048 loss_val: 0.4277 acc_train: 0.8179 acc_val: 0.8130 time: 8 s total_time: 19 min\n",
      "Epoch: 151 loss_train: 0.4030 loss_val: 0.4262 acc_train: 0.8187 acc_val: 0.8138 time: 7 s total_time: 20 min\n",
      "Epoch: 156 loss_train: 0.4012 loss_val: 0.4252 acc_train: 0.8190 acc_val: 0.8131 time: 8 s total_time: 21 min\n",
      "Epoch: 161 loss_train: 0.4026 loss_val: 0.4252 acc_train: 0.8180 acc_val: 0.8116 time: 8 s total_time: 21 min\n",
      "Epoch: 166 loss_train: 0.3991 loss_val: 0.4257 acc_train: 0.8206 acc_val: 0.8116 time: 8 s total_time: 22 min\n",
      "Epoch: 171 loss_train: 0.3972 loss_val: 0.4311 acc_train: 0.8224 acc_val: 0.8089 time: 8 s total_time: 23 min\n",
      "Epoch: 176 loss_train: 0.4021 loss_val: 0.4307 acc_train: 0.8172 acc_val: 0.8125 time: 8 s total_time: 23 min\n",
      "Deviding the learning rate by 10. New learning rate: 0.001\n",
      "Epoch: 181 loss_train: 0.3952 loss_val: 0.4277 acc_train: 0.8208 acc_val: 0.8095 time: 8 s total_time: 24 min\n",
      "Epoch: 186 loss_train: 0.3950 loss_val: 0.4277 acc_train: 0.8219 acc_val: 0.8135 time: 8 s total_time: 25 min\n",
      "Epoch: 191 loss_train: 0.3936 loss_val: 0.4261 acc_train: 0.8225 acc_val: 0.8112 time: 8 s total_time: 25 min\n",
      "Epoch: 196 loss_train: 0.3927 loss_val: 0.4274 acc_train: 0.8223 acc_val: 0.8140 time: 8 s total_time: 26 min\n",
      "Epoch: 201 loss_train: 0.3923 loss_val: 0.4270 acc_train: 0.8228 acc_val: 0.8130 time: 9 s total_time: 27 min\n",
      "Epoch: 206 loss_train: 0.3920 loss_val: 0.4266 acc_train: 0.8228 acc_val: 0.8142 time: 8 s total_time: 27 min\n",
      "Deviding the learning rate by 10. New learning rate: 0.0001\n",
      "Epoch: 211 loss_train: 0.3915 loss_val: 0.4264 acc_train: 0.8235 acc_val: 0.8136 time: 8 s total_time: 28 min\n",
      "Epoch: 216 loss_train: 0.3911 loss_val: 0.4264 acc_train: 0.8234 acc_val: 0.8139 time: 11 s total_time: 29 min\n",
      "Epoch: 221 loss_train: 0.3914 loss_val: 0.4262 acc_train: 0.8231 acc_val: 0.8141 time: 9 s total_time: 29 min\n",
      "Epoch: 226 loss_train: 0.3913 loss_val: 0.4261 acc_train: 0.8233 acc_val: 0.8139 time: 8 s total_time: 30 min\n",
      "Epoch: 231 loss_train: 0.3909 loss_val: 0.4261 acc_train: 0.8231 acc_val: 0.8141 time: 8 s total_time: 31 min\n",
      "Epoch: 236 loss_train: 0.3911 loss_val: 0.4261 acc_train: 0.8233 acc_val: 0.8140 time: 8 s total_time: 32 min\n",
      "Epoch: 241 loss_train: 0.3908 loss_val: 0.4260 acc_train: 0.8235 acc_val: 0.8140 time: 8 s total_time: 32 min\n",
      "Epoch: 246 loss_train: 0.3909 loss_val: 0.4259 acc_train: 0.8234 acc_val: 0.8141 time: 7 s total_time: 33 min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[390], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m run_number \u001b[38;5;241m=\u001b[39m randint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m      6\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m----> 9\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mfeatures_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                            \u001b[49m\u001b[43my_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_indices_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_number\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[329], line 62\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, learning_rate, bart_abstract, features, adj, pairs, y, val_indices, y_val, epochs, run_number, window)\u001b[0m\n\u001b[1;32m     60\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output, y) \u001b[38;5;66;03m# we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m acc_train \u001b[38;5;241m=\u001b[39m accuracy_score(torch\u001b[38;5;241m.\u001b[39margmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), y\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\u001b[38;5;66;03m# just to show it in the out put message of the training\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[43mloss_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\u001b[39;00m\n\u001b[1;32m     63\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# Performs a single optimization step (parameter update).\u001b[39;00m\n\u001b[1;32m     65\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "epochs = 602\n",
    "run_number = randint(0, 1000)\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "trained_model, model, list_loss_val, list_loss_train, list_epochs = train_model(model, learning_rate, text_embedding, \n",
    "                            features_torch, adj_torch, pairs_torch, \n",
    "                            y_torch, val_indices_torch, y_val_torch, epochs, run_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the optimizer with learning rate: 0.01\n",
      "Start training...\n",
      "Epoch: 001 loss_train: 0.7602 loss_val: 0.7479 acc_train: 0.5000 acc_val: 0.5000 time: 12 s total_time: 0 min\n",
      "Epoch: 006 loss_train: 0.7065 loss_val: 0.6908 acc_train: 0.5000 acc_val: 0.5000 time: 7 s total_time: 1 min\n",
      "Epoch: 011 loss_train: 0.6521 loss_val: 0.6157 acc_train: 0.5457 acc_val: 0.5889 time: 7 s total_time: 1 min\n",
      "Epoch: 016 loss_train: 0.5959 loss_val: 0.5776 acc_train: 0.6227 acc_val: 0.7131 time: 6 s total_time: 2 min\n",
      "Epoch: 021 loss_train: 0.5506 loss_val: 0.5312 acc_train: 0.7415 acc_val: 0.7647 time: 7 s total_time: 2 min\n",
      "Epoch: 026 loss_train: 0.5185 loss_val: 0.4993 acc_train: 0.7520 acc_val: 0.7719 time: 6 s total_time: 3 min\n",
      "Epoch: 031 loss_train: 0.4862 loss_val: 0.4654 acc_train: 0.7779 acc_val: 0.7934 time: 6 s total_time: 4 min\n",
      "Epoch: 036 loss_train: 0.4516 loss_val: 0.4313 acc_train: 0.7880 acc_val: 0.8063 time: 7 s total_time: 4 min\n",
      "Epoch: 041 loss_train: 0.4239 loss_val: 0.4104 acc_train: 0.8004 acc_val: 0.8138 time: 7 s total_time: 5 min\n",
      "Epoch: 046 loss_train: 0.4020 loss_val: 0.3972 acc_train: 0.8099 acc_val: 0.8193 time: 7 s total_time: 5 min\n",
      "Epoch: 051 loss_train: 0.3807 loss_val: 0.3821 acc_train: 0.8219 acc_val: 0.8285 time: 6 s total_time: 6 min\n",
      "Epoch: 056 loss_train: 0.3645 loss_val: 0.3702 acc_train: 0.8415 acc_val: 0.8384 time: 7 s total_time: 6 min\n",
      "Epoch: 061 loss_train: 0.3519 loss_val: 0.3633 acc_train: 0.8479 acc_val: 0.8429 time: 6 s total_time: 7 min\n",
      "Epoch: 066 loss_train: 0.3430 loss_val: 0.3573 acc_train: 0.8535 acc_val: 0.8476 time: 6 s total_time: 7 min\n",
      "Epoch: 071 loss_train: 0.3348 loss_val: 0.3508 acc_train: 0.8599 acc_val: 0.8517 time: 7 s total_time: 8 min\n",
      "Epoch: 076 loss_train: 0.3267 loss_val: 0.3426 acc_train: 0.8635 acc_val: 0.8561 time: 9 s total_time: 9 min\n",
      "Epoch: 081 loss_train: 0.3184 loss_val: 0.3353 acc_train: 0.8684 acc_val: 0.8615 time: 6 s total_time: 9 min\n",
      "Epoch: 086 loss_train: 0.3135 loss_val: 0.3305 acc_train: 0.8707 acc_val: 0.8648 time: 6 s total_time: 10 min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[348], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m run_number \u001b[38;5;241m=\u001b[39m randint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m      6\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m----> 9\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mfeatures_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                            \u001b[49m\u001b[43my_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_indices_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_number\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[329], line 62\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, learning_rate, bart_abstract, features, adj, pairs, y, val_indices, y_val, epochs, run_number, window)\u001b[0m\n\u001b[1;32m     60\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output, y) \u001b[38;5;66;03m# we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m acc_train \u001b[38;5;241m=\u001b[39m accuracy_score(torch\u001b[38;5;241m.\u001b[39margmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), y\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\u001b[38;5;66;03m# just to show it in the out put message of the training\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[43mloss_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\u001b[39;00m\n\u001b[1;32m     63\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# Performs a single optimization step (parameter update).\u001b[39;00m\n\u001b[1;32m     65\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "epochs = 602\n",
    "run_number = randint(0, 1000)\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "trained_model = train_model(model, learning_rate, text_embedding, \n",
    "                            features_torch, adj_torch, pairs_torch, \n",
    "                            y_torch, val_indices_torch, y_val_torch, epochs, run_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "BSvrKVnoLGfn",
    "outputId": "59e42e30-9078-48de-e057-054d7dcc4b5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the optimizer with learning rate: 0.01\n",
      "Start training...\n",
      "Epoch: 001 loss_train: 0.6945 loss_val: 0.6931 acc_train: 0.4038 acc_val: 0.5021 time: 14 s total_time: 0 min\n",
      "Epoch: 006 loss_train: 0.6557 loss_val: 0.6425 acc_train: 0.5295 acc_val: 0.7204 time: 7 s total_time: 1 min\n",
      "Epoch: 011 loss_train: 0.6177 loss_val: 0.5952 acc_train: 0.6884 acc_val: 0.6996 time: 8 s total_time: 1 min\n",
      "Epoch: 016 loss_train: 0.5748 loss_val: 0.5468 acc_train: 0.7156 acc_val: 0.7352 time: 7 s total_time: 2 min\n",
      "Epoch: 021 loss_train: 0.5282 loss_val: 0.4967 acc_train: 0.7502 acc_val: 0.7696 time: 7 s total_time: 3 min\n",
      "Epoch: 026 loss_train: 0.4833 loss_val: 0.4552 acc_train: 0.7722 acc_val: 0.7913 time: 7 s total_time: 3 min\n",
      "Epoch: 031 loss_train: 0.4514 loss_val: 0.4271 acc_train: 0.7864 acc_val: 0.8060 time: 8 s total_time: 4 min\n",
      "Epoch: 036 loss_train: 0.4298 loss_val: 0.4141 acc_train: 0.7952 acc_val: 0.8051 time: 6 s total_time: 5 min\n",
      "Epoch: 041 loss_train: 0.4111 loss_val: 0.4040 acc_train: 0.8023 acc_val: 0.8167 time: 7 s total_time: 5 min\n",
      "Epoch: 046 loss_train: 0.3962 loss_val: 0.3888 acc_train: 0.8151 acc_val: 0.8263 time: 7 s total_time: 6 min\n",
      "Epoch: 051 loss_train: 0.3753 loss_val: 0.3766 acc_train: 0.8267 acc_val: 0.8447 time: 6 s total_time: 6 min\n",
      "Epoch: 056 loss_train: 0.3648 loss_val: 0.3632 acc_train: 0.8304 acc_val: 0.8463 time: 8 s total_time: 7 min\n",
      "Epoch: 061 loss_train: 0.3538 loss_val: 0.3553 acc_train: 0.8404 acc_val: 0.8494 time: 8 s total_time: 8 min\n",
      "Epoch: 066 loss_train: 0.3432 loss_val: 0.3504 acc_train: 0.8437 acc_val: 0.8519 time: 7 s total_time: 8 min\n",
      "Epoch: 071 loss_train: 0.3333 loss_val: 0.3449 acc_train: 0.8492 acc_val: 0.8539 time: 6 s total_time: 9 min\n",
      "Epoch: 076 loss_train: 0.3258 loss_val: 0.3424 acc_train: 0.8517 acc_val: 0.8583 time: 7 s total_time: 9 min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[337], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m run_number \u001b[38;5;241m=\u001b[39m randint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m      6\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m----> 9\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mfeatures_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                            \u001b[49m\u001b[43my_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_indices_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_number\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[329], line 62\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, learning_rate, bart_abstract, features, adj, pairs, y, val_indices, y_val, epochs, run_number, window)\u001b[0m\n\u001b[1;32m     60\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output, y) \u001b[38;5;66;03m# we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m acc_train \u001b[38;5;241m=\u001b[39m accuracy_score(torch\u001b[38;5;241m.\u001b[39margmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), y\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\u001b[38;5;66;03m# just to show it in the out put message of the training\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[43mloss_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\u001b[39;00m\n\u001b[1;32m     63\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# Performs a single optimization step (parameter update).\u001b[39;00m\n\u001b[1;32m     65\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# before shuffling\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "epochs = 602\n",
    "run_number = randint(0, 1000)\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "trained_model = train_model(model, learning_rate, text_embedding, \n",
    "                            features_torch, adj_torch, pairs_torch, \n",
    "                            y_torch, val_indices_torch, y_val_torch, epochs, run_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3931040]),\n",
       " torch.Size([3931040]),\n",
       " torch.Size([4, 109195]),\n",
       " torch.Size([982760]))"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(pairs_torch), y_torch.shape, val_indices_torch.shape, y_val_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uihFEZKFsjwH"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "epochs = 602\n",
    "run_number = randint(0, 1000)\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "trained_model = train_model(model, learning_rate, text_embedding, \n",
    "                            features_torch, adj_torch, pairs_torch, \n",
    "                            y_torch, val_indices_torch, y_val_torch, epochs, run_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lJ2Kxomh20h"
   },
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PAjwmZMg4X6"
   },
   "outputs": [],
   "source": [
    "# with dense2 1024 of BART\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "run_number = randint(0, 1000)\n",
    "learning_rate = 0.005\n",
    "\n",
    "\n",
    "trained_model = train_model(model, learning_rate, abstracts_bart_embeddings, features_torch, adj_torch, pairs_torch, y_torch, val_indices_torch, y_val_torch, epochs, run_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZPrUfJnFnzB"
   },
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDQ_DCCJavQZ"
   },
   "outputs": [],
   "source": [
    "# Without dense 1000\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "run_number = randint(0, 1000)\n",
    "\n",
    "\n",
    "trained_model = train_model(model, 0.01, tfidf_matrix_torch, features_torch, adj_torch, pairs_torch, y_torch, val_indices_torch, y_val_torch, epochs, run_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_XPsPMFbZji"
   },
   "source": [
    "# Generate test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "id": "cmFO4qI7_245"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "test_path = 'https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/test.txt'\n",
    "node_pairs = list()\n",
    "f = urlopen(test_path)\n",
    "\n",
    "for line in f:\n",
    "    t = str(line).split(',')\n",
    "    t[0] = int(re.sub(\"[^0-9]\", \"\", t[0]))\n",
    "    t[1] = int(re.sub(\"[^0-9]\", \"\", t[1]))\n",
    "    node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "node_pairs = np.transpose(node_pairs)\n",
    "node_pairs = add_authors_to_pairs(node_pairs, authors)\n",
    "#node_pairs = torch.LongTensor(node_pairs).to(device)\n",
    "\n",
    "adj_torch = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "features_torch = torch.FloatTensor(walks_wv).to(device)\n",
    "\n",
    "test_output = model(features_torch, text_embedding, adj_torch, node_pairs)\n",
    "y_pred = torch.exp(test_output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "model_nb = 4\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns=['predicted']).to_csv(\n",
    "\"{}-submission-{}-{}.csv\".format(today, model_nb, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pD505btvfDG0"
   },
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bfa1O6lLg4X2"
   },
   "source": [
    "# BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Md1ARLd-g4X3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BartTokenizer, BartModel\n",
    "\n",
    "# Load the BART model and tokenizer\n",
    "model = BartModel.from_pretrained('facebook/bart-large')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ov9PcB9wg4X3"
   },
   "outputs": [],
   "source": [
    "abstracts_bart_embeddings = []\n",
    "\n",
    "# Define a function to generate embeddings for text\n",
    "def get_bart_embeddings(text, model):\n",
    "    # Tokenize the input text\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Generate embeddings using the BART model\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "        embeddings = model_output.last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_FBpRYsg4X3"
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(abstracts_bart_embeddings), len(voc.sentences_list))):\n",
    "    abstract = voc.sentences_list[i]\n",
    "    abstracts_bart_embeddings.append(get_bart_embeddings(abstract, model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8RxfG2Eg4X3"
   },
   "outputs": [],
   "source": [
    "saved_abstracts_bart_embeddings = abstracts_bart_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mi0TkxyFg4X3"
   },
   "outputs": [],
   "source": [
    "len(saved_abstracts_bart_embeddings), len(abstracts_bart_embeddings), len(voc.sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "411JkYvrg4X4"
   },
   "outputs": [],
   "source": [
    "abstracts_bart_embeddings = torch.stack(abstracts_bart_embeddings)\n",
    "abstracts_bart_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iLzw5akgg4X4"
   },
   "outputs": [],
   "source": [
    "abstracts_bart_embeddings.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTZC7PDEg4X4"
   },
   "outputs": [],
   "source": [
    "torch.save(abstracts_bart_embeddings, 'bart_embeddings.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6oNCPyubfSd"
   },
   "source": [
    "#Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2UWL4y-K1jd"
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "node_pairs = torch.LongTensor(node_pairs).to(device)\n",
    "\n",
    "test_output = model(features, adj, node_pairs)\n",
    "y_pred = torch.exp(test_output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"{}-submission-{}-{}.csv\".format(today, model_nb, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmUL7tGAR9Yt"
   },
   "outputs": [],
   "source": [
    "#### New script with batches\n",
    "\n",
    "def early_stopping(loss_train, list_loss_train, loss_val, list_loss_val, \n",
    "                   tolerance=0.01, patience=15):\n",
    "    list_loss_val = list(list_loss_val)[-patience:]\n",
    "    list_loss_train = list(list_loss_train)[-patience:]\n",
    "    if (len(list_loss_val) == patience and loss_val > (sum(list_loss_val)/len(list_loss_val)) and loss_train + tolerance < loss_val) or (len(list_loss_train) == patience and loss_train > (sum(list_loss_train)/len(list_loss_train))):\n",
    "        #print('train: {:.5f} val: {:.5f} mean val: {:.5f}'.format(loss_train, loss_val, (sum(list_loss_val)/len(list_loss_val))))\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "    \n",
    "def train_model(model, learning_rate, features, adj, indices_mc, y, val_indices, \n",
    "                y_val, epochs, batch_size, wv_walk_size, \n",
    "                tolerence = 0.01, patience = 15, run_number=randint(0, 1000)):\n",
    "    # Train model\n",
    "    start_time = time()\n",
    "    print('Initializing the optimizer with learning rate:', learning_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) #optimizer with halving learning rate in training\n",
    "    try: os.mkdir('./outputs')\n",
    "    except: pass\n",
    "    print('Preparing the data for training...')        \n",
    "\n",
    "    today = datetime.today().strftime('%Y-%m-%d-%H:%M')\n",
    "    list_loss_val = []\n",
    "    list_loss_train = []\n",
    "\n",
    "    \n",
    "    halving_lr = 0 # counter of the number of halving lr\n",
    "    print('Start training...')\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "\n",
    "        # we create the rand indices corresponding to non edges (their y = 0)\n",
    "        # we could apply a condition on epoch to run rand_indices (for speed purposes)\n",
    "        rand_indices = np.random.randint(0, len(indices_mc), size=(indices_mc.shape[0], indices_mc.shape[1]))\n",
    "        rand_indices = add_authors_to_pairs(rand_indices, authors)\n",
    "        pairs = np.concatenate((indices_mc, rand_indices), axis=1)\n",
    "        pairs = torch.LongTensor(pairs).to(device)\n",
    "\n",
    "        permutation = torch.randperm(pairs.size()[1])\n",
    "        \n",
    "        # batches\n",
    "        for i in range(0, pairs.size()[1], batch_size):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            elts_indices = permutation[i:i+batch_size]\n",
    "            batch_pairs = pairs[:, elts_indices]\n",
    "            batch_y = y[elts_indices]\n",
    "\n",
    "        \n",
    "            model.train()\n",
    "\n",
    "            output = model(features, adj, batch_pairs, wv_walk_size).to(device) # we run the model that gives the output.\n",
    "            loss_train = F.nll_loss(output, batch_y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "            acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), batch_y.cpu().numpy())# just to show it in the out put message of the training\n",
    "            loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "            optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, adj, val_indices, wv_walk_size).to(device)\n",
    "        #y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        list_loss_val.append(loss_val.item())\n",
    "        list_loss_train.append(loss_train.item())\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {} s'.format(int(round(time()) - round(t))),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "        if epoch % 50 == 0:\n",
    "            model_path = \"outputs/{}-model-{}epochs-{}.pt\".format(today, epoch, run_number)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "        if int(loss_val.item()) > 5:\n",
    "            break\n",
    "            \n",
    "        early = early_stopping(loss_train.item(), list_loss_train, loss_val.item(), list_loss_val, patience=15)        \n",
    "        if early:\n",
    "            halving_lr += 1\n",
    "            if halving_lr > 5:\n",
    "                break\n",
    "            list_loss_val=[]\n",
    "            learning_rate = learning_rate/10\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            print('Deviding the learning rate by 2. New learning rate: {:.6f}'.format(learning_rate))\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwNIJmkGn6-G"
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "trained_model = train_model(model, 0.01, features, authors, adj, indices, y, torch.tensor(val_indices).to(device), torch.tensor(y_val).to(device), epochs)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Ubeigqung4X0"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "58e47c2ee268430aaa7f2d4bcd403174": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6d0392accb0540a18da20bca3739e343": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6fbf8bf5039e438e91820792b9d38a1c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73d746420b814d2a932f5f27dfce9deb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6dd31ba6317434fbb78869513d18727",
      "max": 138499,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d0d0137c8095472e8de432293708c8cc",
      "value": 138499
     }
    },
    "9dfbd4b78ff24c8d96e6d5d9473ed2e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ce6b224b0b904cf6b7d0031401d3d8b4",
       "IPY_MODEL_73d746420b814d2a932f5f27dfce9deb",
       "IPY_MODEL_f160946f509943a0b36ab628373c1c1b"
      ],
      "layout": "IPY_MODEL_6fbf8bf5039e438e91820792b9d38a1c"
     }
    },
    "ae332fd482c64880a6a204ccb5708705": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce6b224b0b904cf6b7d0031401d3d8b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f75064ca348f4de9b99b6301edc088be",
      "placeholder": "",
      "style": "IPY_MODEL_58e47c2ee268430aaa7f2d4bcd403174",
      "value": "100%"
     }
    },
    "d0d0137c8095472e8de432293708c8cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e6dd31ba6317434fbb78869513d18727": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f160946f509943a0b36ab628373c1c1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae332fd482c64880a6a204ccb5708705",
      "placeholder": "",
      "style": "IPY_MODEL_6d0392accb0540a18da20bca3739e343",
      "value": " 138499/138499 [00:12&lt;00:00, 11226.56it/s]"
     }
    },
    "f75064ca348f4de9b99b6301edc088be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
