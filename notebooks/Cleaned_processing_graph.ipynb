{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52d34a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from random import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss\n",
    "from random import choice\n",
    "from gensim.models import Word2Vec\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1207335f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499\n",
      "Number of edges: 1091955\n",
      "Number of nodes of training set: 138499\n",
      "Number of edges of training set: 982599\n"
     ]
    }
   ],
   "source": [
    "# Create a graph\n",
    "G = nx.read_edgelist('edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "nodes = list(G.nodes())\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "edges = list(G.edges())\n",
    "\n",
    "print('Number of nodes:', n)\n",
    "print('Number of edges:', m)\n",
    "\n",
    "# We need to create a seperate validation subset \n",
    "# with 0.1 edges from the graph\n",
    "\n",
    "val_edges = list()\n",
    "G_train = G\n",
    "\n",
    "for edge in edges:\n",
    "    if random() < 0.1:\n",
    "        val_edges.append(edge)\n",
    "\n",
    "# We remove the val edges from the graph G\n",
    "for edge in val_edges:\n",
    "    G_train.remove_edge(edge[0], edge[1])\n",
    "\n",
    "n = G_train.number_of_nodes()\n",
    "m = G_train.number_of_edges()\n",
    "train_edges = list(G_train.edges())\n",
    "    \n",
    "print('Number of nodes of training set:', n)\n",
    "print('Number of edges of training set:', m)\n",
    "\n",
    "y_val = [1]*len(val_edges)\n",
    "\n",
    "n_val_edges = len(val_edges)\n",
    "\n",
    "# Create random pairs of nodes\n",
    "for i in range(n_val_edges):\n",
    "    n1 = nodes[randint(0, n-1)]\n",
    "    n2 = nodes[randint(0, n-1)]\n",
    "    (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "    val_edges.append((n1, n2))\n",
    "    \n",
    "# Remove edges that exist in both train and val\n",
    "\n",
    "for edge in list(set(val_edges) & set(train_edges)):\n",
    "    val_edges.remove(edge)\n",
    "    \n",
    "n_val_edges = len(val_edges) - len(y_val) #because we removed from val_edges edges that exist in both\n",
    "y_val.extend([0]*n_val_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be9c0e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that generates a random walk for a given graph node and walk length\n",
    "\n",
    "def random_walk(G, node, walk_length):\n",
    "    # Starts from vertex \"node\" and performs a random walk of length \"walk length\". Returns a list of the visited vertices\n",
    "    walk = [node]\n",
    "  \n",
    "    # your code here\n",
    "    for i in range(walk_length-1) :\n",
    "        neibor_nodes = list(G.neighbors(walk[-1]))\n",
    "        if len(neibor_nodes) > 0 :\n",
    "            #print('neibor_nodes : ', neibor_nodes)\n",
    "            next_node = choice(neibor_nodes)\n",
    "            walk.append(next_node)\n",
    "    \n",
    "    walk = [str(node) for node in walk] # in case the nodes are in string format, \n",
    "    #we don't need to cast into string, but if the nodes are in numeric or integer, \n",
    "    #we need this line to cast into string\n",
    "    return walk\n",
    "\n",
    "\n",
    "# Define a second function that generates num_walks random walks with a given length for all the nodes of a graph\n",
    "\n",
    "def generate_walks(G, num_walks, walk_length):\n",
    "    # Runs \"num_walks\" random walks from each node, and returns a list of all random walk\n",
    "    walks = list()\n",
    "  \n",
    "    # your code here\n",
    "    for i in range(num_walks) :\n",
    "        for node in G.nodes() :\n",
    "            #print(node)\n",
    "            walk = random_walk(G, node, walk_length)\n",
    "            walks.append(walk)\n",
    "        #print('walks : ', walks)\n",
    "    return walks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2609e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate walks in a variable called walks which is a list of walk(s) which is a sequence of nodes\n",
    "walks = generate_walks(G,num_walks=30,walk_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93e74548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1384990"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.wv[0]\n",
    "len(walks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c2d5f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(411509850, 411509850)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use word2vec to reduce the dimensionality and get a non sparse representation\n",
    "vector_size = 50\n",
    "model = Word2Vec(vector_size=vector_size, window=5, min_count=0, sg=1, workers=8)\n",
    "model.build_vocab(walks)\n",
    "model.train(walks, total_examples=model.corpus_count, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "34e1bade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train matrix\n",
    "# Now we start with only 2 features for each edge\n",
    "# which are the sum of degrees of the two nodes and absolute difference of \n",
    "\n",
    "X_train = np.zeros((2*m, 2 + 2*vector_size))\n",
    "y_train = np.zeros(2*m)\n",
    "\n",
    "for i, edge in enumerate(train_edges):\n",
    "    X_train[2*i, 0] = G.degree(edge[0]) + G.degree(edge[1])\n",
    "    X_train[2*i, 1] = abs(G.degree(edge[0]) - G.degree(edge[1]))\n",
    "    X_train[2*i, 2:2+vector_size] = model.wv[str(edge[0])]\n",
    "    X_train[2*i, 2+vector_size:2+2*vector_size] = model.wv[str(edge[1])]    \n",
    "    y_train[2*i] = 1\n",
    "    \n",
    "    # a randomly generated pair of nodes\n",
    "    n1 = nodes[randint(0, n-1)]\n",
    "    n2 = nodes[randint(0, n-1)]\n",
    "    X_train[2*i+1,0] = G.degree(n1) + G.degree(n2)\n",
    "    X_train[2*i+1,1] = abs(G.degree(n1) - G.degree(n2))\n",
    "    X_train[2*i+1, 2:2+vector_size] = model.wv[str(G.degree(min(n1, n2)))]\n",
    "    X_train[2*i+1, 2+vector_size:2+2*vector_size] = model.wv[str(G.degree(max(n1, n2)))]  \n",
    "    y_train[2*i+1] = 0\n",
    "\n",
    "    \n",
    "#Create the validation (test) matrix. Use the same 2 (or more) features as above (for train matrix) \n",
    "#for all edges in new complete val set\n",
    "\n",
    "X_val = np.array(np.zeros((len(val_edges), 2 + 2*vector_size)))\n",
    "\n",
    "for i,edge in enumerate(val_edges):\n",
    "    X_val[i,0] = G.degree(edge[0]) + G.degree(edge[1])\n",
    "    X_val[i,1] = abs(G.degree(edge[0]) - G.degree(edge[1]))\n",
    "    X_val[i, 2:2+vector_size] = model.wv[str(edge[0])]\n",
    "    X_val[i, 2+vector_size:2+2*vector_size] = model.wv[str(edge[1])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6161a2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "([1]*len(y_train)-y_train)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b25e0eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_stack = np.column_stack((y_train, ([1]*len(y_train)-y_train)))\n",
    "y_val_stack = np.column_stack((y_val, ([1]*len(np.array(y_val))-np.array(y_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1278c65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val[150000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5819ad5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1966/1966 [==============================] - 110s 56ms/step - loss: 0.0701 - binary_crossentropy: 0.0701 - val_loss: 3.1642 - val_binary_crossentropy: 3.1642\n",
      "Epoch 2/25\n",
      "1966/1966 [==============================] - 105s 53ms/step - loss: 0.0071 - binary_crossentropy: 0.0071 - val_loss: 4.1616 - val_binary_crossentropy: 4.1616\n",
      "Epoch 3/25\n",
      "1966/1966 [==============================] - 110s 56ms/step - loss: 0.0044 - binary_crossentropy: 0.0044 - val_loss: 4.6760 - val_binary_crossentropy: 4.6760\n",
      "Epoch 4/25\n",
      "1966/1966 [==============================] - 114s 58ms/step - loss: 0.0031 - binary_crossentropy: 0.0031 - val_loss: 5.0772 - val_binary_crossentropy: 5.0772\n",
      "Epoch 5/25\n",
      "1966/1966 [==============================] - 96s 49ms/step - loss: 0.0023 - binary_crossentropy: 0.0023 - val_loss: 5.5200 - val_binary_crossentropy: 5.5200\n",
      "Epoch 6/25\n",
      "1966/1966 [==============================] - 95s 48ms/step - loss: 0.0023 - binary_crossentropy: 0.0023 - val_loss: 5.4966 - val_binary_crossentropy: 5.4966\n",
      "Epoch 7/25\n",
      "1966/1966 [==============================] - 97s 49ms/step - loss: 0.0015 - binary_crossentropy: 0.0015 - val_loss: 5.5382 - val_binary_crossentropy: 5.5382\n",
      "Epoch 8/25\n",
      "1966/1966 [==============================] - 97s 49ms/step - loss: 0.0013 - binary_crossentropy: 0.0013 - val_loss: 5.7504 - val_binary_crossentropy: 5.7504\n",
      "Epoch 9/25\n",
      "1966/1966 [==============================] - 96s 49ms/step - loss: 0.0013 - binary_crossentropy: 0.0013 - val_loss: 5.3713 - val_binary_crossentropy: 5.3713\n",
      "Epoch 10/25\n",
      "1966/1966 [==============================] - 109s 55ms/step - loss: 0.0011 - binary_crossentropy: 0.0011 - val_loss: 6.0498 - val_binary_crossentropy: 6.0498\n",
      "Epoch 11/25\n",
      "1966/1966 [==============================] - 117s 59ms/step - loss: 0.0013 - binary_crossentropy: 0.0013 - val_loss: 5.2643 - val_binary_crossentropy: 5.2643\n",
      "Epoch 12/25\n",
      "1966/1966 [==============================] - 109s 56ms/step - loss: 8.1187e-04 - binary_crossentropy: 8.1187e-04 - val_loss: 6.0122 - val_binary_crossentropy: 6.0122\n",
      "Epoch 13/25\n",
      "1966/1966 [==============================] - 106s 54ms/step - loss: 8.4578e-04 - binary_crossentropy: 8.4578e-04 - val_loss: 6.2742 - val_binary_crossentropy: 6.2742\n",
      "Epoch 14/25\n",
      "1966/1966 [==============================] - 116s 59ms/step - loss: 8.4587e-04 - binary_crossentropy: 8.4587e-04 - val_loss: 5.6459 - val_binary_crossentropy: 5.6459\n",
      "Epoch 15/25\n",
      "1966/1966 [==============================] - 104s 53ms/step - loss: 6.5411e-04 - binary_crossentropy: 6.5411e-04 - val_loss: 6.2105 - val_binary_crossentropy: 6.2105\n",
      "Epoch 16/25\n",
      "1966/1966 [==============================] - 110s 56ms/step - loss: 6.9414e-04 - binary_crossentropy: 6.9414e-04 - val_loss: 6.3050 - val_binary_crossentropy: 6.3050\n",
      "Epoch 17/25\n",
      "1966/1966 [==============================] - 105s 53ms/step - loss: 6.1959e-04 - binary_crossentropy: 6.1959e-04 - val_loss: 6.5058 - val_binary_crossentropy: 6.5058\n",
      "Epoch 18/25\n",
      "1966/1966 [==============================] - 102s 52ms/step - loss: 5.9974e-04 - binary_crossentropy: 5.9974e-04 - val_loss: 6.1938 - val_binary_crossentropy: 6.1938\n",
      "Epoch 19/25\n",
      "1966/1966 [==============================] - 93s 47ms/step - loss: 4.5615e-04 - binary_crossentropy: 4.5615e-04 - val_loss: 6.3610 - val_binary_crossentropy: 6.3610\n",
      "Epoch 20/25\n",
      "1966/1966 [==============================] - 93s 47ms/step - loss: 4.7489e-04 - binary_crossentropy: 4.7489e-04 - val_loss: 6.7996 - val_binary_crossentropy: 6.7996\n",
      "Epoch 21/25\n",
      "1966/1966 [==============================] - 93s 47ms/step - loss: 4.4885e-04 - binary_crossentropy: 4.4885e-04 - val_loss: 6.1755 - val_binary_crossentropy: 6.1755\n",
      "Epoch 22/25\n",
      "1966/1966 [==============================] - 93s 47ms/step - loss: 3.9183e-04 - binary_crossentropy: 3.9183e-04 - val_loss: 6.3330 - val_binary_crossentropy: 6.3330\n",
      "Epoch 23/25\n",
      "1966/1966 [==============================] - 93s 47ms/step - loss: 4.0061e-04 - binary_crossentropy: 4.0061e-04 - val_loss: 6.2148 - val_binary_crossentropy: 6.2148\n",
      "Epoch 24/25\n",
      "1966/1966 [==============================] - 93s 47ms/step - loss: 3.9710e-04 - binary_crossentropy: 3.9710e-04 - val_loss: 6.7518 - val_binary_crossentropy: 6.7518\n",
      "Epoch 25/25\n",
      "1966/1966 [==============================] - 722s 368ms/step - loss: 3.6634e-04 - binary_crossentropy: 3.6634e-04 - val_loss: 6.2513 - val_binary_crossentropy: 6.2513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f96f8ef5d90>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import time\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "def multi_layers (input_dim, output_dim=3):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.Input(shape=(input_dim,)))\n",
    "    model.add(keras.layers.Dense(50, activation='relu'))\n",
    "    model.add(keras.layers.Dense(40, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(units = 30, activation = \"relu\"))\n",
    "    model.add(Dense(units = 30, activation = \"relu\"))\n",
    "    model.add(Dense(units = 30, activation = \"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(units = 20, activation = \"relu\"))\n",
    "    model.add(Dense(units = 20, activation = \"relu\"))\n",
    "    model.add(Dense(units = 20, activation = \"relu\"))\n",
    "    model.add(BatchNormalization())    \n",
    "    \n",
    "    model.add(Dense(units = 10, activation = \"relu\"))\n",
    "    model.add(Dense(units = 10, activation = \"relu\"))\n",
    "    model.add(Dense(units = 10, activation = \"relu\"))\n",
    "    model.add(BatchNormalization()) \n",
    "    \n",
    "    model.add(Dense(units = 5, activation = \"relu\"))\n",
    "    model.add(Dense(units = 5, activation = \"relu\"))\n",
    "    model.add(Dense(units = 5, activation = \"relu\"))\n",
    "    model.add(BatchNormalization())     \n",
    "    \n",
    "    model.add(Dense(units = output_dim, activation = \"softmax\"))\n",
    "    return model \n",
    "\n",
    "multi_layers_model = multi_layers(102, 2) \n",
    "# multi_layers_model.summary()\n",
    "\n",
    "multi_layers_model.compile(\n",
    "    optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"binary_crossentropy\"]\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    \"multi_layers_model.h5\", monitor='binary_crossentropy', save_best_only=True\n",
    ")\n",
    "\n",
    "adam = keras.optimizers.Adam(learning_rate=0.1)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='binary_crossentropy', patience=5, verbose=1)\n",
    "\n",
    "\n",
    "multi_layers_model.fit(\n",
    "    x = X_train, \n",
    "    y = y_train_stack, \n",
    "    epochs = 25,\n",
    "    batch_size = 1000,\n",
    "    validation_data = (X_val, y_val_stack),\n",
    "    callbacks = [model_checkpoint, early_stop], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3e50b62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghassenabdedayem/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/structured_function.py:264: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6835/6835 [==============================] - 98s 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.], dtype=float32)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = multi_layers_model.predict(X_val)\n",
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0046be7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_stack[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5d203995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.43883337796012\n"
     ]
    }
   ],
   "source": [
    "loss = log_loss(y_val, y_pred)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c973ce9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "73eb3f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1966/1966 [==============================] - 110s 56ms/step - loss: 0.1035 - binary_crossentropy: 0.1035 - val_loss: 2.7878 - val_binary_crossentropy: 2.7878\n",
      "Epoch 2/25\n",
      "1966/1966 [==============================] - 103s 52ms/step - loss: 0.0059 - binary_crossentropy: 0.0059 - val_loss: 3.5764 - val_binary_crossentropy: 3.5764\n",
      "Epoch 3/25\n",
      "1966/1966 [==============================] - 115s 58ms/step - loss: 0.0034 - binary_crossentropy: 0.0034 - val_loss: 3.9609 - val_binary_crossentropy: 3.9609\n",
      "Epoch 4/25\n",
      "1966/1966 [==============================] - 154s 79ms/step - loss: 0.0026 - binary_crossentropy: 0.0026 - val_loss: 4.9091 - val_binary_crossentropy: 4.9091\n",
      "Epoch 5/25\n",
      "1966/1966 [==============================] - 152s 78ms/step - loss: 0.0030 - binary_crossentropy: 0.0030 - val_loss: 5.1376 - val_binary_crossentropy: 5.1376\n",
      "Epoch 6/25\n",
      "1966/1966 [==============================] - 146s 74ms/step - loss: 0.0015 - binary_crossentropy: 0.0015 - val_loss: 4.8869 - val_binary_crossentropy: 4.8869\n",
      "Epoch 7/25\n",
      "1966/1966 [==============================] - 138s 70ms/step - loss: 0.0014 - binary_crossentropy: 0.0014 - val_loss: 5.4783 - val_binary_crossentropy: 5.4783\n",
      "Epoch 8/25\n",
      "1966/1966 [==============================] - 140s 71ms/step - loss: 0.0013 - binary_crossentropy: 0.0013 - val_loss: 5.5656 - val_binary_crossentropy: 5.5656\n",
      "Epoch 9/25\n",
      "1966/1966 [==============================] - 137s 70ms/step - loss: 9.8707e-04 - binary_crossentropy: 9.8707e-04 - val_loss: 5.1218 - val_binary_crossentropy: 5.1218\n",
      "Epoch 10/25\n",
      "1966/1966 [==============================] - 140s 71ms/step - loss: 0.0011 - binary_crossentropy: 0.0011 - val_loss: 5.7587 - val_binary_crossentropy: 5.7587\n",
      "Epoch 11/25\n",
      "1966/1966 [==============================] - 141s 72ms/step - loss: 8.4933e-04 - binary_crossentropy: 8.4933e-04 - val_loss: 5.6640 - val_binary_crossentropy: 5.6640\n",
      "Epoch 12/25\n",
      "1966/1966 [==============================] - 136s 69ms/step - loss: 8.5687e-04 - binary_crossentropy: 8.5687e-04 - val_loss: 5.7228 - val_binary_crossentropy: 5.7228\n",
      "Epoch 13/25\n",
      "1966/1966 [==============================] - 141s 72ms/step - loss: 8.3629e-04 - binary_crossentropy: 8.3629e-04 - val_loss: 5.8604 - val_binary_crossentropy: 5.8604\n",
      "Epoch 14/25\n",
      "1966/1966 [==============================] - 134s 68ms/step - loss: 7.8396e-04 - binary_crossentropy: 7.8396e-04 - val_loss: 5.6574 - val_binary_crossentropy: 5.6574\n",
      "Epoch 15/25\n",
      "1966/1966 [==============================] - 144s 73ms/step - loss: 8.1343e-04 - binary_crossentropy: 8.1343e-04 - val_loss: 5.4614 - val_binary_crossentropy: 5.4614\n",
      "Epoch 16/25\n",
      "1966/1966 [==============================] - 138s 70ms/step - loss: 6.3776e-04 - binary_crossentropy: 6.3776e-04 - val_loss: 6.2519 - val_binary_crossentropy: 6.2519\n",
      "Epoch 17/25\n",
      "1966/1966 [==============================] - 134s 68ms/step - loss: 6.2087e-04 - binary_crossentropy: 6.2087e-04 - val_loss: 5.8294 - val_binary_crossentropy: 5.8294\n",
      "Epoch 18/25\n",
      "1966/1966 [==============================] - 135s 69ms/step - loss: 6.6288e-04 - binary_crossentropy: 6.6288e-04 - val_loss: 5.6514 - val_binary_crossentropy: 5.6514\n",
      "Epoch 19/25\n",
      "1966/1966 [==============================] - 136s 69ms/step - loss: 5.8906e-04 - binary_crossentropy: 5.8906e-04 - val_loss: 6.1520 - val_binary_crossentropy: 6.1520\n",
      "Epoch 20/25\n",
      "1966/1966 [==============================] - 146s 74ms/step - loss: 6.6086e-04 - binary_crossentropy: 6.6086e-04 - val_loss: 6.3445 - val_binary_crossentropy: 6.3445\n",
      "Epoch 21/25\n",
      "1966/1966 [==============================] - 141s 72ms/step - loss: 5.3805e-04 - binary_crossentropy: 5.3805e-04 - val_loss: 6.2572 - val_binary_crossentropy: 6.2572\n",
      "Epoch 22/25\n",
      "1966/1966 [==============================] - 132s 67ms/step - loss: 5.0914e-04 - binary_crossentropy: 5.0914e-04 - val_loss: 6.3084 - val_binary_crossentropy: 6.3084\n",
      "Epoch 23/25\n",
      "1966/1966 [==============================] - 130s 66ms/step - loss: 5.7972e-04 - binary_crossentropy: 5.7972e-04 - val_loss: 6.7246 - val_binary_crossentropy: 6.7246\n",
      "Epoch 24/25\n",
      "1966/1966 [==============================] - 130s 66ms/step - loss: 5.1317e-04 - binary_crossentropy: 5.1317e-04 - val_loss: 6.4362 - val_binary_crossentropy: 6.4362\n",
      "Epoch 25/25\n",
      "1966/1966 [==============================] - 132s 67ms/step - loss: 4.4246e-04 - binary_crossentropy: 4.4246e-04 - val_loss: 6.3474 - val_binary_crossentropy: 6.3474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f97762b9730>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_layers_model = multi_layers(102, 1) \n",
    "# multi_layers_model.summary()\n",
    "\n",
    "multi_layers_model.compile(\n",
    "    optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"binary_crossentropy\"]\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    \"multi_layers_model.h5\", monitor='binary_crossentropy', save_best_only=True\n",
    ")\n",
    "\n",
    "adam = keras.optimizers.Adam(learning_rate=0.1)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='binary_crossentropy', patience=5, verbose=1)\n",
    "\n",
    "\n",
    "multi_layers_model.fit(\n",
    "    x = X_train, \n",
    "    y = y_train, \n",
    "    epochs = 25,\n",
    "    batch_size = 1000,\n",
    "    validation_data = (X_val, np.array(y_val)),\n",
    "    callbacks = [model_checkpoint, early_stop], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "49543df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      " [1.000000e+00 4.980952e-12]\n"
     ]
    }
   ],
   "source": [
    "loss = log_loss(y_val, y_pred)\n",
    "print(y_val[400], '\\n', y_pred[400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eb984c",
   "metadata": {},
   "source": [
    "### Logistic regression and random forest to test the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4c39725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8816955490116937\n"
     ]
    }
   ],
   "source": [
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_clf = clf.predict_proba(X_val)\n",
    "y_pred_clf = y_pred_clf[:,1]\n",
    "\n",
    "loss = log_loss(y_val, y_pred_clf)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfed92da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.3793166433572\n"
     ]
    }
   ],
   "source": [
    "# Random forest classifier\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(X_train, y_train)\n",
    "rf_y_pred = rf_clf.predict_proba(X_val)\n",
    "\n",
    "rf_y_pred = rf_y_pred[:,1]\n",
    "\n",
    "loss = log_loss(y_val, rf_y_pred)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
