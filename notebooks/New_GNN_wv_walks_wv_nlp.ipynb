{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbW5vRi3mWS0"
      },
      "source": [
        "# Packages and functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5GfuP1hlTmg",
        "outputId": "8dddec47-7659-4f75-fefb-3cd1a78ab8a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.9/dist-packages (1.3.6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from random import random\n",
        "from random import randint\n",
        "from datetime import datetime\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from time import time\n",
        "from gensim.models import Word2Vec\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "from scipy.sparse import identity, diags\n",
        "from urllib.request import urlopen\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "import re\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from random import choice\n",
        "\n",
        "\n",
        "\n",
        "!pip install unidecode\n",
        "\n",
        "from unidecode import unidecode\n",
        "\n",
        "#from nltk.stem import WordNetLemmatizer\n",
        "#lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "BJSlSwejljfp"
      },
      "outputs": [],
      "source": [
        "\n",
        "def text_to_list(text): # a function that split the text of the authors to a list of authors\n",
        "    return unidecode(text).split(',')\n",
        "\n",
        "def intersection(lst1, lst2): # a function that returns the number of common items of two lists and 1 or 0 if there are common. This function will be used in add_authors_to_pairs to add this features to the pairs.\n",
        "    lst3 = [value for value in lst1 if value in lst2]\n",
        "    is_common = 1 if len(lst3)>0 else 0\n",
        "    return len(lst3), is_common\n",
        "\n",
        "\n",
        "def save_subgraph_in_file(nbr_nodes, source_path='../input_data/edgelist.txt', destination_path='../input_data/small_edgelist.txt'):\n",
        "    G = nx.read_edgelist(source_path, delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
        "    G = G.subgraph(range(nbr_nodes))\n",
        "    nx.write_edgelist(G, path=destination_path, delimiter=',')\n",
        "    print(G.number_of_nodes(), 'nodes,', G.number_of_edges(), 'edges Graph extracted from', source_path[source_path.rfind('/')+1:])\n",
        "    G = nx.read_edgelist(destination_path, delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
        "    print(G.number_of_nodes(), 'nodes,', G.number_of_edges(), 'edges Graph saved in', destination_path[destination_path.rfind('/')+1:])\n",
        "    print(max(G.nodes))\n",
        "    return\n",
        "\n",
        "\n",
        "def read_train_val_graph(path='small_edgelist.txt', val_ratio=0.1):\n",
        "    #gets the data from the file on the distant server\n",
        "    #G = nx.read_edgelist(urlopen('https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/edgelist.txt'), delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
        "    G = nx.read_edgelist(path, delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
        "    nodes = list(G.nodes())\n",
        "    n = G.number_of_nodes()\n",
        "    m = G.number_of_edges()\n",
        "    edges = list(G.edges())\n",
        "\n",
        "    print('Number of nodes:', n, 'number of edges:', m,'in the Complete the set')\n",
        "\n",
        "    node_to_idx = dict()\n",
        "    for i, node in enumerate(nodes):\n",
        "        node_to_idx[node] = i\n",
        "\n",
        "    val_edges = list()\n",
        "    G_train = G.copy()\n",
        "\n",
        "    for edge in edges:\n",
        "        if random() < val_ratio and edge[0] < n and edge[1] < n:\n",
        "            val_edges.append(edge)\n",
        "            G_train.remove_edge(edge[0], edge[1]) # We remove the val edges from the graph G\n",
        "\n",
        "   \n",
        "    #for edge in val_edges:\n",
        "        \n",
        "\n",
        "    n = G_train.number_of_nodes()\n",
        "    m = G_train.number_of_edges()\n",
        "    train_edges = list(G_train.edges())\n",
        "\n",
        "    print('Number of nodes:', n, 'number of edges:', m, 'in the Training set')\n",
        "    print('len(nodes)', len(nodes))\n",
        "\n",
        "    y_val = [1]*len(val_edges)\n",
        "\n",
        "    n_val_edges = len(val_edges)\n",
        "    \n",
        "    print('Creating random val_edges...')\n",
        "    for i in range(n_val_edges):\n",
        "        n1 = nodes[randint(0, n-1)]\n",
        "        n2 = nodes[randint(0, n-1)]\n",
        "        (n1, n2) = (min(n1, n2), max(n1, n2))\n",
        "        while n2 >= n: #or (n1, n2) in train_edges:\n",
        "            if (n1, n2) in train_edges:\n",
        "                print((n1, n2), 'in train_edges:')\n",
        "            n1 = nodes[randint(0, n-1)]\n",
        "            n2 = nodes[randint(0, n-1)]\n",
        "            (n1, n2) = (min(n1, n2), max(n1, n2))\n",
        "        val_edges.append((n1, n2))\n",
        "\n",
        "    y_val.extend([0]*(n_val_edges))\n",
        "    \n",
        "    ### From Giannis /!\\\n",
        "    val_indices = np.zeros((2,len(val_edges)))\n",
        "    for i,edge in enumerate(val_edges):\n",
        "        val_indices[0,i] = node_to_idx[edge[0]]\n",
        "        val_indices[1,i] = node_to_idx[edge[1]]\n",
        "    \n",
        "    print('Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects')\n",
        "    print('Loaded from', path[path.rfind('/')+1:], 'and with a training validation split ratio =', val_ratio)\n",
        "    \n",
        "    \n",
        "    \n",
        "    return G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx\n",
        "\n",
        "def random_walk(G, node, walk_length):\n",
        "    walk = [node]\n",
        "  \n",
        "    for i in range(walk_length-1):\n",
        "        neibor_nodes = list(G.neighbors(walk[-1]))\n",
        "        if len(neibor_nodes) > 0:\n",
        "            next_node = choice(neibor_nodes)\n",
        "            walk.append(next_node)\n",
        "    walk = [node for node in walk] # in case the nodes are in string format, we don't need to cast into string, but if the nodes are in numeric or integer, we need this line to cast into string\n",
        "    return walk\n",
        "\n",
        "\n",
        "def generate_walks(G, num_walks, walk_length):\n",
        "  # Runs \"num_walks\" random walks from each node, and returns a list of all random walk\n",
        "    t = time()\n",
        "    print('Start generating walks....')\n",
        "    walks = list()  \n",
        "    for i in range(num_walks):\n",
        "        for node in G.nodes():\n",
        "            walk = random_walk(G, node, walk_length)\n",
        "            walks.append(walk)\n",
        "        #print('walks : ', walks)\n",
        "    print('Random walks generated in in {}s!'.format(round(time()-t)))\n",
        "    return walks\n",
        "\n",
        "def apply_word2vec_on_features(features, nodes, vector_size=128, window=5, min_count=0, sg=1, workers=8):\n",
        "    t = time()\n",
        "    print('Start applying Word2Vec...')\n",
        "    wv_model = Word2Vec(vector_size=vector_size, window=window, min_count=min_count, sg=sg, workers=workers)\n",
        "    wv_model.build_vocab(features)\n",
        "    wv_model.train(features, total_examples=wv_model.corpus_count, epochs=5) \n",
        "    print('Word2vec model trained on features in {} min!'.format(round((time()-t)/60)))\n",
        "    features_np = []\n",
        "    for node in nodes:\n",
        "        features_np.append(wv_model.wv[node])\n",
        "\n",
        "    features_np = np.array(features_np)\n",
        "    print(features_np.shape, 'features numpy array created in {} min!'.format(round((time()-t)/60)))\n",
        "    return features_np\n",
        "\n",
        "\n",
        "\n",
        "def normalize_adjacency(A):\n",
        "    n = A.shape[0]\n",
        "    A = A + identity(n)\n",
        "    degs = A.dot(np.ones(n))\n",
        "    inv_degs = np.power(degs, -1)\n",
        "    D_inv = diags(inv_degs)\n",
        "    A_hat = D_inv.dot(A)\n",
        "    return A_hat\n",
        "\n",
        "def create_and_normalize_adjacency(G):\n",
        "    adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix of the training graph\n",
        "    adj = normalize_adjacency(adj)\n",
        "    print('Created a normalized adjancency matrix of shape', adj.shape)\n",
        "    indices = np.array(adj.nonzero()) # Gets the positions of non zeros of adj into indices\n",
        "    print('Created indices', indices.shape, 'with the positions of non zeros in adj matrix')\n",
        "    return adj, indices\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)\n",
        " \n",
        "\n",
        "def add_authors_to_pairs (pairs, authors):\n",
        "    #authors = pd.DataFrame(authors)\n",
        "    try: \n",
        "        pairs = pairs.detach().cpu().numpy()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    pairs_df = pd.DataFrame(np.transpose(pairs)).rename(columns={0: \"paper_1\", 1: \"paper_2\"})\n",
        "    #pairs = torch.tensor(pairs).to(device)\n",
        "    pairs_df = pairs_df.merge(authors, left_on='paper_1', right_on='paper_id', how='left').rename(columns={'authors': \"authors_1\"})\n",
        "    pairs_df = pairs_df.merge(authors, left_on='paper_2', right_on='paper_id', how='left').rename(columns={'authors': \"authors_2\"})\n",
        "    pairs_df.drop(['paper_id_x', 'paper_id_y'], axis=1, inplace=True)\n",
        "\n",
        "    pairs_df['nb_common_author'] = pairs_df.apply(lambda row: intersection(row['authors_1'], row['authors_2'])[0], axis=1)\n",
        "    pairs_df['is_common_author'] = pairs_df.apply(lambda row: intersection(row['authors_1'], row['authors_2'])[1], axis=1)\n",
        "\n",
        "    #pairs_tensor = torch.LongTensor(np.transpose(pairs_df[[\"paper_1\", \"paper_2\", 'is_common_author', 'nb_common_author']].values.tolist())).to(device)\n",
        "    \n",
        "    return np.transpose(pairs_df[[\"paper_1\", \"paper_2\", 'is_common_author', 'nb_common_author']].values.tolist())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5ef7pPp7lBeS"
      },
      "outputs": [],
      "source": [
        "def read_and_clean_abstracts (nodes, sample_length=-1, abstracts_path = 'https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/abstracts.txt'):\n",
        "    t = time()\n",
        "    abstracts = dict()\n",
        "    abstracts_list = list()\n",
        "    f = urlopen(abstracts_path)\n",
        "    \n",
        "    for i, line in enumerate(f):\n",
        "        if i == sample_length:\n",
        "            break\n",
        "        if i in nodes:\n",
        "            node, abstract = str(line).lower().split('|--|')\n",
        "            abstract = remove_stopwords(abstract)\n",
        "            abstract = re.sub(r\"[,.;@#?!&$()-]\", \" \", abstract)\n",
        "\n",
        "            for word in abstract.split()[:-1]:\n",
        "                #abstract = abstract.replace(word, stemmer.stem(word))\n",
        "                abstract = abstract.replace(word, lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word), pos='s'), pos='a'), pos='n'), pos='v'), pos='r'))\n",
        "            \n",
        "            node = re.sub(\"[^0-9]\", \"\", node)\n",
        "            if i != int(node):\n",
        "                print('i and node not the same', i, node)\n",
        "            abstracts[int(node)] = abstract\n",
        "            abstracts_list.append(abstract)\n",
        "        \n",
        "    print('Text loaded and cleaned in {:.0f} sec'.format(time()-t))\n",
        "    return abstracts\n",
        "\n",
        "def doc_counter (documents, word): #a function that return the number of documents containing a word\n",
        "    counter = 0\n",
        "    for i in documents:\n",
        "        if word in documents[i]:\n",
        "            counter += 1\n",
        "    return counter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UAOZ3MLP98UR"
      },
      "outputs": [],
      "source": [
        "class Vocabulary:\n",
        "\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {}\n",
        "        self.word_occurrence = {}\n",
        "        self.words_list = []\n",
        "        self.sentences_list = []\n",
        "        self.sentences_list_words = []\n",
        "        self.num_words = 0\n",
        "        self.num_sentences = 0\n",
        "        self.longest_sentence = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2index:\n",
        "            # First entry of word into vocabulary\n",
        "            self.words_list.append(word)\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "            self.word_occurrence[word] = 1\n",
        "        else:\n",
        "            # Word exists; increase word count\n",
        "            self.word2count[word] += 1\n",
        "            self.word_occurrence[word] += 1\n",
        "            \n",
        "    def add_sentence(self, sentence):\n",
        "        sentence_len = 0\n",
        "        for word in sentence.split()[:-1]:\n",
        "            sentence_len += 1\n",
        "            self.add_word(word)\n",
        "        if sentence_len > self.longest_sentence:\n",
        "            # This is the longest sentence\n",
        "            self.longest_sentence = sentence_len\n",
        "        # Count the number of sentences\n",
        "        self.num_sentences += 1\n",
        "        self.sentences_list.append(sentence)\n",
        "        self.sentences_list_words.append(sentence.split()[:-1])\n",
        "\n",
        "    def to_word(self, index):\n",
        "        return self.index2word[index]\n",
        "\n",
        "    def to_index(self, word):\n",
        "        return self.word2index[word]\n",
        "\n",
        "    def words(self):\n",
        "        return self.words_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QlXtTyCe4C_J"
      },
      "outputs": [],
      "source": [
        "def list_words_to_one_sentence_wv_vector(sentences_list_words):\n",
        "    t = time()\n",
        "\n",
        "    embedded_abstracts = dict()\n",
        "    for node, abstract in enumerate(sentences_list_words):\n",
        "        if len(abstract) > 0: #some abstracts are null\n",
        "            embedded_abstracts[node] = np.mean(wv_model.wv[abstract], axis=0)\n",
        "            for quartile in np.percentile(wv_model.wv[abstract], [25, 50, 75], axis=0):\n",
        "                embedded_abstracts[node] = np.concatenate((embedded_abstracts[node], quartile), axis=0)\n",
        "        else: #if the abstract text is null, we fill the embedded text vector by random numbers (it could help to prevent overfittiing)\n",
        "            embedded_abstracts[node] = np.random.uniform(wv_model.wv.vectors.min(), wv_model.wv.vectors.max(), size=embedded_abstracts[0].shape)\n",
        "    print('nodes embeddings generated based on words embeddings in {:.0f} sec in embedded_abstracts'.format(time()-t)) #206 sec\n",
        "    return embedded_abstracts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iHDwUFfhUmJw"
      },
      "outputs": [],
      "source": [
        "def train_wv_on_vocal (voc, vector_size):\n",
        "    t = time()\n",
        "    wv_model = Word2Vec(vector_size=vector_size, window=5, min_count=1, sg=1, workers=8)\n",
        "    wv_model.build_vocab(voc.sentences_list_words)\n",
        "    wv_model.train(voc.sentences_list_words, total_examples=wv_model.corpus_count, epochs=5) \n",
        "    print('word2vec trained in {:.0f} sec'.format(time()-t)) #219 sec\n",
        "    return wv_model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH6hAskFw5AU"
      },
      "source": [
        "# Text processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "oHyYv78NlkXU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3b35a3d-a60a-49d3-e0db-9688a77bdc57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes: 980 number of edges: 2405 in the Complete the set\n",
            "Number of nodes: 980 number of edges: 2188 in the Training set\n",
            "len(nodes) 980\n",
            "Creating random val_edges...\n",
            "Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects\n",
            "Loaded from small_edgelist.txt and with a training validation split ratio = 0.1\n",
            "Start generating walks....\n",
            "Random walks generated in in 0s!\n",
            "Start applying Word2Vec...\n",
            "Word2vec model trained on features in 0 min!\n",
            "(980, 64) features numpy array created in 0 min!\n",
            "Created a normalized adjancency matrix of shape (980, 980)\n",
            "Created indices (2, 5356) with the positions of non zeros in adj matrix\n"
          ]
        }
      ],
      "source": [
        "path = 'small_edgelist.txt' #not used\n",
        "num_walks = 10\n",
        "walk_length=15\n",
        "wv_vector_size = 64\n",
        "\n",
        "G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx = read_train_val_graph(path=path, val_ratio=0.1)\n",
        "walks = generate_walks(G=G_train, num_walks=num_walks, walk_length=walk_length)\n",
        "walks_wv = apply_word2vec_on_features(features=walks, nodes=nodes, vector_size=wv_vector_size)\n",
        "adj, indices = create_and_normalize_adjacency(G_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "sbhiufK3kLG7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20f47db0-0e3f-4e39-fa28-95875205355d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text loaded and cleaned in 13 sec\n",
            "Texe cleaned and vocab built in 13 sec\n"
          ]
        }
      ],
      "source": [
        "t = time()\n",
        "n = -1 #length of the sample to develop and test the pipeline (-1 or negative values to take all the dataset)\n",
        "\n",
        "#takes 4 minutes to process all the abstracts\n",
        "abstracts = read_and_clean_abstracts(nodes, sample_length=n)  #149s #194s\n",
        "abstracts_dict_list_words = {i: abstracts[i].split()[:-1] for i in nodes}\n",
        "abstracts_list_sentences = [list(item)[1][:-3] for item in abstracts.items()]\n",
        "\n",
        "#we create a vacabulary of words and sentences (abstracts)\n",
        "#we take only a sample of 3 abstracts (i=2) to explore the approach\n",
        "voc = Vocabulary('abstracts') \n",
        "for i in nodes:\n",
        "    voc.add_sentence(abstracts[i])\n",
        "\n",
        "print('Texe cleaned and vocab built in {:.0f} sec'.format(time()-t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DesyOWhh86uv",
        "outputId": "25048227-4436-4ea4-ece0-3355127f210b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word2vec trained in 2 sec\n",
            "nodes embeddings generated based on words embeddings in 1 sec in embedded_abstracts\n"
          ]
        }
      ],
      "source": [
        "#apply wv to abstracts\n",
        "vector_size = 64\n",
        "\n",
        "wv_model = train_wv_on_vocal (voc, vector_size=vector_size)\n",
        "embedded_abstracts = list_words_to_one_sentence_wv_vector(voc.sentences_list_words)\n",
        "embedded_abstracts = np.array(list(embedded_abstracts.values()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(walks_wv.shape, embedded_abstracts.shape)\n",
        "features_np = np.concatenate((walks_wv, embedded_abstracts), axis=1)\n",
        "print(walks_wv.shape, embedded_abstracts.shape, features_np.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5RJqWqu8nz1",
        "outputId": "3930b366-9b2b-475d-ae9f-a1b3de5a57c4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(980, 64) torch.Size([980, 256])\n",
            "(980, 64) torch.Size([980, 256]) (980, 320)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Q1TfjcN1GiUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6375d7a-5d10-4158-e0dd-6a846c77a068"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "node features shape: (980, 320)\n"
          ]
        }
      ],
      "source": [
        "indices = torch.LongTensor(indices).to(device)\n",
        "val_indices = torch.LongTensor(val_indices).to(device)\n",
        "y_val = torch.LongTensor(y_val).to(device)\n",
        "\n",
        "# Create class labels\n",
        "y = np.zeros(2*indices.shape[1])\n",
        "y[:indices.shape[1]] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
        "y = torch.LongTensor(y).to(device)\n",
        "\n",
        "features = torch.FloatTensor(features_np).to(device)\n",
        "print('node features shape:', features_np.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhC2OuMhBMKD",
        "outputId": "6ed8b5ae-47b6-49f0-dad6-22a3da5bbad5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([980, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "dWAzHqmxmYff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "10c1788e-5b1d-4ce1-e60e-59ff8880b5e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   paper_id                                            authors\n",
              "0         0  [James H. Niblock, Jian-Xun Peng, Karen R. McM...\n",
              "1         1          [Jian-Xun Peng, Kang Li, De-Shuang Huang]\n",
              "2         2                                      [J. Heikkila]\n",
              "3         3    [L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]\n",
              "4         4  [Long Zhang, Kang Li, Er-Wei Bai, George W. Ir..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-86a1b3e3-66be-477e-b6ed-4a243a620f62\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_id</th>\n",
              "      <th>authors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[James H. Niblock, Jian-Xun Peng, Karen R. McM...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[Jian-Xun Peng, Kang Li, De-Shuang Huang]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[J. Heikkila]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[Long Zhang, Kang Li, Er-Wei Bai, George W. Ir...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86a1b3e3-66be-477e-b6ed-4a243a620f62')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-86a1b3e3-66be-477e-b6ed-4a243a620f62 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-86a1b3e3-66be-477e-b6ed-4a243a620f62');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "authors = pd.read_csv(urlopen('https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/authors.txt'), sep = '|', header=None)\n",
        "authors = authors.rename(columns={0: \"paper_id\", 2: \"authors\"})\n",
        "#authors = authors[authors['paper_id'].isin(nodes) ]\n",
        "authors['authors'] = authors['authors'].apply(text_to_list)\n",
        "authors = authors[[\"paper_id\", \"authors\"]]\n",
        "authors = authors[authors['paper_id'] <= max(G.nodes())]\n",
        "authors.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ROBXxQaZnfQ3"
      },
      "outputs": [],
      "source": [
        "class GNN(nn.Module):\n",
        "    def __init__(self, n_feat, n_hidden, n_class, sub_class, dropout):\n",
        "        super(GNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(n_feat, n_hidden)\n",
        "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
        "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
        "        self.double_fc3 = nn.Linear((3*n_hidden), n_hidden)\n",
        "        self.fc4 = nn.Linear(n_hidden, sub_class)\n",
        "        self.fc5 = nn.Linear(sub_class, n_class)        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        \n",
        "\n",
        "    def forward(self, x_in, adj, pairs):\n",
        "        \n",
        "        h1 = self.fc1(x_in)\n",
        "        z1 = self.relu(torch.mm(adj, h1))\n",
        "        z1 = self.dropout(z1)\n",
        "        del(h1)\n",
        "\n",
        "        h2 = self.fc2(z1)\n",
        "        z2 = self.relu(torch.mm(adj, h2))\n",
        "        z2 = self.dropout(z2)\n",
        "        del(h2, z1, adj)\n",
        "\n",
        "        x = z2[pairs[0]] - z2[pairs[1]] # embedded features (z2) of node 0 - embedded features of node 1 // x_in[:, :64].shape\n",
        "        \n",
        "        print(x_in[:, :64].shape)\n",
        "        \n",
        "        x = pairs[3][:, None] * x #we multiply by the number of common authors by pairs of nodes (papers)\n",
        "        x = x.to(device)\n",
        "        x1 = z2[pairs[0]]#.to(device)\n",
        "        x2 = z2[pairs[1]]#.to(device)\n",
        "        del(pairs)\n",
        "        x = torch.cat((x, x1, x2), dim=1)\n",
        "        del(x1, x2)\n",
        "        \n",
        "\n",
        "        x = self.relu(self.double_fc3(x))        \n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x = self.relu(self.fc4(x))\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x = self.fc5(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "bnRNz5z4nh6x"
      },
      "outputs": [],
      "source": [
        "def early_stopping(loss_train, list_loss_train, loss_val, list_loss_val, window=10, tolerance=0.01):\n",
        "    list_loss_val = list(list_loss_val)[-window:]\n",
        "    list_loss_train = list(list_loss_train)[-window:]\n",
        "    if (len(list_loss_val) == window and loss_val > (sum(list_loss_val)/len(list_loss_val)) and loss_train + tolerance < loss_val) or (len(list_loss_train) == window and loss_train > (sum(list_loss_train)/len(list_loss_train))):\n",
        "        print('train: {:.5f} val: {:.5f} mean val: {:.5f}'.format(loss_train, loss_val, (sum(list_loss_val)/len(list_loss_val))))\n",
        "        return True\n",
        "    return False\n",
        "    \n",
        "\n",
        "    \n",
        "def train_model(model, learning_rate, features, authors, adj, indices, y, val_indices, y_val, epochs, run_number):\n",
        "    # Train model\n",
        "    start_time = time()\n",
        "    print('Initializing the optimizer with learning rate:', learning_rate)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) #optimizer with halving learning rate in training\n",
        "    try: os.mkdir('./outputs')\n",
        "    except: pass\n",
        "    print('Preparing the data for training...')        \n",
        "    val_indices = add_authors_to_pairs(val_indices, authors) #we add the authors to val_pairs\n",
        "    #print(type(indices), np.shape(indices))\n",
        "    #indices = add_authors_to_pairs(indices, authors) #we add the authors to indices\n",
        "    #print(type(indices), np.shape(indices))\n",
        "    rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)# We take random indices each time we run an epoch\n",
        "    #rand_indices = add_authors_to_pairs(rand_indices, authors)\n",
        "    pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices. \n",
        "    pairs = torch.tensor(pairs).to(device)\n",
        "    del(authors, indices, rand_indices)\n",
        "\n",
        "    today = datetime.today().strftime('%Y-%m-%d-%H:%M')\n",
        "    list_loss_val = []\n",
        "    list_loss_train = []\n",
        "    \n",
        "    features = features.to(device)\n",
        "    adj = adj.to(device)\n",
        "    pairs = pairs.to(device)\n",
        "    \n",
        "    halving_lr = 0 # counter of the number of halving lr\n",
        "    print('Start training...')\n",
        "    for epoch in range(epochs):\n",
        "        t = time()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        model.train()\n",
        "\n",
        "        output = model(features, adj, pairs).to(device) # we run the model that gives the output.\n",
        "        loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
        "        acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
        "        loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
        "        optimizer.step() # Performs a single optimization step (parameter update).\n",
        "        \n",
        "        model.eval()\n",
        "        output = model(features, adj, val_indices).to(device)\n",
        "        #y_val = torch.LongTensor(y_val).to(device)\n",
        "        loss_val = F.nll_loss(output, y_val)\n",
        "        list_loss_val.append(loss_val.item())\n",
        "        list_loss_train.append(loss_train.item())\n",
        "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
        "        \n",
        "        if epoch % 5 == 0:\n",
        "            print('Epoch: {:03d}'.format(epoch+1),\n",
        "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "                  'time: {} s'.format(int(round(time()) - round(t))),\n",
        "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
        "        if epoch % 20 == 0:\n",
        "            model_path = \"outputs/{}-model-{}epochs-{}.pt\".format(today, epoch, run_number)\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            \n",
        "        early = early_stopping(loss_train.item(), list_loss_train, loss_val.item(), list_loss_val, window=10)        \n",
        "        if early:\n",
        "            halving_lr += 1\n",
        "            if halving_lr > 10:\n",
        "                break\n",
        "            list_loss_val=[]\n",
        "            learning_rate = learning_rate/2\n",
        "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "            print('Deviding the learning rate by 2. New learning rate: {:.6f}'.format(learning_rate))\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "7i8UpGHuNhQp"
      },
      "outputs": [],
      "source": [
        "n_hidden = 64\n",
        "dropout_rate = 0.2\n",
        "sub_class = 8\n",
        "n_class = 2\n",
        "n_features = features.shape[1]\n",
        "\n",
        "# Creates the model\n",
        "model = GNN(n_features, n_hidden, n_class, sub_class, dropout_rate).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "KDQ_DCCJavQZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "3f0ead87-6edf-4970-9bfc-cb5722610c0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> torch.Size([2, 5356])\n",
            "Initializing the optimizer with learning rate: 0.02\n",
            "Preparing the data for training...\n",
            "pairs <class 'numpy.ndarray'> (2, 434)\n",
            "authors <class 'pandas.core.frame.DataFrame'> (1000, 2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-1ccaec9d9879>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-47-b2966cbd5887>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, learning_rate, features, authors, adj, indices, y, val_indices, y_val, epochs, run_number)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#indices = add_authors_to_pairs(indices, authors) #we add the authors to indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#print(type(indices), np.shape(indices))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mrand_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# We take random indices each time we run an epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m#rand_indices = add_authors_to_pairs(rand_indices, authors)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrand_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Concatenate the edges indices and random indices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: device not found"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "#import gc\n",
        "#del variables\n",
        "#gc.collect()\n",
        "\n",
        "epochs = 1000\n",
        "run_number = randint(0, 1000)\n",
        "\n",
        "print(type(indices), np.shape(indices))\n",
        "\n",
        "trained_model = train_model(model, 0.02, features, authors, adj, indices, y, val_indices, y_val, epochs, run_number)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "intersection(['Jian-Xun Peng', 'Kang Li', 'De-Shuang Huang'], ['J. Heikkila', 'Kang Li', 'De-Shuang Huang'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEpkbzgYDswA",
        "outputId": "0c477666-124a-43e5-b84e-db7d59234f3f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "V42H3MbEh6Ba"
      },
      "outputs": [],
      "source": [
        "authors.to_csv('df_authors.csv', sep=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Von8ehIosDPa"
      },
      "outputs": [],
      "source": [
        "features.get_device()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZcN9lfNA6ly",
        "outputId": "14069858-7dab-418e-e595-009887b5ca1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([138499, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5VIVVzmMCaG"
      },
      "outputs": [],
      "source": [
        "del(adj, G, G_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "158xT56h4_QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZezlQHa24_Xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zev8mABQ4_fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zoayPq8E5AGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "okgeOyoZ5AJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe1G4eUBQLNM"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, filter_sizes, embedding_dim, dropout, n_class=2):\n",
        "        super(CNN, self).__init__()\n",
        "        #self.conv1d = nn.Conv1d(in_channels=vocab_size, out_channels=embedding_dim, kernel_size=3, stride=1, padding=0)\n",
        "        \n",
        "        # self.convs = nn.ModuleList([\n",
        "        #     nn.Conv1d(in_channels=vocab_size, out_channels=embedding_dim, \n",
        "        #               kernel_size=fs, stride=1, padding=0)\n",
        "        #     for fs in filter_sizes\n",
        "        # ])\n",
        "        \n",
        "        self.conv1d = nn.Conv1d(in_channels=vocab_size, out_channels=embedding_dim*4, \n",
        "                      kernel_size=3, stride=1, padding=3)\n",
        "        self.maxpooling = nn.MaxPool1d(kernel_size=5)\n",
        "\n",
        "\n",
        "        self.fc2 = nn.Linear(embedding_dim*4, embedding_dim) \n",
        "        self.fc1 = nn.Linear(embedding_dim*2, n_class)             \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "        \n",
        "\n",
        "    def forward(self, features, pairs):\n",
        "        max_filter = max(filter_sizes)\n",
        "        \n",
        "        features = self.dropout(features)\n",
        "        features = features.unsqueeze(2)\n",
        "\n",
        "        features = self.conv1d(features)\n",
        "        features = self.maxpooling(features)\n",
        "        features = self.sigmoid(features)\n",
        "        features = self.dropout(features)        \n",
        "        features = features.view(features.shape[0], -1)\n",
        "        features = features.squeeze(-1)\n",
        "\n",
        "        features = self.fc2(features)\n",
        "        features = self.relu(features)\n",
        "        features = self.dropout(features)\n",
        "\n",
        "        x1 = features[pairs[0]]\n",
        "        x = features[pairs[1]]        \n",
        "        y = torch.cat((x, x1), 1)\n",
        "        del (features, x1)        \n",
        "        \n",
        "        y = self.fc1(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.dropout(y) ### we distinguish x = embedding and y = output \n",
        "        \n",
        "        return F.log_softmax(y, dim=1), x        \n",
        "         \n",
        "\n",
        "\n",
        "        #print(x.shape)\n",
        "        #print(features.shape)\n",
        "\n",
        "\n",
        "        #features = F.pad(features, (1, max_filter+1), 'constant', 0) #(1, 1): pad last dim in a 'constant' mode with the value 0\n",
        "        #features = features.unsqueeze(2)\n",
        "\n",
        "        #features_conv = self.conv1d(features)\n",
        "\n",
        "        # conv_outputs = []\n",
        "        # for conv in self.convs:\n",
        "            \n",
        "        #     features = conv(features)\n",
        "        #     features = nn.functional.max_pool1d(features, kernel_size=features.shape[2])\n",
        "        #     features = nn.functional.relu(features)\n",
        "        #     features = self.dropout(features)\n",
        "        #     conv_outputs.append(features)\n",
        "\n",
        "        # Concatenate and flatten output when filter sizes is longer than one element\n",
        "        #try: features = torch.cat(features, dim=1)\n",
        "        #except: pass\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "        #print(features.shape)\n",
        "     \n",
        "\n",
        "        # x = self.fc2(x)\n",
        "        # x = self.sigmoid(x)\n",
        "        # y = self.dropout(x)       \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evrXfCMq9MDn"
      },
      "outputs": [],
      "source": [
        "def early_stopping(loss_train, list_loss_train, loss_val, list_loss_val, wait=15, tolerance=0.05):\n",
        "    list_loss_val = list(list_loss_val)[-wait:]\n",
        "    list_loss_train = list(list_loss_train)[-wait:]\n",
        "    if (len(list_loss_val) == wait and loss_val >= (sum(list_loss_val)/len(list_loss_val)) and (loss_train + tolerance) < loss_val):\n",
        "        print('VAL early stop: train = {:.5f} val = {:.5f} mean val = {:.5f}'.format(loss_train, loss_val, (sum(list_loss_val)/len(list_loss_val))))\n",
        "        return True\n",
        "    elif  len(list_loss_train) == wait and loss_train >= (sum(list_loss_train)/len(list_loss_train)):\n",
        "        print('TRAIN early stop: train = {:.5f} val = {:.5f} mean train = {:.5f}'.format(loss_train, loss_val, (sum(list_loss_train)/len(list_loss_train))))\n",
        "        return True\n",
        "    elif (loss_train + tolerance) < loss_val and len(list_loss_val) >= wait:\n",
        "        print('VAL early stop: train = {:.5f} val = {:.5f} mean val = {:.5f}'.format(loss_train, loss_val, (sum(list_loss_val)/len(list_loss_val))))\n",
        "        return True \n",
        "    return False\n",
        "    \n",
        "\n",
        "    \n",
        "def train_NLP_model(model, learning_rate, features, indices, y, val_indices, y_val, epochs, batch_size, run_number):\n",
        "    # Train model\n",
        "    start_time = time()\n",
        "    print('Initializing the optimizer with learning rate:', learning_rate)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) #optimizer with halving learning rate in training\n",
        "    try: os.mkdir('./outputs')\n",
        "    except: pass\n",
        "    print('Preparing the data for training...')\n",
        "    #indices must be a torch tensor to be able to apply size method\n",
        "    rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=indices.device)# We take random indices each time we run an epoch\n",
        "    pairs = torch.cat((indices, rand_indices), dim=1).to(device) # Concatenate the edges indices and random indices. \n",
        "    del(indices, rand_indices)\n",
        "\n",
        "    today = datetime.today().strftime('%Y-%m-%d-%H:%M')\n",
        "    list_loss_val = []\n",
        "    list_loss_train = []\n",
        "    \n",
        "    #features = features.to(device)\n",
        "    #pairs = pairs.to(device)\n",
        "    \n",
        "    halving_lr = 0 # counter of the number of halving lr\n",
        "    print('Start training...')\n",
        "    for epoch in range(epochs):\n",
        "        t = time()\n",
        "        \n",
        "        permutation = torch.randperm(pairs.size()[1])\n",
        "\n",
        "        for i in range(0, pairs.size()[1], batch_size):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            elts_indices = permutation[i:i+batch_size]\n",
        "            batch_pairs = pairs[:, elts_indices]\n",
        "            batch_y = y[elts_indices]\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            output, embedding = model(features, batch_pairs) # we run the model that gives the output.\n",
        "            loss_train = F.nll_loss(output, batch_y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
        "            acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), batch_y.cpu().numpy())# just to show it in the out put message of the training\n",
        "            loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
        "            optimizer.step() # Performs a single optimization step (parameter update).\n",
        "        \n",
        "        model.eval()\n",
        "        output, embedding = model(features, val_indices)\n",
        "        #y_val = torch.LongTensor(y_val).to(device)\n",
        "        loss_val = F.nll_loss(output, y_val)\n",
        "        list_loss_val.append(loss_val.item())\n",
        "        list_loss_train.append(loss_train.item())\n",
        "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
        "        \n",
        "        if epoch % 5 == 0:\n",
        "            print('Epoch: {:03d}'.format(epoch+1),\n",
        "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "                  'time: {} s'.format(int(round(time()) - round(t))),\n",
        "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
        "        if epoch % 20 == 0:\n",
        "            model_path = \"outputs/{}-model-{}epochs-{}.pt\".format(today, epoch, run_number)\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            \n",
        "        early = early_stopping(loss_train.item(), list_loss_train, loss_val.item(), list_loss_val, wait=30)        \n",
        "        if early:\n",
        "            halving_lr += 1\n",
        "            if halving_lr > 5:\n",
        "                break\n",
        "            list_loss_val=[]\n",
        "            list_loss_train=[]\n",
        "            learning_rate = learning_rate/10\n",
        "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "            print('Deviding the learning rate by 10. New learning rate: {:.6f}'.format(learning_rate))\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SD5KLOx86u1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UH0ytHBhGvAa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45910fa6-c693-4f3e-f0fb-1ee632731512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing the optimizer with learning rate: 0.1\n",
            "Preparing the data for training...\n",
            "Start training...\n",
            "Epoch: 001 loss_train: 0.6931 loss_val: 0.6932 acc_train: 0.5109 acc_val: 0.5000 time: 1304 s total_time: 22 min\n",
            "Epoch: 006 loss_train: 0.6931 loss_val: 0.6932 acc_train: 0.5088 acc_val: 0.5000 time: 1302 s total_time: 130 min\n",
            "Epoch: 011 loss_train: 0.6931 loss_val: 0.6932 acc_train: 0.4849 acc_val: 0.5000 time: 1304 s total_time: 239 min\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "vocab_size = embedded_abstracts.shape[1]\n",
        "learning_rate = 0.1\n",
        "features = embedded_abstracts\n",
        "indices = indices\n",
        "epochs = 400\n",
        "batch_size = 2000 #2000 : 10min\n",
        "filter_sizes = [3]\n",
        "embedding_dim=64\n",
        "dropout = 0.3\n",
        "run_number = 1 #an arbitrary number to identify the run number (not really used)\n",
        "\n",
        "model_NLP = CNN(vocab_size=vocab_size, embedding_dim=embedding_dim, filter_sizes=filter_sizes, dropout=dropout).to(device)\n",
        "train_NLP_model(model_NLP, learning_rate=learning_rate, features=embedded_abstracts, indices=indices, y=y, val_indices=val_indices, y_val=y_val, epochs=epochs, batch_size=batch_size, run_number=run_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTTCoAFheRZ2"
      },
      "outputs": [],
      "source": [
        "break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeDK2NXYDWmG"
      },
      "outputs": [],
      "source": [
        "# Generate tf-idf matrix\n",
        "corpus = [\"This is a sample sentence.\", \"Another sample sentence.\"]\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Convert matrix to PyTorch tensor\n",
        "X = torch.tensor(X.toarray(), dtype=torch.float)\n",
        "print(X.shape)\n",
        "\n",
        "# Define Conv1D layer\n",
        "conv = nn.Conv1d(in_channels=X.shape[1], out_channels=32, kernel_size=1)\n",
        "\n",
        "# Apply Conv1D layer to tensor\n",
        "X = X.unsqueeze(2)\n",
        "print(X.shape)\n",
        "\n",
        "X_conv = conv(X)  # add extra dimension to tensor for batch size\n",
        "\n",
        "# Print output shape\n",
        "print(X_conv.shape) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3NB6dirDWmH"
      },
      "outputs": [],
      "source": [
        "#tfidf_reduced[pairs[0][:5]]\n",
        "pairs[:, :5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekTGAyTrneWx"
      },
      "outputs": [],
      "source": [
        "features_np = features_abstracts_wv\n",
        "\n",
        "# Create class labels\n",
        "y = np.zeros(2*indices.shape[1])\n",
        "y[:indices.shape[1]] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
        "\n",
        "# Transforms the numpy matrices/vectors to torch tensors.\n",
        "features = torch.FloatTensor(features_np).to(device)\n",
        "y = torch.LongTensor(y).to(device)\n",
        "if type(adj) != torch.Tensor:\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
        "indices = torch.LongTensor(indices).to(device)\n",
        "val_indices = torch.LongTensor(val_indices).to(device)\n",
        "y_val = torch.LongTensor(y_val).to(device)\n",
        "\n",
        "#del (G, G_train, train_edges, val_edges, nodes, abstracts, embedded_mean_abstracts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dmob3xD4pd5R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfvPqy79pGEW"
      },
      "outputs": [],
      "source": [
        "np.transpose(node_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmFO4qI7_245"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "test_path = 'https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/test.txt'\n",
        "node_pairs = list()\n",
        "f = urlopen(test_path)\n",
        "\n",
        "for line in f:\n",
        "    t = str(line).split(',')\n",
        "    t[0] = int(re.sub(\"[^0-9]\", \"\", t[0]))\n",
        "    t[1] = int(re.sub(\"[^0-9]\", \"\", t[1]))\n",
        "    node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
        "\n",
        "node_pairs = np.transpose(node_pairs)\n",
        "node_pairs = add_authors_to_pairs(node_pairs, authors)\n",
        "node_pairs = torch.LongTensor(node_pairs).to(device)\n",
        "\n",
        "test_output = model(features, adj, node_pairs)\n",
        "y_pred = torch.exp(test_output)\n",
        "y_pred = y_pred.detach().cpu().numpy()\n",
        "\n",
        "y_pred_true = list()\n",
        "for element in y_pred:\n",
        "    y_pred_true.append(element[1])\n",
        "\n",
        "today = datetime.today().strftime('%Y-%m-%d')\n",
        "random_nb = randint(0, 1000)\n",
        "model_nb = 1\n",
        "\n",
        "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
        "\"{}-submission-{}-{}.csv\".format(today, model_nb, random_nb), header=True, index=True, index_label='id'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2UWL4y-K1jd"
      },
      "outputs": [],
      "source": [
        "# Testing\n",
        "model.eval()\n",
        "node_pairs = np.array(np.transpose(node_pairs))\n",
        "node_pairs = torch.LongTensor(node_pairs).to(device)\n",
        "\n",
        "test_output = model(features, adj, node_pairs)\n",
        "y_pred = torch.exp(test_output)\n",
        "y_pred = y_pred.detach().cpu().numpy()\n",
        "\n",
        "y_pred_true = list()\n",
        "for element in y_pred:\n",
        "    y_pred_true.append(element[1])\n",
        "    \n",
        "\n",
        "    \n",
        "today = datetime.today().strftime('%Y-%m-%d')\n",
        "random_nb = randint(0, 1000)\n",
        "\n",
        "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
        "\"{}-submission-{}-{}.csv\".format(today, model_nb, random_nb), header=True, index=True, index_label='id'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwNIJmkGn6-G"
      },
      "outputs": [],
      "source": [
        "epochs = 1000\n",
        "\n",
        "trained_model = train_model(model, 0.01, features, authors, adj, indices, y, torch.tensor(val_indices).to(device), torch.tensor(y_val).to(device), epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OSAu20O0zri"
      },
      "outputs": [],
      "source": [
        "print(type(features), type(adj), type(indices), type(y))\n",
        "print(features.get_device(), adj.get_device(), indices.get_device(), y.get_device())\n",
        "\n",
        "#torch.tensor(np.array(authors)).to(device).get_device()\n",
        "authors\n",
        "print(type(torch.tensor(val_indices)))\n",
        "print(torch.tensor(val_indices).to(device).get_device())\n",
        "\n",
        "type(y_val)\n",
        "\n",
        "####### randindicies to check on which device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_5ih-4EQQqW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print (torch.__version__)\n",
        "!pip install torchvision==0.14.0\n",
        "!pip install torchtext==0.14.0\n",
        "!pip install torchaudio==0.13.0\n",
        "!pip install torch==1.13.0\n",
        "import torch\n",
        "print (torch.__version__)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}