{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dF0MJHXZVRpt"
   },
   "source": [
    "# Packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5GfuP1hlTmg",
    "outputId": "7174b16a-6061-4a5e-ab19-d112e247a2cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\r\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from random import choice\n",
    "from scipy.sparse import identity, diags\n",
    "from unidecode import unidecode\n",
    "from urllib.request import urlopen\n",
    "import gzip\n",
    "import pickle\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "import io\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BJSlSwejljfp"
   },
   "outputs": [],
   "source": [
    "def save_subgraph_in_file(nbr_nodes, source_path='../input_data/edgelist.txt', destination_path='../input_data/small_edgelist.txt'):\n",
    "    G = nx.read_edgelist(source_path, delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "    G = G.subgraph(range(nbr_nodes))\n",
    "    nx.write_edgelist(G, path=destination_path, delimiter=',')\n",
    "    print(G.number_of_nodes(), 'nodes,', G.number_of_edges(), 'edges Graph extracted from', source_path[source_path.rfind('/')+1:])\n",
    "    G = nx.read_edgelist(destination_path, delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "    print(G.number_of_nodes(), 'nodes,', G.number_of_edges(), 'edges Graph saved in', destination_path[destination_path.rfind('/')+1:])\n",
    "    print(max(G.nodes))\n",
    "    return\n",
    "\n",
    "\n",
    "def read_train_val_graph(path='../input_data/edgelist.txt', shuffle=True, val_ratio=0.1):\n",
    "    #gets the data from the file on the distant server\n",
    "    G = nx.read_edgelist(urlopen('https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/edgelist.txt'), delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "    nodes = list(G.nodes())\n",
    "    edges = list(G.edges())\n",
    "    n = G.number_of_nodes()\n",
    "    m = G.number_of_edges()\n",
    "    \n",
    "    permutation = np.array(range(n))\n",
    "    mapping_permutation = dict(zip(range(n), range(n)))\n",
    "    if shuffle:\n",
    "        # shuffle the order of the edges without changing the labels\n",
    "        random.shuffle(edges)\n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(edges)\n",
    "        permutation = np.random.permutation(n)\n",
    "        print(type(permutation))\n",
    "        # create a mapping from old nodes labels to new nodes labels\n",
    "        mapping_permutation = dict(zip(range(n), permutation))\n",
    "\n",
    "        # shuffle G node labels according to the permutation\n",
    "        G = nx.relabel_nodes(G, mapping_permutation)    \n",
    "        \n",
    "        edges = list(G.edges())\n",
    "        nodes = list(G.nodes())\n",
    "\n",
    "\n",
    "    print('Number of nodes:', n, 'number of edges:', m,'in the Complete set')\n",
    "\n",
    "    node_to_idx = dict()\n",
    "    for i, node in enumerate(nodes):\n",
    "        node_to_idx[node] = i\n",
    "\n",
    "    val_edges = list()\n",
    "    G_train = G.copy()\n",
    "\n",
    "    for edge in edges:\n",
    "        if random.random() < val_ratio and edge[0] < n and edge[1] < n:\n",
    "            val_edges.append(edge)\n",
    "            G_train.remove_edge(edge[0], edge[1]) # We remove the val edges from the graph G\n",
    "\n",
    "   \n",
    "    #for edge in val_edges:\n",
    "        \n",
    "\n",
    "    n = G_train.number_of_nodes()\n",
    "    m = G_train.number_of_edges()\n",
    "    train_edges = list(G_train.edges())\n",
    "\n",
    "    print('Number of nodes:', n, 'number of edges:', m, 'in the Training set')\n",
    "    print('len(nodes)', len(nodes))\n",
    "\n",
    "    y_val = [1]*len(val_edges)\n",
    "\n",
    "    n_val_edges = len(val_edges)\n",
    "    \n",
    "    print('Creating random val_edges...')\n",
    "    for i in range(n_val_edges):\n",
    "        n1 = nodes[randint(0, n-1)]\n",
    "        n2 = nodes[randint(0, n-1)]\n",
    "        (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "        while n2 >= n: #or (n1, n2) in train_edges:\n",
    "            if (n1, n2) in train_edges:\n",
    "                print((n1, n2), 'in train_edges:')\n",
    "            n1 = nodes[randint(0, n-1)]\n",
    "            n2 = nodes[randint(0, n-1)]\n",
    "            (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "        val_edges.append((n1, n2))\n",
    "\n",
    "    y_val.extend([0]*(n_val_edges))\n",
    "    \n",
    "    ### From Giannis /!\\\n",
    "    val_indices = np.zeros((2,len(val_edges)))\n",
    "    for i,edge in enumerate(val_edges):\n",
    "        val_indices[0,i] = node_to_idx[edge[0]]\n",
    "        val_indices[1,i] = node_to_idx[edge[1]]\n",
    "    \n",
    "    print('Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects')\n",
    "    print('Loaded from', path[path.rfind('/')+1:], 'and with a training validation split ratio =', val_ratio)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx, permutation, mapping_permutation\n",
    "\n",
    "def random_walk(G, node, walk_length):\n",
    "    walk = [node]\n",
    "  \n",
    "    for i in range(walk_length-1):\n",
    "        neibor_nodes = list(G.neighbors(walk[-1]))\n",
    "        if len(neibor_nodes) > 0:\n",
    "            next_node = choice(neibor_nodes)\n",
    "            walk.append(next_node)\n",
    "    walk = [node for node in walk] # in case the nodes are in string format, we don't need to cast into string, but if the nodes are in numeric or integer, we need this line to cast into string\n",
    "    return walk\n",
    "\n",
    "\n",
    "def generate_walks(G, num_walks, walk_length):\n",
    "  # Runs \"num_walks\" random walks from each node, and returns a list of all random walk\n",
    "    t = time()\n",
    "    print('Start generating walks....')\n",
    "    walks = list()  \n",
    "    for i in range(num_walks):\n",
    "        for node in G.nodes():\n",
    "            walk = random_walk(G, node, walk_length)\n",
    "            walks.append(walk)\n",
    "        #print('walks : ', walks)\n",
    "    print('Random walks generated in in {}s!'.format(round(time()-t)))\n",
    "    return walks\n",
    "\n",
    "def apply_word2vec_on_features(features, nodes, vector_size=128, window=5, min_count=0, sg=1, workers=8):\n",
    "    t = time()\n",
    "    print('Start applying Word2Vec...')\n",
    "    wv_model = Word2Vec(vector_size=vector_size, window=window, min_count=min_count, sg=sg, workers=workers)\n",
    "    wv_model.build_vocab(features)\n",
    "    wv_model.train(features, total_examples=wv_model.corpus_count, epochs=5) \n",
    "    print('Word2vec model trained on features in {} min!'.format(round((time()-t)/60)))\n",
    "    features_np = []\n",
    "    for node in nodes:\n",
    "        features_np.append(wv_model.wv[node])\n",
    "\n",
    "    features_np = np.array(features_np)\n",
    "    print(features_np.shape, 'features numpy array created in {} min!'.format(round((time()-t)/60)))\n",
    "    return features_np\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def normalize_adjacency(A):\n",
    "    n = A.shape[0]\n",
    "    A = A + identity(n)\n",
    "    degs = A.dot(np.ones(n))\n",
    "    inv_degs = np.power(degs, -1)\n",
    "    D_inv = diags(inv_degs)\n",
    "    A_hat = D_inv.dot(A)\n",
    "    return A_hat\n",
    "\n",
    "\n",
    "# a proposed adj normalization, but we will keep the original one in the function after\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "\n",
    "\n",
    "def create_and_normalize_adjacency(G):\n",
    "    adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix of the training graph\n",
    "    #adj = normalize_adjacency(adj)\n",
    "    adj = normalize_adj(adj)\n",
    "    print('Created a normalized adjancency matrix of shape', adj.shape)\n",
    "    indices = np.array(adj.nonzero()) # Gets the positions of non zeros of adj into indices\n",
    "    print('Created indices', indices.shape, 'with the positions of non zeros in adj matrix')\n",
    "    return adj, indices\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "def text_to_list(text):\n",
    "    return unidecode(text).split(',')\n",
    "\n",
    "def intersection(lst1, lst2): # a function that returns the number of common items of two lists and 1 or 0 if there are common. This function will be used in add_authors_to_pairs to add this features to the pairs.\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    is_common = 1 if len(lst3)>0 else 0\n",
    "    return len(lst3)+1, is_common+1\n",
    "\n",
    "\n",
    "def add_authors_to_pairs (pairs, authors):\n",
    "    authors = pd.DataFrame(authors)\n",
    "    try: \n",
    "        pairs = pairs.detach().cpu().numpy()\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "\n",
    "    pairs_df = pd.DataFrame(np.transpose(pairs)).rename(columns={0: \"paper_1\", 1: \"paper_2\"})\n",
    "    pairs_df = pairs_df.merge(authors, left_on='paper_1', right_on='paper_permut', how='left').rename(columns={'authors': \"authors_1\"})\n",
    "    pairs_df = pairs_df.merge(authors, left_on='paper_2', right_on='paper_permut', how='left').rename(columns={'authors': \"authors_2\"})\n",
    "    pairs_df.drop(['paper_id_x', 'paper_id_y'], axis=1, inplace=True)\n",
    "\n",
    "    pairs_df['nb_common_author'] = pairs_df.apply(lambda row: intersection(row['authors_1'], row['authors_2'])[0], axis=1)\n",
    "    pairs_df['is_common_author'] = pairs_df.apply(lambda row: intersection(row['authors_1'], row['authors_2'])[1], axis=1)\n",
    "\n",
    "    pairs_tensor = torch.LongTensor(np.transpose(pairs_df[[\"paper_1\", \"paper_2\", 'is_common_author', 'nb_common_author']].values.tolist())).to(device)\n",
    "    \n",
    "    return pairs_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HLahMzTSg4Xz"
   },
   "outputs": [],
   "source": [
    "def read_and_clean_abstracts (nodes, sample_length=-1, abstracts_path = 'https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/abstracts.txt'):\n",
    "    t = time()\n",
    "    abstracts = dict()\n",
    "    abstracts_list = list()\n",
    "    f = urlopen(abstracts_path)\n",
    "    \n",
    "    for i, line in tqdm(enumerate(f)):\n",
    "        if i == sample_length:\n",
    "            break\n",
    "        if i in nodes:\n",
    "            node, abstract = str(line).lower().split('|--|')\n",
    "            abstract = remove_stopwords(abstract)\n",
    "            #abstract = re.sub(r\"[,.;@#?!&$()-]\", \" \", abstract)\n",
    "            abstract = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", abstract)\n",
    "            #abstract = re.sub(r\"\\\\\", \" \", abstract)\n",
    "            abstract = remove_stopwords(abstract)\n",
    "\n",
    "            for word in abstract.split()[:-1]:\n",
    "                #abstract = abstract.replace(word, stemmer.stem(word))\n",
    "                abstract = abstract.replace(word, lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word), pos='s'), pos='a'), pos='n'), pos='v'), pos='r'))\n",
    "            \n",
    "            node = re.sub(\"[^0-9]\", \"\", node)\n",
    "            if i != int(node):\n",
    "                print('i and node not the same', i, node)\n",
    "            abstracts[int(node)] = abstract\n",
    "            abstracts_list.append(abstract)\n",
    "        \n",
    "    print('Text loaded and cleaned in {:.0f} min'.format((time()-t)/60))\n",
    "    return abstracts\n",
    "\n",
    "def doc_counter (documents, word): #a function that return the number of documents containing a word\n",
    "    counter = 0\n",
    "    for i in documents:\n",
    "        if word in documents[i]:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8HP9n7D5g4Xz"
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        self.word_occurrence = {}\n",
    "        self.word2node = {}\n",
    "        self.words_list = []\n",
    "        self.sentences_list = []\n",
    "        self.sentences_list_words = []\n",
    "        self.num_words = 0\n",
    "        self.num_sentences = 0\n",
    "        self.longest_sentence = 0\n",
    "\n",
    "    def add_word(self, word, node):\n",
    "        if word not in self.word2index:\n",
    "            # First entry of word into vocabulary\n",
    "            self.words_list.append(word)\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "            self.word_occurrence[word] = 1\n",
    "            self.word2node[word] = [node]\n",
    "        else:\n",
    "            # Word exists; increase word count\n",
    "            self.word2count[word] += 1\n",
    "            self.word_occurrence[word] += 1\n",
    "            if node not in self.word2node[word]:\n",
    "                self.word2node[word].append(node)\n",
    "            # self.num_words += 1\n",
    "            \n",
    "    def add_sentence(self, sentence, node):\n",
    "        sentence_len = 0\n",
    "        for word in sentence.split()[:-1]:\n",
    "            sentence_len += 1\n",
    "            self.add_word(word, node)\n",
    "        if sentence_len > self.longest_sentence:\n",
    "            # This is the longest sentence\n",
    "            self.longest_sentence = sentence_len\n",
    "        # Count the number of sentences\n",
    "        self.num_sentences += 1\n",
    "        self.sentences_list.append(sentence)\n",
    "        self.sentences_list_words.append(sentence.split()[:-1])\n",
    "\n",
    "    def to_word(self, index):\n",
    "        return self.index2word[index]\n",
    "\n",
    "    def to_index(self, word):\n",
    "        return self.word2index[word]\n",
    "\n",
    "    def words(self):\n",
    "        return self.words_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "frfxHnMT0zNn"
   },
   "outputs": [],
   "source": [
    "def prepare_data_to_train (features, authors, adj, auth_matrix, indices, val_indices, y_val):\n",
    "    \n",
    "    print('Preparing the data for training...')\n",
    "    \n",
    "    t = time()\n",
    "    \n",
    "    y_val = torch.LongTensor(y_val).to(device)\n",
    "\n",
    "    # Create class labels\n",
    "    y = np.zeros(2*indices.shape[1])\n",
    "    y[:indices.shape[1]] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
    "    y = torch.LongTensor(y).to(device)\n",
    "    \n",
    "    features = torch.FloatTensor(features).to(device)\n",
    "    \n",
    "    indices = torch.LongTensor(indices).to(device)\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "    auth_matrix = sparse_mx_to_torch_sparse_tensor(auth_matrix).to(device)\n",
    "    #tfidf_matrix = sparse_mx_to_torch_sparse_tensor(tfidf_matrix).to(device)\n",
    "    \n",
    "    # the function add_authors_to_pairs converts into torch tensors and sends to Device    \n",
    "    val_indices = add_authors_to_pairs(val_indices, authors) #we add the authors to val_pairs\n",
    "    indices = add_authors_to_pairs(indices, authors) #we add the authors to indices    \n",
    "    rand_indices = np.random.randint(0, features.shape[0], (indices.shape[0],indices.shape[1]))# We take random indices each time we run an epoch\n",
    "    rand_indices = add_authors_to_pairs(rand_indices, authors)\n",
    "\n",
    "    pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices. \n",
    "    del(authors, indices, rand_indices)\n",
    "    \n",
    "    print('Data converted into torch tensors and authors added to indices in {:.0f} min'.format((time()-t)/60))\n",
    "\n",
    "    return features, adj, auth_matrix, pairs, y, val_indices, y_val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VBxraTeM0zNn"
   },
   "outputs": [],
   "source": [
    "def map_features_with_permutation(features, permutation):\n",
    "    new_features = np.zeros((len(features), len(features[0])))\n",
    "    for i in range(len(features)):\n",
    "        new_features[i] = features[permutation[i]]\n",
    "    return new_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ubeigqung4X0"
   },
   "source": [
    "# Load graph and authors data from sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oe6DhotaSuqg",
    "outputId": "582fb456-e255-4e89-d4f2-f235fd1d155d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499 number of edges: 1091955 in the Complete set\n",
      "Number of nodes: 138499 number of edges: 982375 in the Training set\n",
      "len(nodes) 138499\n",
      "Creating random val_edges...\n",
      "Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects\n",
      "Loaded from edgelist.txt and with a training validation split ratio = 0.1\n",
      "graph loaded and seperated, val indices generated and node to index mapping returned in 10 s\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "shuffle = False\n",
    "G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx, permutation, mapping_permutation = read_train_val_graph(val_ratio=0.1, shuffle=shuffle)\n",
    "\n",
    "print('graph loaded and seperated, val indices generated and node to index mapping returned in {:.0f} s'.format(time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EyBibxd6zQhV",
    "outputId": "a29b8f4f-b682-4507-c178-d118b746a298"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pd/glcnl2497w5b6xn3p94tnwlr0000gn/T/ipykernel_51428/1170815680.py:163: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix of the training graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a normalized adjancency matrix of shape (138499, 138499)\n",
      "Created indices (2, 1964750) with the positions of non zeros in adj matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pd/glcnl2497w5b6xn3p94tnwlr0000gn/T/ipykernel_51428/1170815680.py:155: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n"
     ]
    }
   ],
   "source": [
    "adj, indices = create_and_normalize_adjacency(G_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "dWAzHqmxmYff",
    "outputId": "e120169b-23f8-4858-ad05-9f271de065d6",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>authors</th>\n",
       "      <th>paper_permut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[James H. Niblock, Jian-Xun Peng, Karen R. McM...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Jian-Xun Peng, Kang Li, De-Shuang Huang]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[J. Heikkila]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[Long Zhang, Kang Li, Er-Wei Bai, George W. Ir...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id                                            authors  paper_permut\n",
       "0         0  [James H. Niblock, Jian-Xun Peng, Karen R. McM...             0\n",
       "1         1          [Jian-Xun Peng, Kang Li, De-Shuang Huang]             1\n",
       "2         2                                      [J. Heikkila]             2\n",
       "3         3    [L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]             3\n",
       "4         4  [Long Zhang, Kang Li, Er-Wei Bai, George W. Ir...             4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors = pd.read_csv(urlopen('https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/authors.txt'), sep = '|', header=None)\n",
    "authors = authors.rename(columns={0: \"paper_id\", 2: \"authors\"})\n",
    "authors['authors'] = authors['authors'].apply(text_to_list)\n",
    "authors = authors[[\"paper_id\", \"authors\"]]\n",
    "authors = authors[authors['paper_id'] <= max(G.nodes())]\n",
    "authors['paper_permut'] = permutation[authors['paper_id']]\n",
    "authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130,
     "referenced_widgets": [
      "548d4579dc26489da531997cf6862544",
      "d96a4dcb1da743bcb4685c2f3bd8f0f8",
      "9872e75a68d04f45b54815e1bbd8f52e",
      "d7e97039408748c2a49de39396931a7e",
      "60292eade64242fdb1e7de2040c1764e",
      "924c1ba5f01f4809a044a28b72917470",
      "8e4a67e76ce34aabbe6b60feb0859d84",
      "11902059cd7148fe978ca28cab88b10d",
      "048074f63c75476d99a9dec95f42c8d4",
      "c4c63bc81390469b9c305a43f7e4980b",
      "86337e063b1e4d199c0f493ed1f887d1",
      "846969cb1c9c430fb96f488eb0ad727c",
      "b9fa1be8ce0741f595f27d20f1550cc8",
      "9e61bb4f3d054d6c9f3d807203a984ec",
      "47599c391f7f4b85998826354d2afb0f",
      "07765899fda144cca0dc7775800f3215",
      "37d5ab03a8bf48278e439809364d144b",
      "93b4d097061147a185c7f05a80ac9a22",
      "485660547adb473694453175605ffde7",
      "093c3327f5984498ba15badb8db23a17",
      "25f0e533952c4ad8b81fee35ec6025f8",
      "932491a1b72342d8b063fcab699f5728",
      "c99dba6a96804c829a931ac074afff0b",
      "e68689640a564e9fa353886d5f4a45f4",
      "b12d9410df3e4f35b1ed6567a5454e51",
      "aafe73f8ce26421d9704dab62ee2ff88",
      "371aa51a013c4bb4bcad92c54ab1c79e",
      "e63b115ae35e417c997003995eab8e64",
      "150e5c5090414276b13b373710ccf198",
      "96e54fe99c4542c1a97fff3fcce2e4de",
      "77b6df7549534afbbf896fc1d90ecf3c",
      "bd733b795c274acfa068038c066a02b8",
      "532eb4c4869f42a59024ff65ec3d8e2d"
     ]
    },
    "id": "YQw6HQUT9c_t",
    "outputId": "af382e10-afe6-4abc-e2a3-bbf1bcfe0332"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799ae712c05649468cc21848d7070e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0c58be47cc4d59a5e54917d1e2d75c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84bc40f4af2949199f18c0c1a72b0c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138499, 147950)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "# get the unique list of authors\n",
    "authors_lst_ppr = list(set([a for authors_list in tqdm(authors['authors']) for a in authors_list]))\n",
    "\n",
    "# create a mapping of author to index\n",
    "author_to_index = {author: i for i, author in tqdm(enumerate(authors_lst_ppr))}\n",
    "\n",
    "# create an empty sparse matrix\n",
    "nrows = len(authors)\n",
    "ncols = len(authors_lst_ppr)\n",
    "data = np.ones(nrows)\n",
    "row_ind = np.arange(nrows)\n",
    "col_ind = np.zeros(nrows)\n",
    "\n",
    "# fill in the sparse matrix with 1s where authors appear\n",
    "for i, authors_list in tqdm(enumerate(authors['authors'])):\n",
    "    for author in authors_list:\n",
    "        col_ind[i] = author_to_index[author]\n",
    "        row_ind[i] = i\n",
    "        data[i] = 1\n",
    "auth_matrix = csr_matrix((data, (row_ind, col_ind)), shape=(nrows, ncols))\n",
    "\n",
    "# print the resulting sparse matrix\n",
    "print(auth_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ahaTr3Q0C5Vb",
    "outputId": "12626ec2-65fb-4f17-91c9-c8ac2cf91f7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 21089]\n"
     ]
    }
   ],
   "source": [
    "# get the indices of non-zero elements\n",
    "row_idx, col_idx = auth_matrix.nonzero()\n",
    "\n",
    "# display the first non-zero element\n",
    "print([row_idx[0], col_idx[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o5mW4OXyLIGQ",
    "outputId": "723dc16b-39d8-4102-a100-43e78feced94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138499, 147950)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auth_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2czqslmiVfcQ"
   },
   "source": [
    "# Read processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf-_fYkF5xiO",
    "outputId": "a033cf59-6d14-43b7-eb86-2827d89ddaea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wv walks loaded from GCP in 0 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(138499, 64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time()\n",
    "from io import BytesIO\n",
    "\n",
    "walks_url = 'https://storage.googleapis.com/link_prediction_processed_data/walks_wv.npy'\n",
    "with urlopen(walks_url) as url:\n",
    "    data = url.read()\n",
    "\n",
    "# Create a seekable file-like object from the data\n",
    "fileobj = BytesIO(data)\n",
    "\n",
    "# Load the data from the file object\n",
    "walks_wv = np.load(fileobj)\n",
    "print('wv walks loaded from GCP in {:.0f} sec'.format(time()-t))\n",
    "walks_wv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "IrvRUVduhoAI"
   },
   "outputs": [],
   "source": [
    "# Load the BART embedding torch tensor\n",
    "\n",
    "# url = \"https://storage.googleapis.com/link_prediction_processed_data/bart_embeddings.pt\"\n",
    "# response = requests.get(url)\n",
    "\n",
    "# with open(\"bart_embeddings.pt\", \"wb\") as f:\n",
    "#     f.write(response.content)\n",
    "\n",
    "# abstracts_bart_embeddings = torch.load('bart_embeddings.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "LlQCihvfmBQP"
   },
   "outputs": [],
   "source": [
    "# # Load the wv300 mean embedding\n",
    "# url = 'https://storage.googleapis.com/link_prediction_processed_data/embedded_abstracts_local_wv300.npy'\n",
    "\n",
    "# response = requests.get(url)\n",
    "\n",
    "# with open('embedded_abstracts_local_wv300.npy', 'wb') as f:\n",
    "#     f.write(response.content)\n",
    "\n",
    "# # Load the numpy array from the saved file\n",
    "# local_wv300_abstracts = np.load('embedded_abstracts_local_wv300.npy')\n",
    "\n",
    "# local_wv300_abstracts.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "F4jEFSE6in_w"
   },
   "outputs": [],
   "source": [
    "# # Load the goog300 mean embedding\n",
    "# url = 'https://storage.googleapis.com/link_prediction_processed_data/embedded_abstracts_goog300.npy'\n",
    "\n",
    "# response = requests.get(url)\n",
    "\n",
    "# with open('embedded_abstracts_goog300.npy', 'wb') as f:\n",
    "#     f.write(response.content)\n",
    "\n",
    "# # Load the numpy array from the saved file\n",
    "# goog300_abstracts = np.load('embedded_abstracts_goog300.npy')\n",
    "\n",
    "# goog300_abstracts.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3_xIGkD_wuUn"
   },
   "outputs": [],
   "source": [
    "# # Load TF-IDF matrix\n",
    "\n",
    "# url = \"https://storage.googleapis.com/link_prediction_processed_data/tfidf_matrix.npz\"\n",
    "\n",
    "# response = requests.get(url)\n",
    "\n",
    "# with open(\"tfidf_matrix.npz\", \"wb\") as f:\n",
    "#     f.write(response.content)\n",
    "\n",
    "# tfidf_matrix = sparse.load_npz(\"tfidf_matrix.npz\")\n",
    "\n",
    "# tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "mJjyDTiVPblx"
   },
   "outputs": [],
   "source": [
    "# # Load words 192 embedding\n",
    "# import gzip\n",
    "# import pickle\n",
    "\n",
    "# with gzip.open('embedded_abstracts_dict_192array.pkl.gz', 'rb') as f:\n",
    "#     words_embedding_192 = pickle.load(f)\n",
    "\n",
    "# len(words_embedding_192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "RuZSYpPv0zNr"
   },
   "outputs": [],
   "source": [
    "# len(words_embedding_192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "20sWKHiSsgKV"
   },
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "# import numpy as np\n",
    "\n",
    "# words_embedding_192_trunc128 = dict ()\n",
    "# for i in tqdm(range(len(words_embedding_192))):\n",
    "#     if len(words_embedding_192[i])>0:\n",
    "#         arr = np.zeros((128, 192))\n",
    "#         vec = words_embedding_192[i][:128, :]\n",
    "#         arr[:vec.shape[0],:] = vec\n",
    "#         words_embedding_192_trunc128[i] = torch.tensor(arr).to(device)\n",
    "#     else:\n",
    "#         words_embedding_192_trunc128[i] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "fkzNXQF1taxU"
   },
   "outputs": [],
   "source": [
    "# tensor_list = []\n",
    "# i = 0\n",
    "# for i in tqdm(range(len(words_embedding_192_trunc128))):\n",
    "#     if len(words_embedding_192_trunc128[i])>0:\n",
    "#         tensor_list.append(words_embedding_192_trunc128[i])\n",
    "#     else:\n",
    "#         tensor_list.append(torch.zeros((128, 192)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "pM27YApj0zNs"
   },
   "outputs": [],
   "source": [
    "# del(words_embedding_192_trunc128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "tAo6RBG-xy47"
   },
   "outputs": [],
   "source": [
    "# import gzip\n",
    "\n",
    "# filename = 'words_embedding_192_trunc128.pkl.gz'\n",
    "\n",
    "# # open the file in binary mode and write the dictionary to it, compressing the data with gzip\n",
    "# with gzip.open(filename, 'wb') as f:\n",
    "#     pickle.dump(words_embedding_192_trunc128, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "BNgqJRoY0zNt"
   },
   "outputs": [],
   "source": [
    "# tensor_list_float = []\n",
    "# for i, tensor in tqdm(enumerate(tensor_list)):\n",
    "#     tensor_list_float.append(tensor_list[i].float())\n",
    "# del(tensor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ebgY7dPF0zNt"
   },
   "outputs": [],
   "source": [
    "# # Load truncated padded 128 words embeddings from GCP\n",
    "# url = 'https://storage.googleapis.com/link_prediction_processed_data/words_embedding_192_trunc128.pkl.gz'\n",
    "\n",
    "\n",
    "# # download the file from the URL\n",
    "# response = requests.get(url)\n",
    "\n",
    "# # save the file to disk\n",
    "# with open(filename, 'wb') as f:\n",
    "#     f.write(response.content)\n",
    "\n",
    "# # load the data from the file\n",
    "# with gzip.open(filename, 'rb') as f:\n",
    "#     words_embedding_192_trunc128 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load truncated 128 words embeddings from URL\n",
    "\n",
    "url = 'https://storage.googleapis.com/link_prediction_processed_data/words_embedding_192_trunc128.pkl.gz'\n",
    "\n",
    "\n",
    "print('Loading file from URL...')\n",
    "# open the file from URL in binary mode and read the dictionary from it, decompressing the data with gzip\n",
    "with urllib.request.urlopen(url) as f:\n",
    "    with gzip.open(f, 'rb') as f2:\n",
    "        data = pickle.load(f2)\n",
    "\n",
    "print('File loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "nNfoJp-U0zNt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading padded truncated embeddings...\n",
      "padded truncated embeddings loaded in 4 min\n"
     ]
    }
   ],
   "source": [
    "# Load truncated padded 128 words embeddings from file\n",
    "\n",
    "t = time()\n",
    "\n",
    "filename = 'words_embedding_192_trunc128.pkl.gz'\n",
    "\n",
    "print('Loading padded truncated embeddings...')\n",
    "# open the file in binary mode and read the dictionary from it, decompressing the data with gzip\n",
    "with gzip.open(filename, 'rb') as f:\n",
    "    words_embedding_192_trunc128 = pickle.load(f)\n",
    "\n",
    "print('padded truncated embeddings loaded in {:.0f} min'.format((time()-t)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 192])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_embedding_192_trunc128[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abfbc9de9862496180fac42395281ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_tensors = []\n",
    "i = 0\n",
    "for i in tqdm(range(len(words_embedding_192_trunc128))):\n",
    "    if len(words_embedding_192_trunc128[i])>0:\n",
    "        list_tensors.append(words_embedding_192_trunc128[i])\n",
    "    else:\n",
    "        list_tensors.append(torch.zeros((128, 192)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(words_embedding_192_trunc128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1693, -0.5117, -0.1612,  ...,  0.3282,  0.2673, -0.2871],\n",
       "        [ 0.3021, -0.0629,  0.1254,  ...,  0.3228,  0.4747, -0.2569],\n",
       "        [ 0.1363, -0.1517, -0.0926,  ...,  0.1736,  0.0540,  0.4056],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxvIuCP_bFrN"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ROBXxQaZnfQ3"
   },
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_auth, n_feat, n_hidden, n_class, sub_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        self.auth_emb = nn.Linear(n_auth, n_hidden)\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden)        \n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(4*n_hidden, n_hidden)\n",
    "        self.fc4 = nn.Linear(n_hidden, sub_class)\n",
    "        self.fc5 = nn.Linear(sub_class, n_class)        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "       \n",
    "        \n",
    "\n",
    "    def forward(self, x_in, auth, adj, pairs):\n",
    "\n",
    "        y = self.auth_emb(auth)\n",
    "        y = self.relu(y)\n",
    "        y = self.dropout(y)\n",
    "        del(auth)\n",
    "\n",
    "        #x_in = torch.cat((x_in, y), dim=1)\n",
    "        \n",
    "        h1 = self.fc1(x_in)\n",
    "        z1 = self.relu(torch.spmm(adj, h1))\n",
    "        z1 = self.dropout(z1)\n",
    "        del(x_in, h1)\n",
    "\n",
    "        h2 = self.fc2(z1)\n",
    "        z2 = self.relu(torch.spmm(adj, h2))\n",
    "        z2 = self.dropout(z2)\n",
    "        del(h2, z1, adj)\n",
    "        \n",
    "        x = torch.cat((z2[pairs[0]] , y[pairs[0]], z2[pairs[1]] , y[pairs[1]]), dim=1)\n",
    "        del(z2)\n",
    "        # multiply by the authors flag\n",
    "        # x = (pairs[2][:, None])*x       \n",
    "        \n",
    "        #x = torch.cat((x, y), dim=1)        \n",
    "        del(pairs)\n",
    "\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_filters, filter_sizes, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1,\n",
    "                      out_channels=num_filters,\n",
    "                      kernel_size=(fs, embedding_dim))\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, output_dim)\n",
    "\n",
    "        # Sigmoid activation function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text shape: [batch_size, max_sent_len]\n",
    "\n",
    "        # Embedding layer\n",
    "        embedded = self.embedding(text) # shape: [batch_size, max_sent_len, embedding_dim]\n",
    "\n",
    "        # Add a channel dimension for convolutional layers\n",
    "        embedded = embedded.unsqueeze(1) # shape: [batch_size, 1, max_sent_len, embedding_dim]\n",
    "\n",
    "        # Convolutional layers\n",
    "        conved = [nn.functional.relu(conv(embedded)).squeeze(3) for conv in self.convs] # shape: [batch_size, num_filters, max_sent_len - filter_size + 1]\n",
    "\n",
    "        # Max pooling\n",
    "        pooled = [nn.functional.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved] # shape: [batch_size, num_filters]\n",
    "\n",
    "        # Concatenate pooled outputs from different filter sizes\n",
    "        cat = torch.cat(pooled, dim=1) # shape: [batch_size, num_filters * len(filter_sizes)]\n",
    "\n",
    "        # Fully connected layer\n",
    "        output = self.fc(cat) # shape: [batch_size, output_dim]\n",
    "\n",
    "        # Sigmoid activation\n",
    "        output = self.sigmoid(output) # shape: [batch_size, output_dim]\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the optimizer with learning rate: 0.01\n",
      "Start training...\n",
      "Epoch: 001 loss_train: 3.1571 loss_val: 0.6925 acc_train: 0.5009 acc_val: 0.5177 time: 10 s total_time: 0 min\n",
      "Epoch: 006 loss_train: 6.6591 loss_val: 4.1714 acc_train: 2.9600 acc_val: 3.0355 time: 11 s total_time: 1 min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m model_CNN \u001b[38;5;241m=\u001b[39m CNN()\n\u001b[1;32m      3\u001b[0m sampling_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain_CNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_CNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffled_list_tensors\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffled_pairs_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffled_y_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffled_val_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffled_y_val_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[94], line 47\u001b[0m, in \u001b[0;36mtrain_CNN\u001b[0;34m(model, learning_rate, x, pairs, y, val_indices, y_val, epochs, batch_size, run_number, window)\u001b[0m\n\u001b[1;32m     44\u001b[0m pairs2 \u001b[38;5;241m=\u001b[39m pairs[\u001b[38;5;241m1\u001b[39m][mask1 \u001b[38;5;241m&\u001b[39m mask2]\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m-\u001b[39m i \u001b[38;5;66;03m# because x_batch contains elements from 0 to batch_size\u001b[39;00m\n\u001b[1;32m     46\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y[mask1 \u001b[38;5;241m&\u001b[39m mask2]\n\u001b[0;32m---> 47\u001b[0m x1_batch \u001b[38;5;241m=\u001b[39m \u001b[43mx_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpairs1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mint\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     48\u001b[0m x2_batch \u001b[38;5;241m=\u001b[39m x_batch[pairs2\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     50\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m model_CNN(x1_batch, x2_batch) \u001b[38;5;66;03m# we run the model that gives the output.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_CNN = CNN()\n",
    "\n",
    "sampling_size = 20000\n",
    "\n",
    "train_CNN(model_CNN, 0.01, shuffled_list_tensors[: sampling_size], shuffled_pairs_torch, shuffled_y_torch, shuffled_val_indices, shuffled_y_val_torch, 50, 2000, run_number=4, window = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "k41qUGs60zNt"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, dtype=torch.float32):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=192, out_channels=64, kernel_size=8, stride=4, padding=3)\n",
    "        #self.conv2 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=8, stride=4, padding=3)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc1 = nn.Linear(2112, 128)\n",
    "        self.fc2 = nn.Linear(128*2, 64)\n",
    "        self.fc5 = nn.Linear(64, 2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.dtype = dtype\n",
    "    \n",
    "\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = x1.permute(0, 2, 1)\n",
    "        x2 = x2.permute(0, 2, 1)\n",
    "        \n",
    "        x1 = self.conv1(x1.to(self.dtype))\n",
    "        x2 = self.conv1(x2.to(self.dtype))\n",
    "\n",
    "        x1 = self.sigmoid(x1)\n",
    "        x2 = self.sigmoid(x2)\n",
    "\n",
    "        x1 = self.pool(x1)\n",
    "        x2 = self.pool(x2)\n",
    "#         print(x1.shape)\n",
    "\n",
    "#         x1 = self.conv2(x1)\n",
    "#         x2 = self.conv2(x2)\n",
    "#         print(x1.shape)\n",
    "\n",
    "#         x1 = self.relu(x1)\n",
    "#         x2 = self.relu(x2)\n",
    "\n",
    "#         x1 = self.pool(x1)\n",
    "#         x2 = self.pool(x2)\n",
    "#         print(x1.shape)\n",
    "\n",
    "        x1 = x1.view(x1.shape[0], -1)\n",
    "        x2 = x2.view(x2.shape[0], -1)\n",
    "\n",
    "        x1 = self.fc1(x1)\n",
    "        x2 = self.fc1(x2)\n",
    "\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        del(x2)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1), x1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ognzQbsFDb13"
   },
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "0azz_rVlDyn9"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, model, patience, delta, path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.model = model\n",
    "        self.val_loss_min = np.Inf\n",
    "        \n",
    "    def __call__(self, val_loss, path='checkpoint.pt'):\n",
    "        score = val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(path)\n",
    "        elif score > self.best_score + 0:\n",
    "            self.counter += 1\n",
    "            #print(self.counter)\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def load_checkpoint(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "p-4Vl4Nu0zNu"
   },
   "outputs": [],
   "source": [
    "def train_CNN(model, learning_rate, x, pairs, y, val_indices, y_val, epochs, batch_size, run_number, window = 10):\n",
    "    # Train model\n",
    "    start_time = time()\n",
    "    \n",
    "    print('Initializing the optimizer with learning rate:', learning_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) #optimizer with halving learning rate in training\n",
    "\n",
    "    today = datetime.today().strftime('%Y-%m-%d-%H:%M')\n",
    "    list_loss_val = []\n",
    "    list_loss_train = []\n",
    "    list\n",
    "    num_batches = len(x) // batch_size\n",
    "    if len(x) % batch_size != 0:\n",
    "        num_batches += 1\n",
    "    \n",
    "    \n",
    "    halving_lr = 0 # counter of the number of halving lr\n",
    "    patience = 16\n",
    "    early_stopping = EarlyStopping(model, patience=patience, delta=0.1, path='checkpoint.pt')\n",
    "    print('Start training...')\n",
    "\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total_loss_val = 0\n",
    "    total_acc_val = 0\n",
    "    num_pairs = pairs.shape[1]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # print(f'epoch {epoch}')\n",
    "        t = time()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model.train()\n",
    "        for i in (range(0, len(x), batch_size)):\n",
    "            x_batch = torch.stack(x[i : i+batch_size])\n",
    "            lst = range(i , i+batch_size)\n",
    "            \n",
    "            mask1 = np.isin(pairs[0], lst)\n",
    "            mask2 = np.isin(pairs[1], lst)\n",
    "            \n",
    "\n",
    "            \n",
    "            pairs1 = pairs[0][mask1 & mask2].numpy() - i # we soustract the number of batch * batch_size\n",
    "            pairs2 = pairs[1][mask1 & mask2].numpy() - i # because x_batch contains elements from 0 to batch_size\n",
    "            \n",
    "            y_batch = y[mask1 & mask2]\n",
    "            x1_batch = x_batch[pairs1.astype('int')]\n",
    "            x2_batch = x_batch[pairs2.astype('int')]\n",
    "            \n",
    "            output, _ = model_CNN(x1_batch, x2_batch) # we run the model that gives the output.\n",
    "            output.to(device)\n",
    "            loss_train = F.nll_loss(output, y_batch) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "            acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_batch.cpu().numpy())# just to show it in the out put message of the training\n",
    "            loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "            optimizer.step() # Performs a single optimization step (parameter update).\n",
    "            total_loss += loss_train.item()\n",
    "            total_acc += acc_train\n",
    "            \n",
    "            \n",
    "        model.eval()    \n",
    "        for i in (range(0, len(x), batch_size)):\n",
    "            x_batch = torch.stack(x[i : i+batch_size])\n",
    "            lst = range(i , i+batch_size)\n",
    "            \n",
    "            mask_val1 = np.isin(val_indices[0], lst)\n",
    "            mask_val2 = np.isin(val_indices[1], lst)\n",
    "            \n",
    "            val_indices_batch_1 = val_indices[0][mask_val1 & mask_val2].numpy() - i # already a numpy\n",
    "            val_indices_batch_2 = val_indices[1][mask_val1 & mask_val2].numpy() - i\n",
    "            y_val_batch = y_val[mask_val1 & mask_val2]\n",
    "            \n",
    "            x1_batch = x_batch[val_indices_batch_1.astype('int')]\n",
    "            x2_batch = x_batch[val_indices_batch_2.astype('int')]\n",
    "            \n",
    "            output, _ = model(x1_batch, x2_batch)\n",
    "            output.to(device)\n",
    "            loss_val = F.nll_loss(output, y_val_batch)\n",
    "            acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val_batch.cpu().numpy())\n",
    "            total_loss_val += loss_val\n",
    "            total_acc_val += acc_val            \n",
    "            \n",
    "            \n",
    "        loss_train = total_loss / num_batches\n",
    "        acc_train = total_acc / num_batches\n",
    "        loss_val = total_loss_val / num_batches\n",
    "        acc_val = total_acc_val / num_batches\n",
    "        list_loss_val.append(loss_val)\n",
    "        list_loss_train.append(loss_train)\n",
    "        \n",
    "        \n",
    "        #val_indices_torch = torch.Tensor(val_indices).long()\n",
    "        #val_indices_np = np.array(val_indices) #transform into a tensor\n",
    "#         print(type(val_indices_np))\n",
    "#         x1 = x[val_indices_np[0].astype(int)]\n",
    "#         x2 = x[val_indices_np[1].astype(int)]\n",
    "        \n",
    "#         x1 = x[val_indices[0]]\n",
    "#         x2 = x[val_indices[1]]\n",
    "\n",
    "#         x1 = x[val_indices[0].astype('int')]\n",
    "#         x2 = x[val_indices[1].astype('int')]\n",
    "#         output, _ = model(x1, x2)\n",
    "#         output.to(device)\n",
    "#         #y_val = torch.LongTensor(y_val).to(device)\n",
    "#         loss_val = F.nll_loss(output, y_val)\n",
    "#         list_loss_val.append(loss_val.item())\n",
    "#         list_loss_train.append(loss_train.item())\n",
    "#         acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "        \n",
    "\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train),\n",
    "                  'loss_val: {:.4f}'.format(loss_val),\n",
    "                  'acc_train: {:.4f}'.format(acc_train),\n",
    "                  'acc_val: {:.4f}'.format(acc_val),\n",
    "                  'time: {} s'.format(int(round(time()) - round(t))),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "            \n",
    "            if epoch % 50 == 0:\n",
    "                model_path = \"outputs/{}-model-{}epochs-{}.pt\".format(today, epoch, run_number)\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "\n",
    "        \n",
    "        early_stopping(loss_val)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            halving_lr += 1\n",
    "            if halving_lr > 4:\n",
    "                break\n",
    "            learning_rate = learning_rate/10\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            early_stopping = EarlyStopping(model, patience=patience, delta=0.01, path='checkpoint.pt')\n",
    "            print('Deviding the learning rate by 10. New learning rate: {}'.format(learning_rate))\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for key, tensor in words_embedding_dict.items():\n",
    "#     output_dict[key] = model(tensor.unsqueeze(0), pairs).squeeze()\n",
    "\n",
    "# # Print the output shape of each tensor\n",
    "# for key, tensor in output_dict.items():\n",
    "#     print(key, tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the data for training...\n",
      "Data converted into torch tensors and authors added to indices in 2 min\n"
     ]
    }
   ],
   "source": [
    "features_torch, adj_torch, auth_torch, pairs_torch, y_torch, val_indices_torch, y_val_torch = prepare_data_to_train(walks_wv, authors, adj, auth_matrix, indices, val_indices, y_val)\n",
    "\n",
    "#abstracts_bart_embeddings = abstracts_bart_embeddings.to(device)\n",
    "#goog300_abstracts_torch = torch.FloatTensor(goog300_abstracts).to(device)\n",
    "#local_wv300_abstracts_torch = torch.FloatTensor(local_wv300_abstracts).to(device)\n",
    "#tfidf_matrix_torch = sparse_mx_to_torch_sparse_tensor(tfidf_matrix).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23bfb88468564c3d9d8b523ac19251c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3929500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c225b7ad594ed8beb2feb2f21b19b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/219160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# shuffle the list of tensors and get the mapping dictionary\n",
    "mapping_dict = {}\n",
    "perm = np.random.permutation(len(list_tensors))\n",
    "shuffled_list_tensors = [list_tensors[i] for i in perm]\n",
    "for i, p in enumerate(perm):\n",
    "    mapping_dict[i] = p\n",
    "\n",
    "\n",
    "shuffled_pairs_torch = torch.zeros(size=pairs_torch.shape)\n",
    "shuffled_val_indices = torch.zeros(size=val_indices.shape)\n",
    "\n",
    "perm_pairs = torch.randperm(shuffled_pairs_torch.shape[1])\n",
    "perm_val_indices = torch.randperm(shuffled_val_indices.shape[1])\n",
    "\n",
    "\n",
    "pairs_dict = dict()\n",
    "\n",
    "# Update pairs_torch tensor with the new indices\n",
    "for i in tqdm(range(len(pairs_torch[0]))):\n",
    "    old_idx1 = int(pairs_torch[0][i])\n",
    "    old_idx2 = int(pairs_torch[1][i])\n",
    "    new_idx1 = mapping_dict[old_idx1]\n",
    "    new_idx2 = mapping_dict[old_idx2]\n",
    "    shuffled_pairs_torch[0][i] = new_idx1\n",
    "    shuffled_pairs_torch[1][i] = new_idx2\n",
    "shuffled_pairs_torch = torch.index_select(shuffled_pairs_torch, dim=1, index=perm_pairs)\n",
    "shuffled_y_torch = torch.index_select(y_torch, dim=0, index=perm_pairs)\n",
    "    \n",
    "for i in tqdm(range(len(val_indices[0]))):\n",
    "    old_idx1 = int(val_indices[0][i])\n",
    "    old_idx2 = int(val_indices[1][i])\n",
    "    new_idx1 = mapping_dict[old_idx1]\n",
    "    new_idx2 = mapping_dict[old_idx2]\n",
    "    shuffled_val_indices[0][i] = new_idx1\n",
    "    shuffled_val_indices[1][i] = new_idx2\n",
    "shuffled_val_indices = torch.index_select(shuffled_val_indices, dim=1, index=perm_val_indices)\n",
    "shuffled_y_val_torch = torch.index_select(y_val_torch, dim=0, index=perm_val_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the optimizer with learning rate: 0.01\n",
      "Start training...\n",
      "torch.Size([864, 128, 192])\n",
      "torch.Size([864, 192, 128])\n",
      "torch.Size([864, 64, 32])\n",
      "torch.Size([864, 64, 8])\n",
      "torch.Size([864, 32, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (32x1x2). Calculated output size: (32x1x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m model_CNN \u001b[38;5;241m=\u001b[39m CNN()\n\u001b[1;32m      3\u001b[0m sampling_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain_CNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_CNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffled_list_tensors\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffled_pairs_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffled_y_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffled_val_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffled_y_val_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[94], line 50\u001b[0m, in \u001b[0;36mtrain_CNN\u001b[0;34m(model, learning_rate, x, pairs, y, val_indices, y_val, epochs, batch_size, run_number, window)\u001b[0m\n\u001b[1;32m     47\u001b[0m x1_batch \u001b[38;5;241m=\u001b[39m x_batch[pairs1\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     48\u001b[0m x2_batch \u001b[38;5;241m=\u001b[39m x_batch[pairs2\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m---> 50\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_CNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2_batch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# we run the model that gives the output.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m output\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     52\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output, y_batch) \u001b[38;5;66;03m# we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[102], line 41\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     38\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x1)\n\u001b[1;32m     39\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x2)\n\u001b[0;32m---> 41\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x2)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(x1\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/pooling.py:92\u001b[0m, in \u001b[0;36mMaxPool1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/_jit_internal.py:485\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:696\u001b[0m, in \u001b[0;36m_max_pool1d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    695\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (32x1x2). Calculated output size: (32x1x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "model_CNN = CNN()\n",
    "\n",
    "sampling_size = 10000\n",
    "\n",
    "train_CNN(model_CNN, 0.01, shuffled_list_tensors[: sampling_size], shuffled_pairs_torch, shuffled_y_torch, shuffled_val_indices, shuffled_y_val_torch, 50, 2000, run_number=4, window = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the optimizer with learning rate: 0.01\n",
      "Start training...\n",
      "epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36968123d524ce28bd7404f0fde29d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e022189a9f1a49bdb1e3b912d1c6ddb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 3.0146 loss_val: 0.9280 acc_train: 0.7814 acc_val: 0.8846 time: 484 s total_time: 8 min\n",
      "epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3846fda103624189a2dbecf99f84f5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_CNN \u001b[38;5;241m=\u001b[39m CNN()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain_CNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_CNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[57], line 54\u001b[0m, in \u001b[0;36mtrain_CNN\u001b[0;34m(model, learning_rate, x, pairs, y, val_indices, y_val, epochs, batch_size, run_number, window)\u001b[0m\n\u001b[1;32m     52\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output, y_batch) \u001b[38;5;66;03m# we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m acc_train \u001b[38;5;241m=\u001b[39m accuracy_score(torch\u001b[38;5;241m.\u001b[39margmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), y_batch\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\u001b[38;5;66;03m# just to show it in the out put message of the training\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[43mloss_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\u001b[39;00m\n\u001b[1;32m     55\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# Performs a single optimization step (parameter update).\u001b[39;00m\n\u001b[1;32m     56\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_train\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_CNN = CNN()\n",
    "\n",
    "train_CNN(model_CNN, 0.01, list_tensors, pairs_torch, y_torch, val_indices, y_val_torch, 50, 2000, run_number=4, window = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e566bce78b410384b1bb163ed3e434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(219160, torch.Size([128, 192]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_val_1 = []\n",
    "x_val_2 = []\n",
    "\n",
    "for pair in tqdm(zip(val_indices_torch[0], val_indices_torch[1])):\n",
    "    if random() < 0.1:\n",
    "        x_val_1.append(list_tensors[int(pair[0])])\n",
    "        x_val_2.append(list_tensors[int(pair[1])])\n",
    "    \n",
    "len(x_val_1), x_val_1[0].shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_torch_1 = torch.stack(x_val_1).to(device)\n",
    "#x_val_torch_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN(x_val_1, x_val_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_CNN(model_CNN, 0.01, list_tensors, pairs_torch, y_torch, val_indices, y_val_torch, 50, 1000, run_number=4, window = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_indices[0].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN((list_tensors[0]), (list_tensors[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN(torch.stack(list_tensors[0:1]), torch.stack(list_tensors[2:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 30\n",
    "indices[0][m:m+40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tensors[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = range(0, 4)\n",
    "arr1 = indices[0]\n",
    "arr2 = indices[1]\n",
    "\n",
    "# check which elements belong to the list\n",
    "mask1 = np.isin(arr1, lst)\n",
    "mask2 = np.isin(arr2, lst)\n",
    "\n",
    "# select the elements that satisfy the condition\n",
    "result1 = arr1[mask1 & mask2]\n",
    "result2 = arr2[mask1 & mask2]\n",
    "\n",
    "print(result1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val[5:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnRNz5z4nh6x"
   },
   "outputs": [],
   "source": [
    "def train_model(model, learning_rate, auth, features, adj, pairs, y, val_indices, y_val, epochs, run_number, window = 10):\n",
    "    # Train model\n",
    "    start_time = time()\n",
    "    \n",
    "    print('Initializing the optimizer with learning rate:', learning_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) #optimizer with halving learning rate in training\n",
    "    try: os.mkdir('./outputs')\n",
    "    except: pass\n",
    "\n",
    "    today = datetime.today().strftime('%Y-%m-%d-%H:%M')\n",
    "    list_loss_val = []\n",
    "    list_loss_train = []\n",
    "    list_epochs = []\n",
    "    \n",
    "    halving_lr = 0 # counter of the number of halving lr\n",
    "    patience = 16\n",
    "    early_stopping = EarlyStopping(model, patience=patience, delta=0.1, path='checkpoint.pt')\n",
    "    print('Start training...')\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        output = model(features, auth, adj, pairs).to(device) # we run the model that gives the output.\n",
    "        loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "        acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "        loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "        optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, auth, adj, val_indices).to(device)\n",
    "        #y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        list_loss_val.append(loss_val.item())\n",
    "        list_loss_train.append(loss_train.item())\n",
    "        list_epochs.append(epoch)\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "        \n",
    "\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {} s'.format(int(round(time()) - round(t))),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "            \n",
    "            if epoch % 50 == 0:\n",
    "                model_path = \"outputs/{}-model-{}epochs-{}.pt\".format(today, epoch, run_number)\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "\n",
    "        \n",
    "        early_stopping(loss_val)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            halving_lr += 1\n",
    "            if halving_lr > 4:\n",
    "                break\n",
    "            learning_rate = learning_rate/10\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            early_stopping = EarlyStopping(model, patience=patience, delta=0.01, path='checkpoint.pt')\n",
    "            print('Deviding the learning rate by 10. New learning rate: {}'.format(learning_rate))\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model, list_loss_val, list_loss_train, list_epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2eA1kmI4fV8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJc0hof64gH8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aYciYrx4hEC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJqiI7fxbVhl"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C1UJ9IMw0zNw"
   },
   "outputs": [],
   "source": [
    "# shuffle features the same way as the Graph\n",
    "walks_wv_shuffled = map_features_with_permutation(walks_wv, permutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZqvqcdPg4X6",
    "outputId": "6172e2b4-6c6f-44d9-f532-f0cf5ab28974"
   },
   "outputs": [],
   "source": [
    "features_torch, adj_torch, auth_torch, pairs_torch, y_torch, val_indices_torch, y_val_torch = prepare_data_to_train(walks_wv, authors, adj, auth_matrix, indices, val_indices, y_val)\n",
    "\n",
    "#abstracts_bart_embeddings = abstracts_bart_embeddings.to(device)\n",
    "#goog300_abstracts_torch = torch.FloatTensor(goog300_abstracts).to(device)\n",
    "#local_wv300_abstracts_torch = torch.FloatTensor(local_wv300_abstracts).to(device)\n",
    "#tfidf_matrix_torch = sparse_mx_to_torch_sparse_tensor(tfidf_matrix).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ADpiZYBnHcNs",
    "outputId": "5f1b7ae8-25d3-4b13-f0c5-7bf4e1a8fd51"
   },
   "outputs": [],
   "source": [
    "auth_torch.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47r-jwjb4jl4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7i8UpGHuNhQp"
   },
   "outputs": [],
   "source": [
    "n_hidden = 64\n",
    "dropout_rate = 0.2\n",
    "sub_class = 32\n",
    "n_class = 2\n",
    "text_embedding = 0 #tfidf_matrix_torch\n",
    "n_auth = auth_torch.shape[1]\n",
    "n_features = features_torch.shape[1]\n",
    "\n",
    "\n",
    "# Creates the model\n",
    "model = GNN(n_auth, n_features, n_hidden, n_class, sub_class, dropout_rate).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5llTPM64kQj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tiyrR1an0zNx",
    "outputId": "f35099de-20d8-4c85-fdac-2f33a2d38c23"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "epochs = 400\n",
    "run_number = randint(0, 1000)\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "trained_model, list_loss_val, list_loss_train, list_epochs = train_model(model, learning_rate, auth_torch, \n",
    "                            features_torch, adj_torch, pairs_torch, \n",
    "                            y_torch, val_indices_torch, y_val_torch, epochs, run_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "idFEWIJO4k5L",
    "outputId": "570c9cf7-821d-4d61-afab-42175298b055"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "y1 = pd.Series(list_loss_val[:400])\n",
    "y2 = pd.Series(list_loss_train[:400])\n",
    "x = pd.Series(list_epochs[:400])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(x, y1, label='Validation Loss')\n",
    "plt.plot(x, y2, label='Training Loss')\n",
    "\n",
    "plt.title('Loss over Epochs, G not shuffled')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oErtNBi1Ra66",
    "outputId": "b78af35f-a1dc-47a2-9fdf-6451be807cff"
   },
   "outputs": [],
   "source": [
    "pairs_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "id": "0KKqG0rk1HM4",
    "outputId": "8573e1f0-0197-4650-fe4e-80e7e1cbb6b6"
   },
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDss4SBl0zNx"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "epochs = 602\n",
    "run_number = randint(0, 1000)\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "trained_model = train_model(model, learning_rate, text_embedding, \n",
    "                            features_torch, adj_torch, pairs_torch, \n",
    "                            y_torch, val_indices_torch, y_val_torch, epochs, run_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSvrKVnoLGfn"
   },
   "outputs": [],
   "source": [
    "# before shuffling\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "epochs = 602\n",
    "run_number = randint(0, 1000)\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "trained_model = train_model(model, learning_rate, text_embedding, \n",
    "                            features_torch, adj_torch, pairs_torch, \n",
    "                            y_torch, val_indices_torch, y_val_torch, epochs, run_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoZHWc0G0zNy"
   },
   "outputs": [],
   "source": [
    "np.shape(pairs_torch), y_torch.shape, val_indices_torch.shape, y_val_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uihFEZKFsjwH"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "epochs = 602\n",
    "run_number = randint(0, 1000)\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "trained_model = train_model(model, learning_rate, text_embedding, \n",
    "                            features_torch, adj_torch, pairs_torch, \n",
    "                            y_torch, val_indices_torch, y_val_torch, epochs, run_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lJ2Kxomh20h"
   },
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PAjwmZMg4X6"
   },
   "outputs": [],
   "source": [
    "# with dense2 1024 of BART\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "run_number = randint(0, 1000)\n",
    "learning_rate = 0.005\n",
    "\n",
    "\n",
    "trained_model = train_model(model, learning_rate, abstracts_bart_embeddings, features_torch, adj_torch, pairs_torch, y_torch, val_indices_torch, y_val_torch, epochs, run_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZPrUfJnFnzB"
   },
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDQ_DCCJavQZ"
   },
   "outputs": [],
   "source": [
    "# Without dense 1000\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "run_number = randint(0, 1000)\n",
    "\n",
    "\n",
    "trained_model = train_model(model, 0.01, tfidf_matrix_torch, features_torch, adj_torch, pairs_torch, y_torch, val_indices_torch, y_val_torch, epochs, run_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_XPsPMFbZji"
   },
   "source": [
    "# Generate test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmFO4qI7_245"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "test_path = 'https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/test.txt'\n",
    "node_pairs = list()\n",
    "f = urlopen(test_path)\n",
    "\n",
    "for line in f:\n",
    "    t = str(line).split(',')\n",
    "    t[0] = int(re.sub(\"[^0-9]\", \"\", t[0]))\n",
    "    t[1] = int(re.sub(\"[^0-9]\", \"\", t[1]))\n",
    "    node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "node_pairs = np.transpose(node_pairs)\n",
    "node_pairs = add_authors_to_pairs(node_pairs, authors)\n",
    "#node_pairs = torch.LongTensor(node_pairs).to(device)\n",
    "\n",
    "adj_torch = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "features_torch = torch.FloatTensor(walks_wv).to(device)\n",
    "\n",
    "test_output = model(features_torch, text_embedding, adj_torch, node_pairs)\n",
    "y_pred = torch.exp(test_output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "model_nb = 4\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns=['predicted']).to_csv(\n",
    "\"{}-submission-{}-{}.csv\".format(today, model_nb, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pD505btvfDG0"
   },
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bfa1O6lLg4X2"
   },
   "source": [
    "# BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Md1ARLd-g4X3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BartTokenizer, BartModel\n",
    "\n",
    "# Load the BART model and tokenizer\n",
    "model = BartModel.from_pretrained('facebook/bart-large')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ov9PcB9wg4X3"
   },
   "outputs": [],
   "source": [
    "abstracts_bart_embeddings = []\n",
    "\n",
    "# Define a function to generate embeddings for text\n",
    "def get_bart_embeddings(text, model):\n",
    "    # Tokenize the input text\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Generate embeddings using the BART model\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "        embeddings = model_output.last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_FBpRYsg4X3"
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(abstracts_bart_embeddings), len(voc.sentences_list))):\n",
    "    abstract = voc.sentences_list[i]\n",
    "    abstracts_bart_embeddings.append(get_bart_embeddings(abstract, model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8RxfG2Eg4X3"
   },
   "outputs": [],
   "source": [
    "saved_abstracts_bart_embeddings = abstracts_bart_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mi0TkxyFg4X3"
   },
   "outputs": [],
   "source": [
    "len(saved_abstracts_bart_embeddings), len(abstracts_bart_embeddings), len(voc.sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "411JkYvrg4X4"
   },
   "outputs": [],
   "source": [
    "abstracts_bart_embeddings = torch.stack(abstracts_bart_embeddings)\n",
    "abstracts_bart_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iLzw5akgg4X4"
   },
   "outputs": [],
   "source": [
    "abstracts_bart_embeddings.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTZC7PDEg4X4"
   },
   "outputs": [],
   "source": [
    "torch.save(abstracts_bart_embeddings, 'bart_embeddings.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6oNCPyubfSd"
   },
   "source": [
    "#Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2UWL4y-K1jd"
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "node_pairs = torch.LongTensor(node_pairs).to(device)\n",
    "\n",
    "test_output = model(features, adj, node_pairs)\n",
    "y_pred = torch.exp(test_output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"{}-submission-{}-{}.csv\".format(today, model_nb, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmUL7tGAR9Yt"
   },
   "outputs": [],
   "source": [
    "#### New script with batches\n",
    "\n",
    "def early_stopping(loss_train, list_loss_train, loss_val, list_loss_val, \n",
    "                   tolerance=0.01, patience=15):\n",
    "    list_loss_val = list(list_loss_val)[-patience:]\n",
    "    list_loss_train = list(list_loss_train)[-patience:]\n",
    "    if (len(list_loss_val) == patience and loss_val > (sum(list_loss_val)/len(list_loss_val)) and loss_train + tolerance < loss_val) or (len(list_loss_train) == patience and loss_train > (sum(list_loss_train)/len(list_loss_train))):\n",
    "        #print('train: {:.5f} val: {:.5f} mean val: {:.5f}'.format(loss_train, loss_val, (sum(list_loss_val)/len(list_loss_val))))\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "    \n",
    "def train_model(model, learning_rate, features, adj, indices_mc, y, val_indices, \n",
    "                y_val, epochs, batch_size, wv_walk_size, \n",
    "                tolerence = 0.01, patience = 15, run_number=randint(0, 1000)):\n",
    "    # Train model\n",
    "    start_time = time()\n",
    "    print('Initializing the optimizer with learning rate:', learning_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) #optimizer with halving learning rate in training\n",
    "    try: os.mkdir('./outputs')\n",
    "    except: pass\n",
    "    print('Preparing the data for training...')        \n",
    "\n",
    "    today = datetime.today().strftime('%Y-%m-%d-%H:%M')\n",
    "    list_loss_val = []\n",
    "    list_loss_train = []\n",
    "\n",
    "    \n",
    "    halving_lr = 0 # counter of the number of halving lr\n",
    "    print('Start training...')\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "\n",
    "        # we create the rand indices corresponding to non edges (their y = 0)\n",
    "        # we could apply a condition on epoch to run rand_indices (for speed purposes)\n",
    "        rand_indices = np.random.randint(0, len(indices_mc), size=(indices_mc.shape[0], indices_mc.shape[1]))\n",
    "        rand_indices = add_authors_to_pairs(rand_indices, authors)\n",
    "        pairs = np.concatenate((indices_mc, rand_indices), axis=1)\n",
    "        pairs = torch.LongTensor(pairs).to(device)\n",
    "\n",
    "        permutation = torch.randperm(pairs.size()[1])\n",
    "        \n",
    "        # batches\n",
    "        for i in range(0, pairs.size()[1], batch_size):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            elts_indices = permutation[i:i+batch_size]\n",
    "            batch_pairs = pairs[:, elts_indices]\n",
    "            batch_y = y[elts_indices]\n",
    "\n",
    "        \n",
    "            model.train()\n",
    "\n",
    "            output = model(features, adj, batch_pairs, wv_walk_size).to(device) # we run the model that gives the output.\n",
    "            loss_train = F.nll_loss(output, batch_y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "            acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), batch_y.cpu().numpy())# just to show it in the out put message of the training\n",
    "            loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "            optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, adj, val_indices, wv_walk_size).to(device)\n",
    "        #y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        list_loss_val.append(loss_val.item())\n",
    "        list_loss_train.append(loss_train.item())\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {} s'.format(int(round(time()) - round(t))),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "        if epoch % 50 == 0:\n",
    "            model_path = \"outputs/{}-model-{}epochs-{}.pt\".format(today, epoch, run_number)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "        if int(loss_val.item()) > 5:\n",
    "            break\n",
    "            \n",
    "        early = early_stopping(loss_train.item(), list_loss_train, loss_val.item(), list_loss_val, patience=15)        \n",
    "        if early:\n",
    "            halving_lr += 1\n",
    "            if halving_lr > 5:\n",
    "                break\n",
    "            list_loss_val=[]\n",
    "            learning_rate = learning_rate/10\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            print('Deviding the learning rate by 2. New learning rate: {:.6f}'.format(learning_rate))\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwNIJmkGn6-G"
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "trained_model = train_model(model, 0.01, features, authors, adj, indices, y, torch.tensor(val_indices).to(device), torch.tensor(y_val).to(device), epochs)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Ubeigqung4X0"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "048074f63c75476d99a9dec95f42c8d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "07765899fda144cca0dc7775800f3215": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "093c3327f5984498ba15badb8db23a17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "11902059cd7148fe978ca28cab88b10d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "150e5c5090414276b13b373710ccf198": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "25f0e533952c4ad8b81fee35ec6025f8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "371aa51a013c4bb4bcad92c54ab1c79e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37d5ab03a8bf48278e439809364d144b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47599c391f7f4b85998826354d2afb0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25f0e533952c4ad8b81fee35ec6025f8",
      "placeholder": "​",
      "style": "IPY_MODEL_932491a1b72342d8b063fcab699f5728",
      "value": " 147950/? [00:00&lt;00:00, 1423383.99it/s]"
     }
    },
    "485660547adb473694453175605ffde7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "532eb4c4869f42a59024ff65ec3d8e2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "548d4579dc26489da531997cf6862544": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d96a4dcb1da743bcb4685c2f3bd8f0f8",
       "IPY_MODEL_9872e75a68d04f45b54815e1bbd8f52e",
       "IPY_MODEL_d7e97039408748c2a49de39396931a7e"
      ],
      "layout": "IPY_MODEL_60292eade64242fdb1e7de2040c1764e"
     }
    },
    "60292eade64242fdb1e7de2040c1764e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77b6df7549534afbbf896fc1d90ecf3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "846969cb1c9c430fb96f488eb0ad727c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b9fa1be8ce0741f595f27d20f1550cc8",
       "IPY_MODEL_9e61bb4f3d054d6c9f3d807203a984ec",
       "IPY_MODEL_47599c391f7f4b85998826354d2afb0f"
      ],
      "layout": "IPY_MODEL_07765899fda144cca0dc7775800f3215"
     }
    },
    "86337e063b1e4d199c0f493ed1f887d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8e4a67e76ce34aabbe6b60feb0859d84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "924c1ba5f01f4809a044a28b72917470": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "932491a1b72342d8b063fcab699f5728": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93b4d097061147a185c7f05a80ac9a22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "96e54fe99c4542c1a97fff3fcce2e4de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "9872e75a68d04f45b54815e1bbd8f52e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_11902059cd7148fe978ca28cab88b10d",
      "max": 138499,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_048074f63c75476d99a9dec95f42c8d4",
      "value": 138499
     }
    },
    "9e61bb4f3d054d6c9f3d807203a984ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_485660547adb473694453175605ffde7",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_093c3327f5984498ba15badb8db23a17",
      "value": 1
     }
    },
    "aafe73f8ce26421d9704dab62ee2ff88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd733b795c274acfa068038c066a02b8",
      "placeholder": "​",
      "style": "IPY_MODEL_532eb4c4869f42a59024ff65ec3d8e2d",
      "value": " 138499/? [00:00&lt;00:00, 351148.49it/s]"
     }
    },
    "b12d9410df3e4f35b1ed6567a5454e51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_96e54fe99c4542c1a97fff3fcce2e4de",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_77b6df7549534afbbf896fc1d90ecf3c",
      "value": 1
     }
    },
    "b9fa1be8ce0741f595f27d20f1550cc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37d5ab03a8bf48278e439809364d144b",
      "placeholder": "​",
      "style": "IPY_MODEL_93b4d097061147a185c7f05a80ac9a22",
      "value": ""
     }
    },
    "bd733b795c274acfa068038c066a02b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4c63bc81390469b9c305a43f7e4980b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c99dba6a96804c829a931ac074afff0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e68689640a564e9fa353886d5f4a45f4",
       "IPY_MODEL_b12d9410df3e4f35b1ed6567a5454e51",
       "IPY_MODEL_aafe73f8ce26421d9704dab62ee2ff88"
      ],
      "layout": "IPY_MODEL_371aa51a013c4bb4bcad92c54ab1c79e"
     }
    },
    "d7e97039408748c2a49de39396931a7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4c63bc81390469b9c305a43f7e4980b",
      "placeholder": "​",
      "style": "IPY_MODEL_86337e063b1e4d199c0f493ed1f887d1",
      "value": " 138499/138499 [00:00&lt;00:00, 1751708.75it/s]"
     }
    },
    "d96a4dcb1da743bcb4685c2f3bd8f0f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_924c1ba5f01f4809a044a28b72917470",
      "placeholder": "​",
      "style": "IPY_MODEL_8e4a67e76ce34aabbe6b60feb0859d84",
      "value": "100%"
     }
    },
    "e63b115ae35e417c997003995eab8e64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e68689640a564e9fa353886d5f4a45f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e63b115ae35e417c997003995eab8e64",
      "placeholder": "​",
      "style": "IPY_MODEL_150e5c5090414276b13b373710ccf198",
      "value": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
