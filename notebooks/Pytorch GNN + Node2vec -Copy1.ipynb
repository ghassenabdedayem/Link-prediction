{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "46a7427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from random import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from random import choice\n",
    "from gensim.models import Word2Vec\n",
    "import keras\n",
    "\n",
    "from scipy.sparse import identity, diags\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e11dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ea6cfcc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669cdbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dcf6da2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adjacency(A):\n",
    "    n = A.shape[0]\n",
    "    A = A + identity(n)\n",
    "    degs = A.dot(np.ones(n))\n",
    "    inv_degs = np.power(degs, -1)\n",
    "    print('inv_degs', inv_degs.shape)\n",
    "    D_inv = diags(inv_degs)\n",
    "    print('D_inv shape:', D_inv.shape)\n",
    "    A_hat = D_inv.dot(A)\n",
    "    print('A_hat shape:', A_hat.shape)\n",
    "    return A_hat\n",
    "\n",
    "def new_normalize_adjacency(A):\n",
    "    n = A.shape[0]\n",
    "    A = A + identity(n)\n",
    "    #degs = A.dot(np.ones(n))\n",
    "    #inv_degs = np.power(degs, -1)\n",
    "    #D_inv = diags(inv_degs)\n",
    "    #A_hat = D_inv.dot(A)\n",
    "    return A\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    print(type(sparse_mx))\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def random_walk(G, node, walk_length):\n",
    "    walk = [node]\n",
    "  \n",
    "    for i in range(walk_length-1):\n",
    "        neibor_nodes = list(G.neighbors(walk[-1]))\n",
    "        if len(neibor_nodes) > 0 :\n",
    "        #print('neibor_nodes : ', neibor_nodes)\n",
    "            next_node = choice(neibor_nodes)\n",
    "            walk.append(next_node)\n",
    "    \n",
    "    walk = [str(node) for node in walk] # in case the nodes are in string format, we don't need to cast into string, but if the nodes are in numeric or integer, we need this line to cast into string\n",
    "    return walk\n",
    "\n",
    "def generate_walks(G, num_walks, walk_length):\n",
    "  # Runs \"num_walks\" random walks from each node, and returns a list of all random walk\n",
    "    walks = list()\n",
    "  \n",
    "    for i in range(num_walks):\n",
    "        for node in G.nodes():\n",
    "            walk = random_walk(G, node, walk_length)\n",
    "            walks.append(walk)\n",
    "        #print('walks : ', walks)\n",
    "    return walks\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_feat, n_hidden, n_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc4 = nn.Linear(n_hidden, n_class)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x_in, adj, pairs):\n",
    "        \n",
    "        h1 = self.fc1(x_in)\n",
    "        z1 = self.relu(torch.mm(adj, h1))\n",
    "        z1 = self.dropout(z1)\n",
    "\n",
    "        h2 = self.fc2(z1)\n",
    "        z2 = self.relu(torch.mm(adj, h2))\n",
    "        \n",
    "        x = z2[pairs[0,:],:] - z2[pairs[1,:],:]\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "66e47b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2',\n",
       " '25',\n",
       " '58',\n",
       " '43',\n",
       " '428',\n",
       " '122789',\n",
       " '134684',\n",
       " '131280',\n",
       " '134682',\n",
       " '131280',\n",
       " '134682',\n",
       " '122789',\n",
       " '428',\n",
       " '43',\n",
       " '405',\n",
       " '461',\n",
       " '70359',\n",
       " '461',\n",
       " '70359',\n",
       " '465']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_walk(G=G_train, node=2, walk_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "03325ac0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parallel, delayed\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parallel_generate_walks\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNode2Vec\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     FIRST_TRAVEL_KEY \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_travel_key\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import gensim\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pkg_resources\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from .parallel import parallel_generate_walks\n",
    "\n",
    "\n",
    "class Node2Vec:\n",
    "    FIRST_TRAVEL_KEY = 'first_travel_key'\n",
    "    PROBABILITIES_KEY = 'probabilities'\n",
    "    NEIGHBORS_KEY = 'neighbors'\n",
    "    WEIGHT_KEY = 'weight'\n",
    "    NUM_WALKS_KEY = 'num_walks'\n",
    "    WALK_LENGTH_KEY = 'walk_length'\n",
    "    P_KEY = 'p'\n",
    "    Q_KEY = 'q'\n",
    "\n",
    "    def __init__(self, graph: nx.Graph, dimensions: int = 128, walk_length: int = 80, num_walks: int = 10, p: float = 1,\n",
    "                 q: float = 1, weight_key: str = 'weight', workers: int = 1, sampling_strategy: dict = None,\n",
    "                 quiet: bool = False, temp_folder: str = None, seed: int = None):\n",
    "        \"\"\"\n",
    "        Initiates the Node2Vec object, precomputes walking probabilities and generates the walks.\n",
    "        :param graph: Input graph\n",
    "        :param dimensions: Embedding dimensions (default: 128)\n",
    "        :param walk_length: Number of nodes in each walk (default: 80)\n",
    "        :param num_walks: Number of walks per node (default: 10)\n",
    "        :param p: Return hyper parameter (default: 1)\n",
    "        :param q: Inout parameter (default: 1)\n",
    "        :param weight_key: On weighted graphs, this is the key for the weight attribute (default: 'weight')\n",
    "        :param workers: Number of workers for parallel execution (default: 1)\n",
    "        :param sampling_strategy: Node specific sampling strategies, supports setting node specific 'q', 'p', 'num_walks' and 'walk_length'.\n",
    "        :param seed: Seed for the random number generator.\n",
    "        Use these keys exactly. If not set, will use the global ones which were passed on the object initialization\n",
    "        :param temp_folder: Path to folder with enough space to hold the memory map of self.d_graph (for big graphs); to be passed joblib.Parallel.temp_folder\n",
    "        \"\"\"\n",
    "\n",
    "        self.graph = graph\n",
    "        self.dimensions = dimensions\n",
    "        self.walk_length = walk_length\n",
    "        self.num_walks = num_walks\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.weight_key = weight_key\n",
    "        self.workers = workers\n",
    "        self.quiet = quiet\n",
    "        self.d_graph = defaultdict(dict)\n",
    "\n",
    "        if sampling_strategy is None:\n",
    "            self.sampling_strategy = {}\n",
    "        else:\n",
    "            self.sampling_strategy = sampling_strategy\n",
    "\n",
    "        self.temp_folder, self.require = None, None\n",
    "        if temp_folder:\n",
    "            if not os.path.isdir(temp_folder):\n",
    "                raise NotADirectoryError(\"temp_folder does not exist or is not a directory. ({})\".format(temp_folder))\n",
    "\n",
    "            self.temp_folder = temp_folder\n",
    "            self.require = \"sharedmem\"\n",
    "\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self._precompute_probabilities()\n",
    "        self.walks = self._generate_walks()\n",
    "\n",
    "    def _precompute_probabilities(self):\n",
    "        \"\"\"\n",
    "        Precomputes transition probabilities for each node.\n",
    "        \"\"\"\n",
    "\n",
    "        d_graph = self.d_graph\n",
    "\n",
    "        nodes_generator = self.graph.nodes() if self.quiet \\\n",
    "            else tqdm(self.graph.nodes(), desc='Computing transition probabilities')\n",
    "\n",
    "        for source in nodes_generator:\n",
    "\n",
    "            # Init probabilities dict for first travel\n",
    "            if self.PROBABILITIES_KEY not in d_graph[source]:\n",
    "                d_graph[source][self.PROBABILITIES_KEY] = dict()\n",
    "\n",
    "            for current_node in self.graph.neighbors(source):\n",
    "\n",
    "                # Init probabilities dict\n",
    "                if self.PROBABILITIES_KEY not in d_graph[current_node]:\n",
    "                    d_graph[current_node][self.PROBABILITIES_KEY] = dict()\n",
    "\n",
    "                unnormalized_weights = list()\n",
    "                d_neighbors = list()\n",
    "\n",
    "                # Calculate unnormalized weights\n",
    "                for destination in self.graph.neighbors(current_node):\n",
    "\n",
    "                    p = self.sampling_strategy[current_node].get(self.P_KEY,\n",
    "                                                                 self.p) if current_node in self.sampling_strategy else self.p\n",
    "                    q = self.sampling_strategy[current_node].get(self.Q_KEY,\n",
    "                                                                 self.q) if current_node in self.sampling_strategy else self.q\n",
    "\n",
    "                    try:\n",
    "                        if self.graph[current_node][destination].get(self.weight_key):\n",
    "                            weight = self.graph[current_node][destination].get(self.weight_key, 1)\n",
    "                        else: \n",
    "                            ## Example : AtlasView({0: {'type': 1, 'weight':0.1}})- when we have edge weight\n",
    "                            edge = list(self.graph[current_node][destination])[-1]\n",
    "                            weight = self.graph[current_node][destination][edge].get(self.weight_key, 1)\n",
    "                            \n",
    "                    except:\n",
    "                        weight = 1 \n",
    "                    \n",
    "                    if destination == source:  # Backwards probability\n",
    "                        ss_weight = weight * 1 / p\n",
    "                    elif destination in self.graph[source]:  # If the neighbor is connected to the source\n",
    "                        ss_weight = weight\n",
    "                    else:\n",
    "                        ss_weight = weight * 1 / q\n",
    "\n",
    "                    # Assign the unnormalized sampling strategy weight, normalize during random walk\n",
    "                    unnormalized_weights.append(ss_weight)\n",
    "                    d_neighbors.append(destination)\n",
    "\n",
    "                # Normalize\n",
    "                unnormalized_weights = np.array(unnormalized_weights)\n",
    "                d_graph[current_node][self.PROBABILITIES_KEY][\n",
    "                    source] = unnormalized_weights / unnormalized_weights.sum()\n",
    "\n",
    "            # Calculate first_travel weights for source\n",
    "            first_travel_weights = []\n",
    "\n",
    "            for destination in self.graph.neighbors(source):\n",
    "                first_travel_weights.append(self.graph[source][destination].get(self.weight_key, 1))\n",
    "\n",
    "            first_travel_weights = np.array(first_travel_weights)\n",
    "            d_graph[source][self.FIRST_TRAVEL_KEY] = first_travel_weights / first_travel_weights.sum()\n",
    "\n",
    "            # Save neighbors\n",
    "            d_graph[source][self.NEIGHBORS_KEY] = list(self.graph.neighbors(source))\n",
    "\n",
    "    def _generate_walks(self) -> list:\n",
    "        \"\"\"\n",
    "        Generates the random walks which will be used as the skip-gram input.\n",
    "        :return: List of walks. Each walk is a list of nodes.\n",
    "        \"\"\"\n",
    "\n",
    "        flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "        # Split num_walks for each worker\n",
    "        num_walks_lists = np.array_split(range(self.num_walks), self.workers)\n",
    "\n",
    "        walk_results = Parallel(n_jobs=self.workers, temp_folder=self.temp_folder, require=self.require)(\n",
    "            delayed(parallel_generate_walks)(self.d_graph,\n",
    "                                             self.walk_length,\n",
    "                                             len(num_walks),\n",
    "                                             idx,\n",
    "                                             self.sampling_strategy,\n",
    "                                             self.NUM_WALKS_KEY,\n",
    "                                             self.WALK_LENGTH_KEY,\n",
    "                                             self.NEIGHBORS_KEY,\n",
    "                                             self.PROBABILITIES_KEY,\n",
    "                                             self.FIRST_TRAVEL_KEY,\n",
    "                                             self.quiet) for\n",
    "            idx, num_walks\n",
    "            in enumerate(num_walks_lists, 1))\n",
    "\n",
    "        walks = flatten(walk_results)\n",
    "\n",
    "        return walks\n",
    "\n",
    "    def fit(self, **skip_gram_params) -> gensim.models.Word2Vec:\n",
    "        \"\"\"\n",
    "        Creates the embeddings using gensim's Word2Vec.\n",
    "        :param skip_gram_params: Parameters for gensim.models.Word2Vec - do not supply 'size' / 'vector_size' it is\n",
    "            taken from the Node2Vec 'dimensions' parameter\n",
    "        :type skip_gram_params: dict\n",
    "        :return: A gensim word2vec model\n",
    "        \"\"\"\n",
    "\n",
    "        if 'workers' not in skip_gram_params:\n",
    "            skip_gram_params['workers'] = self.workers\n",
    "\n",
    "        # Figure out gensim version, naming of output dimensions changed from size to vector_size in v4.0.0\n",
    "        gensim_version = pkg_resources.get_distribution(\"gensim\").version\n",
    "        size = 'size' if gensim_version < '4.0.0' else 'vector_size'\n",
    "        if size not in skip_gram_params:\n",
    "            skip_gram_params[size] = self.dimensions\n",
    "\n",
    "        if 'sg' not in skip_gram_params:\n",
    "            skip_gram_params['sg'] = 1\n",
    "\n",
    "        return gensim.models.Word2Vec(self.walks, **skip_gram_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d22582c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes of training set: 138499\n",
      "Number of edges of training set: 982679\n"
     ]
    }
   ],
   "source": [
    "\n",
    "G = nx.read_edgelist('../input_data/edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "nodes = list(G.nodes())\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "edges = list(G.edges())\n",
    "\n",
    "val_edges = list()\n",
    "G_train = G\n",
    "\n",
    "for edge in edges:\n",
    "    if random() < 0.1:\n",
    "        val_edges.append(edge)\n",
    "\n",
    "# We remove the val edges from the graph G\n",
    "for edge in val_edges:\n",
    "    G_train.remove_edge(edge[0], edge[1])\n",
    "\n",
    "n = G_train.number_of_nodes()\n",
    "m = G_train.number_of_edges()\n",
    "train_edges = list(G_train.edges())\n",
    "    \n",
    "print('Number of nodes of training set:', n)\n",
    "print('Number of edges of training set:', m)\n",
    "\n",
    "y_val = [1]*len(val_edges)\n",
    "\n",
    "n_val_edges = len(val_edges)\n",
    "\n",
    "# Create random pairs of nodes\n",
    "for i in range(n_val_edges):\n",
    "    n1 = nodes[randint(0, n-1)]\n",
    "    n2 = nodes[randint(0, n-1)]\n",
    "    (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "    val_edges.append((n1, n2))\n",
    "    \n",
    "# Remove from val_edges edges that exist in both train and val\n",
    "\n",
    "for edge in list(set(val_edges) & set(train_edges)):\n",
    "    val_edges.remove(edge)\n",
    "    \n",
    "n_val_edges = len(val_edges) - len(y_val) #because we removed from val_edges edges that exist in both\n",
    "y_val.extend([0]*n_val_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "08a930b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218546"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fe3e518f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pd/glcnl2497w5b6xn3p94tnwlr0000gn/T/ipykernel_44001/3833020208.py:1: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G_train) # Obtains the adjacency matrix of the training graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv_degs (138499,)\n",
      "D_inv shape: (138499, 138499)\n",
      "A_hat shape: (138499, 138499)\n"
     ]
    }
   ],
   "source": [
    "adj = nx.adjacency_matrix(G_train) # Obtains the adjacency matrix of the training graph\n",
    "\n",
    "indices = np.array(adj.nonzero()) # Gets the positions of non zeros of adj into indices\n",
    "adj = normalize_adjacency(adj) # Normalizes the adjacency matrix only by adding ones to diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0e097cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138499, 8)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features initializaed randomly because not yet ready\n",
    "features_np = np.random.randn(G_train.number_of_nodes(), 8) # Generates node features randomly\n",
    "features_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4fbed3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138499, 138499)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d6458fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Create class labels\n",
    "y = np.zeros(4*G_train.number_of_edges())\n",
    "y[:2*G_train.number_of_edges()] = 1 # Concatenated ones for edges indices and zeros for random indices.\n",
    "\n",
    "# Transforms the numpy matrices/vectors to torch tensors.\n",
    "features = torch.FloatTensor(features_np).to(device)\n",
    "y = torch.LongTensor(y).to(device)\n",
    "print(type(adj))\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "print(type(adj))\n",
    "indices = torch.LongTensor(indices).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f5e5be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "14a08291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 20\n",
    "n_hidden = 128\n",
    "dropout_rate = 0.2\n",
    "n_class = 2\n",
    "n_features = features.shape[1]\n",
    "\n",
    "# Creates the model and specifies the optimizer\n",
    "model = GNN(n_features, n_hidden, n_class, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "daac32ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 0.6931 acc_train: 0.4967 time: 24.7683s total_time: 0min\n",
      "Epoch: 006 loss_train: 0.5850 acc_train: 0.6306 time: 20.8846s total_time: 2min\n",
      "Epoch: 011 loss_train: 0.5159 acc_train: 0.7750 time: 19.0353s total_time: 4min\n",
      "Epoch: 016 loss_train: 0.4665 acc_train: 0.7829 time: 18.0593s total_time: 5min\n",
      "Optimization Finished in 7 min!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Train model\n",
    "model.train()\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    t = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)# We take random indices each time we run an epoch\n",
    "    pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices.   \n",
    "    output = model(features, adj, pairs) # we run the model that gives the output.\n",
    "    loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "    #print(type(loss_train), '\\n', loss_train.shape)\n",
    "    acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "    loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "    optimizer.step() # Performs a single optimization step (parameter update).\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "              'time: {:.4f}s'.format(time.time() - t),\n",
    "             'total_time: {}min'.format(round((time.time() - start_time)/60)))\n",
    "\n",
    "print(\"Optimization Finished in {} min!\".format(round((time.time() - start_time)/60)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f7a25b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1965358])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e74c8f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_pairs = np.array(np.transpose(val_edges))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "output = model(features, adj, pairs)\n",
    "y_pred = torch.exp(output)\n",
    "y_pred = y_pred.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "932f2680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9594832 , 0.04051685], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b278d11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 1.029002465304126\n"
     ]
    }
   ],
   "source": [
    "print('Log loss:', log_loss(y_val, y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0a9d9fbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read test data. Each sample is a pair of nodes\u001b[39;00m\n\u001b[1;32m      2\u001b[0m node_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m         t \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test.txt'"
     ]
    }
   ],
   "source": [
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "output = model(features, adj, pairs)\n",
    "y_pred = torch.exp(output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "# Compute log loss\n",
    "y_test = np.loadtxt('y_test.txt', delimiter=',')[:,1]\n",
    "y_pred = y_pred[:,1]\n",
    "y_pred[y_pred>0.9999] = 0.9999\n",
    "y_pred[y_pred<0.0001] = 0.0001\n",
    "print('Log loss:', log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fbbba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4447804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adj = normalize_adjacency(adj) # Normalizes the adjacency matrix\n",
    "#indices = np.array(adj.nonzero())\n",
    "\n",
    "# Do we create the adjencency matrix based on the Training G ? And then we need to create one for the val G ?\n",
    "\n",
    "# Combine input and output to get indices matrix undirected for the undirected Graph.\n",
    "# You need to experiment with different hyperparameters and number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9842ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee79f472",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484c8b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features, adj, pairs\n",
    "rand_indices.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6f9afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m)\n",
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd9e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a non(?) directed graph\n",
    "G = nx.read_edgelist('edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "nodes = list(G.nodes())\n",
    "node_to_idx = dict()\n",
    "for i, node in enumerate(nodes):\n",
    "    node_to_idx[node] = i\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "print('Number of nodes:', n)\n",
    "print('Number of edges:', m)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 1000\n",
    "n_hidden = 256\n",
    "dropout_rate = 0.2\n",
    "\n",
    "n_class = 2\n",
    "n_nodes = G.number_of_nodes()\n",
    "adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix\n",
    "indices = np.array(adj.nonzero())\n",
    "adj = normalize_adjacency(adj) # Normalizes the adjacency matrix\n",
    "features_np = np.random.randn(n_nodes, 32) # Generates node features\n",
    "\n",
    "# Create class labels\n",
    "y = np.zeros(4*m)\n",
    "y[:2*m] = 1\n",
    "\n",
    "# Transforms the numpy matrices/vectors to torch tensors\n",
    "features = torch.FloatTensor(features_np).to(device)\n",
    "y = torch.LongTensor(y).to(device)\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "indices = torch.LongTensor(indices).to(device)\n",
    "\n",
    "# Creates the model and specifies the optimizer\n",
    "model = GNN(features.shape[1], n_hidden, n_class, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train model\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    t = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)\n",
    "    pairs = torch.cat((indices, rand_indices), dim=1)\n",
    "    output = model(features, adj, pairs)\n",
    "    loss_train = F.nll_loss(output, y)\n",
    "    acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print()\n",
    "\n",
    "# # Read test data. Each sample is a pair of nodes\n",
    "# node_pairs = list()\n",
    "# with open('test.txt', 'r') as f:\n",
    "#     for line in f:\n",
    "#         t = line.split(',')\n",
    "#         node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "# # Testing\n",
    "# model.eval()\n",
    "# node_pairs = np.array(np.transpose(node_pairs))\n",
    "# pairs = torch.LongTensor(node_pairs).to(device)\n",
    "# output = model(features, adj, pairs)\n",
    "# y_pred = torch.exp(output)\n",
    "# y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "# # Compute log loss\n",
    "# y_test = np.loadtxt('y_test.txt', delimiter=',')[:,1]\n",
    "# y_pred = y_pred[:,1]\n",
    "# y_pred[y_pred>0.9999] = 0.9999\n",
    "# y_pred[y_pred<0.0001] = 0.0001\n",
    "# print('Log loss:', log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2872d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "output = model(features, adj, pairs)\n",
    "y_pred = torch.exp(output)\n",
    "y_pred = y_pred.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b86084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588c6627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_df.head()\n",
    "\n",
    "y_pred_array = np.column_stack((range(len(y_pred)), y_pred_true))\n",
    "# y_pred_df = pd.DataFrame(y_pred)\n",
    "df_pred = pd.DataFrame({range(len(y_pred)), y_pred_true}, columns={'id','predicted'})#, columns={'id', 'predicted'}).astype({'id':'int'})\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5539c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(y_pred_array, columns={'id', 'predicted'}).astype({'id':'int'}).head()\n",
    "\n",
    "# pd.DataFrame(y_pred_array, columns={'id', 'predicted'}).astype({'id':'int'}).to_csv(\n",
    "# \"submission.csv\", header=True, index=False\n",
    "# )\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"submission.csv\", header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf97f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to a file\n",
    "predictions = zip(range(len(y_pred)), y_pred)\n",
    "with open(\"submission.csv\",\"w\") as pred:\n",
    "    csv_out = csv.writer(pred)\n",
    "    csv_out.writerow(['id','predicted'])\n",
    "    for row in y_pred_array:\n",
    "        csv_out.writerow(row) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22a327",
   "metadata": {},
   "source": [
    "CNN with Sigmoid to predict if they connected based on the abstract."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cad23c",
   "metadata": {},
   "source": [
    "A good way to aggregate the text is to calculate the average (mean)\n",
    "\n",
    "Another approach is to use directly a GNN. Take the abstract, compute the embeddng of the words. Take the mean of the node.\n",
    "Then you can take the features.\n",
    "\n",
    "For thr GN, we have only the \n",
    "\n",
    "pairs is a tensor, contains a pair of nodes that contains all the positive samples and some of the negative samples. y: half of them are equal to one, and half of them are connected. rand_indices are random pairs that are considered as not connected.\n",
    "\n",
    "As we have a non directed. We can take twice every edge (2*m instead of 2*m for y. Or we can take the edges only once.\n",
    "\n",
    "One vector for the abstract using the word2vec embedding or any other similar approach.\n",
    "\n",
    "Or we can directly use a CNN.We can take a CNN and feed pais of abstracts in the CNN, the CNN will produce one vector for the first abstract and one vector for the second. We can combine these two vectors.\n",
    "\n",
    "Then we can use an MLP to produce a vector, and then we can concatenate the two vectors from CNN and MLP.\n",
    "\n",
    "There is a pretrained word embedding (Google provided a pretrained embedding).\n",
    "\n",
    "Embedding of each word. Then the CNN will provide one vector for the abstract.\n",
    "\n",
    "Each abstract has a different number of words. the representation of the CNN will have a fixed size of the embedding vector.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
