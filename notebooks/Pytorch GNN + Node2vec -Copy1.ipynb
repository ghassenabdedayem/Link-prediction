{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46a7427c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 13:59:18.977272: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from random import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from random import choice\n",
    "from gensim.models import Word2Vec\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77e11dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import identity, diags\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fbf0d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "669cdbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adjacency(A):\n",
    "    n = A.shape[0]\n",
    "    A = A + identity(n)\n",
    "    degs = A.dot(np.ones(n))\n",
    "    print('degs shape:', degs.shape)\n",
    "    inv_degs = np.power(degs, -1)\n",
    "    print('inv_degs', inv_degs.shape)\n",
    "    D_inv = diags(inv_degs)\n",
    "    print('D_inv shape:', D_inv.shape)\n",
    "    A_hat = D_inv.dot(A)\n",
    "    print('A_hat shape:', A_hat.shape)\n",
    "    return A_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcf6da2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_normalize_adjacency(A):\n",
    "    n = A.shape[0]\n",
    "    A = A + identity(n)\n",
    "    #degs = A.dot(np.ones(n))\n",
    "    #inv_degs = np.power(degs, -1)\n",
    "    #D_inv = diags(inv_degs)\n",
    "    #A_hat = D_inv.dot(A)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "189d36bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    print(type(sparse_mx))\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "# Is it equivalent to Word2vec for dimensionality reduction ? --> NO !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56e0db3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_feat, n_hidden, n_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc4 = nn.Linear(n_hidden, n_class)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x_in, adj, pairs):\n",
    "        \n",
    "        h1 = self.fc1(x_in)\n",
    "        z1 = self.relu(torch.mm(adj, h1))\n",
    "        z1 = self.dropout(z1)\n",
    "\n",
    "        h2 = self.fc2(z1)\n",
    "        z2 = self.relu(torch.mm(adj, h2))\n",
    "        \n",
    "        x = z2[pairs[0,:],:] - z2[pairs[1,:],:]\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d22582c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes of training set: 138499\n",
      "Number of edges of training set: 982222\n"
     ]
    }
   ],
   "source": [
    "\n",
    "G = nx.read_edgelist('../input_data/edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "nodes = list(G.nodes())\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "edges = list(G.edges())\n",
    "\n",
    "val_edges = list()\n",
    "G_train = G\n",
    "\n",
    "for edge in edges:\n",
    "    if random() < 0.1:\n",
    "        val_edges.append(edge)\n",
    "\n",
    "# We remove the val edges from the graph G\n",
    "for edge in val_edges:\n",
    "    G_train.remove_edge(edge[0], edge[1])\n",
    "\n",
    "n = G_train.number_of_nodes()\n",
    "m = G_train.number_of_edges()\n",
    "train_edges = list(G_train.edges())\n",
    "    \n",
    "print('Number of nodes of training set:', n)\n",
    "print('Number of edges of training set:', m)\n",
    "\n",
    "y_val = [1]*len(val_edges)\n",
    "\n",
    "n_val_edges = len(val_edges)\n",
    "\n",
    "# Create random pairs of nodes\n",
    "for i in range(n_val_edges):\n",
    "    n1 = nodes[randint(0, n-1)]\n",
    "    n2 = nodes[randint(0, n-1)]\n",
    "    (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "    val_edges.append((n1, n2))\n",
    "    \n",
    "# Remove from val_edges edges that exist in both train and val\n",
    "\n",
    "for edge in list(set(val_edges) & set(train_edges)):\n",
    "    val_edges.remove(edge)\n",
    "    \n",
    "n_val_edges = len(val_edges) - len(y_val) #because we removed from val_edges edges that exist in both\n",
    "y_val.extend([0]*n_val_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08a930b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218715"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe3e518f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pd/glcnl2497w5b6xn3p94tnwlr0000gn/T/ipykernel_44001/65278059.py:1: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G_train) # Obtains the adjacency matrix of the training graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degs shape: (138499,)\n",
      "inv_degs (138499,)\n",
      "D_inv shape: (138499, 138499)\n",
      "A_hat shape: (138499, 138499)\n"
     ]
    }
   ],
   "source": [
    "adj = nx.adjacency_matrix(G_train) # Obtains the adjacency matrix of the training graph\n",
    "adj = normalize_adjacency(adj) # Normalizes the adjacency matrix only by adding ones to diag\n",
    "indices = np.array(adj.nonzero()) # Gets the positions of non zeros of adj into indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e097cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138499, 8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features initializaed randomly because not yet ready\n",
    "features_np = np.random.randn(G_train.number_of_nodes(), 8) # Generates node features randomly\n",
    "features_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fbed3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1964444"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_train.number_of_edges()*2 # = 1965036 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6458fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Create class labels\n",
    "y = np.zeros(4*G_train.number_of_edges())\n",
    "y[:2*G_train.number_of_edges()] = 1 # Concatenated ones for edges indices and zeros for random indices.\n",
    "\n",
    "# Transforms the numpy matrices/vectors to torch tensors.\n",
    "features = torch.FloatTensor(features_np).to(device)\n",
    "y = torch.LongTensor(y).to(device)\n",
    "print(type(adj))\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "print(type(adj))\n",
    "indices = torch.LongTensor(indices).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c77b8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'tocoo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msparse_mx_to_torch_sparse_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m, in \u001b[0;36msparse_mx_to_torch_sparse_tensor\u001b[0;34m(sparse_mx)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msparse_mx_to_torch_sparse_tensor\u001b[39m(sparse_mx):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(sparse_mx))\n\u001b[0;32m----> 3\u001b[0m     sparse_mx \u001b[38;5;241m=\u001b[39m \u001b[43msparse_mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtocoo\u001b[49m()\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      4\u001b[0m     indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mvstack((sparse_mx\u001b[38;5;241m.\u001b[39mrow, sparse_mx\u001b[38;5;241m.\u001b[39mcol))\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint64))\n\u001b[1;32m      5\u001b[0m     values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(sparse_mx\u001b[38;5;241m.\u001b[39mdata)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'tocoo'"
     ]
    }
   ],
   "source": [
    "sparse_mx_to_torch_sparse_tensor(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76f5e5be",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (668683560.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[24], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14a08291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 20\n",
    "n_hidden = 128\n",
    "dropout_rate = 0.2\n",
    "n_class = 2\n",
    "n_features = features.shape[1]\n",
    "\n",
    "# Creates the model and specifies the optimizer\n",
    "model = GNN(n_features, n_hidden, n_class, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "daac32ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> \n",
      " torch.Size([])\n",
      "Epoch: 001 loss_train: 59.4502 acc_train: 0.4955 time: 15.8311s total_time: 0min\n",
      "Optimization Finished in 0 min!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Train model\n",
    "model.train()\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    t = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)# We take random indices each time we run an epoch\n",
    "    pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices.   \n",
    "    output = model(features, adj, pairs) # we run the model that gives the output.\n",
    "    loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "    print(type(loss_train), '\\n', loss_train.shape)\n",
    "    acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "    loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "    optimizer.step() # Performs a single optimization step (parameter update).\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "              'time: {:.4f}s'.format(time.time() - t),\n",
    "             'total_time: {}min'.format(round((time.time() - start_time)/60)))\n",
    "\n",
    "print(\"Optimization Finished in {} min!\".format(round((time.time() - start_time)/60)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7a25b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method backward in module torch._tensor:\n",
      "\n",
      "backward(gradient=None, retain_graph=None, create_graph=False, inputs=None) method of torch.Tensor instance\n",
      "    Computes the gradient of current tensor w.r.t. graph leaves.\n",
      "    \n",
      "    The graph is differentiated using the chain rule. If the tensor is\n",
      "    non-scalar (i.e. its data has more than one element) and requires\n",
      "    gradient, the function additionally requires specifying ``gradient``.\n",
      "    It should be a tensor of matching type and location, that contains\n",
      "    the gradient of the differentiated function w.r.t. ``self``.\n",
      "    \n",
      "    This function accumulates gradients in the leaves - you might need to zero\n",
      "    ``.grad`` attributes or set them to ``None`` before calling it.\n",
      "    See :ref:`Default gradient layouts<default-grad-layouts>`\n",
      "    for details on the memory layout of accumulated gradients.\n",
      "    \n",
      "    .. note::\n",
      "    \n",
      "        If you run any forward ops, create ``gradient``, and/or call ``backward``\n",
      "        in a user-specified CUDA stream context, see\n",
      "        :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.\n",
      "    \n",
      "    .. note::\n",
      "    \n",
      "        When ``inputs`` are provided and a given input is not a leaf,\n",
      "        the current implementation will call its grad_fn (though it is not strictly needed to get this gradients).\n",
      "        It is an implementation detail on which the user should not rely.\n",
      "        See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.\n",
      "    \n",
      "    Args:\n",
      "        gradient (Tensor or None): Gradient w.r.t. the\n",
      "            tensor. If it is a tensor, it will be automatically converted\n",
      "            to a Tensor that does not require grad unless ``create_graph`` is True.\n",
      "            None values can be specified for scalar Tensors or ones that\n",
      "            don't require grad. If a None value would be acceptable then\n",
      "            this argument is optional.\n",
      "        retain_graph (bool, optional): If ``False``, the graph used to compute\n",
      "            the grads will be freed. Note that in nearly all cases setting\n",
      "            this option to True is not needed and often can be worked around\n",
      "            in a much more efficient way. Defaults to the value of\n",
      "            ``create_graph``.\n",
      "        create_graph (bool, optional): If ``True``, graph of the derivative will\n",
      "            be constructed, allowing to compute higher order derivative\n",
      "            products. Defaults to ``False``.\n",
      "        inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be\n",
      "            accumulated into ``.grad``. All other Tensors will be ignored. If not\n",
      "            provided, the gradient is accumulated into all the leaf Tensors that were\n",
      "            used to compute the attr::tensors.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(loss_train.backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74c8f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_pairs = np.array(np.transpose(val_edges))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "output = model(features, adj, pairs)\n",
    "y_pred = torch.exp(output)\n",
    "y_pred = y_pred.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f2680",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[190000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b278d11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Log loss:', log_loss(y_val, y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d9fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "output = model(features, adj, pairs)\n",
    "y_pred = torch.exp(output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "# Compute log loss\n",
    "y_test = np.loadtxt('y_test.txt', delimiter=',')[:,1]\n",
    "y_pred = y_pred[:,1]\n",
    "y_pred[y_pred>0.9999] = 0.9999\n",
    "y_pred[y_pred<0.0001] = 0.0001\n",
    "print('Log loss:', log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fbbba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4447804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adj = normalize_adjacency(adj) # Normalizes the adjacency matrix\n",
    "#indices = np.array(adj.nonzero())\n",
    "\n",
    "# Do we create the adjencency matrix based on the Training G ? And then we need to create one for the val G ?\n",
    "\n",
    "# Combine input and output to get indices matrix undirected for the undirected Graph.\n",
    "# You need to experiment with different hyperparameters and number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9842ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee79f472",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484c8b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features, adj, pairs\n",
    "rand_indices.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6f9afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m)\n",
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd9e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a non(?) directed graph\n",
    "G = nx.read_edgelist('edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "nodes = list(G.nodes())\n",
    "node_to_idx = dict()\n",
    "for i, node in enumerate(nodes):\n",
    "    node_to_idx[node] = i\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "print('Number of nodes:', n)\n",
    "print('Number of edges:', m)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 1000\n",
    "n_hidden = 256\n",
    "dropout_rate = 0.2\n",
    "\n",
    "n_class = 2\n",
    "n_nodes = G.number_of_nodes()\n",
    "adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix\n",
    "indices = np.array(adj.nonzero())\n",
    "adj = normalize_adjacency(adj) # Normalizes the adjacency matrix\n",
    "features_np = np.random.randn(n_nodes, 32) # Generates node features\n",
    "\n",
    "# Create class labels\n",
    "y = np.zeros(4*m)\n",
    "y[:2*m] = 1\n",
    "\n",
    "# Transforms the numpy matrices/vectors to torch tensors\n",
    "features = torch.FloatTensor(features_np).to(device)\n",
    "y = torch.LongTensor(y).to(device)\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "indices = torch.LongTensor(indices).to(device)\n",
    "\n",
    "# Creates the model and specifies the optimizer\n",
    "model = GNN(features.shape[1], n_hidden, n_class, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train model\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    t = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)\n",
    "    pairs = torch.cat((indices, rand_indices), dim=1)\n",
    "    output = model(features, adj, pairs)\n",
    "    loss_train = F.nll_loss(output, y)\n",
    "    acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print()\n",
    "\n",
    "# # Read test data. Each sample is a pair of nodes\n",
    "# node_pairs = list()\n",
    "# with open('test.txt', 'r') as f:\n",
    "#     for line in f:\n",
    "#         t = line.split(',')\n",
    "#         node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "# # Testing\n",
    "# model.eval()\n",
    "# node_pairs = np.array(np.transpose(node_pairs))\n",
    "# pairs = torch.LongTensor(node_pairs).to(device)\n",
    "# output = model(features, adj, pairs)\n",
    "# y_pred = torch.exp(output)\n",
    "# y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "# # Compute log loss\n",
    "# y_test = np.loadtxt('y_test.txt', delimiter=',')[:,1]\n",
    "# y_pred = y_pred[:,1]\n",
    "# y_pred[y_pred>0.9999] = 0.9999\n",
    "# y_pred[y_pred<0.0001] = 0.0001\n",
    "# print('Log loss:', log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2872d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "output = model(features, adj, pairs)\n",
    "y_pred = torch.exp(output)\n",
    "y_pred = y_pred.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b86084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588c6627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_df.head()\n",
    "\n",
    "y_pred_array = np.column_stack((range(len(y_pred)), y_pred_true))\n",
    "# y_pred_df = pd.DataFrame(y_pred)\n",
    "df_pred = pd.DataFrame({range(len(y_pred)), y_pred_true}, columns={'id','predicted'})#, columns={'id', 'predicted'}).astype({'id':'int'})\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5539c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(y_pred_array, columns={'id', 'predicted'}).astype({'id':'int'}).head()\n",
    "\n",
    "# pd.DataFrame(y_pred_array, columns={'id', 'predicted'}).astype({'id':'int'}).to_csv(\n",
    "# \"submission.csv\", header=True, index=False\n",
    "# )\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"submission.csv\", header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf97f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to a file\n",
    "predictions = zip(range(len(y_pred)), y_pred)\n",
    "with open(\"submission.csv\",\"w\") as pred:\n",
    "    csv_out = csv.writer(pred)\n",
    "    csv_out.writerow(['id','predicted'])\n",
    "    for row in y_pred_array:\n",
    "        csv_out.writerow(row) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22a327",
   "metadata": {},
   "source": [
    "CNN with Sigmoid to predict if they connected based on the abstract."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cad23c",
   "metadata": {},
   "source": [
    "A good way to aggregate the text is to calculate the average (mean)\n",
    "\n",
    "Another approach is to use directly a GNN. Take the abstract, compute the embeddng of the words. Take the mean of the node.\n",
    "Then you can take the features.\n",
    "\n",
    "For thr GN, we have only the \n",
    "\n",
    "pairs is a tensor, contains a pair of nodes that contains all the positive samples and some of the negative samples. y: half of them are equal to one, and half of them are connected. rand_indices are random pairs that are considered as not connected.\n",
    "\n",
    "As we have a non directed. We can take twice every edge (2*m instead of 2*m for y. Or we can take the edges only once.\n",
    "\n",
    "One vector for the abstract using the word2vec embedding or any other similar approach.\n",
    "\n",
    "Or we can directly use a CNN.We can take a CNN and feed pais of abstracts in the CNN, the CNN will produce one vector for the first abstract and one vector for the second. We can combine these two vectors.\n",
    "\n",
    "Then we can use an MLP to produce a vector, and then we can concatenate the two vectors from CNN and MLP.\n",
    "\n",
    "There is a pretrained word embedding (Google provided a pretrained embedding).\n",
    "\n",
    "Embedding of each word. Then the CNN will provide one vector for the abstract.\n",
    "\n",
    "Each abstract has a different number of words. the representation of the CNN will have a fixed size of the embedding vector.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
