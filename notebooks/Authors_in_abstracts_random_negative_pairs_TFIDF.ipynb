{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dF0MJHXZVRpt"
   },
   "source": [
    "# Packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5GfuP1hlTmg",
    "outputId": "b655d8cd-a8e8-4766-8d27-d5fcb463db72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /Users/ghassenabdedayem/opt/anaconda3/lib/python3.8/site-packages (1.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from random import choice\n",
    "from scipy.sparse import identity, diags\n",
    "from unidecode import unidecode\n",
    "from urllib.request import urlopen\n",
    "import gzip\n",
    "import pickle\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "import io\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BJSlSwejljfp"
   },
   "outputs": [],
   "source": [
    "def save_subgraph_in_file(nbr_nodes, source_path='../input_data/edgelist.txt', destination_path='../input_data/small_edgelist.txt'):\n",
    "    G = nx.read_edgelist(source_path, delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "    G = G.subgraph(range(nbr_nodes))\n",
    "    nx.write_edgelist(G, path=destination_path, delimiter=',')\n",
    "    print(G.number_of_nodes(), 'nodes,', G.number_of_edges(), 'edges Graph extracted from', source_path[source_path.rfind('/')+1:])\n",
    "    G = nx.read_edgelist(destination_path, delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "    print(G.number_of_nodes(), 'nodes,', G.number_of_edges(), 'edges Graph saved in', destination_path[destination_path.rfind('/')+1:])\n",
    "    print(max(G.nodes))\n",
    "    return\n",
    "\n",
    "\n",
    "def read_train_val_graph(path='../input_data/edgelist.txt', shuffle=True, val_ratio=0.1):\n",
    "    #gets the data from the file on the distant server\n",
    "    G = nx.read_edgelist(urlopen('https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/edgelist.txt'), delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "    nodes = list(G.nodes())\n",
    "    edges = list(G.edges())\n",
    "    n = G.number_of_nodes()\n",
    "    m = G.number_of_edges()\n",
    "    \n",
    "    permutation = np.array(range(n))\n",
    "    mapping_permutation = dict(zip(range(n), range(n)))\n",
    "    if shuffle:\n",
    "        # shuffle the order of the edges without changing the labels\n",
    "        random.shuffle(edges)\n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(edges)\n",
    "        permutation = np.random.permutation(n)\n",
    "        print(type(permutation))\n",
    "        # create a mapping from old nodes labels to new nodes labels\n",
    "        mapping_permutation = dict(zip(range(n), permutation))\n",
    "\n",
    "        # shuffle G node labels according to the permutation\n",
    "        G = nx.relabel_nodes(G, mapping_permutation)    \n",
    "        \n",
    "        edges = list(G.edges())\n",
    "        nodes = list(G.nodes())\n",
    "\n",
    "\n",
    "    print('Number of nodes:', n, 'number of edges:', m,'in the Complete set')\n",
    "\n",
    "    node_to_idx = dict()\n",
    "    for i, node in enumerate(nodes):\n",
    "        node_to_idx[node] = i\n",
    "\n",
    "    val_edges = list()\n",
    "    G_train = G.copy()\n",
    "\n",
    "    for edge in edges:\n",
    "        if random.random() < val_ratio and edge[0] < n and edge[1] < n:\n",
    "            val_edges.append(edge)\n",
    "            G_train.remove_edge(edge[0], edge[1]) # We remove the val edges from the graph G\n",
    "\n",
    "   \n",
    "    #for edge in val_edges:\n",
    "        \n",
    "\n",
    "    n = G_train.number_of_nodes()\n",
    "    m = G_train.number_of_edges()\n",
    "    train_edges = list(G_train.edges())\n",
    "\n",
    "    print('Number of nodes:', n, 'number of edges:', m, 'in the Training set')\n",
    "    print('len(nodes)', len(nodes))\n",
    "\n",
    "    y_val = [1]*len(val_edges)\n",
    "\n",
    "    n_val_edges = len(val_edges)\n",
    "    \n",
    "    print('Creating random val_edges...')\n",
    "    for i in range(n_val_edges):\n",
    "        n1 = nodes[randint(0, n-1)]\n",
    "        n2 = nodes[randint(0, n-1)]\n",
    "        (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "        while n2 >= n: #or (n1, n2) in train_edges:\n",
    "            if (n1, n2) in train_edges:\n",
    "                print((n1, n2), 'in train_edges:')\n",
    "            n1 = nodes[randint(0, n-1)]\n",
    "            n2 = nodes[randint(0, n-1)]\n",
    "            (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "        val_edges.append((n1, n2))\n",
    "\n",
    "    y_val.extend([0]*(n_val_edges))\n",
    "    \n",
    "    ### From Giannis /!\\\n",
    "    val_indices = np.zeros((2,len(val_edges)))\n",
    "    for i,edge in enumerate(val_edges):\n",
    "        val_indices[0,i] = node_to_idx[edge[0]]\n",
    "        val_indices[1,i] = node_to_idx[edge[1]]\n",
    "    \n",
    "    print('Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects')\n",
    "    print('Loaded from', path[path.rfind('/')+1:], 'and with a training validation split ratio =', val_ratio)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx, permutation, mapping_permutation\n",
    "\n",
    "def random_walk(G, node, walk_length):\n",
    "    walk = [node]\n",
    "  \n",
    "    for i in range(walk_length-1):\n",
    "        neibor_nodes = list(G.neighbors(walk[-1]))\n",
    "        if len(neibor_nodes) > 0:\n",
    "            next_node = choice(neibor_nodes)\n",
    "            walk.append(next_node)\n",
    "    walk = [node for node in walk] # in case the nodes are in string format, we don't need to cast into string, but if the nodes are in numeric or integer, we need this line to cast into string\n",
    "    return walk\n",
    "\n",
    "\n",
    "def generate_walks(G, num_walks, walk_length):\n",
    "  # Runs \"num_walks\" random walks from each node, and returns a list of all random walk\n",
    "    t = time()\n",
    "    print('Start generating walks....')\n",
    "    walks = list()  \n",
    "    for i in range(num_walks):\n",
    "        for node in G.nodes():\n",
    "            walk = random_walk(G, node, walk_length)\n",
    "            walks.append(walk)\n",
    "        #print('walks : ', walks)\n",
    "    print('Random walks generated in in {}s!'.format(round(time()-t)))\n",
    "    return walks\n",
    "\n",
    "def apply_word2vec_on_features(features, nodes, vector_size=128, window=5, min_count=0, sg=1, workers=8):\n",
    "    t = time()\n",
    "    print('Start applying Word2Vec...')\n",
    "    wv_model = Word2Vec(vector_size=vector_size, window=window, min_count=min_count, sg=sg, workers=workers)\n",
    "    wv_model.build_vocab(features)\n",
    "    wv_model.train(features, total_examples=wv_model.corpus_count, epochs=5) \n",
    "    print('Word2vec model trained on features in {} min!'.format(round((time()-t)/60)))\n",
    "    features_np = []\n",
    "    for node in nodes:\n",
    "        features_np.append(wv_model.wv[node])\n",
    "\n",
    "    features_np = np.array(features_np)\n",
    "    print(features_np.shape, 'features numpy array created in {} min!'.format(round((time()-t)/60)))\n",
    "    return features_np\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def normalize_adjacency(A):\n",
    "    n = A.shape[0]\n",
    "    A = A + identity(n)\n",
    "    degs = A.dot(np.ones(n))\n",
    "    inv_degs = np.power(degs, -1)\n",
    "    D_inv = diags(inv_degs)\n",
    "    A_hat = D_inv.dot(A)\n",
    "    return A_hat\n",
    "\n",
    "\n",
    "# a proposed adj normalization, but we will keep the original one in the function after\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "\n",
    "\n",
    "def create_and_normalize_adjacency(G):\n",
    "    adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix of the training graph\n",
    "    #adj = normalize_adjacency(adj)\n",
    "    adj = normalize_adj(adj)\n",
    "    print('Created a normalized adjancency matrix of shape', adj.shape)\n",
    "    indices = np.array(adj.nonzero()) # Gets the positions of non zeros of adj into indices\n",
    "    print('Created indices', indices.shape, 'with the positions of non zeros in adj matrix')\n",
    "    return adj, indices\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "def text_to_list(text):\n",
    "    text = unidecode(text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s.,]\", \"\", text)\n",
    "    return text.split(',')\n",
    "\n",
    "def intersection(lst1, lst2): # a function that returns the number of common items of two lists and 1 or 0 if there are common. This function will be used in add_authors_to_pairs to add this features to the pairs.\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    is_common = 1 if len(lst3)>0 else 0\n",
    "    return len(lst3)+1, is_common+1\n",
    "\n",
    "\n",
    "def add_authors_to_pairs (pairs, authors):\n",
    "    authors = pd.DataFrame(authors)\n",
    "    try: \n",
    "        pairs = pairs.detach().cpu().numpy()\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "\n",
    "    pairs_df = pd.DataFrame(np.transpose(pairs)).rename(columns={0: \"paper_1\", 1: \"paper_2\"})\n",
    "    pairs_df = pairs_df.merge(authors, left_on='paper_1', right_on='paper_permut', how='left').rename(columns={'authors': \"authors_1\"})\n",
    "    pairs_df = pairs_df.merge(authors, left_on='paper_2', right_on='paper_permut', how='left').rename(columns={'authors': \"authors_2\"})\n",
    "    pairs_df.drop(['paper_id_x', 'paper_id_y'], axis=1, inplace=True)\n",
    "\n",
    "    pairs_df['nb_common_author'] = pairs_df.apply(lambda row: intersection(row['authors_1'], row['authors_2'])[0], axis=1)\n",
    "    pairs_df['is_common_author'] = pairs_df.apply(lambda row: intersection(row['authors_1'], row['authors_2'])[1], axis=1)\n",
    "\n",
    "    pairs_tensor = torch.LongTensor(np.transpose(pairs_df[[\"paper_1\", \"paper_2\", 'is_common_author', 'nb_common_author']].values.tolist())).to(device)\n",
    "    \n",
    "    return pairs_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HLahMzTSg4Xz"
   },
   "outputs": [],
   "source": [
    "def read_and_clean_abstracts (nodes, sample_length=-1, abstracts_path = 'https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/abstracts.txt'):\n",
    "    t = time()\n",
    "    abstracts = dict()\n",
    "    abstracts_list = list()\n",
    "    f = urlopen(abstracts_path)\n",
    "    \n",
    "    for i, line in tqdm(enumerate(f)):\n",
    "        if i == sample_length:\n",
    "            break\n",
    "        if i in nodes:\n",
    "            node, abstract = str(line).lower().split('|--|')\n",
    "            abstract = remove_stopwords(abstract)\n",
    "            #abstract = re.sub(r\"[,.;@#?!&$()-]\", \" \", abstract)\n",
    "            abstract = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", abstract)\n",
    "            #abstract = re.sub(r\"\\\\\", \" \", abstract)\n",
    "            abstract = remove_stopwords(abstract)\n",
    "\n",
    "            for word in abstract.split()[:-1]:\n",
    "                #abstract = abstract.replace(word, stemmer.stem(word))\n",
    "                abstract = abstract.replace(word, lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word), pos='s'), pos='a'), pos='n'), pos='v'), pos='r'))\n",
    "            \n",
    "            node = re.sub(\"[^0-9]\", \"\", node)\n",
    "            if i != int(node):\n",
    "                print('i and node not the same', i, node)\n",
    "            abstracts[int(node)] = abstract\n",
    "            abstracts_list.append(abstract)\n",
    "        \n",
    "    print('Text loaded and cleaned in {:.0f} min'.format((time()-t)/60))\n",
    "    return abstracts\n",
    "\n",
    "def doc_counter (documents, word): #a function that return the number of documents containing a word\n",
    "    counter = 0\n",
    "    for i in documents:\n",
    "        if word in documents[i]:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8HP9n7D5g4Xz"
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        self.word_occurrence = {}\n",
    "        self.word2node = {}\n",
    "        self.words_list = []\n",
    "        self.sentences_list = []\n",
    "        self.sentences_list_words = []\n",
    "        self.num_words = 0\n",
    "        self.num_sentences = 0\n",
    "        self.longest_sentence = 0\n",
    "\n",
    "    def add_word(self, word, node):\n",
    "        if word not in self.word2index:\n",
    "            # First entry of word into vocabulary\n",
    "            self.words_list.append(word)\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "            self.word_occurrence[word] = 1\n",
    "            self.word2node[word] = [node]\n",
    "        else:\n",
    "            # Word exists; increase word count\n",
    "            self.word2count[word] += 1\n",
    "            self.word_occurrence[word] += 1\n",
    "            if node not in self.word2node[word]:\n",
    "                self.word2node[word].append(node)\n",
    "            # self.num_words += 1\n",
    "            \n",
    "    def add_sentence(self, sentence, node):\n",
    "        sentence_len = 0\n",
    "        for word in sentence.split()[:-1]:\n",
    "            sentence_len += 1\n",
    "            self.add_word(word, node)\n",
    "        if sentence_len > self.longest_sentence:\n",
    "            # This is the longest sentence\n",
    "            self.longest_sentence = sentence_len\n",
    "        # Count the number of sentences\n",
    "        self.num_sentences += 1\n",
    "        self.sentences_list.append(sentence)\n",
    "        self.sentences_list_words.append(sentence.split()[:-1])\n",
    "\n",
    "    def to_word(self, index):\n",
    "        return self.index2word[index]\n",
    "\n",
    "    def to_index(self, word):\n",
    "        return self.word2index[word]\n",
    "\n",
    "    def words(self):\n",
    "        return self.words_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "frfxHnMT0zNn"
   },
   "outputs": [],
   "source": [
    "def prepare_data_to_train (features, authors, adj, auth_matrix, indices, val_indices, y_val):\n",
    "    \n",
    "    print('Preparing the data for training...')\n",
    "    \n",
    "    t = time()\n",
    "    \n",
    "    y_val = torch.LongTensor(y_val).to(device)\n",
    "\n",
    "    # Create class labels\n",
    "    y = np.zeros(2*indices.shape[1])\n",
    "    y[:indices.shape[1]] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
    "    y = torch.LongTensor(y).to(device)\n",
    "    \n",
    "    features = torch.FloatTensor(features).to(device)\n",
    "    \n",
    "    indices = torch.LongTensor(indices).to(device)\n",
    "    val_indices = torch.LongTensor(val_indices).to(device)\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "    auth_matrix = sparse_mx_to_torch_sparse_tensor(auth_matrix).to(device)\n",
    "    #tfidf_matrix = sparse_mx_to_torch_sparse_tensor(tfidf_matrix).to(device)\n",
    "    \n",
    "    # the function add_authors_to_pairs converts into torch tensors and sends to Device    \n",
    "    #val_indices = add_authors_to_pairs(val_indices, authors) #we add the authors to val_pairs\n",
    "    #indices = add_authors_to_pairs(indices, authors) #we add the authors to indices    \n",
    "    #rand_indices = np.random.randint(0, features.shape[0], (indices.shape[0],indices.shape[1]))# We take random indices each time we run an epoch\n",
    "    #rand_indices = add_authors_to_pairs(rand_indices, authors)\n",
    "\n",
    "    #pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices.\n",
    "    #indices = torch.LongTensor(indices).to(device)\n",
    "    #del(authors, indices, rand_indices)\n",
    "    \n",
    "    print('Data converted into torch tensors and authors added to indices in {:.0f} min'.format((time()-t)/60))\n",
    "\n",
    "    return features, adj, auth_matrix, indices, y, val_indices, y_val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VBxraTeM0zNn"
   },
   "outputs": [],
   "source": [
    "def map_features_with_permutation(features, permutation):\n",
    "    new_features = np.zeros((len(features), len(features[0])))\n",
    "    for i in range(len(features)):\n",
    "        new_features[i] = features[permutation[i]]\n",
    "    return new_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ubeigqung4X0"
   },
   "source": [
    "# Load graph and authors data from sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oe6DhotaSuqg",
    "outputId": "9b3c3b2d-8645-455a-88dc-1f8005764a26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499 number of edges: 1091955 in the Complete set\n",
      "Number of nodes: 138499 number of edges: 982833 in the Training set\n",
      "len(nodes) 138499\n",
      "Creating random val_edges...\n",
      "Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects\n",
      "Loaded from edgelist.txt and with a training validation split ratio = 0.1\n",
      "graph loaded and seperated, val indices generated and node to index mapping returned in 11 s\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "shuffle = False\n",
    "G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx, permutation, mapping_permutation = read_train_val_graph(val_ratio=0.1, shuffle=shuffle)\n",
    "\n",
    "print('graph loaded and seperated, val indices generated and node to index mapping returned in {:.0f} s'.format(time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EyBibxd6zQhV",
    "outputId": "2dc19344-a28c-4f59-a835-7846b8c109e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pd/glcnl2497w5b6xn3p94tnwlr0000gn/T/ipykernel_34535/2408168445.py:163: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix of the training graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a normalized adjancency matrix of shape (138499, 138499)\n",
      "Created indices (2, 1965666) with the positions of non zeros in adj matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pd/glcnl2497w5b6xn3p94tnwlr0000gn/T/ipykernel_34535/2408168445.py:155: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n"
     ]
    }
   ],
   "source": [
    "adj, indices = create_and_normalize_adjacency(G_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "dWAzHqmxmYff",
    "outputId": "030ce381-c869-4555-e0d5-364e8c57dff6",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>authors</th>\n",
       "      <th>paper_permut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[James H. Niblock, JianXun Peng, Karen R. McMe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[JianXun Peng, Kang Li, DeShuang Huang]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[J. Heikkila]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[Long Zhang, Kang Li, ErWei Bai, George W. Irwin]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id                                            authors  paper_permut\n",
       "0         0  [James H. Niblock, JianXun Peng, Karen R. McMe...             0\n",
       "1         1            [JianXun Peng, Kang Li, DeShuang Huang]             1\n",
       "2         2                                      [J. Heikkila]             2\n",
       "3         3    [L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]             3\n",
       "4         4  [Long Zhang, Kang Li, ErWei Bai, George W. Irwin]             4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors = pd.read_csv(urlopen('https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/authors.txt'), sep = '|', header=None)\n",
    "authors = authors.rename(columns={0: \"paper_id\", 2: \"authors\"})\n",
    "authors['authors'] = authors['authors'].apply(text_to_list)\n",
    "authors = authors[[\"paper_id\", \"authors\"]]\n",
    "authors = authors[authors['paper_id'] <= max(G.nodes())]\n",
    "authors['paper_permut'] = permutation[authors['paper_id']]\n",
    "authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130,
     "referenced_widgets": [
      "b6221c60dfdb4309b6d69db47f5621f1",
      "b768368f52cb4f3d9539caf48fc459c1",
      "1888371c988242c48ba436784b533166",
      "f9282ecb07174c38be1a3dc45ea4a5c2",
      "5faac6ba5e604a3198ad9a5c49f1e2a7",
      "0b776c922cd9412a8dfd8d49ae48bf43",
      "15f1079aff04427d9eab2d7c97fb014e",
      "3f0f112951ca4057960fe2bc33b10acd",
      "029723804c3b420bacce0e1ddd6bc87a",
      "059d6431e6f14c66b9fc272836660c56",
      "ef74a17c91af4def9f5082d345ba5b33",
      "aa38ef49eae14de1800c0d7fb216754b",
      "121f5c36582c4275807e15cb4ba84f30",
      "2c2b5fbf8e41412c949964f2b6cef0d7",
      "7c4a409918d24f71a2b24ea4bf9b1049",
      "8b37044a7d4a4da1adfd9f40b9e27303",
      "dea8d64c83754027b88f54ebc4ffd50c",
      "f428b21f4ed24ac49092742038b20606",
      "6da88af0c4414d758f684ff1258953bd",
      "74cbbff36ac0492b937ad026df2803b1",
      "35ed948eaffd448eb483d863af5b492c",
      "2b681147457b4f859a7d9274012a004a",
      "9b86291845ec4f5b83c8764ed836289e",
      "2cb1d8e27a5346388abcebec1a1ece58",
      "acaa2745e8fc468fadcd0fcc0ff9d324",
      "e7460e2510704b458993133be5fc07bf",
      "c5812332ac6849d1b6e0b28f40d98f5f",
      "d524bbfc74a346f6954f15dad0eeedf1",
      "980cc9d1a24d459f901915403956e030",
      "7bd39cd6acfb4f21ace5dd23f1a82360",
      "6b26bf5844884b34b5cc3c2b8765f6d5",
      "18700b2d971f42d5973f0573dbe397c3",
      "88102ef32915417da9df4562ec493adb"
     ]
    },
    "id": "YQw6HQUT9c_t",
    "outputId": "870561ed-320a-420e-cfda-12833c817a7e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad2732f543f5419c8f8afb3cd923f967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca986526d89340759df4b826cf5bc84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f42b536452143b4b5b13158d5b7577c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138499, 147481)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "# get the unique list of authors\n",
    "authors_lst_ppr = list(set([a for authors_list in tqdm(authors['authors']) for a in authors_list]))\n",
    "\n",
    "# create a mapping of author to index\n",
    "author_to_index = {author: i for i, author in tqdm(enumerate(authors_lst_ppr))}\n",
    "\n",
    "# create an empty sparse matrix\n",
    "nrows = len(authors)\n",
    "ncols = len(authors_lst_ppr)\n",
    "data = np.ones(nrows)\n",
    "row_ind = np.arange(nrows)\n",
    "col_ind = np.zeros(nrows)\n",
    "\n",
    "# fill in the sparse matrix with 1 where authors appear\n",
    "for i, authors_list in tqdm(enumerate(authors['authors'])):\n",
    "    for author in authors_list:\n",
    "        col_ind[i] = author_to_index[author]\n",
    "        row_ind[i] = i\n",
    "        data[i] = 1\n",
    "auth_matrix = csr_matrix((data, (row_ind, col_ind)), shape=(nrows, ncols))\n",
    "\n",
    "# print the resulting sparse matrix\n",
    "print(auth_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JianXun Peng', 'Kang Li', 'DeShuang Huang']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors['authors'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ahaTr3Q0C5Vb",
    "outputId": "243d3042-30c6-4530-da6a-e769a8448eb5"
   },
   "outputs": [],
   "source": [
    "# get the indices of non-zero elements\n",
    "row_idx, col_idx = auth_matrix.nonzero()\n",
    "\n",
    "# display the first non-zero element\n",
    "print([row_idx[0], col_idx[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06e29879d12464b906c214f08549fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text loaded and cleaned in 7 min\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "n = -1 #length of the sample to develop and test the pipeline (-1 or negative values to take all the dataset)\n",
    "\n",
    "#takes 4 minutes to process all the abstracts\n",
    "abstracts = read_and_clean_abstracts(nodes, sample_length=n)  #149s #194s\n",
    "#abstracts_dict_list_words = {i: abstracts[i].split()[:-1] for i in nodes}\n",
    "#abstracts_list_sentences = [list(item)[1][:-3] for item in abstracts.items()]\n",
    "\n",
    "#we create a vacabulary of words and sentences (abstracts)\n",
    "#we take only a sample of 3 abstracts (i=2) to explore the approach\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paper propose novel hybrid forward algorithm hfa construction radial basis function rbf neural network tunable node main objective efficiently effectively produce parsimonious rbf neural network generalize study achieve simultaneous network structure determination parameter optimization continuous parameter space mix integer hard problem propose hfa tackle problem integrate analytic framework lead significantly improve network performance reduce memory usage network construction computational complexity analysis confirm efficiency propose algorithm simulation result demonstrate effectivenessn'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81ce6a73cd74a26ae7a21732957f926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_auth(text):\n",
    "    text = unidecode(text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s.,]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "authors = pd.read_csv(urlopen('https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/authors.txt'), sep = '|', header=None)\n",
    "authors = authors.rename(columns={0: \"paper_id\", 2: \"authors\"})\n",
    "authors['authors'] = authors['authors'].apply(text_to_list)\n",
    "authors = authors[[\"paper_id\", \"authors\"]]\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(abstracts))):\n",
    "    for author in authors['authors'][i]:\n",
    "        abstracts[i] = author + \" \" + abstracts[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['James H. Niblock', 'JianXun Peng', 'Karen R. McMenemy', 'George W. Irwin']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors['authors'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pietro Morerio'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors['authors'][i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27994cc1f1d34155a7c49c7ef4d115a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaned and vocab built in 45 min\n"
     ]
    }
   ],
   "source": [
    "voc_auth = Vocabulary('abstracts') \n",
    "for i in tqdm(nodes):\n",
    "    voc_auth.add_sentence(abstracts[i], i)\n",
    "\n",
    "print('Text cleaned and vocab built in {:.0f} min'.format((time()-t)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['George W. Irwin Karen R. McMenemy JianXun Peng James H. Niblock development automate quality assessment aerodrome grind light agl accordance associate standard recommendation present compose image sensor place inside cockpit aircraft record image agl normal descent aerodrome modelbased methodology ascertain optimum match template agl actual image data order calculate position orientation camera instant image acquire camera position orientation data pixel grey level image luminaire estimate value luminous intensity give luminaire compare expect brightness luminaire ensure operate require standard metric quality agl pattern determine experiment real image data present demonstrate application effectiveness systemn',\n",
       " 'DeShuang Huang Kang Li JianXun Peng paper propose novel hybrid forward algorithm hfa construction radial basis function rbf neural network tunable node main objective efficiently effectively produce parsimonious rbf neural network generalize study achieve simultaneous network structure determination parameter optimization continuous parameter space mix integer hard problem propose hfa tackle problem integrate analytic framework lead significantly improve network performance reduce memory usage network construction computational complexity analysis confirm efficiency propose algorithm simulation result demonstrate effectivenessn',\n",
       " 'J. Heikkila modern ccd camera usually capable spatial accuracy great 150 pixel size accuracy easily attain error source affect image formation process current calibration meethod typically assume observation unbiased error zeromean independent identically distribute random noise observe image coordinate camera model completely explain map 3d coordinate image coordinate general condition meet cause calibration result accurate expect paper calibration procedure precise 3d vision application describe introduce bias correction circular control point nonrecursive meethod reverse distortion model accuracy analysis present error source reduce theoretical accuracy discuss test synthetic image indicate improvement calibration result limit error condition real image suppression external error source prerequisite successful calibrationn',\n",
       " 'I. Skrjanc O. Nelles B. Hartmann L. Teslic paper deal problem fuzzy nonlinear model identification framework local model network lmn new iterative identification approach propose supervise unsupervise learn combine optimize structure lmn purpose fit clustercenter process nonlinearity gustafssonkessel gk fuzzy cluster unsupervise learn apply combination lmn learn procedure new incremental method define number initial location cluster center gk cluster algorithm propose data cluster correspond local region process model local linear model validity function calculate fuzzy covariance matrix cluster highly adaptable process describe sparse local model parsimonious lmn model propose method construct lmn finally test drug absorption spectral process compare method lolimot hilomot comparison experimental result method show usefulness propose identification algorithmn',\n",
       " 'C. J. Harris B. L. Luk X. Hong S. Chen unify approach propose data model include supervise regression classification application unsupervise probability density function estimation orthogonalleastsquares regression base leaveoneout test criterion formulate unify datamodel framework construct sparse kernel model generalise example regression classification density estimation application illustrate effectiveness generic datamodel approach construct parsimonious kernel model excellent generalisation capabilityn',\n",
       " 'Minrui Fei Kang Li Dajun Du paper investigate center selection multioutput radial basis function rbf network multioutput fast recursive algorithm mfra propose method reveal significance candidate center base reduction trace error covariance matrix estimate network weight simultaneously substitution approach main contribution center selection procedure weight estimation perform welldefined regression context lead significantly reduce computational complexity efficiency algorithm confirm computational complexity analysis simulation result demonstrate effectivenessn',\n",
       " 'GuangBin Huang Yeng Chai Soh Yuan Lan extreme learn machine elm propose huang et al develop generalize single hide layer feedforward network slfns wide variety hide node prove fast effective especially solve function approximation problem predetermine network structure method determine network structure preliminary elm tedious lead parsimonious solution paper systematic twostage algorithm name tselm introduce handle problem stage forward recursive algorithm apply select hide node candidate randomly generate step add network stop criterion achieve minimum significance hide node review second stage insignificance one remove network drastically reduce network complexity effectiveness tselm verify empirical study papern',\n",
       " 'M. van Tooren Yong Zhao Xiaoqian Chen Wen Yao radial basis function neural network rbfnns widely nonlinear function approximation challenge rbfnn model determine effectively optimize width parameter improve approximation accuracy solve problem width optimization method concurrent subspace width optimization cswo propose base decomposition coordination strategy method decompose largescale width optimization problem subspace optimization sso problem single optimization variable small train validation data set greatly simplify optimization complexity ssos solve concurrently computational time effectively reduce toplevel coordination optimization ssos converge consistent optimum equivalent optimum original width optimization problem propose method test mathematical example practical engineer approximation problem result demonstrate efficiency robustness cswo optimize width parameter traditional width optimization methodsn',\n",
       " 'G.W. Irwin Kang Li JianXun Peng paper investigate learn wide class singlehidelayer feedforward neural network slfns set adjustable parameter nonlinear parameter hide node linear output weight main objective speed convergence secondorder learn algorithm levenbergmarquardt lm improve network performance achieve reduce dimension solution space introduce new jacobian matrix unlike conventional supervise learn method optimize set parameter simultaneously linear output weight convert dependent parameter remove need explicit computation consequently neural network nn learn perform solution space reduce dimension new jacobian matrix propose use popular secondorder learn method order achieve accurate approximation cost function efficacy propose method show analysis computational complexity present simulation result different examplesn',\n",
       " 'JyhHorng Jeng YihLon Lin JerGuang Hsieh know statistic result linear regressors rankbase wilcoxon approach linear regression problem usually robust insensitive outlier motivate introduce paper wilcoxon approach area machine learn specifically investigate new learn machine wilcoxon neural network wnn wilcoxon generalize radial basis function network wgrbfn wilcoxon fuzzy neural network wfnn kernelbase wilcoxon regressor kwr provide alternative learn machine face general nonlinear learn problem simple weight update rule base gradient descent derive numerical example provide compare robustness outlier learn machine simulation result wilcoxon learn machine propose paper good robustness outlier firmly believe wilcoxon approach provide promise methodology machine learn problemn',\n",
       " 'M. J. Korenberg K. M. Adeney abstract generalize singlelayer network gsln architecture implement sum arbitrary basis function define input potentially flexible efficient structure approximate arbitrary nonlinear function drawback gslns large number weight basis function require provide satisfactory approximation paper present new approach algorithm know iterative fast orthogonal search ifos couple minimum description length mdl criterion provide automatic structure selection parameter estimation gslns result algorithm dub ifosxe2x80x93mdl perform network growth prune construct sparse gslns potentially large space candidate basis functionn',\n",
       " 'Henry Leung Nan Xie paper propose novel blind equalization approach base radial basis function rbf neural network exploit shortterm predictability input rbf neural net predict inverse filter output show prediction error rbf neural net minimize coefficient inverse identical unknown enhance identification performance noisy environment improve square il method base concept orthogonal distance reduce estimation bias cause additive measurement noise propose perform train convergence rate il learn analyze asymptotic mean square error mse propose predictive rbf identification method derive theoretically monte carlo simulation propose method effective blind identification new blind technique apply practical application equalization reallife radar sea clutter collect east coast canada deconvolution real speech signal case propose blind equalization technique perform satisfactory channel effect measurement noise strongn',\n",
       " 'Y. Toyoda V. HagganOzaki T. Ozaki Hui Peng paper consider nonlinear system model problem control structure nonlinear parameter optimization method snpom adapt radial basis function rbf network rbf networktyle coefficient autoregressive model exogenous variable model parameter estimation present offline nonlinear model parameter optimization method depend partly levenbergmarquardt method nonlinear parameter optimization partly leastsquares method singular value decomposition linear parameter estimation compare algorithm snpom accelerate computational convergence parameter optimization search process rbftype model usefulness approach illustrate mean examplesn',\n",
       " 'P.K.S. Tam S.H. Ling H.K. Lam F.H.F. Leung paper present tune structure parameter neural network improve genetic algorithm ga show improve ga perform good standard ga base benchmark test function neural network switch introduce link propose propose neural network learn inputoutput relationship application network structure improve ga number hide node choose manually increase small number learn performance term fitness value good application example sunspot forecast associative memory give merit improve ga propose neural networkn',\n",
       " 'N. Sundararajan P. Saratchandran GuangBin Huang work present new sequential learn algorithm radial basis function rbf network refer generalize grow prune algorithm rbf ggaprbf paper introduce concept significance hide neuron u learn algorithm realize parsimonious network grow prune strategy ggaprbf base link require learn accuracy significance near intentionally add new neuron significance neuron measure average information content neuron ggaprbf algorithm arbitrary sample density train sample derive rigorous statistical point view simulation result bench mark problem function approximation area ggaprbf outperform sequential learn algorithm term learn speed network size generalization performance regardless sample density function train datan',\n",
       " 'O. Kaynak Xianyi Zhuang Sheng Qiang Yahui Li paper different backstepping neural network nn control approach present class affine nonlinear system strictfeedback form unknown nonlinearities special design scheme controller singularity problem avoid perfectly approach furthermore close loop signal guarantee semiglobally uniformly ultimately bound output prove converge small neighborhood desire trajectory control performance closeloop system shape desire suitably choose design parameter simulation result obtain demonstrate effectiveness approach propose difference observe input controller analyze brieflyn',\n",
       " 'C. Manzie D. Ralph M. Palaniswami C. Panchapakesan radial basis function rbf network placement center say significant effect performance network supervise learn center location application superior network center locate unsupervise method network train time sigmoid network increase time need supervise learn offset train time regular rbf network way overcome train network set center select unsupervise method fine tune location center evaluate move center decrease error depend require level accuracy change center location paper provide new result bind gradient hessian error consider function independent set parameter center width weight function center width linear weight function basis function parameter network fix size bind hessian provide line begin initial set parameter bind possible estimate reduce error change center step size specify achieve guarantee reduction errorn',\n",
       " 'ZhiWei Chen ChienYu Chen YuYen Ou ShienChing Hwang YenJen Oyang work present novel learn algorithm efficient construction radial basis function rbf network deliver level accuracy support vector machine svms data classification application propose learn algorithm work construct rbf subnetwork approximate probability density function class object train data set respect algorithm design main distinction propose learn algorithm novel kernel density estimation algorithm feature average time complexity onlogn n number sample train data set important advantage propose learn algorithm comparison svm propose learn algorithm generally take far time construct data classifier optimize parameter set feature significance contemporary application particular application new object continuously add large database desirable feature propose learn algorithm rbf network construct capable carry data classification class object single run word unlike svm need resort mechanism oneagainstone oneagainstall handle dataset class object comparison svm particular show number recent study svm generally able deliver high classification accuracy exist data classification algorithm propose learn algorithm instancebased data reduction issue address paper interest observation regard data set data reduction experiment number train sample remain naspl inodotspl umlve data reduction mechanism apply close number support vector identify svm software paper compare performance rbf network construct propose learn algorithm construct conventional clusterbased learn algorithm interest observation learn respect data classification distribution train sample near boundary different class object carry crucial information distribution sample inner part clustersn',\n",
       " 'A.F. Diaz F.J. Fernandez H. Pomares J. Ortega I. Rojas J. Gonzalez paper present multiobjective evolutionary algorithm optimize radial basis function neural network rbfnns order approach target function set inputoutput pair procedure allow application heuristic improve solution problem hand include new genetic operator evolutionary process new operator base wellknown matrix transformation singular value decomposition svd orthogonal square ols define new mutation operator produce local global modification radial basis function rbfs network individual population evolutionary procedure analyze efficiency different operator show global mutation operator yield improve procedure adjust parameter rbfnnsn',\n",
       " 'Shiqian Wu W. Chen Meng Joo Er paper efficient method highspeed face recognition base discrete cosine transform dct fisheru0027s linear discriminant fld radial basis function rbf neural network present dimensionality original face image reduce dct large area illumination variation alleviate discard lowfrequency dct coefficient truncate dct coefficient vector cluster propose cluster algorithm process make subsequent fld efficient implement fld discriminate invariant facial feature maintain train sample cluster consequence parameter estimation rbf neural network fulfil easily facilitate fast train rbf neural network simulation result propose achieve excellent performance high train recognition speed high recognition rate good illumination robustnessn',\n",
       " 'Moncef Gabbouj Serkan Kiranyaz Jenni Raitoharju train radial basis function neural network rbfnns location gaussian neuron commonly determine cluster train input cluster fully unsupervised manner input cluster supervision introduce example concatenate input vector weight output vector inputxe2x80x93output cluster paper propose apply cluster separately class classspecific cluster idea previous work evaluate benefit approach compare classspecific input inputxe2x80x93output cluster approach term classification performance computational efficiency train rbfnns accomplish objective apply different cluster algorithm conduct experiment 25 benchmark data set classspecific approach significantly reduce overall complexity cluster experimental result demonstrate lead significant gain classification performance especially network relatively gaussian neuron apply cluster algorithm combine time dynamic evolutionary optimization method multidimensional particle swarm optimization classspecific cluster optimize number cluster centroid locationn',\n",
       " 'Bo Zhang Xiaolin Hu Qingtian Zhang support vector regression svr popular function estimation technique base vapnikxe2x80x99s concept support vector machine variant ell 1 norm svr know good select useful feature feature redundant sparse cod sc technique widely area number efficient algorithm available ell 1 norm svr sc linear regression brief close connection ell 1 norm svr sc reveal typical algorithm compare linear regression result sc algorithm outperform newton linear program algorithm efficient ell 1 norm svr algorithm efficiency algorithm design radial basis function rbf neural network experiment benchmark data set demonstrate high efficiency sc algorithm particular sc algorithm orthogonal match pursuit order magnitude fast wellknow rbf network design algorithm orthogonal square algorithmn',\n",
       " 'Elli Angelopoulou Michael Balda Christian Schaller Hannes G. Hofmann Etienne Assoumou Mengue Peter Fursattel Simon Placht present new checkerboard detection algorithm able detect checkerboard extreme pose checkerboard highly distort lens distortion lowresolution image detect pattern apply surface fit base subpixel refinement specifically tailor checkerboard xjunctions finally investigate accuracy checkerboard detector affect overall calibration result multicamera setup propose method evaluate real image capture different camera model wide applicability quantitative comparison opencvxe2x80x99s checkerboard detector propose method detect 80 checkerboard detect corner point accurately strong perspective distortion present wide baseline stereo setupn',\n",
       " 'Hassan Foroosh Imran N. Junejo shadow trajectory stationary object scene demonstrate set photograph sufficient accurately calibrate camera present novel application point shadow trajectory object accurately determine geolocation camera longitude ambiguity date image acquisition gps special instrument refer geotemporal localization consider possible case ambiguity remove additional information available method require knowledge date time picture take geotemporal information recover directly image demonstrate accuracy technique step calibration geotemporal localization synthetic real datan',\n",
       " 'M.J. Brooks R. Hill A. van den Hengel camera calibration require identification point image correspond know location scene typically determine use calibration pattern design facilitate feature localisation present paper novel method generate pattern subregion individually identifiable cross ratio method aim minimise probability misidentify subregion key advantage method ability place constraint size element constitute pattern allow calibration object wide variety view condition increase flexibility calibration processn',\n",
       " 'Weidong Geng Hao Lei Bingwen Jin low precision consumergrade depth sensor calibrate jointly color camera joint calibration present undesired interaction paper propose novel method carry highaccuracy intrinsic calibration depth sensor merely depth camera traditional calibration rig checkerboard pattern replace set cuboid know size objective function calibration base length width height cuboid angle neighbor surface directly robustly calculate depthmap experimentally evaluate accuracy calibrate depth camera measure angle size cubic object empirically show result calibration accuracy high stateoftheart calibration procedure make commodity depth sensor applicable interest application scenario 3d measurement shape model etcn',\n",
       " 'Alexandra Branzan Albu Robert Bergevin Frederic Jean propose new method viewinvariant gait model single calibrate camera piecewisecontinuous body part trajectory extract video sequence rectify appear observe frontoparallel view standard gait characteristic compute combine rectify gait halfcycles trajectory method use walk model allow change direction change speed order decouple gait characteristic distract factor observe sequence contrast previous work method suit clinical surveillance application simulate real trajectory indoor set validate propose methodn',\n",
       " 'Zengfu Wang Yang Cao Shuping Liu Yu Liu chessboard corner detection fundamental work popular chessboard patternbased camera calibration technique paper fast robust algorithm chessboard corner detection present method initial corner set obtain improve hessian corner detector novel strategy take textural geometrical characteristic chessboard consideration employ eliminate fake corner initial corner set propose algorithm require userinput total number chessboard inner corner parameter adaptively calculate statistical approach experimental result public data set demonstrate propose method outperform commonly opencv method term detection rate computational efficiencyn',\n",
       " 'Sergio R. Neves Jonathan N. Gois Carla L. Pagliari Eduardo A. B. da Silva Andreas Ellmauthaler accurate geometric calibration thermal infrared ir camera vital importance vision application general calibration procedure consist localize set calibration point calibration image set subsequently solve camera parameter physical limitation ir acquisition process localization calibration point pose difficulty subsequently lead unsatisfying calibration result work novel ir camera calibration approach introduce able localize calibration point image conventional calibration board consist miniature lightbulb improve accuracy algorithm model radiation pattern light bulb ellipse consider center mass extract ellipsoidal region start calibration point refine iteratively alternate map undistorted grid model propose process chain lead significantly reduce calibration error compare stateoftheart furthermore propose methodology calibrate visiblelight camera suitable calibration multiple camera rig involve visiblelight ir cameran',\n",
       " 'N. Krouglicof T. Rahman field machine vision camera calibration refer experimental determination set parameter image formation process give analytical model machine vision researcher work lowcost digital camera offtheshelf lens generally favor camera calibration technique rely specialize optical equipment modification hardware priori knowledge vision commonly calibration technique base observation single 3d target multiple planar 2d target large number control point paper present novel calibration technique offer improve accuracy robustness efficiency wide range lens distortion technique operate minimize error reconstruct image point experimentally determine counterpart xe2x80x9cdistortion freexe2x80x9d space facilitate incorporation exact lens distortion model addition express spatial orientation term unit quaternion greatly enhance propose calibration solution formulate minimally redundant equation free singularity extensive performance benchmarking consist simulation experiment confirm high accuracy calibration regardless lens distortion present optic camera paper experimentally confirm comprehensive lens distortion model include high order radial tangential distortion term improve calibration accuracyn',\n",
       " 'Takeo Kanade JunSik Kim Ankur Datta novel camera calibration algorithm square circle ring planar calibration pattern iterative refinement approach propose utilize parameter obtain traditional calibration algorithm initialization perform undistortion unprojection calibration image canonical frontoparallel plane canonical plane localize calibration pattern control point recompute camera parameter iterative refinement convergence undistorting unprojecting calibration pattern canonical plane increase accuracy control point localization consequently camera calibration conduct extensive set experiment real synthetic image square circle ring pattern pixel reprojection error obtain method 50 low opencv camera calibration toolbox increase accuracy camera calibration directly lead improvement application demonstrate recovery fine object structure visual hull reconstruction recovery precise epipolar geometry stereo camera calibrationn',\n",
       " 'Olivier Morel David Fofi Abd El Rahman Shabayek calibration determination coordinate pixelsu0027 ray common coordinate enable compute 3d ray light travel image pixel paper propose novel noncentral catadioptric autocalibration approach u polarization image know specular surface shape order estimate catacaustics catadioptric geometrically fully image catacaustic locus viewpoint pixel image map point caustic surface point map unique light ray scene polarization image help widen range catadioptric system include type specular surface novel approach help autocalibrate combination specular surface lensesn',\n",
       " 'Adrian G. Bors Matthew Grum paper propose new approach multiobject 3d scene model scene multiple object characterize object occlusion view complex illumination condition multiple reflection shadow variety object shape surface property factor raise huge challenge attempt model real 3d multiobject scene exist approach design mainly single object model propose method rely initialization provide rough 3d model scene estimate give set multiview image contribution describe paper consist new method identify correct error reconstruct 3d scene approach correct location 3d patch scene detect disparity pair projection image second approach call shapefromcontour identify discrepancy projection 3d object correspond contour segment image unsupervise supervise segmentation define contour object highlightsmultiple object 3d scene reconstruction multiple imagereconstruction scene display object occlusion illumination variationusing image disparity pair image 3d scene correctionusing unconsistencies object contour 3d scene correctionn',\n",
       " 'Dongcheng Hu Yupin Luo Qihe Li Fei Qi paper focus problem camera calibration onedimensional 1d object general motion pattern suit solve calibration problem b improve robustness accuracy method firstly sufficient necessary condition solvability 1d calibration general motion prove special motion toss 1d object provide example illustrate correctness feasibility condition practical issue obtain solution inspect avoid singularity precision robustness method improve relative mean error reduce 5 noise level pixel surpass stateoftheart method categoryn',\n",
       " 'P.F. Whelan J. Mallon A.K. Dunne generic camera calibration nonparametric calibration technique applicable type vision sensor standard generic calibration method develop goal generality suboptimal common case camera single centre projection pinhole fisheye hyperboloidal catadioptric paper propose novel improvement standard generic calibration method central camera reduce complexity improve accuracy robustness improvement achieve take advantage geometric constraint result single centre projection input data algorithm acquire active grid performance characterise new linear estimation stage generic algorithm propose incorporate classical pinhole calibration technique show significantly accurate linear estimation stage standard method linear method pose estimation propose evaluate exist polynomial method distortion correction motion reconstruction experiment conduct real data hyperboloidal catadioptric sensor standard propose method result accuracy robustness propose method superior standard methodn',\n",
       " 'Fuchao Wu Qiulei Dong Kunfeng Shi paper weight similarityinvariant linear algorithm camera calibration rotate 1d object propose propose new estimation method compute relative depth free endpoint 1d object prove robustness noise compare previous literature introduce estimator invariant image similarity transform result similarityinvariant linear calibration algorithm slightly accurate wellknown normalize linear algorithm use reciprocal standard deviation estimate relative depth different image weight constraint equation similarityinvariant linear calibration algorithm propose weight similarityinvariant linear calibration algorithm high accuracy experimental result synthetic data real image data effectiveness propose algorithmn',\n",
       " 'Juliani Chico Piai Maria Bernadete De M. FrancA Marcelo Ricardo Stemmer Jose Alexandre De FrancA vision camera calibration necessary process retrieval information angle distance require paper address multicamera calibration problem single dimension calibration pattern general motion currently know algorithm solve problem base estimation vanish point estimate susceptible noise make method unsuitable practical application instead paper present new calibration algorithm camera divide binocular set fundamental matrix binocular set estimate allow perform projective calibration camera calibration update euclidean space end process calibration possible impose restriction movement pattern prior information camera motion experiment synthetic real image validate new method accuracy make suitable practical applicationn',\n",
       " 'Robert S. Allison Karim Benzeroual Jianhui Chen geometrical calibration highdefinition camera rig important step 3d film make vision application large image data highdefinition maintain execution speed appropriate onset online adjustment procedure big challenge machine vision base calibration method aim provide lowcost fast accurate calibrate intrinsic extrinsic parameter stereo camera rig propose novel calibration target marker chessboard speed corner detection develop automatic key frame selection algorithm optimize frame calibration propose bundle adjustment method overcome geometrical inaccuracy chessboard finally introduce online stereo camera calibration base improvementsn',\n",
       " 'Lina J. Karam Charan D. Prakash paper present novel camera calibration method circular calibration pattern disadvantage issue exist stateoftheart method discuss overcome work propose iterative method capture image circular pattern undistorted project fronto parallel plane control point localize fronto parallel plane novel approach accurately localize control point image base adaptive segmentation ellipse fit localize control point project original plane estimate camera calibration parameter refine minimize reprojection error simulation result present illustrate performance propose scheme result propose method reduce error 57 compare stateoftheart highresolution image propose scheme robust blur image calibration patternn',\n",
       " 'K. Kanatani present new technique calibrate ultrawide fisheye lens camera impose constraint collinear point rectify collinear parallel line parallel orthogonal line orthogonal exploit fact line fit reduce eigenvalue problem 3d rigorous perturbation analysis obtain practical calibration procedure experiment point spurious solution exist collinearity parallelism impose technique desirable property example metric information require reference pattern camera position separate stripe pattern display video screen generate virtual grid eliminate grid point extraction processingn',\n",
       " 'S.S. Brandt J. Kannala fisheye lens convenient application wide angle view need use measurement purpose limit lack accurate generic easytouse calibration procedure propose generic camera model suitable fisheye lens camera conventional wideangle lens camera calibration method estimate parameter model achieve level calibration accuracy comparable previously report stateoftheartn',\n",
       " 'H. Foroosh Xiaochun Cao paper address problem calibrate pinhole camera image symmetric object assume unit aspect ratio zero skew interimage homographies express function principal point minimize symmetric transfer error geometric distance obtain accurate solution calibration parameter approach extend calibration technique image 1d object fix pivot point advantage approach exist method rely interimage homgraphies knowledge world image homography require demonstrate effectiveness approach case symmetric object 1d object present process result synthetic real image provide quantitative comparison zhangu0027s flexible calibration technique 2000 1d calibration method 2002n',\n",
       " 'Hongbin Zha Xianghua Ying concentric circle calibration feature posse good geometric property paper present efficient method detection project concentric circle image plane consider special geometric property propose method capable detect partially visible concentric circle experimental result demonstrate validity propose approachn',\n",
       " 'Christopher Jaynes Matt Steele present novel matchpoint acquisition method capable produce accurate correspondence subpixel precision give know representation point match project fiducial structure light method estimate fiducial location expect uncertainty improve matchpoint precision application number calibration task uncertainty estimate significantly improve overall calibration result simple parametric model capture relationship know fiducial correspond position shape intensity image plane matchpoint pair unknow model parameter recover maximum likelihood estimation determine subpixel center fiducial uncertainty matchpoint center estimate perform forward error analysis expect image noise uncertainty estimate conjunction accurate matchpoints improve calibration accuracy multiview systemsn',\n",
       " 'Janne Heikkil x E Juho Kannala C Daniel Herrera present algorithm simultaneously calibrate color camera depth camera relative pose method design key feature accurate practical applicable wide range sensor method require planar surface image pose calibration use depth discontinuity depth image make flexible robust noise apply calibration kinect device present new depth distortion model depth sensor perform experiment improve accuracy respect manufactureru0027s calibrationn',\n",
       " 'K. Chihara D. Douxchamps accurate measurement position feature image subject fundamental compromise feature small limit effect nonlinear distortion large limit effect noise discretization constrain accuracy robustness image measurement play important role geometric camera calibration subsequent measurement base calibration paper present new geometric camera calibration technique exploit complete camera model localization control marker abolish marker size compromise large marker allow dense pattern instead simple disc result significant increase accuracy robustness highly planar marker geometric camera calibration base synthetic image lead true error 0002 pixel presence artifact noise illumination gradient compression blur limit dynamic range camera parameter accurately recover complex camera modelsn',\n",
       " 'H. Bischof R. Perko P. Elbischger K. Karner J. Bauer A. Klaus present simple universal camera calibration method instead extensive setup exploit accurate angular position fix star high precision achieve compensate interfere error source approach u star catalog require single input image additional user input information focal length exposure date position require fully automatic process fast convergence achieve perform consecutive step star segmentation centroid find algorithm extract subpixel position luminary second initial solution essential parameter determine combinatorial analysis finally levenbergmarquardt algorithm apply solve result nonlinear experimental result digital consumer camera demonstrate high robustness accuracy introduce method advisable application large calibration target requiren',\n",
       " 'Long Quan Guang Jiang geometry planebased calibration method understand user interaction need practice feature detection paper present fully automatic calibration u pattern pair concentric circle key observation introduce geometric method construct sequence point strictly convergent image circle center arbitrary point method automatically detect point pattern feature construction method identify invariant take advantage homological constraint consistently optimally estimate feature image experiment demonstrate robustness accuracy new methodn',\n",
       " 'Hideki Koike Hiroyuki Arai Isao Miyagawa propose simple practical calibration technique effectively estimate camera parameter point orthogonal 1d object collinear point share derive basic equation need realize camera calibration point observe single image capture object new camera calibration algorithm estimate camera parameter base basic equation optimize bundle adjustment technique method validate simulate data real image result camera parameter yield method close yield exist method test demonstrate method effective practicaln',\n",
       " 'InSo Kweon P. Gurdjos JunSik Kim investigate projective property feature consist concentric circle demonstrate exist geometric algebraic constraint projection constraint greatly simplify recovery affine euclidean structure 3d plane application as performance camera calibration algorithmsn',\n",
       " 'J.M. Menendez J. Torres paper new practical simple method remove distortion image acquisition camera describe pattern image straight horizontal vertical line automatically compute radial tangential distortion base classical model rely observe apparent distortion line obtain cannyu0027s edge detector make use houghu0027s transform apply model robust complete practical result real image shownn',\n",
       " 'In So Kweon Jiyoung Jung Kyungdon Joo Yunsu Bok Hyowon Ha propose novel camera calibration method defocused image smartphone assumption defocus blur model convolution sharp image gaussian point spread function psf contrast exist calibration approach require wellfocused image propose method achieve accurate camera calibration severely defocused image robustness defocus propose set unidirectional binary pattern simplify 2d gaussian deconvolution 1d gaussian deconvolution problem multiple observation capture set pattern consecutively display smartphone formulate feature extraction deconvolution problem estimate feature point location subpixel accuracy blur kernel location compensate error camera parameter refraction glass panel display device evaluate performance propose method synthetic real data severe defocus method show accurate camera calibration resultn',\n",
       " 'Rafael Grompone von Gioi Pierre Gurdjos Viorica Patraucean propose line segment elliptical arc detector produce reduce number false detection type image parameter tune give region pixel greyscale image detector decide line segment elliptical arc present model validation interpretation possible region detector choose best explain data model selection statistical criterion base contrario theory serf validation model selection experimental result highlight performance propose approach compare stateoftheart detector apply synthetic real imagen',\n",
       " 'Lina J. Karam Juan Andrade robust radial distortion correction method require single image distort image pattern present method robust orientation image pattern require availability ideal reference regularly structure pattern recreate detect feature distort image radial distortion parameter center distortion cod radial distortion coefficient rdc optimize minimize correspond cost function alternate manner test synthetic real data present order illustrate performance robustness propose algorithmn',\n",
       " 'Yaser Sheikh In So Kweon Hatem Alismail Michal Perdoch Hyowon Ha recent proliferation high resolution camera present opportunity achieve unprecedented level precision visual 3d reconstruction camera calibration pipeline develop decade ago checkerboard remain facto standard paper ask question checkerboard optimal pattern high precision calibration empirically demonstrate deltille grid regular triangular tile produce high precision calibration possible tiles euclidean plane posit new standard highprecision calibration present complete ecosystem calibration deltille grid include 1 highly precise corner detection algorithm base polynomial surface fit 2 index scheme base polarity extract fit surface 3 2d cod deltille grid refer deltags lieu conventional matrix barcodes demonstrate stateoftheart performance apply calibration ecosystem use 3d calibration object multiview camera calibrationn',\n",
       " 'XiaoLin Tian WenLiang Du evaluate image registration method image dense feature point challenge itu0027s hard discriminate real inliers thousand result correspondence process image registration method paper present dense feature point simulation provide grind truth correspondence evaluate image registration method automatically dense feature point create simulate pinhole camera model parallax reference image sense image radial distortion camera lens random outlier performance stateofart image registration method evaluate discuss dense correspondence simulate propose model evaluation result propose model offer practical way evaluate image registration method dense feature pointn',\n",
       " 'Luis Gerardo de la Fraga Heriberto CruzHernandez abstract work introduce novel visual fiducial tag appropriate application automatic identification propose tag base order type construction define computational geometry invariant 3d translation rotation projective transformation main contribution present design propose tag procedure detect image algorithm compute identifier second analyze feasibility proposal different condition tagxe2x80x99s rotation distance tag effect noise point position recognition process applicability propose tag simulate image conduct experiment indicate tag robust image generation process suitable automatic identification 3472 different tag pose estimation vision applicationn',\n",
       " 'Evgeniy Martyushev internal calibration pinhole camera give parameter combine uppertriangular 3times 3 calibration matrix skew parameter zero aspect ratio equal camera say euclidean image plane paper propose noniterative selfcalibration algorithm camera euclidean image plane case remain internal parameter xe2x80x94 focal length principal point coordinate xe2x80x94 fix unknown algorithm require set n ge 7 point correspondence view measure relative rotation angle view problem generically solution include complex onesn',\n",
       " 'JenHui Chuang MuTien Lu paper novel fully automatic camera calibration procedure propose estimate camera principal point intersection optical axis image plane basic idea derive orthogonal projection camera optical axis flatscreen monitor changinganalyzing simple edge pattern display flat monitor principal point estimate intersection image line feature derive screen call calibration line multiple monitor configuration calibration perform automatically fix camera multiple monitor multiple pose movable monitor experiment develop evaluate propose method accuracy robustness result propose approach compare favorably previous work include classic zhangu0027s algorithm derive principal point camera parameter timen',\n",
       " 'Yun Fu Ming Shao Siyu Xia Changsheng Lu year ellipse detection algorithm spring study broadly critical issue detect ellipsis accurately efficiently realworld image remain challenge paper propose valuable industryoriented ellipse detector arcsupport line segment simultaneously reach high detection accuracy efficiency simplify complicate curve image retain general property include convexity polarity arcsupport line segment extract grind successful detection ellipsis arcsupport group form iteratively robustly link arcsupport line segment latently belong common ellipse afterward complementary approach locally select arcsupport group high saliency globally search valid pair group adopt fit initial ellipsis fast way ellipse candidate set formulate hierarchical cluster 5d parameter space initial ellipsis finally salient ellipse candidate select refine detection subject stringent effective verification extensive experiment public datasets implement method achieve best fmeasure score compare stateoftheart method source code available httpsgithubcomalanlusunhighqualityellipsedetection n',\n",
       " 'Vassilios Chatzis Stelios Krinidis paper present variation fuzzy cmeans fcm algorithm provide image cluster propose algorithm incorporate local spatial information gray level information novel fuzzy way new algorithm call fuzzy local information cmeans flicm flicm overcome disadvantage know fuzzy cmeans algorithm time enhance cluster performance major characteristic flicm use fuzzy local spatial gray level similarity measure aim guarantee noise insensitiveness image preservation furthermore propose algorithm fully free empirically adjust parameter g s incorporate fuzzy cmeans algorithm propose literature experiment perform synthetic realworld image flicm algorithm effective efficient provide robustness noisy imagen',\n",
       " 'Shanika Karunasekera Christopher Leckie Sutharshan Rajasegarar Masud Moshtaghi cluster widely fundamental data mine tool automate analysis complex datasets grow need use cluster algorithm embed system restrict computational capability wireless sensor node order support automate knowledge extraction system considerable research cluster algorithm propose method computationally expensive propose robust cluster algorithm low computational complexity suitable computationally constrain environment evaluation synthetic reallife datasets demonstrate low computational complexity comparable accuracy approach compare range exist methodn',\n",
       " 'Krista Rizman alik cluster validity index estimate quality partition produce cluster algorithm determine number cluster data cluster validation difficult task data set partition exist level detail fit natural group give data set cluster validity index exist inefficient cluster widely differ density size propose cluster validity index address issue base compactness overlap measure overlap measure indicate degree overlap fuzzy cluster obtain calculate overlap rate data object belong strongly cluster compactness measure indicate degree similarity data object cluster calculate membership value data object strongly associate cluster propose ratio summation type index compactness overlap measure maximal value index denote optimal fuzzy partition expect high compactness low degree overlap cluster test wellknown previously formulate propose index wellknown data set show superior reliability effectiveness propose index comparison index especially evaluate partition cluster widely differ size densityn',\n",
       " 'G. Lightbody G. Gregorcic bayesian gaussian process gp model approach recently introduce modelbased control strategy estimate variance predict output useful advantage gps comparison neural network nns fuzzy model gp model computationally demand nontransparent reduce computation load increase transparency local linear gp model network propose paper propose methodology combine local model network principle gp prior approach novel algorithm structure determination optimization introduce widely applicable train local model network model procedure local linear gp lgp model network demonstrate example nonlinear laboratory scale process rign',\n",
       " 'S. Marco G. Sberveglieri M. Pardo A. Gutierrez M. Falasconi important goal cluster analysis internal validation result objective criterion particular relevance respect estimation optimum number cluster capture intrinsic structure data paper propose method determine optimum number base evaluation fuzzy partition stability bootstrap resampling method characterize synthetic data respect hyperparameter like fuzzifier spatial cluster parameter feature space dimensionality cluster degree overlap number cluster method validate experimental datasets furthermore performance propose method compare obtain number traditional fuzzy validity rule base cluster compactnesstoseparation criterion propose method provide accurate reliable result offer good generalization capability classical approachesn',\n",
       " 'Alessandro Artusi Andreas Brandstetter neural network nns area show potential limitation main limitation long time require train process useful case fast train process require respond change application domain possible way accelerate learn process nn implement hardware high cost reduce flexibility original central process unit cpu implementation solution choose recently power graphic process unit gpu market increase start application particular kind nn name radial basis function network rbfn extensively prove power limit time performance reduce application area brief paper gpu implementation entire learn process rbfn show ability reduce computational cost order magnitude respect cpu implementationn',\n",
       " 'C.J. Harris Qiang Gan fuzzy local linearization fll useful divideandconquer method cop complex problem model unknown nonlinear system data state estimation control base probabilistic interpretation fll paper propose hybrid learn scheme fll model u modify adaptive spline model masmod algorithm construct antecedent part membership function fll model expectationmaximization em algorithm parameterize consequent part local linear model hybrid method approximation ability good neurofuzzy network model produce parsimonious network structure gain masmod provide covariance information model error gain em valuable application state estimation control numerical example nonlinear timeseries analysis nonlinear trajectory estimation fll model present validate derive algorithmn',\n",
       " 'HuaLiang Wei S.A. Billings sparse representation satisfactory approximation accuracy usually desirable nonlinear identification signal process problem new forward orthogonal regression algorithm mutual information interference propose sparse model selection parameter estimation new algorithm construct parsimonious linearintheparameters modelsn',\n",
       " 'Long Chen ChiHsu Wang Jing Wang C. L. Philip Chen traditional neurofuzzy transform equivalent fully connect layer neural network nn fully connect neurofuzzy inference system fconfis fconfis differ traditional nns dependent repeat weight input hide layer consider variation kind multilayer nn efficient learn algorithm fconfis cope repeat weight derive furthermore dynamic learn rate propose neurofuzzy system fconfis premise hide consequent portion sidered simulation result indicate propose approach achieve good accuracy fast convergencen',\n",
       " 'Dianhui Wang Tianyou Chai Yajun Zhang model nonlinear system combine linear model nonlinear compensation term virtual unmodeled dynamic vud parameter estimation linear model learningbased vud estimate influence interact simultaneously paper aim develop alternate identification scheme resolve challenge problem projection algorithm employ identify linear model feedforward neural network model vud class nonlinear dynamical system openloop estimation algorithm vud present know linear model follow alternate identification algorithm completely unknow nonlinear system algorithm description give simulation study multiple input multiple output nonlinear system carry illustrate effectiveness propose model techniquesn',\n",
       " 'George W. Irwin ErWei Bai Kang Li Long Zhang number neural network formulate linearintheparameters model train network transform model selection problem compact model select candidate subset selection algorithm forward selection method popular fast subset selection approach produce suboptimal model trap local minimum recently twostage fast recursive algorithm tsfra combine forward selection backward model refinement propose improve compactness generalization performance model paper propose unify twostage orthogonal square method instead fast recursivebased method contrast tsfra paper derive new simplify relationship forward backward stage avoid repetitive computation inherent orthogonal property square method furthermore new term exchange scheme backward model refinement introduce reduce computational demand finally give error reduction ratio criterion effective efficient forward backward subset selection procedure propose extensive example present demonstrate improve model compactness construct propose technique comparison popular methodn',\n",
       " 'James Theiler Kevin Lacker Simon Perkins present novel flexible approach problem feature selection call graft consider feature selection separate learn graft treat selection suitable feature integral learn predictor regularize learn framework regularize learn process sufficiently fast large scale problem graft operate incremental iterative fashion gradually build feature set train predictor model gradient descent iteration fast gradientbased heuristic quickly as feature likely improve exist model feature add model model incrementally optimize gradient descent algorithm scale linearly number data point quadratically number feature graft variety predictor model class linear nonlinear classification regression experiment report variant graft classification linear nonlinear model logistic regressioninspired loss function result variety synthetic real world data set present finally relationship graft stagewise additive model boost exploredn',\n",
       " 'CheeKheong Siew Lei Chen GuangBin Huang accord conventional neural network theory singlehidelayer feedforward network slfns additive radial basis function rbf hide node universal approximators parameter network allow adjustable observe neural network implementation tune parameter network cause learn complicate inefficient difficult train network nondifferential activation function threshold network unlike conventional neural network theory paper prof incremental constructive method order let slfns work universal approximators simply randomly choose hide node need adjust output weight link hide layer output layer slfns implementation activation function additive node bound nonconstant piecewise continuous function grxe2x86x92r activation function rbf node integrable piecewise continuous function grxe2x86x92r xe2x88xabrgxdxxe2x89xa00 propose incremental method efficient sflns continuous include nondifferentiable activation function slfns piecewise continuous threshold activation function compare popular method new network fully automatic user need intervene learn process manually tune control parametern',\n",
       " 'DeShuang Huang Lin Zhu abstract regularize discriminant analysis rda special case uncorrelated linear discriminant analysis ulda important subspace learn method propose recently handle small sample size s problem linear discriminant analysis lda important unsolved issue rda automatically determine appropriate regularization parameter resort unscalable procedure like crossvalidation cv paper develop novel efficient algorithm automatically estimate regularization parameter base geometric interpretation rda provide formal analysis propose method robust perturbation feature space train data extensive experiment benchmark datasets verify scalability effectiveness approach compare stateoftheart algorithmsn',\n",
       " 'D.L. Yu J.B. Gomm recursive orthogonal square rols numerically robust method solve output layer weight radial basis function rbf network require memory batch alternative paper use rols extend select center rbf network show information available rols algorithm network train sequentially select center minimize network output error provide efficient method network reduction achieve small architecture acceptable accuracy retrain selection method develop forward backward method illustrate application rbf network model nonlinear time series real multiinputmultioutput chemical process final network model obtain achieve acceptable accuracy significant reduction number require centern',\n",
       " 'JiXiang Du DeShuang Huang paper novel heuristic structure optimization methodology radial basis probabilistic neural network rbpnns propose minimum volume cover hyperspheres mvch algorithm propose select initial hiddenlayer center rbpnn recursive orthogonal square algorithm rolsa combine particle swarm optimization pso algorithm adopt optimize initial structure rbpnn propose algorithm evaluate benchmark classification problem realworld application problem plant specie identification task involve 50 plant specie palmprint recognition task experimental result propose algorithm feasible efficient structure optimization rbpnn rbpnn achieve high recognition rate good classification efficiency multilayer perceptron network mlpns radial basis function neural network rbfnns task experimental result illustrate generalization performance optimize rbpnn plant specie identification task markedly good optimize rbfnnn',\n",
       " 'C.J. Harris X. Hong efficient learn algorithm model subset selection introduce base new composite cost function simultaneously optimize model approximation ability model robustness adequacy derive model parameter estimate forward orthogonal square model subset selection cost function include doptimality design criterion maximize determinant design matrix subset ensure model robustness adequacy parsimony final model propose approach base forward orthogonal square ols algorithm new doptimalitybase cost function construct base orthogonalization process gain computational advantage maintain inherent advantage computational efficiency associate conventional forward ols approach illustrative example include demonstrate effectiveness new approachn',\n",
       " 'Gerard Dreyfus Yacine Oussar abstract present original initialization procedure parameter feedforward wavelet network prior train gradientbased technique take advantage wavelet frame stem discrete wavelet transform u selection method determine set best wavelet center dilation parameter initial value subsequent train result obtain model simulate process compare obtain heuristic initialization procedure effectiveness propose method demonstratedn',\n",
       " 'Michael A. Balikhin HuaLiang Wei Stephen A. Billings novel model framework propose construct parsimonious flexible multiscale radial basis function network rbf unlike conventional standard single scale rbf network basis function common kernel width new network structure adopt multiscale gaussian function base select centre multiple kernel width provide flexible representation good generalization property general nonlinear dynamical system direct extension traditional single scale gaussian network new multiscale network easy implement quick learn standard learn algorithm kmeans cluster algorithm improve orthogonal square ols algorithm determine unknown parameter network model include centre width basis function weight basis function demonstrate new network lead parsimonious model good generalization property compare traditional single width rbf networkn',\n",
       " 'DeShuang Huang CanYi Lu dimensionality reduction dr method commonly principled way understand highdimensional data facial image paper propose new supervise dr method call optimize projection sparse representation base classification opsrc base recent face recognition method sparse representation base classification src src seek sparse linear combination train data give query image make decision minimal reconstruction residual opsrc design decision rule src aim reduce withinclass reconstruction residual simultaneously increase betweenclass reconstruction residual train data projection optimize match mechanism src src perform opsrc transform space feasibility effectiveness propose method verify yale orl umist database promise resultsn',\n",
       " 'HuaLiang Wei S.A. Billings new class wavelet network wns propose nonlinear identification new network model structure highdimensional choose superimposition number function few variable expand function truncate wavelet decomposition multivariate nonlinear network convert linearintheparameter regression solve leastsquare type method efficient model term selection approach base forward orthogonal square ols algorithm error reduction ratio err apply solve linearintheparameters problem present study main advantage new wn exploit attractive feature multiscale wavelet decomposition capability traditional neural network adopt analysis variance anova expansion wns handle nonlinear identification problem high dimensionsn',\n",
       " 'DeShuang Huang Chao Wang Bo Li paper supervise feature extraction method name orthogonal discriminant projection odp present extension spectral map method propose algorithm maximize weight difference nonlocal scatter local scatter weight node graph adjust accord class information local information experiment feret face data yale face data mnist handwrite digit data validate odp offer good recognition rate feature extraction method local preserve projection lpp unsupervise discriminant projection udp orthogonal lpp olppn',\n",
       " 'C.J. Harris X. Hong efficient learn algorithm model subset selection introduce base new composite cost function simultaneously optimize model approximation ability model adequacy derive model parameter estimate forward orthogonal square subset selection cost function include aoptimality design criterion minimize variance parameter estimate ensure adequacy parsimony final model illustrative example include demonstrate effectiveness new approachn',\n",
       " 'Xiaolin Wang Long Zhang Xiaoquan Tang abstract huge class nonlinear dynamic system approximate nonlinear autoregressive exogenous input narx model paper propose novel method sparse augment lagrangian sal narx model variable selection parameter estimation firstly split augment lagrangian shrinkage algorithm salsa apply produce intermediate model subsampling technique model term high select probability choose final model follow model parameter estimation salsa model sparsity algorithm convergence guarantee theoretical analysis nonlinear example realworld application process industry demonstrate effectiveness advantage propose method comparison popular methodsn',\n",
       " 'Haiyan Zhang Xiaoping Zhou Jianping Li James T. Kwok Xun Liang Yuefeng Ma semisupervise square support vector machine lss3vm important enhancement square support vector machine semisupervise learn give data collect real world label semisupervise approach applicable standard supervise approach train method lss3vm exist problem derive optimal decision hyperplane efficiently effectually solve paper fully weight model lss3vm propose simple integer program ip model introduce equivalent transformation solve model base distance unlabeled data decision hyperplane new indicator design represent possibility label unlabeled datum reverse iteration train indicator construct extend candidate set consist index unlabeled data high possibility integrate information unlabeled data algorithm degenerate special scenario previous algorithm extend candidate set reduce set element strategy utilize determine descent direction base extend candidate set furthermore develop novel method locate good start point base property equivalent ip model combine extend candidate set carefully compute start point fast algorithm solve lss3vm quasioptimally propose choice quasioptimal solution result low computational cost avoidance overfitting experiment algorithm equip design strategy effective algorithm follow aspect 1 computational complexity 2 generalization ability 3 flexibility algorithm algorithm similar level performance remain aspectn',\n",
       " 'Michael E. Tipping paper introduce general bayesian framework obtain sparse solution regression classification task utilise model linear parameter framework fully general illustrate approach particular specialisation denote u0027relevance vector machineu0027 rvm model identical functional form popular stateoftheart u0027support vector machineu0027 svm demonstrate exploit probabilistic bayesian learn framework derive accurate prediction model typically utilise dramatically few basis function comparable svm offer number additional advantage include benefit probabilistic prediction automatic estimation u0027nuisanceu0027 parameter facility utilise arbitrary basis function nonu0027merceru0027 kernel bayesian framework associate learn algorithm rvm illustrative example application comparative benchmark offer explanation exceptional degree sparsity obtain discus demonstrate advantageous feature potential extension bayesian relevance learnn',\n",
       " 'Michael I. Jordan Laurent El Ghaoui Peter Bartlett Nello Cristianini Gert R. G. Lanckriet kernelbased learn algorithm work embed data euclidean space search linear relation embed data point embed perform implicitly specify inner product pair point embed space information contain socalled kernel matrix symmetric positive semidefinite matrix encode relative position point specify matrix amount specify geometry embed space induce notion similarity input spaceclassical model selection problem machine learn paper kernel matrix learn data semidefinite program sdp technique apply kernel matrix associate train test data give powerful transductive algorithmusing label data learn embed unlabel similarity test point infer train point label importantly learn problem convex obtain method learn model class function local minimum furthermore approach lead directly convex method learn 2norm soft margin parameter support vector machine solve important open problemn',\n",
       " 'Sayan Mukherjee Olivier Bousquet Vladimir Vapnik Olivier Chapelle problem automatically tune multiple parameter pattern recognition support vector machine svms consider minimize estimate generalization error svms gradient descent algorithm set parameter usual method choose parameter base exhaustive search intractable soon number parameter exceed experimental result as feasibility approach large number parameter 100 demonstrate improvement generalization performancen',\n",
       " 'K.R. Muller T. Onoda G. Ratsch recently ensemble method like adaboost apply successfully problem seemingly defy problem 18rarely overfits low noise regime clearly high noise level central understand fact margin distribution adaboost view constraint gradient descent error function respect margin adaboost asymptotically achieve hard margin distribution algorithm concentrate resource hardtolearn pattern interestingly similar support vector hard margin clearly suboptimal strategy noisy case regularization case xe2x80x9cmistrustxe2x80x9d data introduce algorithm alleviate distortion single difficult pattern outlier cause margin distribution propose regularization method generalization original adaboost algorithm achieve soft margin particular suggest 1 regularize adaboostreg gradient decent directly respect soft margin 2 regularize linear quadratic program lpqp adaboost soft margin attain introduce slack 195simulations demonstrate propose regularize adaboosttype algorithm useful yield competitive result noisy datan',\n",
       " 'Chao He M. Girolami requirement reduce computational cost evaluate point probability density estimate employ parzen window estimator wellknown problem paper present reduce set density estimator provide kernelbased density estimator employ small percentage available data sample optimal lsub 2 sense require spl oscrnsup 2 optimization routine estimate require kernel weight coefficient propose method provide similar level performance accuracy sparseness representation support vector machine density estimation require spl oscrnsup 3 optimization routine previously show consistently outperform gaussian mixture model demonstrate propose density estimator consistently provide superior density estimate similar level data reduction provide recently propose densitybased multiscale data condensation algorithm addition comparable computational scale additional advantage propose method extra free parameter introduce regularization bin width condensation ratio make method simple straightforward approach provide reduce set density estimator comparable accuracy sample parzen density estimatorn',\n",
       " 'Aun Neow Poo S.Sathiya Keerthi Kaibo Duan choose optimal hyperparameter value support vector machine important step svm design usually minimize estimate generalization error relate performance measure paper empirically study usefulness simple performance measure inexpensive compute sense require expensive matrix operation involve kernel matrix result point measure adequate functionals tune svm hyperparameters svms l1 softmargin formulation simple measure yield performance uniformly good kfold cross validation joachimsxe2x80x99 xialpha bind gacv wahba et al come perform reasonably svms l2 softmargin formulation radius margin bind give good prediction optimal hyperparameter valuen',\n",
       " 'Yoshua Bengio Pascal Vincent match pursuit algorithm learn function weight sum basis function sequentially append function initially basis approximate target function leastsquares sense match pursuit extend use nonsquared error loss function build kernelbased solution machine learn problem keep control sparsity solution present version algorithm make optimal choice basis weight previously choose base finally link boost algorithm rbf train procedure extensive experimental comparison svms classification give show comparable result typically sparse modelsn',\n",
       " 'Min Han Meng Joo Er Ning Wang novel constructive destructive parsimonious extreme learn machine cp dpelm propose paper virtue propose elm parsimonious structure excellent generalization multiinputmultioutput single hidelayer feedforward network slfns obtain propose elm develop innovative decomposition recursive orthogonal square procedure sequential partial orthogonalization spo salient feature propose approach follow 1 initial hide node randomly generate elm methodology recursively orthogonalized upper triangular matrix dramatic reduction matrix size 2 constructive spo cpelm focus partial matrix subcolumn select regressor include nonzeros column destructive spo dpelm operate partial matrix include element determine remove regressor 3 termination criterion cp dpelm simplify additional residual error reduction method 4 output weight slfn need solve model selection procedure derive final upper triangular equation backward substitution single multioutput realworld regression data set verify effectiveness superiority cp dpelm term parsimonious architecture generalization accuracy innovative application nonlinear timeseries model demonstrate superior identification resultsn',\n",
       " 'C. J. Harris X. Hong S. Chen classical parzen window pw estimate desire response kernel density estimation formulate regression problem orthogonal forward regression technique adopt construct sparse kernel density skd estimate propose algorithm incrementally minimise leaveoneout test score select sparse kernel model local regularisation method incorporate density construction process enforce sparsity kernel weight select sparse model finally update multiplicative nonnegative quadratic program algorithm ensure nonnegative unity constraint kernel weight desire ability reduce model size kernel width propose method parameter need tune user require specify additional criterion terminate density construction procedure example demonstrate ability simple regressionbased approach effectively construct skd estimate comparable accuracy fullsample optimise pw density estimaten',\n",
       " 'S. Chen locally regularize orthogonal square lrols algorithm propose construct parsimonious sparse regression model generalize associate orthogonal weight regression model individual regularization parameter ability orthogonal square model selection produce sparse model good generalization performance greatly enhance furthermore assistance local regularization terminate model selection procedure clear comparison stateoftheart method construct sparse regression model know relevance vector machine give propose lrols algorithm show posse considerable computational advantage include condition solution fast convergence speedn',\n",
       " 'Daniel D. Lee Lawrence K. Saul Fei Sha derive multiplicative update solve nonnegative quadratic program problem support vector machine svms update simple close form prove converge monotonically solution maximum margin hyperplane update optimize traditionally propose objective function svms involve heuristic choose learn rate decide variable update iteration adjust quadratic program variable parallel guarantee improvement iteration analyze asymptotic convergence update coefficient nonsupport vector decay geometrically zero rate depend margin practice update converge rapidly good classifiersn',\n",
       " 'C.J. Harris Sheng Chen Xia Hong classical parzen window pw estimate target function sparse kernel density estimator construct forwardconstrained regression fcr manner propose algorithm select significant kernel time leaveoneout loo test score minimize subject simple positivity constraint forward stage model parameter estimation forward stage simply solution jackknife parameter estimator single parameter subject positivity constraint check select kernel associate kernel width update gaussnewton method model parameter estimate fix propose approach simple implement associate computational cost low numerical example employ demonstrate efficacy propose approachn',\n",
       " 'HaiBo Zhang ZhiAn Zhang ZhongHua Du JianGuo Sun YongPing Zhao kernel minimum square error kmse computationally simple need solve linear equation set suffer drawback test phase computational efficiency decrease seriously train sample increase underlie reason solution naive kmse represent train sample feature space paper method select significant node kmse propose calculation round present algorithm prune train sample make contribution objective function call plockmse accelerate train procedure batch socall nonsignificant node prune instead plockmse speedup algorithm name mplockmse short efficacy feasibility propose plockmse mplockmse experiment benchmark data set realworld instance report experimental result demonstrate plockmse mplockmse require few significant node compare algorithm computational efficiency test phase best suitable environment have strict demand computational efficiency addition perform experiment easily know propose mplockmse accelerate train procedure sacrifice computational efficiency test phase reach generalization performance finally ploc mploc propose regression domain easily extend classification problem algorithm kernel ridge regressionn',\n",
       " 'Gerard Dreyfus Gaetan Monari abstract nonlinear parameterised model effect withdraw example train set predict focus prediction error leftout example confidence interval prediction example derive rigorous expression firstorder expansion parameter space gradient quadratic cost function specify validity condition consequence derive approximate expression prediction error give example confidence interval thereof example withdraw train set influence example model summarise single parameter result applicable leaveoneout crossvalidation considerable decrease computation time respect conventional leaveoneout paper focus theoretical aspect question academic illustration largescale industrial example describe 9n',\n",
       " 'Jie Yang Shengzheng Wang abstract far square regression lsr widely data model method statistic mathematics effectiveness completeness play important underlie role extension regularize lsr weight lsr lasso lsr discriminative model allow sample target variable condition observation paper present latent lsr llsr generative model enable lsr exploit structural information hide explanatory variable impose sparsityencouraging prior precision matrix latent variable maximum posteriori map estimate apply obtain point estimate model parameter toy example real data test suggest effectiveness llsrn',\n",
       " 'Johan A. Suykens Hugo Hamme Kris Brabanter Kristiaan Pelckmans Peter Karsmakers work study optimization scheme compute sparse approximate solution overdetermined linear system sparse conjugate direction pursuit scdp aim construct solution small number nonzero nonsparse coefficient motivation work set machine learn sparse model typically exhibit good generalization performance lead fast evaluation exploit define scalable algorithm main idea build iteratively conjugate set vector increase cardinality iteration solve small linear subsystem exploit structure conjugate basis algorithm converge d iteration ddimensional system ii computational complexity close classical conjugate gradient algorithm iii especially efficient iteration suffice produce good approximation example application scdp fixedsize square support vector machine fslssvm discuss result scheme efficiently find good model size fslssvm set scalable largescale machine learn task algorithm empirically verify classification context discussion include algorithmic issue component selection criterion computational analysis influence additional hyperparameters determination suitable stop criterionn',\n",
       " 'HaiBo Zhang YuChen Zhang ZhiAn Zhang ZhongHua Du JianGuo Sun YongPing Zhao abstract recently algorithm recursive reduce square support vector regression rrlssvr propose reduce number support vector demonstrate good sparseness compare algorithm consider effect previously select support vector willselect one selection process actually independent paper improve scheme name irrlssvr propose update support weight immediately new sample select support vector result train sample lead large reduction target function choose construct approximation subset efficacy feasibility propose irrlssvr lot experiment favorable viewpoint irrlssvr need number support vector reach generalization performance rrlssvr beneficial reduce test time favorable realtimen',\n",
       " 'HaiBo Zhang ZhiAn Zhang ZhongHua Du YongPing Zhao paper fast method select feature kernel minimum square error kmse propose mitigate computational burden case size train pattern large compare existent algorithm select feature kmse iterative kmse viz ikmse show good property enhance computational efficiency sacrifice generalization performance experimental report benchmark data set nonlinear autoregressive model real problem address efficacy feasibility propose ikmse addition ikmse easily extend classification fieldsn',\n",
       " 'Hong Liu Jane You Yan Chen Qi Zhu Xiaozhao Fang Yong Xu paper improve minimum square error mse algorithm classification modify classification rule differ conventional mse algorithm obtain map best transform train sample class label exploit obtain map predict class label test sample modify minimum square error classification mmsec algorithm simultaneously predict class label test sample train sample near combine predict result ultimately classify test sample paper time propose idea advantage predict class label train sample classification test sample devise weight fusion scheme fuse predict class label train sample test sample paper interpret rationale mmsec mmsec generalize good conventional mse lead robust classification decision face recognition experiment mmsec obtain promise performancen',\n",
       " 'YanCheng Liu Meng Joo Er JingChao Sun Ning Wang paper hybrid recursive square hrls algorithm online identification sequential chunkbychunk observation propose employ optimizationbased square ols hrls initialize chunk data sample work successively recursive procedure update inverse matrix minimal dimension rankdeficiency contribute fast stable online identification norm output weight train error minimize simultaneously hrls achieve high accuracy term generalization approximation simulation study comprehensive comparison demonstrate hrls numerically stable superior algorithm term accuracy speedn',\n",
       " 'Haikuan Wang Minrui Fei Xue Li Kang Li Dajun Du paper investigate construction linearintheparameter litp model multioutput regression problem exist stepwise forward algorithm choose regressor term time maximize model error reduction ratio drawback procedure guarantee sparse model especially highly noisy learn condition main objective paper improve sparsity generalization capability model multioutput regression problem reduce computational complexity achieve propose novel multioutput twostage locally regularize model construction mtlrmc method extreme learn machine elm new algorithm nonlinear parameter term width gaussian function power polynomial term firstly determine elm initial multioutput litp model generate accord termination criterion stage significance select regressor check insignificant one replace second stage propose method produce optimize compact model regularize parameter reduce computational complexity proper regression context allow fast implementation propose method simulation result confirm effectiveness propose techniquen',\n",
       " 'Fatemeh Ghorbani Reza Sabzevari Gh. A. Montazer paper present novel approach learn algorithm commonly train radial basis function rbf neural network approach application need realtime capability retrain rbf neural network propose method threephase learn algorithm optimize functionality optimum steep decent osd learn method methodology focus attain great precision initialize center width rbf unit rbf neural network welladjusted rbf unit train process result good performance network response method propose reach good performance rbf neural network few train iteration critical issue realtime application compare result employ different learn strategy show interest outcome come papern',\n",
       " 'Jian Xu Yuwang Yang Qianmu Li Yaping Li Fang Qian Lei Xu work study energyefficient resource allocation problem base chanceconstrained program overlay cognitive orthogonal frequency division multiplexing ofdm objective function minimize total power consumption constraint condition include requirement outage probability feasibility subcarrier allocation solution order solve chanceconstrained resource allocation problem step take develop hybrid quantum particle swarm optimization hqpso step define uncertain function accord outage probability constraint condition utilize radial basis function brf neural network second step hqpso include quantum particle swarm optimization qpso rbf neural network propose simulation result demonstrate total power consumption hqpso small algorithm outage probability satisfy welln',\n",
       " 'Minrui Fei Xue Li Kang Li Dajun Du paper investigate gene selection problem microarray data small sample variant correlation exist algorithm usually require expensive computational effort especially thousand gene condition main objective paper effectively select informative gene microarray data make computational expense affordable achieve propose novel forward gene selection algorithm fgsa overcome small sampleu0027 problem augment data technique firstly employ produce augment data set take inspiration gene selection method l2norm penalty introduce recently propose fast regression algorithm achieve group selection ability finally define proper regression context propose method fast implement software significantly reduce computational burden computational complexity analysis simulation result confirm effectiveness propose algorithm comparison approachesn',\n",
       " 'Witold Pedrycz Mingli Song study concern construction granular neural network gnnsarchitectures form direct result reconciliation result produce collection local neural network construct basis individual data set cognizant diversity result produce collection network arrive concept granular neural network produce result form information granule plain numeric entity reflective diversity result generate contribute network design granular neural network exploit concept justifiable granularity introduce performance index quantify quality information granule generate granular neural network study illustrate aid machine learn data set experimental result provide detail insight develop granular neural networkn',\n",
       " 'Alejandro Pazos Juan Rabunal Julian Dorado Daniel Rivero development artificial neural network anns traditionally slow process human expert need experiment different architectural procedure present correct result solve specific problem work describe new technique u genetic program gp order automatically develop simple anns low number neuron connection experiment carry order measure behavior compare result obtain ann generation train method evolutionary computation ec tool obtain result bad case comparable exist technique case substantially good explain important feature variable discrimination provide new information problem solvedn',\n",
       " 'JianXun Peng Kang Li neural input selection important stage neural network configuration neural model control nonlinear dynamic system input neural network include variable time lag choose set significant input combinational problem selection procedure time consume paper modelbased neural input selection method propose essentially neural input selection transform problem identify significant term linearintheparameters model fast method propose identify significant nonlinear term function neural input group select theoretic analysis simulation example demonstrate effectiveness efficiency propose modelbased approachn',\n",
       " 'W. Ser K.C. Tan K.Z. Mao network structure determination important issue pattern classification base probabilistic neural network study supervise network structure determination algorithm propose propose algorithm consist part run iterative way identify appropriate smooth parameter genetic algorithm second determine suitable pattern layer neuron forward regression orthogonal algorithm propose algorithm capable offer fairly small network structure satisfactory classification accuracyn',\n",
       " 'GuangBin Huang K.Z. Mao central problem train radial basis function neural network selection hide layer neuron paper propose select hide layer neuron base data structure preserve criterion data structure denote relative location sample highdimensional space preserve data structure sample include close separation boundary different class neuron subset select retain separation margin underlie set hide layer neuron direct result network obtain tend generalize welln',\n",
       " 'A.F. AlAjlouni J.J. Carroll R.J. Schilling technique approximate continuous function n variable radial basis function rbf neural network present method u ndimensional raisedcosine type rbf smooth compact support rbf network coefficient loworder polynomial function input simple computational procedure present significantly reduce network train evaluation time storage space reduce allow nonuniform grid point rbfs center network output show continuous continuous derivative network approximate nonlinear dynamic result boundedinput boundedoutput stable special case linear rbf network representation exact domain define optimal term number distinct storage parameter require example present illustrate effectiveness techniquen',\n",
       " 'P. Saratchandran N. Sundararajan GuangBin Huang Runxuan Zhang paper present performance evaluation recently develop grow prune radial basis function gaprbf algorithm classification problem early gaprbf evaluate function approximation problem improvement gaprbf enhance performance accuracy speed describe result algorithm refer fast gaprbf fgaprbf performance comparison fgaprbf algorithm gaprbf minimal resource allocation network mran algorithm base benchmark classification problem viz phoneme segment satimage dna present result indicate fgaprbf produce high classification accuracy reduce computational complexityn',\n",
       " 'K.Z. Mao classification application role hide layer neuron radial basis function rbf neural network interpret function map input pattern nonlinear separable space linear separable space new space response hide layer neuron form new feature vector discriminative power determine rbf center present study propose choose rbf center base fisher ratio class separability measure objective achieve maximum discriminative power implement idea multistep procedure combine fisher ratio orthogonal transform forward selection search method motivation employ orthogonal transform decouple correlation response hide layer neuron class separability provide individual rbf neuron evaluate independently strength method double fold method select parsimonious network architecture second method select center provide large class separationn',\n",
       " 'C.J. Harris Sheng Chen Xia Hong brief propose orthogonal forward regression ofr algorithm base principle branch bind bb aoptimality experimental design forward regression step candidate pool candidate regressors refer s evaluate turn possible decision 1 select include model 2 remain s evaluation forward regression step 3 rest permanently eliminate s base bb principle combination aoptimality composite cost function model structure determination simple adaptive diagnostics test propose determine decision bindary 2 3 propose algorithm significantly reduce computational cost aoptimality ofr algorithm numerical example demonstrate effectiveness propose algorithmn',\n",
       " 'Chee Kheong Siew QinYu Zhu GuangBin Huang abstract clear learn speed feedforward neural network general far slow require major bottleneck application past decade key reason 1 slow gradientbase learn algorithm extensively train neural network 2 parameter network tune iteratively learn algorithm unlike conventional implementation paper propose new learn algorithm call e xtreme l earn m achine elm s inglehide l ayer f eedforward neural n etworks slfns randomly choose hide node analytically determine output weight slfns theory algorithm tend provide good generalization performance extremely fast learn speed experimental result base artificial real benchmark function approximation classification problem include large complex application new algorithm produce good generalization performance case learn thousand time fast conventional popular learn algorithm feedforward neural network 1n',\n",
       " 'Lei Chen GuangBin Huang unlike conventional neural network theory implementation huang et al universal approximation incremental constructive feedforward network random hide node ieee transaction neural network 174 2006 879892 recently propose new theory singlehidelayer feedforward network slfns randomly generate additive radial basis function rbf hide node accord continuous sample distribution work universal approximators result incremental extreme learn machine ielm outperform popular learn algorithm ielm randomly generate hide node analytically calculate output weight slfns ielm recalculate output weight exist node new node add paper show retain simplicity convergence rate ielm improve recalculate output weight exist node base convex optimization method new hide node randomly add furthermore give type piecewise continuous computational hide node possibly neural alike node slfns fnxi1nbigxaibi work universal approximators adjustable hide node parameter function approximation point view hide node parameter u0027u0027generalizedu0027u0027 slfns include sigmoid network rbf network trigonometric network threshold network fuzzy inference system fully complex neural network highorder network ridge polynomial network wavelet network actually randomly generate accord continuous sample distribution theory parameter slfns analytically determine elm instead tunedn',\n",
       " 'Lei Chen GuangBin Huang recently incremental algorithm refer incremental extreme learn machine ielm propose huang et al gb huang l chen ck siew universal approximation incremental constructive feedforward network random hide node ieee trans neural network 174 2006 879892 randomly generate hide node analytically determine output weight huang et al gb huang l chen ck siew universal approximation incremental constructive feedforward network random hide node ieee trans neural network 174 2006 879892 prove theory additive rbf hide node generate randomly network construct ielm work universal approximator recent study hide node network play minor role network output eventually increase network complexity order avoid issue obtain compact network architecture paper propose enhance method ielm refer eielm learn step hide node randomly generate hide node lead large residual error decrease add exist network output weight network calculate simple way original ielm generally speak propose enhance ielm work widespread type piecewise continuous hide noden',\n",
       " 'Dong Sun Park Zhihui Wang Jianwei Zhao abstract ensemble online sequential extreme learn machine eoselm average online sequential extreme learn machine oselms learn data onebyone chunkbychunk fix vary chunk size eoselm provide high accuracy few train time good generalization performance stability popular sequential learn algorithm plenty practical application stock forecast weather forecast train data timeliness datum period validity order reflect timeliness train data process learn improve eoselm call online sequential extreme learn machine forget mechanism foselm propose paper propose foselm retain advantage eoselm improve learn effect discard outdated data quickly process learn reduce bad affection follow learn detail performance comparison foselm carry eoselm stock price shortterm prediction experimental result foselm high accuracy few train time good stability shortterm predictability eoselmn',\n",
       " 'David M.W. Powers Junfa Liu Runyuan Wang Xibin Jia paper propose learn algorithm call semisupervised online sequential elm denote soselm aim provide solution stream data application learn newly arrive observation call chunk addition soselm utilize label unlabel train data combine advantage exist algorithm online sequential elm oselm semisupervised elm sselm rationale algorithm exploit optimal condition alleviate empirical risk structure risk sselm combination block calculation matrix similar oselm efficient implementation soselm algorithm viable additional assumption negligible structural relationship chunk different time experiment perform standard benchmark problem regression balance binary classification unbalance binary classification multiclass classification compare performance propose soselm oselm sselm experimental result soselm outperform oselm generalization performance similar train speed addition outperform sselm low supervision overheadsn',\n",
       " 'GuangBin Huang Yeng Chai Soh Yuan Lan paper attempt address architectural design elm regressor apply constructive method basis elm algorithm nonlinearities elm network fix randomly generate parameter network correspond linear regression model selection hide node regard subset model selection linear regression propose constructive hide node selection elm refer cselm select optimal number hide node unbiased risk estimation base criterion cp reach minimum value comparison propose cselm model selection algorithm elm evaluate real benchmark regression application empirical study show cselm lead compact network structure automaticallyn',\n",
       " 'Meng Joo Er Zhifei Shao extreme learn machine elm novel learn scheme single hide layer feedforward neural network attract great deal research attention decade extremely fast learn speed popular variant elm online sequential elm oselm deal sequential learn task limitation exist oselm require initialization phase predefined important parameter run singularity problem inconsistent potentially unreliable performance paper online sequential regularize elm osrelm propose address aforementioned issue main idea incorporate regularization method improve generalization performance new update formula eliminate initialization phase enable osrelm adapt new data effective reliable manner efficient leaveoneout crossvalidation method implement finally matrix reconstruction method employ address unstable update issue unlike elm variant greatly jeopardize speed advantage elm strive limit computational load propose scheme simulation result benchmark problem osrelm reliable efficient algorithm superior generalization performance oselmn',\n",
       " 'Amaury Lendasse KajMikael Bjork Dusan Sovilj make accurate prediction difficult task encounter research domain certain case number available sample scarce provide reliable estimate challenge problem paper interest give accurate prediction possible base extreme learn machine type neural network small sample data scenario extreme learn machine literature focus choose particular model pool candidate approach usually ignore model selection uncertainty inferior performance compare combine method empirically examine model selection criterion couple new model combine approach recently propose result obtain indicate careful choice combination perform order accurate stable predictionn',\n",
       " 'Xiaofang Yuan Yaonan Wang Yimin Yang clear learn effectiveness learn speed neural network general far slow require major bottleneck application recently simple efficient learn method refer extreme learn machine elm propose huang show compare conventional method train time neural network reduce thousand time open problem elm research number hide node reduce affect learn effectiveness brief propose new learn algorithm call bidirectional extreme learn machine belm hide node randomly select theory algorithm tend reduce network output error 0 extremely early learn stage furthermore relationship network output error network output weight propose belm simulation result demonstrate propose method ten hundred time fast incremental elm algorithmsn',\n",
       " 'Amaury Lendasse Eric Severin Yoan Miche Qi Yu bankruptcy prediction widely study binary classification problem financial ratio methodology paper leaveoneoutincremental extreme learn machine looielm explore task looielm operate incremental way avoid inefficient unnecessary calculation stop automatically neuron number unknown combo method ensemble model investigate base different looielm model specific financial indicator indicator choose different strategy accord financial expertise entire process show good performance fast speed help interpret model special ration',\n",
       " 'Pedro M. Mateo Beatriz Lacruz David Lahoz extreme learn machine elm methodology learn singlehide layer feedforward neural network slfn prove extremely fast provide good generalization performance elm work randomly choose weight bias hide node analytically obtain output weight bias slfn number hide node previously fix work develop multiobjective micro genetic elm mgelm provide appropriate number hide node problem solve weight bias minimize mse multiobjective algorithm conduct criterion number hide node mean square error mse furthermore novelty mgelm incorporate regression device order decide number hide node individual population increase decrease unchanged general propose algorithm reach good error imply small number hide node data set competitor consideredn',\n",
       " 'Ning Wang Meng Joo Er Zhifei Shao abstract extreme learn machine elm attract comprehensive attention universal function approximator extremely fast learn speed good generalization performance compare learn method single layer feedforward network slfns unique feature elm input parameter hide neuron randomly generate iteratively tune dramatically reduce computational burden point randomness elm parameter result fluctuate performance paper systematically investigate performance stabilization effect bring regularize variant elm name regularize elm relm furthermore prediction sum square press statistic formula unique property relm propose semicrossvalidation algorithm effectively realize robust relmbased model selection slfns term automatic regularize extreme learn machine leaveoneout crossvalidation arelmloo simulation result arelmloo significantly reduce randomness performance elm produce nearly identical result crossvalidation proceduren',\n",
       " 'Feilong Cao Zhihui Wang Jianwei Zhao extreme learn machine elm widely train singlehide layer feedforward neural network slfns good generalization fast speed improve elm usually discus approximation problem sample data output noise sample data noise input output value errorinvariable eiv model paper novel algorithm call regularize tlelm propose approximate eiv model base elm total square tl method propose tlelm u idea elm choose hide weight apply tl method determine output weight furthermore perturbation quantity hide output matrix observe value give simultaneously comparison experiment propose tlelm square method tl method elm propose tlelm good accuracy train timen',\n",
       " 'Qingping Lin GuangBin Huang Guorui Feng open problem neural network research automatically determine network architecture give application brief propose simple efficient approach automatically determine number hide node generalize singlehidelayer feedforward network slfns need neural alike approach refer error minimize extreme learn machine emelm add random hide node slfns group group vary group size growth network output weight update incrementally convergence approach prove brief simulation result demonstrate verify new approach fast sequentialincrementalgrowing algorithm good generalization performancen',\n",
       " 'H.A. Babri YanQiu Chen GuangBin Huang multilayer perceptrons hardlimiting signum activation function form complex decision region know threelayer perceptron hide layer form arbitrary disjoint decision region twolayer perceptron hide layer form single convex decision region paper prof single hide layer feedforward neural network slfn continuous bound nonconstant activation function arbitrary bound continuous continuous activation function unequal limit infinity perceptrons form disjoint decision region arbitrary shape multidimensional case slfn unbound activation function form disjoint decision region arbitrary shapen',\n",
       " 'R.F. Stengel S. Ferrari algebraic approach represent multidimensional nonlinear function feedforward neural network present paper approach implement approximation smooth batch data contain functionu0027s input output possibly gradient information train set associate network adjustable parameter nonlinear weight equation cascade structure equation reveal treat set linear system train process network approximation property investigate linear algebra algorithm develop achieve exact approximate match inputoutput andor gradientbased train set application design forward feedback neurocontrollers show algebraic train characterize fast execution speed good generalization property contemporary optimization techniquesn',\n",
       " 'Zexuan Zhu AhHwee Tan YewSoon Ong HaiJun Rong extreme learn machine elm represent recent successful approach machine learn particularly perform pattern classification key strength elm significantly low computational time require train new classifier weight hide output node randomly choose analytically determine respectively paper address architectural design elm classifier network fewmany hide node employ lead underfittingoverfitting issue pattern classification particular propose pruneelm pelm algorithm systematic automate approach design elm classifier network pelm u statistical method measure relevance hide node begin initial large number hide node irrelevant node prune consider relevance class label result architectural design elm network classifier automate empirical study pelm commonly classification benchmark problem diverse form hide node function propose approach lead compact network classifier generate fast response robust prediction accuracy unseen data compare traditional elm popular machine learn approachn',\n",
       " 'GuangBin Huang problem necessary complexity neural network application paper learn capability storage capacity feedforward neural network consider markedly improve recent result introduce neuralnetwork modularity logically paper rigorously prof constructive method twohidelayer feedforward network tlfns 2spl radicm2n spl ltn hide neuron learn n distinct sample xsub tsub arbitrarily small error m require number output neuron imply require number hide neuron need feedforward network decrease significantly compare previous result conversely tlfn q hide neuron store qsup 24m2 distinct data xsub tsub desire precisionn',\n",
       " 'George W. Irwin Kang Li Jing Deng abstract convenient effective solve nonlinear problem model linearintheparameter litp structure nonlinear parameter width gaussian function model term need predetermine expert experience exhaustive search alternative approach optimize gradientbase technique newtonu0027s method unfortunately method need lot computation recently extreme learn machine elm show advantage term fast learn data sparsity construct model guarantee paper propose novel algorithm automatic construction nonlinear model base extreme learn machine achieve effectively integrate elm leaveoneout loo cross validation twostage stepwise construction procedure 1 main objective improve compactness generalization capability model construct elm method numerical analysis show propose algorithm involve half computation orthogonal square ols base method simulation example include confirm efficacy superiority propose techniquen',\n",
       " 'Kapil Deepak Gupta S. Balasundaram paper novel 1norm extreme learn machine elm regression multiclass classification propose linear program problem solution obtain solve dual exterior penalty problem unconstrained minimization problem fast newton method algorithm converge start point easily implement matlab main advantage propose approach lead sparse model representation mean component optimal solution vector zero decision function determine number hide node comparison elm numerical experiment perform number interest realworld benchmark datasets result compare elm additive radial basis function rbf hide node optimally prune elm opelm support vector machine svm method similar good generalization performance propose method test data elm opelm svm clearly illustrate applicability usefulnessn',\n",
       " 'Siow Wee Chang Shing Chiang Tan Hwa Jen Yap Keem Siah Yap Shen Yuong Wong paper present fuzzy extreme learn machine felm embed fuzzy membership function rule hide layer extreme learn machine elm similar concept elm employ random initialization technique parameter felm randomly assign standard deviation membership function matrixc rulecombination matrix matrixd donxe2x80x99t care dc matrix fuzzy ifthen rule formulate rulecombination matrix felm dc approach adopt minimize number input attribute rule furthermore felm utilize output weight elm form target class confidence factor rule indicate correspond consequent parameter determine analytically operation felm equivalent fuzzy inference benchmark data set real world fault detection diagnosis problem empirically evaluate efficacy propose felm handle pattern classification task result accuracy rate felm comparable superior elm distinctive ability provide explicit knowledge form interpretable rule basen',\n",
       " 'Meng Joo Er Zhifei Shao know leaveoneout crossvalidation loocv highly reliable procedure term model selection unfortunately extremely tedious method rarely deploy practical application paper highly efficient leaveoneout crossvalidation loocv formula develop integrate popular regularize extreme learn machine relm main contribution paper propose algorithm term efficient loocvbased relm eloorelm effectively efficiently update loocv error regularization parameter automatically select optimal model limit user intervention rigorous analysis computational complexity show eloorelm include tune process achieve similar efficiency original relm predefined parameter scale linearly size train data early termination criterion introduce speed learn process experimentation study benchmark datasets eloorelm achieve comparable generalization performance support vector machine svm significantly high learn efficiency importantly compare trial error tune procedure employ original relm eloorelm provide reliable result virtue incorporate loocv proceduren',\n",
       " 'Zixiao Guan Chaoxing He Baihai Zhang Fenxi Yao Weidong Zou prediction solar greenhouse temperature humidity important play critical role greenhouse cultivation account important set predictive model temperature humidity precisely predict temperature humidity reduce potential financial loss paper present novel temperature humidity prediction model base convex bidirectional extreme learn machine cbelm simulation result convergence rate bidirectional extreme learn machine belm improve retain simplicity simply recalculate output weight exist node base convex optimization method new hide node randomly add performance cbelm model compare model approach apply predict solar greenhouse temperature humidity experiment result cbelm model prediction accurate belm propagation neural network bpnn support vector machine svm radial basis function rbf consider suitable effective method predict solar greenhouse temperature humidityn',\n",
       " 'Senchun Chai Fenxi Yao Baihai Zhang Guoqiang Zeng abstract incremental extreme learn machine prove efficient simple universal approximator network architecture large inefficient node tiny effect reduce residual error point output weight square solution reduce inefficient node method call bidirectional elm belm analytically calculate input weight node propose analyze belm improve achieve good performance compact structure paper propose modify belm mbelm orthogonalization method involve belm orthogonalize output vector hide node result vector take output vector mbelm greatly diminish inefficient node obtain preferable output weight vector square solution good convergence rate compact network architecture specifically prove theory mbelm reduce residual error zero add node network simulation result verify conclusion mbelm reach small low limit residual error ielm methodsn',\n",
       " 'Wei Wu Jie Yang Atlas Khan learn scheme base extreme learn machine elm l12 regularization propose double parallel feedforward neural network elm widely fast learn method feedforward network single hide layer key problem elm choice minimum number hide node resolve problem propose combine l12 regularization method popular recent year informatics elm show experiment involvement l12 regularizer dpfnn elm result hide node equally good performancen',\n",
       " 'P.M. Mateo D. Lahoz B. Lacruz xc2xb5gelm multiobjective evolutionary algorithm look best term mse compact artificial neural network elm methodology work present xc2xb5g2elm upgrade version xc2xb5gelm previously present author upgrade base key element specifically design approach initialization weight initial artificial neural network introduction resowing process select population evolve change process modify weight artificial neural network test proposal consider stateoftheart extreme learn machine elm algorithm confront wide wellknown set continuous regression classification problem conduct experiment prove xc2xb5g2elm show good general performance previous version competitor guess combination evolutionary algorithm elm methodology promise subject study allow design good train algorithm artificial neural networkn',\n",
       " 'Jing Deng George W. Irwin Kang Li Dajun Du paper investigate design linearintheparameter litp regression classifier twoclass problem exist algorithm generally learn classifier model available train data base stop criterion akaikeu0027s final prediction error fpe drawback classifier obtain directly obtain base generalization capability main objective paper improve sparsity generalization capability classifier reduce computational expense produce achieve propose automatic twostage locally regularize classifier construction tslrcc method extreme learn machine elm new algorithm nonlinear parameter term width gaussian function power polynomial term firstly determine elm initial classifier generate direct evaluation candidate model accord leaveoneout loo misclassification rate stage significance select regressor term check insignificant one replace second stage reduce computational complexity proper regression context define allow fast implementation propose method simulation result confirm effectiveness propose techniquen',\n",
       " 'YeBo Li Bing Li YongPing Zhao constructive destructive parsimonious extreme learn machine cpelm dpelm recently propose sparsify elm comparison cpelm dpelm own advantage number hide node lose edge respect train time paper equivalent measure propose accelerate dpelm adpelm result adpelm keep hide node dpelm need train time cpelm especially important train time sensitive scenario similar idea extend regularize elm relm yield adprelm adprelm accelerate train process dprelm work good cprelm term number hide node train time addition computational complexity propose accelerate scheme analyze theory report result benchmark data set effectiveness usefulness propose accelerate scheme paper confirm experimentallyn',\n",
       " 'Kang Li Shiji Song Bing Li abstract square support vector machine lssvms express train term solve linear equation equivalent quadratic program qp linear equality constraint contrast qp low upper bind linear equality constraint conventional support vector machine svms large scale problem presence linear equality constraint impede application develop method paper eliminate linear equality constraint qp train lssvm unconstrained propose fast iterative single data approach stepsize acceleration unconstrained qp result combine selection rule variable coordinate descent approach propose approach superior successive overrelaxation sor method update variable iteration make propose approach simple flexible sequential minimal optimization smo method computational experiment result benchmark data set propose approach efficient exist single data approach smo methodn',\n",
       " 'Carlos Henggeler Antunes Rui Araujo Francisco Souza Tiago Matias paper propose learn framework singlehidden layer feedforward neural network slfn call optimize extreme learn machine oelm oelm structure parameter slfn determine optimization method output weight like batch elm obtain square algorithm tikhonovu0027s regularization order improve slfn performance presence noisy data optimization method set input variable hiddenlayer configuration bias input weight tikhonovu0027s regularization factor propose framework test optimization method genetic algorithm simulate anneal differential evolution 16 benchmark problem available public repositoriesn',\n",
       " 'Qian Leng Aruna Tiwari Chandan Gautam abstract oneclass classification occ prime concern researcher effectively employ discipline traditional method base oneclass classifier time consume iterative process parameter tune paper present occ method thirteen variant base extreme learn machine elm online sequential elm oselm propose classifier mainly lie category reconstruction base boundary base propose classifier belong reconstruction base belong boundary base present type learn viz online offline learn occ method offline remain online method offline method method perform random feature map method perform kernel feature map present comprehensive discussion method comparison kernel feature map base approach test rbf kernel online version oneclass classifier test type node viz additive rbf know fact threshold decision crucial factor case occ different threshold decide criterion employ far analyze effectiveness threshold decide criterion method test artificial datasets check boundary construction capability benchmark datasets different discipline evaluate performance classifier propose classifier exhibit good performance compare traditional oneclass classifier elm base oneclass classifier propose oneclass classifier intend expand functionality toolbox occ dd toolbox method totally compatible present feature toolboxn',\n",
       " 'Marko RobnikSikonja plenty problem data available scarce expensive propose generator semiartificial data similar property original data enable development test different data mine algorithm optimization parameter generate data allow largescale experimentation simulation danger overfitting propose generator base radial basis function network learn set gaussian kernel gaussian kernel generative mode generate new data distribution as quality generate data evaluate statistical property generate data structural similarity predictive similarity supervise unsupervise learn technique determine usability propose generator conduct large scale evaluation 51 data set result considerable similarity original generate data indicate method useful development simulation scenario analyze possible improvement classification performance add different amount generate data train set performance highdimensional data set condition propose approach successfuln',\n",
       " 'H. Sarimveis E. Chondrodima A. Alexandridis paper present novel algorithm train radial basis function rbf network order produce model increase accuracy parsimony propose methodology base nonsymmetric variant fuzzy mean fm algorithm ability determine number location hiddennode rbf center synaptic weight calculate linear regression take advantage short computational time require fm algorithm wrap particle swarm optimization pso base engine design optimize fuzzy partition result integrate framework fully determine parameter rbf network propose approach evaluate application 12 realworld synthetic benchmark datasets compare neural network train technique result rbf network model produce psobase nonsymmetric fm algorithm outperform model produce technique exhibit high prediction accuracy short computational time accompany simple network structuresn',\n",
       " 'A. R. FigueirasVidal A. Omari functional weight conventional linear combination architecture way obtain expressive power represent alternative classical trainable implicit nonlinear transformation brief explore way construct binary classifier take advantage possibility generate functional weight mean gate fix radial basis function particular form gate permit train machine directly maximal margin algorithm result scheme xe2x80x9cfeature combiners gate generate weight classificationxe2x80x9d experimental result architecture outperform support vector machine svms real adaboost ensemble consider benchmark example increase computational design effort crossvalidation demand price pay obtain advantage operational effort usually low need svmsn',\n",
       " 'Peter Andras function approximation core task solve neural network context engineer problem good approximation result need good sample data space usually require exponentially increase volume data dimensionality data increase time highdimensional data arrange low dimensional manifold propose break function approximation task highdimensional data step 1 map highdimensional data low dimensional space correspond manifold data reside 2 approximation function map low dimensional data use overcomplete selforganizing map som map unsupervise learn single hide layer neural network function approximation supervise learn extend twostep procedure consider support vector machine bayesian som determination best parameter nonlinear neuron hide layer neural network function approximation compare approximation performance propose neural network set function neural network combine unsupervise supervise learn outperform case neural network learn function approximation original highdimensional datan',\n",
       " 'Xuli Han Muzhou Hou know single hide layer feedforward network radial basis function rbf kernel universal approximators parameter network obtain kind algorithm observe neural network implementation tune parameter network cause learn complicate poor generalization overtrain unstable unlike conventional neural network theory brief give constructive proof fact decay rbf neural network n 1 hide neuron interpolate n 1 multivariate sample zero error prove give decay rbfs uniformly approximate continuous multivariate function arbitrary precision train fast convergence good generalization performance conventional rbf algorithm bp algorithm extreme learn machine support vector machine show mean numerical experimentsn',\n",
       " 'J.L. Junkins K. Subbarao Puneet Singla directiondependent scale shape rotation gaussian basis function introduce maximal trend sense minimal parameter representation input output approximation show shape rotation radial basis function help reduce total number function unit require approximate give inputoutput data improve accuracy alternate formulation enforce minimal parameterization general radial basis function present novel direct graph base algorithm introduce facilitate intelligent direction base learn adaptation parameter appear radial basis function network parameter estimation algorithm incorporate establish start estimate model parameter multiple window inputoutput data efficacy directiondependent shape rotation function approximation evaluate modify minimal resource allocate network consider different test example example draw recent literature benchmark new algorithm versus exist methodsn',\n",
       " 'Dawood S. Javan Modjtaba Rouhani paper present new radial basis function rbf learn meethod classification problem propose meethod use heuristic determine spread center number hide neuron network way high efficiency achieve few number neuron learn algorithm remain fast simple retain network size limit neuron add network recursively termination condition meet neuron cover train data termination condition cover train data reach maximum number neuron step center spread new neuron select base maximization coverage maximization coverage neuron lead network few neuron low vc dimension good generalization property power exponential distribution function activation function hide neuron light new learn approach prove data linearly separable space hide layer output imply exist linear output layer weight zero train error propose meethod apply wellknown datasets simulation result compare svm lead rbf learn meethod satisfactory comparable performancen',\n",
       " 'Jiashu Zhang Haiquan Zhao paper propose novel computational efficient adaptive nonlinear equalizer base combination finite impulse response fir filter functional link artificial neural network cfflann compensate linear nonlinear distortion nonlinear communication channel convex nonlinear combination result improve speed retain low steadystate error addition cfflann need hide layer exist conventional neuralnetworkbase equalizer exhibite simple structure traditional neural network nns require computational burden train mode appropriate adaptation algorithm propose equalizer derive modify mean square mlms result obtain simulation clearly propose equalizer mlms algorithm availably eliminate intensity linear nonlinear distortion provide good antijamming performance furthermore comparison mean square error mse bite error rate ber effect eigenvalue ratio evr input correlation matrix presentedn',\n",
       " 'T.H. Lee K.K. Tan S.N. Huang letter solve problem decentralize adaptive asymptotic track class large scale system significant nonlinearities uncertainty neural network nns control cancel effect unknown nonlinearity semiglobal asymptotic stability result obtain track error converge zeron',\n",
       " 'Jun Wang Shubao Liu design analysis application new recurrent neural network quadratic program call simplify dual neural network discuss analysis mainly concentrate convergence property computational complexity neural network simplify dual neural network show globally convergent exact optimal solution complexity neural network architecture reduce number neuron equal number inequality constraint application kwinnerstakeall kwta operation discuss demonstrate solve problem neural networkn',\n",
       " 'S.J. Perantonis N. Ampazis present highly efficient secondorder algorithm train multilayer feedforward neural network algorithm base iteration form employ levenbergmarquardt lm method nonlinear square problem inclusion additional adaptive momentum term arise formulation train task constrain optimization problem implementation require minimal additional computation compare standard lm iteration simulation large scale classical neuralnetwork benchmark present reveal power method obtain solution difficult problem standard secondorder technique include lm fail convergen',\n",
       " 'TungKuan Liu JyhHorng Chou JinnTsong Tsai paper hybrid taguchigenetic algorithm htga apply solve problem tune network structure parameter feedforward neural network htga approach method combine traditional genetic algorithm tga powerful global exploration capability taguchi method exploit optimum offspring taguchi method insert crossover mutation operation tga systematic reason ability taguchi method incorporate crossover operation select good gene achieve crossover consequently enhance genetic algorithm htga approach robust statistically sound quickly convergent author evaluate performance present htga approach study global numerical optimization problem present htga approach effectively apply solve example forecast sunspot number tune associative memory solve xor problem number hide node link feedforward neural network choose increase small number learn performance good result partially connect feedforward neural network obtain tune imply cost implementation neural network reduce study problem tune network structure parameter feedforward neural network parameter numerous local optimum study problem challenge evaluate performance propose gabased approach computational experiment present htga approach obtain good result exist method report recently literaturen',\n",
       " 'Sichun Wang Titus Lo Henry Leung paper consider problem optimum prediction noisy chaotic time series basis function neural network particular radial basis function rbf network noiseless environment predict chaotic time series equivalent approximate nonlinear function optimal generalization achieve number hide unit rbf predictor approach infinity noise exist show optimal rbf predictor use finite number hide unit determine structure optimal rbf predictor propose new technique call crossvalidated subspace method estimate optimum number hide unit subspace technique identify suitable number hide unit detect dimension subspace span signal eigenvectors cross validation method apply prevent problem overfitting effectiveness new method evaluate simulate noisy chaotic time series reallife oceanic radar signal result propose method correct number hide unit rbf network optimal predictionn',\n",
       " 'Hao Yu B M Wilamowski method introduce paper allow train arbitrarily connect neural network powerful neural network architecture connection layer efficiently train propose method simplify neural network train forwardonly computation instead traditionally forward backward computationn',\n",
       " 'Hao Yu Bogdan M Wilamowski improve computation present paper aim optimize neural network learn process levenbergmarquardt lm algorithm quasihessian matrix gradient vector compute directly jacobian matrix multiplication storage memory limitation problem lm train solve consider symmetry quasihessian matrix element upperlower triangular array need calculate train speed improve significantly small array store memory reduce operation quasihessian matrix calculation improve memory time efficiency especially true large size pattern trainn',\n",
       " 'S. Usui T. Hayasaka P.P. Palmes evolve gradientlearn artificial neural network anns evolutionary algorithm ea popular approach address local optimum design problem ann typical approach combine strength backpropagation bp weight learn eau0027s capability search architecture space bpu0027s gradient descent approach require highly computerintensive operation relatively restrict search coverage ea compel use small population size address problem utilize mutationbased genetic neural network mgnn replace bp mutation strategy local adaptation evolutionary program ep effect weight learn mgnnu0027s mutation enable network dynamically evolve structure adapt weight time mgnnu0027s epbased encode scheme allow flexible restrict formulation fitness function make fitness computation fast efficient make feasible use large population size allow mgnn relatively wide search coverage architecture space mgnn implement stop criterion overfitness occurrence monitor slidingwindows avoid premature learn overlearn statistical analysis performance wellknown classification problem demonstrate good generalization capability reveal locally adapt schedule strategy parameter embed individual network provide proper balance local global search capability mgnnn',\n",
       " 'JunFei Qiao LiDan Wang HongGui Han show extensively dynamic behavior neural strongly influence network architecture learn process establish artificial neural network ann selforganizing architecture suitable learn algorithm nonlinear model automatic axonneural network aann investigate follow respect network architecture construct automatically change number hide neuron topology neural network train process approach introduce adaptive connectandprune algorithm acp type mix mode operation equivalent prune add connect neuron insert require neuron directly secondly weight adjust feedforward computation fc obtain information gradient learn computation unlike previous study aann able selforganize architecture weight improve network performance propose aann test number benchmark problem range nonlinear function approximate nonlinear system model experimental result aann good performance exist neural networksn',\n",
       " 'H.N. Koivo M.E. Celebi C. Guzelis Z. Uykan key point design radial basis function network specify number location center heuristic hybrid learn method apply cluster algorithm locate center subsequently linear leastsquares method linear weight previously suggest hybrid method group call input cluster ic inputoutput cluster ioc depend output vector involve cluster process idea concatenate output vector input vector cluster process independently propose paper literature present theoretical analysis procedure demonstrate effectiveness application main contribution paper present approach investigate relationship cluster process inputoutput train sample mean square output error context radial basis function network rbfn summarize investigation matter follow 1 weight mean square inputoutput quantization error minimize ioc yield upper bind mean square output error 2 upper bind consequently output error arbitrarily small zero limit case decrease quantization error accomplish increase number hide unitsn',\n",
       " 'S.X. Yang Anmin Zhu paper neural network approach task assignment base selforganizing map som propose multirobot dynamic environment subject uncertainty capable dynamically control group mobile robot achieve multiple task different location desire number robot arrive target location arbitrary initial location propose approach robot motion plan integrate task assignment robot start overall task give robot navigation dynamically adjust guarantee target location desire number robot uncertainty robot break propose approach capable deal change environment effectiveness efficiency propose approach demonstrate simulation studiesn',\n",
       " 'JunFei Qiao HongGui Han paper propose constructingandprune cp approach optimise structure feedforward neural network fnn single hide layer number hide node neuron determine contribution ratio calculate fourier decomposition variance fnnu0027s output hide node sufficiently small contribution ratio eliminate new node add fnn satisfy certain design objective procedure similar grow prune process observe biological neural network performance propose method evaluate number example reallife date classification dynamic identification key variable model wastewater treatment experimental result propose method effectively optimise network structure perform good exist algorithmsn',\n",
       " 'Bogdan M. Wilamowski Pawel Rozycki Xing Wu singlelayer feedforward network slfns prove universal approximator parameter allow adjustable widely classification regression problem slfn learn involve task determine network size train parameter current algorithm satisfactory side algorithm focus construction tune parameter able achieve compact network gradientbased optimization algorithm focus parameter tune network size preset user trialanderror approach search optimal network size result trial reuse trial cost computation paper hybrid constructive hcalgorithm propose slfn learn train parameter determine network size simultaneously combine levenbergxe2x80x93marquardt algorithm leastsquare method hybrid algorithm present train slfn fix network size thenwith hybrid algorithm incremental constructive scheme propose new randomly initialize neuron add time train entrap local minimum train continue previous result add new neuron propose hc algorithm work efficiently practical problem give comparison popular algorithm experimental result demonstrate hc algorithm work efficiently optimization method trial error achieve compact slfn construction algorithmn',\n",
       " 'C. J. Harris X. Hong S. Chen paper derive efficient algorithm construct sparse kernel density skd estimate algorithm select small subset significant kernel orthogonal forward regression ofr procedure base doptimality experimental design criterion weight result sparse kernel model calculate modify multiplicative nonnegative quadratic program algorithm unlike skd estimator propose doptimality regression approach unsupervised construction algorithm require empirical desire response kernel selection task strength doptimality ofr owe fact algorithm automatically select small subset significant kernel relate large eigenvalue kernel design matrix count energy kernel train data guarantee accurate kernel weight estimate propose method computationally attractive comparison exist skd construction algorithm extensive numerical investigation demonstrate ability regressionbase approach efficiently construct sparse kernel density estimate excellent test accuracy result propose method compare favourably exist sparse method term test accuracy model sparsity complexity construct kernel density estimaten',\n",
       " 'F. Lai FokChing Chong BorShing Lin B.S. Lin paper highorderstatistics hobased radial basis function rbf network signal enhancement introduce propose scheme high order cumulants reference signal input hobased rbf hobased supervise learn algorithm mean square error obtain high order cumulants desire input output learn criterion adapt weight motivation ho effectively suppress gaussian symmetrically distribute nongaussian noise influence gaussian noise input hobased rbf hobased learn algorithm mitigate simulate result indicate hobased rbf provide good performance signal enhancement different noise level performance insensitive selection learn rate efficiency hobased rbf nonstationary gaussian noise stablen',\n",
       " 'M.N. Vrahatis G.D. Magoulas V.P. Plagianakos present deterministic nonmonotone learn strategy multilayer perceptrons mlps deterministic train algorithm error function value allow increase epoch end argue current error function value satisfy nonmonotone criterion respect maximum error function value m previous epoch propose subprocedure dynamically compute m nonmonotone strategy incorporate batch train algorithm provide fast stable reliable learn experimental result different class problem approach improve convergence speed success percentage firstorder train algorithm alleviate need finetuning problemdepended heuristic parametersn',\n",
       " 'R.L. Cheu D. Srinivasan Min Chee Choy paper propose new hybrid neural network nn model employ multistage online learn process solve distribute control problem infinite horizon technique reinforcement learn evolutionary algorithm design multistage online learn process paper infinite horizon distribute control problem implement form realtime distribute traffic signal control intersection largescale traffic network hybrid neural network model design local traffic signal controller respective intersection state traffic network change random fluctuation traffic volume nnbased local controller need adapt change dynamic order provide effective traffic signal control prevent traffic network overcongested problem especially challenge local controller infinite horizon problem online learn place continuously controller implement traffic network comprehensive simulation model section central business district cbd singapore develop paramics microscopic simulation program complexity simulation increase result hybrid nn model provide significant improvement traffic condition evaluate exist traffic signal control algorithm new continuously update simultaneous perturbation stochastic approximationbased neural network spsann hybrid nn model total mean delay vehicle reduce 78 total mean stoppage time vehicle reduce 84 compare exist traffic signal control algorithm show efficacy hybrid nn model solve largescale traffic signal control problem distribute manner indicate possibility hybrid nn model application similar nature infinite horizon distribute control problemn',\n",
       " 'J.M. Zurada C. Guzelis M.K. Muezzinoglu energy functionbased autoassociative memory design method store give set unipolar binary memory vector attractive fix point asynchronous discrete hopfield network dhn present discrete quadratic energy function local minimum correspond attractive fix point network construct solve linear inequality derive strict local minimumlity condition weight threshold calculate energy function inequality infeasible conclude asynchronous dhn exist extend method design discrete piecewise quadratic energy function minimize generalize version conventional dhn propose spite computational complexity simulation indicate original method perform good conventional design method sense memory store provide attractiveness memory set cardinality equal dimension element overall method extension guarantee storage arbitrary collection memory vector mutually ham distance away result networkn',\n",
       " 'Z. Bandar Clive Mingham D. McLean D. Ingram D. Wedge present hybrid radial basis function rbf sigmoid neural network threestep train algorithm utilize global search gradient descent train algorithm intend identify global feature inputoutput relationship add local approximate function aim achieve efficient function approximation separate identification aspect relationship express universally vary particular region input space test effectiveness method regression task use synthetic datasets problem u realworld data wave overtop seawall show hybrid architecture superior architecture contain neuron single type way low mean square error achievable few hide neuron need regularization globallocal artificial neural network glann see compare favorably perceptron radial basis net regression tree derive rbfs number issue concern train glanns discuss use regularization inclusion gradient descent optimization step choice rbf spread model selection development appropriate stop criterian',\n",
       " 'Dianhui Wang Honggui Han Sanyi Li Junfei Qiao abstract feeforward neural network fnns single hide layer widely apply data model itsxe2x80x99 universal approximation capability nonlinear map theoretical result provide guideline determine architecture model practice research selforganization fnns useful critical effective data model paper propose hybrid construct prune strategy hcps problem solve mutual information mi sensitivity analysis sa employ measure internal information neuron hide layer contribution rate hide neuron respectively hcps merge hide neuron mi value high delete hide neuron contribution rate sufficiently small split hide neuron contribution rate big instant pattern fee model train sample weight neural network update ensure modelu0027s output unchanged structural adjustment hcps aim condense model eliminate redundant neuron degrade instant model performance associate modelu0027s generalization property propose algorithm evaluate benchmark data set include classification problem nonlinear identification problem timeseries prediction problem real world application pm 25 prediction simulation result comparison demonstrate propose method perform favorably improve exist work term model performancen',\n",
       " 'Wenjing Li Xi Meng Junfei Qiao abstract paper novel incremental radial basis function rbf neural network propose nonlinear system model hide layer construct dynamically basis neuronal activity na measure local field potential lfp average fire rate afr goal enhance structural compactness simultaneously modify secondorder algorithm utilize train neuronal activitybased rbf narbf neural network decrease convergence time improve generalization performance benchmark nonlinear model simulation employ evaluate propose narbf neural network indicate propose neural network obtain good generalization performance compact structure fast train finally narbf neural network apply wastewater treatment process model demonstrate propose algorithm predict key water quality variable preciselyn',\n",
       " 'ChihChing Hsiao JinTsong Jeng ShunFeng Su ChenChia Chuang support vector regression svr employ support vector machine svm tackle problem function approximation regression estimation svr show good robust property noise parameter svr improperly select overfitting phenomenon occur selection parameter straightforward svr outlier possibly take support vector inclusion outlier support vector lead seriously overfitting phenomenon paper novel regression approach term robust support vector regression rsvr network propose enhance robust capability svr approach traditional robust learn approach employ improve learn performance select parameter simulation result rsvr improve performance learn system case train last long period test error word overfitting phenomenon suppressedn',\n",
       " 'ChiSing Leung KwokWo Wong Yong Xu recursive square rls efficient approach neural network train classical rls algorithm explicit decay energy function lead unsatisfactory generalization ability train network paper propose generalize rls grls model include general decay term energy function train feedforward neural network particular different weight decay function quadratic weight decay constant weight decay newly propose multimodal quartic weight decay discuss grls approach generalization ability train network significantly improve unnecessary weight prune obtain compact network furthermore computational complexity grls remain standard rls algorithm advantage tradeoff different decay function analyze demonstrate example simulation result approach able meet design goal improve generalization ability train network get compact networkn',\n",
       " 'ChinChing Hsiao ShunFeng Su ChenChia Chuang multilayer feedforward neural network refer universal approximators train data corrupt large noise outlier traditional backpropagation learn scheme come acceptable performance robust learn algorithm propose literature approach suffer initialization problem robust learn algorithm socalled mestimator employ mestimation type learn algorithm loss function play role discriminate outlier majority degrade effect outlier learn loss function algorithm correctly discriminate outlier paper anneal robust backpropagation learn algorithm arbp adopt anneal concept robust learn algorithm propose deal problem model existence outlier propose algorithm employ example result demonstrate superiority robust learn algorithm independent outlier paper anneal concept adopt robust learn algorithm anneal schedule kt experimentally achieve best performance anneal schedule k constant t epoch numbern',\n",
       " 'E. Ricci R. Perfetti analog neural network support vector machine learn propose base partially dual formulation quadratic program problem result simple circuit implementation respect exist neural solution application effectiveness propose network show simulation concern benchmark problemsn',\n",
       " 'L.E. Banta Sheng Wan paper novel stochastic online train algorithm neural network name parameter incremental learn pil algorithm propose develop main idea pil strategy learn algorithm adapt newly present inputoutput train pattern adjust parameter preserve prior result general pil algorithm feedforward neural network accordingly present firstorder approximate solution optimization problem performance index combination proper measure preservation adaptation pil algorithm multilayer perceptron mlp subsequently derive numerical study benchmark problem paper pil algorithm mlp measurably superior standard online backpropagation bp algorithm stochastic diagonal levenbergmarquardt sdlm algorithm term convergence speed accuracy appeal feature pil algorithm computationally simple bp algorithm easy use bp algorithm apply good performance situation standard online bp algorithm applicablen',\n",
       " 'D. Toppo E. Romero support vector machine svms usually need large number support vector form output recently model propose build svms small number basis function maintain property hidelayer weight subset data support vector property present algorithm feedforward neural network fnns construct network sequentially lead sparse model number hide unit explicitly control experimental study benchmark data set compare svms aforementioned sequential fnns carry experiment perform condition model see comparison svms fnns model restrict use similar hidelayer weight accuracy similar number support vector sequential fnns construct model hide unit standard svms range sparse svms computational time low svmsn',\n",
       " 'JianLong Hao ZhenQiu Feng ZengGuang Hou GuiBin Bian XiaoLiang Xie fact linear estimator rankbase wilcoxon approach linear regression problem usually insensitive outlier know statistic outlier data point differ greatly pattern set bulk data inspire fact hsieh et al introduce wilcoxon approach area machine learn investigate new learn machine wilcoxon neural network wnn develop gradient descent base backpropagation algorithm train learn machine performance machine good ordinary nonrobust neural network outlier exist task hard balance learn speed stability algorithm inherently drawback gradient descent base algorithm paper new algorithm train output weight singlelayer feedforward neural network slfn input weight bias randomly choose algorithm call wilcoxonnorm base robust extreme learn machine wrelm shortn',\n",
       " 'Marcos Andre Goncalves Wellington Santos Martins Thierson Couto Rosa Sergio Daniel Canuto Daniel Xavier De Sousa learn rank l2r currently essential task basically type information system give huge increase data available solution propose improve l2r function relatively little attention pay task improve quality feature space l2r strategy usually rely dense feature representation contain noisy redundant feature increase cost learn process benefit feature selection f strategy apply reduce dimensionality noise effect procedure neglect risk get poor prediction important query paper propose multiobjective f strategy optimize aspect time rank performance risksensitive evaluation approximate paretooptimal set multiobjective optimization new original application l2r contribution include novel f method l2r optimize multiple potentially conflict criterion particular objective risksensitive evaluation optimize context f l2r experimental evaluation show propose method select feature effective rank performance lowrisk select stateoftheart f methodn',\n",
       " 'K.Z. Mao feature selection important issue pattern classification present study develop fast orthogonal forward selection fofs algorithm feature subset selection fofs algorithm employ orthogonal transform decompose correlation candidate feature perform orthogonal decomposition implicit way consequently fast algorithm demand computational effort compare conventional orthogonal forward selection ofsn',\n",
       " 'Paul D. Kelly Karen Rafferty Stuart Ferguson JianXun Peng paper present feature selection method data classification combine modelbased variable selection technique fast twostage subset selection algorithm relationship specify complete set candidate feature class label model nonlinear regression model linearintheparameters performance submodel measure sum squarederrors sse score informativeness subset feature involve submodel twostage subset selection algorithm approach solution submodel sse locally minimize feature involve solution submodel select input support vector machine svms classification memory requirement algorithm independent number train pattern property make method suitable application execute mobile device physical ram memory limit application develop activity recognition implement propose feature selection algorithm svm train procedure experiment carry application run pda human activity recognition accelerometer data comparison information gainbased feature selection method demonstrate effectiveness efficiency propose algorithmn',\n",
       " 'J.M. Zurada Jun Wang Ying Tan paper propose novel neuralnetwork approach blind source separation nonlinear mixture approach utilize radial basis function rbf neuralnetwork approximate inverse nonlinear mix map assume exist able approximate rbf network contrast function consist mutual information partial moment output separation define separate nonlinear mixture minimization contrast function result independence output desirable moment original source separate properly learn algorithm parametric rbf network develop stochastic gradient descent method unsupervised cluster method virtue rbf neural network propose approach take advantage high learn convergence rate weight hide layer output layer natural unsupervised learn characteristic modular structure universal approximation capability simulation result present demonstrate feasibility robustness computability propose methodn',\n",
       " 'S. Ridella A. Boni D. Anguita paper propose digital architecture support vector machine svm learn discus implementation field programmable gate array fpga analyze briefly quantization effect performance svm classification problem robustness feedforward phase respect fixedpoint math implementation address problem svm learn architecture describe make use new algorithm svm learn sensitive quantization error respect solution appear far literature algorithm compose part exploit recurrent network find parameter svm second u bisection process compute threshold architecture implement algorithm describe map real currentgeneration fpga xilinx virtex ii effectiveness test channel equalization problem realtime performance paramount importancen',\n",
       " 'SungKwun Oh W. Pedrycz HoSung Park study present new architecture granular neural network provide comprehensive design methodology elaborate algorithmic setup support development propose neural network relate broad category radial basis function neural network rbfnns sense topology involve collection receptive field contrast standard architecture encounter rbfnns form individual receptive field subspace original input space entire input space subspace different different receptive field architecture network fully reflective structure encounter train data granulate aid cluster technique specifically output space granulate use kmeans cluster information granule multidimensional input space form socalled contextbased fuzzy cmeans take account structure form output space innovative development facet network involve dynamic reduction dimensionality input space information granule form subspace overall input space form select suitable subset input variable subspace retain structure entire space search combinatorial character use technique genetic optimization genetic algorithm gas specific determine optimal input subspace series numeric study exploit synthetic data data come machine learn repository university california irvine provide detail insight nature algorithm parameter offer comparative analysisn',\n",
       " 'Zheng Rong Yang novel radial basis function neural network discriminant analysis present paper contrast research work focus exploitation weight structure radial basis function neural network bayesian method expect performance radial basis function neural network wellexplored weight structure improve weight structure radial basis function neural network commonly unknown bayesian method paper study priori structure weight structure investigate study singlegaussian structure twogaussian structure expectationmaximization learn algorithm estimate weight simulation result show propose radial basis function neural network weight structure gaussians outperform algorithmsn',\n",
       " 'Henry Leung Youshen Xia problem spatialtemporal signal process model great recent year new spatialtemporal prediction method present paper optimal fusion scheme base fourthorder statistic employ combine receive signal different spatial domain fuse signal construct spatialtemporal predictor support vector machine show theoretically propose method improve performance nongaussian environment demonstrate practicality spatialtemporal predictor apply model reallife radar sea scatter signal experimental result propose method provide accurate model sea clutter conventional methodsn',\n",
       " 'M. Bettayeb A. Zerguine S. Abrar stopandgo decisiondirected su0026gdd equalization primitive blind equalization method cancel intersymbolinterference data communication system recently scheme apply complexvalued multilayer feedforward neural network give robust result low meansquare error expense slow convergence overcome problem work fast converge recursive square rlsbased complexvalued backpropagation learn algorithm derive su0026gdd blind equalization simulation result effectiveness propose algorithm term initial convergencen',\n",
       " 'L. GomezChova J. CalpeMaravilla A.J. SerranoLopez G. CampsValls J.D. MartinGuerrero E. SoriaOlivas novel fuzzybased activation function artificial neural network propose approach provide easy hardware implementation straightforward interpretability basis ifthen rule backpropagation learn new activation function low computational complexity application example xor gate chaotic timeseries prediction channel equalization independent component analysis support potential propose schemen',\n",
       " 'F. Piazza A. Uncini paper neural network base adaptive nonlinear function suitable blind complex time domain signal separation blind frequency domain signal deconvolution present activation function shape modify learn base couple spline function real imaginary input shape control point adaptively change gradientbase technique bsplines allow impose simple constraint control parameter order ensure monotonously increase characteristic new adaptive function apply output onelayer neural network order separate complex signal mixture maximize entropy function output derive simple form adaptation algorithm present experimental result demonstrate effectiveness propose methodn',\n",
       " 'TeWon Lee Sooyong Choi paper introduce investigate new adaptive equalization method base minimize approximate negentropy estimation error finitelength equalizer consider approximate negentropy nonpolynomial expansion estimation error new performance criterion improve performance linear equalizer base minimize minimum mean square error mmse negentropy include high order statistical information minimization provide improve converge performance accuracy compare traditional method mmse term bitee error rate ber propose negentropy minimization negmin equalizer kind solution mmse solution depend ratio normalization parameter negmin equalizer best ber performance ratio normalization parameter properly adjust maximize output powervariance negmin equalizer simulation experiment ber performance negmin equalizer solution mmse similar characteristic adaptive minimum bitee error rate amber equalizer main advantage propose equalizer need significantly few train symbol amber equalizer furthermore propose equalizer robust nonlinear distortion mmse equalizern',\n",
       " 'Tianrui Li Xiaomin Wang Yangguang Liu Jiashu Zhang Xiangping Zeng Haiquan Zhao eliminate nonlinear channel distortion chaotic communication system novel jointprocessing adaptive nonlinear equalizer base pipelined recurrent neural network jprnn propose modify realtime recurrent learn rtrl algorithm furthermore adaptive amplitude rtrl algorithm adopt overcome deteriorate effect introduce nest process simulation illustrate propose equalizer outperform pipelined recurrent neural network prnn recurrent neural network rnn equalizersn',\n",
       " 'George W. Irwin Minrui Fei Xue Li Dajun Du paper investigate automatic construction radial basis function rbf neural model nonlinear dynamic system main objective automatically effectively produce parsimonious rbf neural model generalize achieve propose locally regularize automatic construction lrac method combine recently propose fast recursive algorithm fra leaveoneout loo crossvalidation criterion new method offer distinctive advantage exist approach firstly u error criterion original model parameter regularize contrast orthogonal square ols base approach transform model parameter regularize enable determination significance original candidate center produce compact neural model automatically determine network size iteratively minimize loo meansquareerror mse need specify additional termination criterion finally define proper regression context network construction process concisely formulate easily implement significantly reduce computation analysis computational complexity confirm efficiency propose method simulation result reveal effectiveness comparison alternative approach produce sparse rbf neural modeln',\n",
       " 'Jun Wu Hui Peng Garba Inoussa paper present functional weight wavelet neural networkbased statedependent ar fwwnnar model main objective address model prediction problem nonlinear time series fwwnnar model statedependent autoregressive sdar model coefficient approximate set functional weight wavelet neural network fwwnn fwwnn enhance type wavelet neural network comprise layer input wavelet product output functional weight layer compute weight function input make weight vary input share dynamic wavelet compartment fwwnnar model posse advantage statedependent ar model description nonlinear dynamic node fwwnn functional approximation consider mutually time frequency space learn nonlinear dynamic distinct level ar level wavelet compartment level functional weight level structure nonlinear parameter optimization method snpom apply estimate fwwnnar model parameter learn approach divide parameter search space linear nonlinear subspace center search nonlinear subspace iteration optimization process search nonlinear linear subspace execute basis estimate value obtain linear nonlinear subspace search nonlinear subspace u method similar levembergmarquardt method lmm search linear subspace u square method lsm propose model validate compare performance effectiveness achieve know model generate real nonlinear time seriesn',\n",
       " 'Feng Zhou Hui Peng Xiaoyong Zeng recently radial basis function rbf networkstyle coefficient autoregressive exogenous input rbfarx model identify structure nonlinear parameter optimization method snpom attract considerable significant performance nonlinear model promise technique occasionally confront problem parameter divergent optimization process potential issue ignore researcher paper regularize snpom regularization parameter detection technique present estimate parameter rbfarx model approach separate parameter rbfarx model linear parameter set nonlinear parameter set combine gradientbased nonlinear optimization algorithm estimate nonlinear parameter regularize square method estimate linear parameter example demonstrate propose approach effective cope potential unstable problem parameter search process yield good similar multistep forecast accuracy good robustness previous methodn',\n",
       " 'C. L. Philip Chen Feng Ding Min Gan GuangYong Chen separable nonlinear model common research field machine learn identification variable projection vp approach efficient optimization model paper study vp algorithm base different matrix decomposition compare previous method use analytical expression jacobian matrix instead finite difference improve efficiency vp algorithm particular base modify gramxe2x80x93schmidt mg method robust implementation vp algorithm introduce separable nonlinear leastsquares problem numerical experiment compare performance different implementation vp algorithm numerical result efficiency robustness propose mg methodbase vp algorithmn',\n",
       " 'K. Khorasani Liying Ma paper constructive onehidelayer network introduce hide unit employ polynomial function activation function different unit specifically structure level function level adaptation methodology utilize construct network functional level adaptation scheme ensure grow constructive network different activation function neuron network able capture underlie inputoutput map effectively activation function consider consist orthonormal hermite polynomial show extensive simulation propose network yield improve performance compare network have identical sigmoidal activation functionn',\n",
       " 'Roberto Battiti Mauro Brunato paper propose new algorithm base multiscale stochastic local search binary representation train neural network binary learn machine blm study effect neighborhood evaluation strategy effect number bite weight maximum weight range map binary string real value follow preliminary investigation propose telescopic multiscale version local search number bite increase adaptive manner lead fast search local minimum good quality analysis relate adapt number bite dynamic way present control number bite happen natural manner propose method effective increase generalization performance learn dynamic discuss validate highly nonlinear artificial problem realworld task application domain blm finally apply problem require feedforward recurrent architecture feedback controln',\n",
       " 'Tomas Maul current paper introduce concept neural diversity machine ndm refer hybrid artificial neural network hann condition minimum number function available network property paper demonstrate ndm network optimize solve different problem result demonstrate feasibility approach bolster biological computational argument favor neural diversity substantial number optimization experiment conduct generate correspond number diverse neural architecture reveal unexpected statistic include relative commonality node combine innerproduct gaussian function paper confirm advantage hann demonstrate potential increase focus neural diversity hint possible new neural computational strategiesn',\n",
       " 'Zhiyu You Zhiling Jiang Yunfang Zhu Weirong Chen Chaohua Dai seeker optimization algorithm soa novel populationbase heuristic stochastic search algorithm base concept simulate act human search soa search direction determine seekeru0027s egotistic behavior altruistic behavior proactiveness behavior step length give uncertainty reaon behavior paper application soa tune structure parameter artificial neural network anns present new evolutionary method ann train simulation experiment pattern classification function approximation perform comparison soa bp algorithm evolutionary algorithm ea study simulation result performance soa good equivalent ea variation pso list problem anns link switch train soa provide good comparable learn capability number link one bp algorithm gdx rp o scg soa simultaneously tune structure weight value soa computationally intensive believe soa promise candidate train annsn',\n",
       " 'Ricardo de A. Araujo paper present hybrid intelligent methodology design increase translation invariant morphological operator apply brazilian stock market prediction overcome random walk dilemma propose translation invariant morphological robust automatic phaseadjustment timraa method consist hybrid intelligent model compose modular morphological neural network mmnn quantuminspired evolutionary algorithm qiea search best time lag reconstruct phase space time series generator phenomenon determine initial suboptimal parameter mmnn individual qiea population train propagation bp algorithm improve mmnn parameter supply qiea prediction model generate u behavioral statistical test phase fix procedure adjust time phase distortion observe stock market time series furthermore experimental analysis conduct propose method brazilian stock market time series achieve result discuss compare result random walk model previously introduce timedelay add evolutionary forecast taef morphologicalranklinear timelag add evolutionary forecast mrltaef methodsn',\n",
       " 'Tiago A. E. Ferreira Ricardo de A. Araujo paper morphologicalranklinear timelag add evolutionary forecast mrltaef method propose order overcome random walk dilemma financial time series prediction consist intelligent hybrid model compose morphologicalranklinear mrl filter combine modify genetic algorithm mga search particular time lag capable fine tune characterization time series estimation initial suboptimal parameter mrl filter individual mga population train average mean square lm algorithm improve parameter mrl filter supply mga initially propose mrltaef method choose tune predictive model time series representation perform behavioral statistical test attempt adjust time phase distortion appear financial time series experiment conduct propose mrltaef method realworld financial time series accord group relevant performance metric result compare multilayer perceptron mlp network mrl filter previously introduce timedelay add evolutionary forecast taef methodn',\n",
       " 'YonPing Chen ShihHung Yang propose method design artificial neural network anns prediction problem base evolutionary constructive prune algorithm ecpa propose ecpa begin set anns simple possible structure hide neuron connect input node employ crossover mutation operator increase complexity ann population additionally clusterbase prune cbp agebase survival selection ab propose new operator ann prune cbp operator retain significant neuron prune insignificant neuron probability basis prevent exponential growth ann ab operator delete old anns potentially complex structure introduce new anns simple structure anns likely trap fully connect topology ecpa framework incorporate constructive prune approach attempt efficiently evolve compact anns demonstration method ecpa apply prediction problem mackeyglass time series number sunspot traffic flow numerical result ecpa make design anns feasible practical realworld applicationsn',\n",
       " 'ChihSheng Wu LinYu Tseng WenChing Chen artificial neural network anns successfully apply area key success properly tune architecture connection weight ann train ann high generalization ability complicate problem artificial neural network ensemble classifier instead single ann classifier consider straight forward construct ann ensemble paper propose unify evolutionary train scheme uets train generalize feedforward neural network construct ann ensemble performance uets evaluate apply solve nbit parity problem classification problem datasets uci machine learn repository compare previous study experimental result reveal neural network ensemble train uets good classification ability unseen casesn',\n",
       " 'J. C. Fernandez M. Carbonero C. Hervas P. A. Gutierrez paper propose hybrid neural network model possible combination different transfer projection function sigmoidal unit su product unit pu kernel function radial basis function rbf hide layer feedforward neural network evolutionary algorithm adapt model apply learn architecture weight node typology different combine basis function model propose different pair obtain su pu rbf node productsigmoidal unit psu neural network productradial basis function prbf neural network sigmoidalradial basis function srbf neural network compare correspond pure model product unit neural network punn multilayer perceptron mlp rbf neural network proposal test benchmark classification problem know machine learn problem combine function projection kernel function good pure basis function task classification datasetsn',\n",
       " 'A. AlMamun K. C. Tan J. H. Ang paper novel evolutionary algorithm ea base newly formulate parameter growth probability pg evolve near optimuml weight number hide neuron neural network nns train nns growth probability base evolution nngp initialize network hide neuron network allow grow suitable size grow neuron restrict hide neuron time optimuml number hide neuron nns neuron represent solution search space far network add number hide neuron growth rate base gaussian distribution provide way escape local optimum selfadaptive version nnsagp aim evolve growth probability parallel nns generation propose evolve network apply widely realworld benchmark problem simulation result propose approach effective evolve nns good classification accuracy low complexityn',\n",
       " 'Lucio C. Pessoa Robson De Sousa Tiago E. Ferreira Francisco Madeiro Ricardo A. Araujo paper present improve evolutionary hybrid method design morphological operator matheron banon barrera decomposition translation invariant operator consist hybrid model compose modular morphological neural network mmnn improve genetic algorithm iga have optimal genetic operator accelerate convergence genetic algorithm propose design method look initial weight architecture number module mmnn element iga population train propagation bp algorithm optimal morphological operator apply image restoration edge extraction binary image corrupt salt pepper noise method propose capable perform simultaneous edge extraction noise removal operation allow seamless efficient design morphological operator increase nonincrease typesn',\n",
       " 'JyhChing Juang GwoRuey Yu TzyyChyang Lu paper present quantumbased algorithm evolve artificial neural network anns aim design ann connection high classification performance simultaneously optimize network structure connection weight unlike previous study propose algorithm u quantum bite representation codify network result connectivity bites indicate actual link probability existence connection alleviate map problem reduce risk throw away potential candidate addition propose model weight space decompose subspace term quantum bites algorithm perform region region exploration evolve gradually promise subspace exploitation helpful provide set appropriate weight evolve network structure alleviate noisy fitness evaluation problem propose model test benchmark problem breast cancer iris heart diabetes problem experimental result propose algorithm produce compact ann structure good generalization ability compare algorithmsn',\n",
       " 'Ricardo de A. Araujo linear nonlinear technique propose solve stock market forecast problem limitation arise technique know random walk dilemma rwd scenario forecast generate arbitrary model characteristic step ahead delay respect time series value time phase distortion stock market phenomenon reconstruction paper propose suitable model inspire concept mathematical morphology mm lattice theory lt model generically call increase morphological perceptron imp present gradient steep descent method design propose imp base idea backpropagation bp algorithm systematic approach overcome problem nondifferentiability morphological operation learn process include procedure overcome rwd automatic correction step gear eliminate time phase distortion occur stock market phenomenon furthermore experimental analysis conduct imp complex nonlinear problem time series forecast brazilian stock market additionally natural phenomenon time series as forecast performance propose imp non financial time series end obtain result discuss compare result model recently propose literaturen',\n",
       " 'Andres M. Gonzalez Alejandro Correa Bahnsen neural network powerful tool classification regression difficult time consume determine best architecture give problem paper evolutionary algorithm genetic algorithm ga binary particle swarm optimization bps optimize architecture multilayer perceptron neural network mlp order improve predictive power credit risk scorecard result method outperform logistic regression default neural network term predictability ga time consume bps predictive power method similar global optimum reasonable timen',\n",
       " 'F.M. Ham M.H. Thursby H.J. Delgado novel artificial neural network synthesisann present design computationally intensive problem apply optimization antenna microwave device antenna example present optimize respect voltage standingwave ratio bandwidth frequency operation simple microstrip transmission line problem ann effectiveness microstrip line width optimize respect line impedance anns exploit unique number representation input output data conjunction standard neural network architecture ann consist heteroassociative memory provide efficient method compute necessary geometrical value antenna conjunction new randomization process number representation provide significant insight new method faulttolerant compute work need evaluate potential new paradigmn',\n",
       " 'B. A. Tolson S. Razavi feedforward neural network commonly function approximation technique apply wide variety problem arise discipline neural network blackbox model have multiple challengesdifficulties associate train generalization paper initially look internal behavior neural network develop detail interpretation neural network functional geometry base geometrical interpretation new set variable describe neural network propose effective geometrically interpretable alternative traditional set network weight bias paper develop new formulation neural network respect newly define variable reformulate neural network renn equivalent common feedforward neural network complex error response surface demonstrate learn ability renn paper train method involve derivativebase variation backpropagation derivativefree optimization algorithm employ new measure regularization basis develop geometrical interpretation propose evaluate improve generalization ability neural network value propose geometrical interpretation renn approach new regularization measure demonstrate multiple test problem result renn train effectively efficiently compare common neural network propose regularization measure effective indicator network perform term generalizationn',\n",
       " 'J.G. Taylor G. Palmas C. Orovas D. Malchiodi A. Esposito B. Apolloni aim get understandable symbolic rule explain give phenomenon split task learn rule sensory data phase multilayer perceptron map feature propositional variable set subsequent layer operate paclike algorithm learn boolean expression variable special feature procedure neural network train produce boolean output have principal task discriminate class input ii symbolic direct compute rule family know priori iii weld point learn system represent feedback base suitability evaluation compute rule procedure propose base computational learn paradigm set recently paper field theoretical science artificial intelligence cognitive system present article focus information management aspect procedure deal lack prior information rule learn strategy affect mean variable description length rule combine paper u task learn formally discriminate emotional state work example test bench comparison previous symbolic subsymbolic method fieldn',\n",
       " 'J F Ralph Tingting Mu J Y Goulermas E RodriguezMartinez projection technique frequently principal mean implementation feature extraction dimensionality reduction machine learn application establish broad class projection technique projection pursuit pp core design parameter projection index drive force obtain transformation function optimization represent explicit implicit way useru0027s perception useful information contain datasets paper seek address problem relate design pp index function linear feature extraction case achieve evolutionary search framework capable build new index fit property available datasets high expressive power framework sustain rich set function primitive performance pp index previously propose human expert compare automatically generate index task classification result decrease classification errorsn',\n",
       " 'HsiaoDong Chiang Bin Wang ensemble optimal inputpruned neural network trusttech elite method construct highquality ensemble optimal linear combination accurate diverse neural network develop optimization problem propose methodology solve global optimization global optimization method call transformation stabilityretrain equilibrium characterization trusttech main feature include capability identify multiple local optimal solution deterministic systematic tierbytier manner elite create diverse population feature selection procedure different local optimal neural network obtain tier1 trusttech search addition capability inputpruned network fully exploit trusttechbased optimal train finally find optimal linear combination weight ensemble model nonlinear program problem solve trusttech interior point method issue nonconvexity effectively handle extensive numerical experiment carry pattern classification synthetic benchmark datasets numerical result elite consistently outperform exist method benchmark datasets result elite promise construct highquality neural network ensemblesn',\n",
       " 'N. A. M. Isa Tatt Hee Oong paper present new evolutionary approach call hybrid evolutionary artificial neural network heann simultaneously evolve artificial neural network anns topology weight evolutionary algorithm ea strong global search capability likely provide promise region efficient finetuning search space locally heann emphasize balance global search local search evolutionary process adapt mutation probability step size weight perturbation distinguishable previous study incorporate ea search network topology gradient learn weight update benchmark function test evolutionary framework heann addition heann test seven classification benchmark problem uci machine learn repository experimental result superior performance heann finetuning network complexity small number generation preserve generalization capability compare algorithmn',\n",
       " 'Jorge MartinezMunoz Luis VillaVargas Leonid Sheremetov Igor Aizenberg paper discus longterm time series forecast multilayer neural network multivalued neuron mlmvn complexvalued neural network derivativefree backpropagation learn algorithm evaluate propose approach realworld data set describe dynamic behavior oilfield asset locate coastal swamp gulf mexico mlmvn efficiently apply univariate multivariate onestep multistep ahead prediction reservoir dynamic paper intend propose use complexvalued neural network forecast deep study important aspect application ann model time series forecast particular pattern recognition community longterm forecast oil production mlmvnunivariate multivariate forecast model developedmlmvn efficient multivariate univariate forecast modelmlmvnbased prediction combine regression pattern recognition approachesn',\n",
       " 'Chenkun Yin Zhongsheng Hou Shida Liu brief enhance genetic backpropagation neural network link switch egabpnnls propose address datadriven model problem gasification process inside unite gas improvement ugi gasifiers onlinemeasured temperature crude gas produce gasification process play dominant role syngas industry difficult model temperature dynamic principle practical complexity gasification process especially reflect severe change gas temperature result infrequent manipulation gasifier practice propose datadriven model approach egabpnnls incorporate nnls ega levenbergmarquardt lm algorithm approach learn relationship control input output historical data optimize network structure combination ega nnls make use network gradient information lm algorithm egabpnnls apply set data collect field model ugi gasification process effectiveness egabpnnls verifiedn',\n",
       " 'Jingbo Zhu Dongxiang Zhang Gang Chen Chengchao Yu Weichao Ren Sai Wu user session ecommerce model sequence web page indicate user interact make hisher purchase typical recommendation approach collaborative filter generate result begin session list likely purchase item approach fail exploit current view history user unable provide realtime customize recommendation service paper build deep recurrent neural network address problem network track user browse website multiple hide layer hide layer model combination webpage access order reduce process cost network record finite number state old state collapse single history state model refresh recommendation result time user open new web page useru0027s session continue recommendation result gradually refine furthermore integrate recurrent neural network feedfoward network represent useritem correlation increase prediction accuracy approach apply kaola httpwwwkaolacom ecommerce website power netease technology show significant improvement previous recommendation servicen',\n",
       " 'Kenli Li Xiaosong Zhang Wenjing Yang Guowu Yang Fengmao Lv quantuminspired evolutionary algorithm qea prove effective method design neural network connection high classification performance quantuminspired evolutionary neural network qenn converge train phase subsequent train fruitless timewasting important control number generation qenn analysis convergence property quantum bite evolution contribute design safe termination criterion reach paper propose appropriate termination criterion base average convergence rate acr experiment classification task conduct demonstrate effectiveness method result termination criterion base acr duly stop train process qenn overcome limitation termination criterion base probability generate best solution pbsn',\n",
       " 'Hao Li Xiaogang Wang Qiguang Miao Maoguo Gong Jia Liu paper focus connect structure deep neural network propose layerwise structure learn method base multiobjective optimization model good generalization obtain reduce connect parameter deep network aim optimal structure high representation ability good generalization layer visible data model respect structure base product expert order mitigate difficulty estimate denominator poe denominator simplify take objective connect sparsity consideration contradictory nature representation ability network connect sparsity multiobjective model establish improve multiobjective evolutionary algorithm solve model trick design decrease computational cost accord property input data experiment singlelayer level hierarchical level application level demonstrate effectiveness propose algorithm learn structure improve performance deep neural networkn',\n",
       " 'Silvio Romero de Lemos Meira Adriano Lorena Inacio de Oliveira Ricardo de A. Araujo abstract paper present study time series generator phenomenon air pollutant concentration forecast problem provide evidence suggest kind generator phenomenon nonlinear characteristic longterm dependency base present nonlinear morphological model able forecast time series descend gradientbase method idea backpropagation algorithm present design propose model furthermore empirical analysis conduct propose model set relevant air pollutant concentration forecast problem employ statistical measure as forecast performancen',\n",
       " 'Mehmet Emin Yuksel Abdullah Caliskan Alper Basturk Hasan Badem abstract work deep learn technique require profound understand mechanism underlie optimization internal parameter complex structure major factor limit understand exist optimization method gradient descent limitedxe2x80x93memory broydenxe2x80x93fletcherxe2x80x93goldfarbxe2x80x93shannon lbfgs best local minimum problem space complex structure deep neural network dnn paper represent new train approach name hybrid artificial bee colony base train strategy habcbts tune parameter dnn structure include autoencoder layer cascade softmax classification layer strategy derivativefree optimization algorithm xe2x80x9cabcxe2x80x9d combine derivativebase algorithm xe2x80x9clbfgsxe2x80x9d construct xe2x80x9chabcxe2x80x9d habcbts detail simulation result support statistical analysis propose train strategy result good classification performance compare dnn classifier train lbfgs abc modify abc obtain classification result compare stateoftheart classifier include mlp svm knn dt nb 15 data set different dimension sizesn',\n",
       " 'Nguyen Thanh Nam Nguyen Ngoc Son Ho Pham Huy Anh abstract paper propose novel adaptive joint position control highly nonlinear scara serial robot pneumatic artificial muscle pam actuator new inverse forward neural narx ifnn model propose dynamically identify nonlinear hysteresis feature scara serial pambase robot parameter new ifnn model optimize modify differential evolution mde algorithm secondly new ifnn model apply novel propose adaptive evolutionary neural ifnnimc controller apply improve precision reject steadystate error joint position scara serial robot control finally novel adaptive backpropagation abp algorithm base fuzzy reason apply online update weight value ifnn model help novel propose adaptive evolutionary neural ifnnimc controller adapt external disturbance dynamic variation operation experimental test confirm performance advantage propose control scheme comparison nonlinear control methodsn',\n",
       " 'Timothy Marler Mohammad Bataineh abstract powerful successful application artificial neural network anns typically perform complex problem limit number train case collect additional train data feasible costly work present new radialbasis network rbn design overcome limitation anns accurately model regression problem minimal train data new design involve multistage train process couple orthogonal square ols technique gradientbased optimization new termination criterion introduce improve accuracy addition algorithm design require minimal heuristic parameter improve ease use consistency performance propose approach test experimental practical regression problem result compare typical network model result new design demonstrate improve accuracy reduce dependence train data demonstrate new ann provide platform approximate potentially slow highfidelity computational model foster intermodel connectivity multiscale modelingn',\n",
       " 'Xiaoli Li Wenjing Li Gongming Wang Junfei Qiao abstract nonlinear model play important role practical engineer deep learnbase deep belief network dbn popular nonlinear model identification strong learn ability exist weight optimization dbn base gradient lead local optimum poor train result paper dbn partial square regression plsrdbn propose nonlinear model focus problem weight optimization dbn plsr firstly unsupervised contrastive divergence cd algorithm weight initialization secondly initial weight derive cd algorithm optimize layerbylayer plsr model layer layer instead gradient method plsrdbn determine optimal weight plsr model good performance plsrdbn achieve analysis convergence theoretically give guarantee effectiveness propose plsrdbn model finally propose plsrdbn test benchmark nonlinear system actual wastewater treatment handwritten digit recognition nonlinear map model highdimension input data experiment result propose plsrdbn good performance time accuracy nonlinear model methodsn',\n",
       " 'Ralf Wittmann KlausHenning Noffz Michael Heizmann Norbert Mitschke decrease hardware price machine learn increasingly interest industrial application automatic visual inspection avi paper present metaheuristic approach automatic generation suit convolutional neural network cnn base differential evolution make possible suitable architecture cnn give task little prior knowledge aim reduce resource need inference possible choose function consider accuracy resource measure fitness cnn typical industrial datasets obtain cnns accuracy 98 average relatively short process timen',\n",
       " 'Keinosuke Matsumoto Naoki Mori Taichi Hatanaka Saya Fujino abstract recently image recognition base deep learn gain considerable research attention study focus anime storyboards apply deep convolutional neural network dcnns data difficult tune dcnn hyperparameters gird search method propose novel method call evolutionary deep learn evodl adopt genetic algorithm ga solve problem effectiveness evodl validate simulation take real anime storyboard recognition problem examplen',\n",
       " 'Scott Yang Mehryar Mohri Vitaly Kuznetsov Xavier Gonzalvo Corinna Cortes present new algorithm adaptively learn artificial neural network algorithm ada net adaptively learn structure network weight base solid theoretical analysis include datadependent generalization guarantee prove discus report result largescale experiment algorithm binary classification task extract cifar10 dataset criteo dataset result demonstrate algorithm automatically learn network structure competitive performance accuracy compare achieve neural network standard approachesn',\n",
       " 'Meftahul Ferdaus current technological advancement autonomy numerous system raise significantly consequently complicate highly nonlinear dynamical system attain desire control autonomy system significant research develop learn machine base autonomous intelligent controller aicon observe recent time paper brief introduction aicon recent advancement design structure implementation nonlinear dynamical system discuss successful employment aicons positive result attach paper include challenge aiconsu0027 realtime implementation possible solution future research directionsn',\n",
       " 'Silvio Meira Sergio Soares Adriano L. I. Oliveira Ricardo De A. ArauJo work present evolutionary morphological approach solve software development cost estimation sdce problem propose approach consist hybrid artificial neuron base framework mathematical morphology mm algebraic foundation complete lattice theory clt refer dilationerosion perceptron dep present evolutionary learn process call depmga modify genetic algorithm mga design dep model drawback arise gradient estimation morphological operator classical learn process dep differentiable usual way furthermore experimental analysis conduct propose model complex sdce problem wellknown performance metric demonstrate good performance dep model solve sdce problemn',\n",
       " 'N. Sundararajan P. Saratchandran GuangBin Huang NanYing Liang paper develop online sequential learn algorithm single hide layer feedforward network slfns additive radial basis function rbf hide node unify framework algorithm refer online sequential extreme learn machine oselm learn data onebyone chunkbychunk block data fix vary chunk size activation function additive node oselm bound nonconstant piecewise continuous function activation function rbf node integrable piecewise continuous function oselm parameter hide node input weight bias additive node center impact factor rbf node randomly select output weight analytically determine base sequentially arrive data algorithm u idea elm huang develop batch learn show extremely fast generalization performance good batch train method apart select number hide node control parameter manually choose detail performance comparison oselm popular sequential learn algorithm benchmark problem draw regression classification time series prediction area result oselm fast sequential algorithm produce good generalization performancen',\n",
       " 'Edwin Lughofer Sreenatha G. Anavatti Mahardhika Pratama novel evolve fuzzy rulebased classifier parsimonious classifier pclass propose paper pclass set learn process scratch rule base initially train fuzzy model importantly pclass adopt open structure concept automatic knowledge build process cultivate train process wellknown main pillar learn stream example incorporate socalled plugandplay principle learn module couple train process order diminish requirement pre postprocessing step undermine firm logic online classifier follow pclass equip rule grow prune recall input weight technique fully perform fly train process viability pclass test exploit realworld synthetic data stream contain sort concept drift compare stateoftheart classifier pclass deliver encourage numerical result term classification rate number fuzzy rule number rule base parameter runtimen',\n",
       " 'Edwin Lughofer Plamen P. Angelov Sreenatha G. Anavatti Mahardhika Pratama dynamic realworld system compile shift drift uneasy overcome omnipresent neurofuzzy system nonetheless learn nonstationary environment entail own high degree flexibility capable assemble rule base autonomously accord degree nonlinearity contain practice rule grow prune carry merely benefit small snapshot complete train data truncate computational load memory demand low level exposure novel algorithm parsimonious network base fuzzy inference panfis end present panfis commence learn process scratch rule base fuzzy rule stitch expel virtue statistical contribution fuzzy rule inject datum afterward identical fuzzy set allude blend fuzzy set pursuit transparent rule base escalate humanu0027s interpretability learn model performance propose panfis numerically validate benchmark problem realworld synthetic dataset validation include comparison stateoftheart evolve neurofuzzy method showcase new method compete case outperform approach term predictive fidelity model complexityn',\n",
       " 'Imam Arifin Edwin Lughofer Richard J. Oentaryo Xiang Li Meng Joo Er Mahardhika Pratama paper novel fuzzy neural network term dynamic parsimonious fuzzy neural network dpfnn propose dpfnn layer network feature coalescence tsk takagisugenokang fuzzy architecture multivariate gaussian kernel membership function train procedure characterize aspect 1 dpfnn evolve fuzzy rule new train datum arrive enable cope nonstationary process propose criterion rule generation error ecompleteness reflect performance sample coverage exist rule base 2 insignificant fuzzy rule observe time base statistical contribution prune truncate rule base complexity redundancy 3 extend self organize map esom theory employ dynamically update center ellipsoidal basis function accordance input train sample 4 optimal fuzzy consequent parameter update time localize square tll method exploit concept slide window order reduce computational burden square l method viability new method intensively investigate base realworld artificial problem show method arguably deliver compact parsimonious network structure achieve low predictive error stateoftheart approachesn',\n",
       " 'O.K. Prasad M.J. Er C.T. Lin Y.Y. Lin M. Prasad paper novel fuzzy rule transfer mechanism selfconstructing neural fuzzy inference network propose feature propose method term datadriven neural fuzzy collaborative fuzzy cluster mechanism ddnfscfcm 1 fuzzy rule generate facilely fuzzy cmeans fcm adapt preprocessed collaborative fuzzy cluster pcfc technique 2 structure parameter learn perform simultaneously select initial parameter ddnfscfcm apply deal big data problem virtue pcfc technique capable deal immense datasets preserve privacy security datasets initially entire dataset organize individual datasets pcfc procedure dataset cluster separately knowledge prototype variable cluster center matrix halve dataset collaborative technique deploy ddnfscfcm able achieve consistency presence collective knowledge pcfc boost model process parameter learn ability selfconstructing neural fuzzy inference network sonfin propose method outperform exist method time series prediction problemn',\n",
       " 'Shuoru Li Mei Bai Donghong Han Guoren Wang Keyan Cao recent year generation uncertain data attention pay mine uncertain data paper study problem classify uncertain data extreme learn machine elm propose uuelm algorithm classification uncertain data uniformly distribute furthermore nuelm algorithm propose classify uncertain data nonuniformly distribute calculate bind probability efficiency algorithm improve finally performance method verify large number simulate experiment experimental result method effective way solve problem uncertain data classification reduce execution time improve efficiencyn',\n",
       " 'Francisco Maldonado Jiang Li Michael T. Manry Walter H. Delashmit Pramod L. Narasimha order facilitate complexity optimization feedforward network algorithm develop combine grow prune grow scheme present iteratively add new hide unit fulltrained network nonheuristic onepass prune technique present utilize orthogonal square base prune onepass approach develop generate validation error versus network size curve combine approach describe network continually prune grow process result hide unit order accord usefulness useful unit eliminate example network design combine method train validation error grow prune combine method exhibit reduce sensitivity initial weight generate monotonic error versus network size curve show perform good wellknown grow methodsconstructive backpropagation cascade correlationn',\n",
       " 'Zoran Miljkovic Najdan Vukovic feedforward neural network ffnn neural network model nonlinear problem engineer sequential especially real time process neural network model fail face outlier outlier wide range engineer problem recent research result field show avoid overfitting divergence model new approach need especially ffnn run sequentially real time accommodate limitation ffnn train data contain certain number outlier paper present new learn algorithm base improvement conventional extend kalman filter ekf extend kalman filter robust outlier ekfor probabilistic generative model measurement noise covariance constant sequence noise measurement covariance model stochastic process set symmetric positivedefinite matrix prior model inverse wishart distribution iteration ekfor simultaneously estimate noise estimate current best estimate ffnn parameter bayesian framework enable mathematically derive expression analytical intractability bayesu0027 update step solve structure variational approximation mathematical expression paper derive principle extensive experimental study show ffnn train develop learn algorithm achieve low prediction error good generalization quality regardless outlieru0027 presence train datan',\n",
       " 'Soon Cheol Park Peng Chen Wei Song paper propose novel learn classifier utilize stag learnbased resource allocation network slrun text categorization light learn progress slrun divide preliminary learn phase refine learn phase phase reduce sensitivity correspond input data agglomerate hierarchical kmeans method utilize create initial structure hide layer subsequently novelty criterion forward dynamically regulate hide layer center phase square method enhance convergence rate network improve ability classification stag learnbased approach build compact structure decrease computational complexity network boost learn capability order implement slrun text categorization utilize semantic similarity approach reduce input scale neural network reveal latent semantics text feature benchmark reuter 20newsgroup datasets test experiment extensive experimental result reveal dynamic learn process slrun improve classify performance comparison conventional classifier run bp rbf neural network svmn',\n",
       " 'Bogdan M. Wilamowski Tomasz Bartczak Tiantian Xie Philip D. Reiner Hao Yu paper propose offline algorithm incrementally construct train radial basis function rbf network iteration error correction errcor algorithm rbf unit add fit eliminate high peak low valley error surface process repeat desire error level reach experimental result real world data set errcor algorithm design compact rbf network compare investigate algorithm benchmark test duplicate pattern test spiral problem apply robustness errcor algorithm propose errcor algorithm generate compact network compactness lead greatly reduce computation time train networkn',\n",
       " 'Nor Ashidi Mat Isa Tatt Hee Oong brief present new order algorithm data presentation fuzzy artmap fam ensemble propose order algorithm manipulate presentation order train data member fam ensemble category create ensemble member bias vector choose input feature diversity create vary train presentation order base ascend order value uncorrelated input feature analysis show category create fams compulsively diverse choose input feature determine presentation order train data uncorrelated propose order algorithm test 10 classification benchmark problem university california irvine machine learn repository cervical cancer problem case study experimental result propose method produce diverse generalize fam ensemblen',\n",
       " 'Min Liu JingHua Hao Lu Guo fast outstanding incremental learn algorithm require meet demand online application data come chunk chunk avoid retrain save precious time interest research result achieve lot difficulty real application unsatisfying generalization performance intensive computation cost paper present incremental extreme learn machine ielm develop base extreme learn machine elm unify framework lssvm psvm present hang et al 2011 15 different application demand different computational cost efficiency different alternative solution ielm achieve detail comparison ielm algorithm incremental algorithm achieve simulation benchmark problem real critical dimension cd prediction problem lithography actual semiconductor production line result kernel base ielm solution perform best square ielm solution fast alterative solution number train data huge result present ielm algorithm good performance incremental algorithm online sequential elm oselm present liang et al 2006 8 fix size lssvm present espinoza et al 2006 11n',\n",
       " 'M. Azzi H. Duc H. Wahid Q.P. Ha assessment air pollutant profile measurement involve limitation implementation deterministic air quality model simulation usually need high computational requirement complex chemical reaction involve paper neural networkbased metamodel approach conjunction deterministic model measure data approximate nonlinear ozone concentration relationship algorithm performance enhancement radial basis function neural network rbfnn develop propose method apply estimate spatial distribution ozone concentration sydney basin experimental comparison propose rbfnn algorithm conventional rbfnn algorithm demonstrate effectiveness efficiency estimate spatial distribution ozone leveln',\n",
       " 'Chee Peng Lim M.V.C. Rao Shing Chiang Tan abstract paper present hybrid network famddat comprise fuzzy artmap fam neural network dynamic decay adjustment dda algorithm online prune strategy benchmark datasets demonstrate effectiveness famddat result famddat compare famdda prune radial basis function network dda rbfndda prune version rbfnddat observe compare ddabased network famddat able form parsimonious network structure time maintain high level network generalization tackle pattern classification problemsn',\n",
       " 'YingHong Li XunKai Wei novel optimum extreme learn machine elm construction method propose define extend cover matrix smooth function relax objective constraint formulate general linear program method minimum sphere set cover problem method linear program minimum sphere set cover lpmssc present correspond kernelized lpmssc extend lpmssc noneuclidean l1 linfinity metric propose apply lpmssc method elm propose data dependent elm ddelm algorithm obtain compact elm pattern classification lpmssc investigate performance propose method uci benchmark data setsn',\n",
       " 'See Kiong Ng Chai Quek Eng Yeow Cheu appetitive operant condition aplysia feed behavior electrical stimulation esophageal nerve contingently reinforce spontaneous bite feed process result acquisition operant memory contingently reinforce animal analysis cellular molecular mechanism feed motor circuitry reveal activitydependent neuronal modulation occur interneurons mediate feed behavior provide evidence interneurons possible locus plasticity constitute mechanism memory storage addition memory storage attribute activitydependent synaptic plasticity paper associative ambiguity correctionbase neurofuzzy network call appetitive rewardbase pseudoouterproductcompositional rule inference arpopcris train base appetitive rewardbase learn algorithm biologically inspire appetitive operant condition feed behavior aplysia variant hebbian learn rule call hebbian concomitant learn propose build block neurofuzzy network learn algorithm propose algorithm posse distinguish feature sequential learn algorithm addition propose arpopcris neurofuzzy encode fuzzy knowledge form linguistic rule satisfy semantic criterion lowlevel fuzzy model interpretability arpopcris evaluate compare model technique benchmark timeseries datasets experimental result encourage arpopcris viable model technique timevariant problem domainsn',\n",
       " 'Ying Tan JianXin Xu paper adaptive waveletnetworkbased control approach propose highly nonlinear uncertain dynamical system wavelet network kind universal approximator novel propertyorthonormality multiresolution orthonormal property ensure add new resolution new wavelet affect exist wavelet network tune sequel online adjustment structure nonlinear adaptive wavelet controller awc constructive manner gradually increase network resolution multiresolution property hand assure guarantee improvement approximation precision new resolution add real life problem unable know adequate size network neural network nn wavelet network produce require approximation precision virtue novel wavelet network property coarse simple structure select fail converge elapse dwell time new wavelet resolution consider necessary add directly manner awc easily construct tune coarse fine level performance requirement satisfy trial error way select network structure lead inadequate highly redundant structure avoid paper propose adaptive wavelet network apply class nonlinear dynamical system partially know model affineininput structure adaptive wavelet network apply class nonlinear nonaffine dynamical systemn',\n",
       " 'Feng Luo Shunjun Wu Shuyuan Yang Min Wang problem direction arrival doa estimation ultra wideband uwb electromagnetic em wave address radialbasisfunction neural network rbfnn base approach present paper doa estimation achieve map model rbfnn train inputoutput pair rbfnn characteristic accurate approximation good generalization robustness interference scatter antenna propose method learn source direction find uwb array work existence manufacture error mutual couple uwb array antenna order rapid train avoid large network size hybrid lean scheme rbfnn adopt firstly unsupervised kmeans cluster algorithm employ determine center hide neuron recursive square rls algorithm obtain linear weight output layer order solve multiple source track uwb array combine eigenvalue decomposition evd rbfnn extract doas multiple source effectiveness scheme demonstrate numerical example result method characteristic high accuracy robustness mutual couple compare presently available method literaturesn',\n",
       " 'J. A. Farrell Yiming Chen Yuanyuan Zhao Wenjie Dong paper consider track control singleinput singleoutput nonaffine dynamic system performancedependent selforganizing approximationbased approach propose designer specify positive track error criterion selforganizing approximationbased controller monitor track performance add basis element need achieve track specification affine approach define approximate function independent control variable u stability prove selforganization derive lyapunovbased methodology illustrate certain novel aspect propose controller numerical example includedn',\n",
       " 'NiNi Wang Feng Xu ZaoJian Zou JianChuan Yin online prediction nonlinear system characteristic timevarying dynamic uncertainty sequential grey prediction approach propose base online sequential extreme learn machine oselm grey process time series alleviate unfavorable effect uncertainty measurement data extremely fast learn speed high generalization accuracy oselm enable online application sequential grey prediction approach shipu0027s roll motion sea complex nonlinear process timevarying dynamic dynamic involve uncertainty cause wind random wave rudder action paper propose oselmbase grey prediction approach implement online ship roll prediction simulation prediction base measurement data obtain sea trial scientific research train ship yu kun simulation result ship roll prediction demonstrate effectiveness efficiency propose grey neural prediction approach deal timevarying nonlinear uncertaintyn',\n",
       " 'FeiYue Wang Cheng Chen effective method provide information influence input variation output variance base sensitivity analysis widely determine structure neural network past global sensitivity analysis method total effect structure learn neural network grow prune algorithm develop paper neurofuzzy network characteristic additive model order effect index influence provide comprehensive information total effect index need analyze order effect input output layer base observation lowcost effective method order effect global sensitivity develop selforganize neurofuzzy network specifically random balance design employ sensitivity analysis addition introduce concept systemic fluctuation neurofuzzy network determine adjustment need network concept help build new procedure lean selforganize neurofuzzy network accelerate speed convergence learn organize example simulation demonstrate propose method perform good exist procedure selforganize neurofuzzy network especially learn network structuren',\n",
       " 'GuangBin Huang Qing Chen Shaoyuan Li dynamic model quality control real largescale continuous anneal process study paper continuous anneal process consist subprocesses exist unknown complex nonlinear map subprocess set point final anneal quality quality model construct update base new data sequentially collect real process order optimize set point subprocess dynamically meet demand late develop sequential learn algorithm call generalize grow prune rbf ggaprbf neural network establish require dynamic quality control model online application quality model continuous anneal furnace steel factory conduct actual performance good requiren',\n",
       " 'JunFei Qiao Qili Chen HongGui Han paper present flexible structure radial basis function rbf neural network fsrbfnn application water quality prediction fsrbfnn vary structure dynamically order maintain prediction accuracy hide neuron rbf neural network add remove online base neuron activity mutual information mi achieve appropriate network complexity maintain overall computational efficiency convergence algorithm analyze dynamic process phase phase follow modification structure propose fsrbfnn test compare algorithm apply problem identify nonlinear dynamic experimental result fsrbfnn design rbf structure few hide neuron train time fast algorithm apply predict water quality wastewater treatment process result demonstrate effectivenessn',\n",
       " 'Zoran Miljkovic Najdan Vukovic radial basis function rbf neural network construct certain number rbf neuron network neural network model nonlinear problem engineer conventional rbf neuron usually base gaussian type activation function single width activation function feature restrict neuron performance model complex nonlinear problem accommodate limitation single scale paper present neural network similar different activation functionhyper basis function hbf hbf allow different scale input dimension provide good generalization property deal complex nonlinear problem engineer practice hbf base generalization gaussian type neuron apply mahalanobislike distance distance metric input train sample prototype vector compare rbf hbf neuron parameter optimize hbf neural network need number hbf neuron memorize relationship input output set order achieve good generalization property recent research result hbf neural network performance show optimal way construct type neural network need paper address issue modify sequential learn algorithm hbf neural network exploit concept neuronu0027s significance allow grow prune hbf neuron learn process extensive experimental study show hbf neural network train develop learn algorithm achieve low prediction error compact neural networkn',\n",
       " 'Francesco Marcelloni Beatrice Lazzerini Witold Pedrycz Mario G. C. A. Cimino paper propose new neural network architecture base family referential multilayer perceptrons rmlps play role generalize receptive field contrast u0027u0027standardu0027u0027 radial basis function rbf neural network propose topology network offer considerable level flexibility result receptive field highly diversify capable adjust characteristic locally available experimental data discus design strategy novel architecture fully exploit model capability contribute rmlps strategy comprise phase phase form u0027u0027blueprintu0027u0027 network employ specialize version commonly encounter fuzzy cmeans fcm cluster algorithm conditional contextbase fcm phase intent generate collection information granule fuzzy set space input output variable narrow certain context second phase base global view structure refine inputoutput relationship engage collection rmlps rmlp train subset data associate correspond context fuzzy set train receptive field focus characteristic locally available data build nonlinear map referential mode finally connection receptive field optimize global minimization linear aggregation unit locate output layer overall architecture include series numeric experiment involve synthetic realworld data set provide thorough comparative analysis standard rbf neural networkn',\n",
       " 'Yan Chen GuangBin Huang Changyun Wen Guoqi Li exist online algorithm support vector machine svm grow support vector paper propose online error tolerance base support vector machine etsvm grow prune support vector similar square support vector machine lssvm etsvm convert original quadratic program qp standard svm group easily solve linear equation different lssvm etsvm remain support vector sparse realize compact structure etsvm significantly reduce computational time ensure satisfactory learn accuracy simulation result verify effectiveness newly propose algorithmn',\n",
       " 'S.H. Zak S.D. Sudhoff Yonggon Lee Jianming Lian realtime approximators continuoustime dynamical system input present approximators employ novel selforganizing radial basis function rbf network vary structure dynamically prescribe approximation accuracy rbfs add remove online order achieve appropriate network complexity realtime approximation dynamical system maintain overall computational efficiency performance variable structure rbf network approximator gaussian rbf grbf raisedcosine rbf rcrbf analyze compact support rcrbf enable fast train easy output evaluation network network grbf propose realtime selforganizing rbf network approximator employ approximate linear nonlinear dynamical system illustrate effectiveness propose approximation scheme especially high order dynamical system uniform ultimate boundedness approximation error prove second method lyapunovn',\n",
       " 'Masato Okada Seiji Sugita Kenji Nagata analytical method deconvolute spectral data number simple band extremely important analysis chemical property matter fundamental problem deconvolution method determine number band resort heuristic difficulty avoid parameter solution trap local minimum hierarchy nonlinearity study propose novel method spectral deconvolution base bayesian estimation exchange monte carlo method application integral approximation stochastic complexity exchange monte carlo method experimentally effectiveness synthetic data reflectance spectral data olivine common mineral terrestrial planetsn',\n",
       " 'M. Pasadas L. Marquez H. Pomares L. J. Herrera A. Guillen F. Rojas O. Valenzuela I. Rojas challenge predict future value time series cover variety discipline fundamental problem select order identify time vary parameter autoregressive move average model arma concern important field linear prediction identification spectral analysis recent research activity forecast artificial neural network anns suggest anns promise alternative traditional arma structure linear model anns compare mix conclusion term superiority forecast performance study design investigate hybrid methodology combine ann arma model b resolve important problem time series arma structure boxjenkins methodology identification model paper present new procedure predict time series paradigm fuzzy system neural network evolutionary algorithm goal obtain expert base paradigm artificial intelligence linear model identify automatically need human expert participation obtain linear model combine ann make hybrid outperform forecast resultn',\n",
       " 'I.Z. Abidi Chee Peng Lim Keem Siah Yap brief new neural network model call generalize adaptive resonance theory gart introduce gart hybrid model comprise modify gaussian adaptive resonance theory mga generalize regression neural network grnn enhance version grnn preserve online learn property adaptive resonance theory art series empirical study as effectiveness gart classification regression time series prediction task conduct result demonstrate gart able produce good performance compare method include online sequential extreme learn machine oselm sequential learn radial basis function rbf neural network modelsn',\n",
       " 'Francesco Gianfelici paper present innovative technique base joint approximation capability radial basis function rbf network estimation capability multivariate iterate hilbert transform iht statistical demodulation pathological tremor electromyography emg signal patient parkinsonu0027s disease define stochastic model multichannel highdensity surface emg mean rbf network apply reconstruction stochastic process characterize disease model multivariate relationship generate karhunenloeve transform hilbert space perform demodulation entire random field mean estimation capability multivariate iht statistical set propose method apply simulate signal data record parkinsonian patient result amplitude modulation component tremor oscillation estimate signaltonoise ratio close 30 db rootmeanquare error estimate tremor instantaneous frequency additionally comparison large number technique base combination rbf extreme learn machine backpropagation support vector machine step algorithm iht empirical mode decomposition multiband energy separation algorithm periodic algebraic separation energy demodulation second step algorithm clearly effectiveness technique result propose approach potential useful tool advance neurorehabilitation technology aim tremor characterization suppressionn',\n",
       " 'I. Santamaria M. LazaroGredilla S. Van Vaerenbergh paper introduce kernel recursive leastsquares krls algorithm able track nonlinear timevarying relationship data purpose derive standard krls equation bayesian perspective include sensible approach prune advantage framework incorporate forget consistent way enable algorithm perform track nonstationary scenario result method kernel adaptive filter algorithm include forget factor principled numerically stable manner addition track ability number appeal property online require fix memory computation time step incorporate regularization natural manner provide confidence interval prediction include experimental result support theory illustrate efficiency propose algorithmn',\n",
       " 'J. C. Principe Pingping Zhu Songlin Zhao Badong Chen paper propose quantization approach alternative sparsification curb growth radial basis function structure kernel adaptive filter basic idea method quantize compress input feature space different sparsification new approach u xe2x80x9credundantxe2x80x9d data update coefficient close center particular quantize kernel mean square qklms algorithm develop base simple online vector quantization method analytical study mean square convergence carry energy conservation relation qklms establish basis arrive sufficient condition mean square convergence low upper bind theoretical value steadystate excess mean square error static function estimation shortterm chaotic timeseries prediction example present demonstrate excellent performancen',\n",
       " 'Armin SalimiBadr Mohammad Mehdi Ebadzadeh abstract paper new fuzzy neural network model correlate fuzzy rule cfnn base levenbergxe2x80x93marquardt lm optimization method propose propose method new fuzzy network structure present approximate nonlinear function especially function high correlation input variable number fuzzy rule multivariable gaussian fuzzy membership function introduce consider correlation input variable consequently model nonseparable relation interactive variable lm optimization method learn parameter premise consequent part fuzzy rule suggest algorithm successfully apply seven test example include static function approximation timeseries prediction nonlinear dynamic identification realworld complex regression problem accord test observation approximate nonlinear function good past algorithm compact structure number fuzzy rulen',\n",
       " 'A. Luchetta paper new procedure selection prune threshold feedforward artificial neural network fann present base evaluation local sensitivity index previously calculate respect single output network special emphasis give particular class neural network multiple heterogeneous output effectiveness propose method show development neural architecture devote specific multioutput inversion propose prune technique provide criterion decide u0027u0027whenu0027u0027 u0027u0027how muchu0027u0027 prune design neural networkn',\n",
       " 'Xianyao Meng Meng Joo Er Ning Wang paper present fast accurate online selforganizing scheme parsimonious fuzzy neural network faospfnn novel structure learn algorithm incorporate prune strategy new growth criterion develop propose grow procedure prune speed online learn process facilitate parsimonious fuzzy neural network achieve comparable performance accuracy virtue grow prune strategy faospfnn start hide neuron parsimoniously generate new hide unit accord propose growth criterion learn proceed parameter learn phase free parameter hide unit regardless newly create originally exist update extend kalman filter ekf method effectiveness superiority faospfnn paradigm compare popular approach like resource allocation network run run extend kalman filter runekf minimal resource allocation network mrun adaptivenetworkbased fuzzy inference anfis orthogonal square ols rbfafs dynamic fuzzy neural network dfnn generalize dfnn gdfnn generalize gaprbf ggaprbf online sequential extreme learn machine oselm selforganizing fuzzy neural network sofnn benchmark problem area function approximation nonlinear dynamic identification chaotic timeseries prediction realworld regression problem simulation result demonstrate propose faospfnn algorithm achieve fast learn speed compact network structure comparably high accuracy approximation generalizationn',\n",
       " 'Feng Cheng Hong Wu Dehua Li Peng Zhou orthogonal square ols algorithm extensively basis selection rbf network unable perform model selection automatically tolerance r specify manually introduce noise difficult implement parametric complexity realtime generic criterion detect optimum number basis function propose paper bayesian information criterion bic method fitness calculation incorporate basis function selection process ols algorithm assign appropriate number new method develop optimize width gaussian function order improve generalization performance augment algorithm employ radial basis function neural network rbfnn know unknow noise nonlinear dynamic system performance compare standard ols experimental result efficacy bic fitness calculation importance proper choice basis function width significantn',\n",
       " 'Xusheng Qian Tingwen Huang He Huang Yuanshan Liu abstract know important step train complexvalued radial basis function neural network effectively determine center width neuron hide layer paper improve maximum spread algorithm propose solve issue basic idea choice center depend distance sample different class heavily affect average distance sample class relationship external inner distance take account determine center performance algorithm test datasets show good performance achieve develop algorithm exist onesn',\n",
       " 'QuanMin Zhu DongYa Zhao Christian Mathews DingLi Yu D.K. Siong Tok adaptive structure radial basis function rbf network model propose paper model nonlinear process operate point migration recursive orthogonal square algorithm rols adopt select new center online train network weight base r matrix orthogonal decomposition initial center bank form update sample period new learn strategy propose gain information new data network structure adaptation center group algorithm develop divide center active nonactive group structure small size maintain final network model output prediction propose rbf model evaluate compare exist adaptive structure rbf network model nonlinear timevarying numerical example simulation result demonstrate propose algorithm advantage term adaptive track ability good recovery speed exist method migration system operate pointn',\n",
       " 'M. Aladjem M. Bortman recently publish generalize grow prune ggap train algorithm radial basis function rbf neural network study modify ggap resourceallocating network run algorithm mean create network unit consistently make little contribution networku0027s performance remove train ggap state formula compute significance network unit require dfold numerical integration arbitrary probability density function px input data x x isin r d work ggap formula approximate gaussian mixture model gmm px analytical solution approximate unit significance derive make possible employ modify ggap input data have complex highdimensional px possible original ggap result extensive experimental study modify algorithm outperform original ggap achieve low prediction error reduce complexity train networkn',\n",
       " 'F. Flentge paper propose new approach function approximation base grow neural gas gng selforganizing map som able adapt local dimension possible highdimensional input distribution local model build interpolate value associate mapu0027s neuron model combine weight sum yield final approximation value value position local range neuron adapt improve approximation quality method able adapt change target function follow nonstationary input distribution new approach compare radial basis function rbf extension grow neural gas locally weight projection regression lwpr stateoftheart algorithm incremental nonlinear function approximationn',\n",
       " 'G. Biagetti M. Pirani P. Crippa C. Turchetti learn capability neural network equivalent model physical event occur real environment early work demonstrate neural network belong class universal approximators inputoutput deterministic function recent work extend ability neural network approximate random function class network name stochastic neural network snn language theory approximation deterministic stochastic function fall identification nonlinear nomemory system result present far restrict case gaussian stochastic process sps linear transformation guarantee property paper aim investigate ability stochastic neural network approximate nonlinear inputoutput random transformation widen range applicability network nonlinear system memory particular study show network belong class name nongaussian stochastic approximate identity neural network sainns capable approximate solution large class nonlinear random ordinary differential transformation effectiveness approach demonstrate discuss application examplesn',\n",
       " 'A. Likas C. Constantinopoulos probabilistic radial basis function prbf network constitute probabilistic version rbf network classification extend typical mixture model approach classification allow share mixture component class typical learn method prbf classification task employ expectationmaximization em algorithm depend strongly initial parameter value paper propose technique incremental train prbf network classification propose algorithm start single component incrementally add component appropriate position data space addition new component base criterion detect region data space crucial classification task addition component algorithm split component network subcomponent correspond different class experimental result wellknown classification data set indicate incremental method provide solution superior classification performance compare hierarchical prbf train method conduct comparative experiment support vector machine method present obtain result qualitative comparison approachesn',\n",
       " 'R. Araujo mobile robot able build map navigate unknown world expand previously propose method base fuzzy art neural architecture fartna paper introduce new online method learn map unknown dynamic world purpose new pruneable fuzzy adaptive resonance theory neural architecture pafartna introduce extend fartna selforganizing neural network novel mechanism provide important dynamic adaptation capability relevant pafartna property formulate demonstrate method propose perception object removal integrate pafartna propose method integrate navigation architecture new navigation architecture mobile robot able navigate change world degree optimality maintain associate short path plan approach implement realtime underlie global world model experimental result obtain nomad 200 robot present demonstrate feasibility effectiveness propose methodn',\n",
       " 'Alberto Prieto Carlos Garcia Puntonet Julio Ortega Moises Salmeron new learn strategy timeseries prediction radial basis function rbf network introduce potential examine particular case resource allocate network model idea apply extend procedure early stage learn addition successive new group rbfs provide increase rate convergence time optimum lag structure determine orthogonal technique qr factorization singular value decomposition svd claim technique apply prune problem useful tool compaction information comparison original run algorithm show comparable error measure smallersized network extra effort require qr svd balance simplicity mean square iterative parameter adaptationn',\n",
       " 'Alberto Prieto Francisco J. Pelayo Begona Pino Julio Ortega Jose Luis Bernier Hector Pomares Ignacio Rojas paper propose framework construct train radial basis function rbf neural network purpose sequential learn algorithm present adapt structure network possible create new hide unit detect remove inactive unit structure gaussian function modify pseudogaussian function pg scale parameter xcfx83 introduce eliminate symmetry restriction provide neuron hide layer great flexibility respect function approximation important characteristic propose neural activation hide neuron normalize describe bibliography provide good performance nonnormalization instead single parameter output weight function input variable lead significant reduction number hide unit compare classical rbf network finally examine result apply propose algorithm time series predictionn',\n",
       " 'Jarkko Tikka input selection advantageous regression problem example decrease train time model reduce measurement cost assist circumvent problem high dimensionality inclusion useless input model increase likelihood overfitting neural network provide good generalization case interpretability usually limit select subset variable estimate relative importance valuable real world application present work simultaneous input basis function selection method radial basis function rbf network propose selection perform minimize constrain optimization problem sparsity network control continuous value shrinkage parameter input dimension weight constraint impose weight output layer coefficient direct alternate optimization ao procedure present solve problem propose method apply simulate benchmark data comparison exist method result rbf network similar prediction accuracy small number input basis functionsn',\n",
       " 'Yi Zhang Zhidong Deng paper propose scalefree highly cluster echo state network shesn design shesn include naturally evolve state reservoir accord incremental growth rule account follow feature 1 short characteristic path length 2 high cluster coefficient 3 scalefree distribution 4 hierarchical distribute architecture new state reservoir contain large number internal neuron sparsely interconnect form domain domain comprise backbone neuron number local neuron backbone natural efficient recurrent neural essentially interpolate completely regular elman network completely random echo state network esn propose jaeger investigate collective characteristic propose complex network model successfully apply challenge problem mackeyglass mg dynamic laser timeseries prediction compare esn experimental result shesn model significantly enhance echo state property good performance approximate highly complex nonlinear dynamic word large scale dynamic complex network reflect natural characteristic biological neural system aspect power law smallworld property hierarchical architecture strong compute power fast signal propagation speed coherent synchronizationn',\n",
       " 'Chai Quek Javan Tan selforganizing neurofuzzy approach mature online learn fuzzyassociative structure timeinvariant condition maximize operative value online reason selfsustaining mechanism able reorganize fuzzyassociative knowledge realtime dynamic environment critical recognize require selfreorganizational skill rebuild fluid associative structure exist organization fail respond change circumstance light hebbian theory hebb 1949 basic computational framework associative learn attractive timevariant online learn suffer stability limitation impede unlearn instead paper adopt bienenstockcoopermunro bcm theory neurological learn metaplasticity principle bienenstock et al 1982 provide online associative dissociative learn decade bcm theory show effectively brace physiological evidence synaptic potentiation association depression dissociation sound mathematical framework computational learn paper propose interpretation bcm theory metaplasticity online selfreorganizing fuzzyassociative learn realize onlinereason capability experimental find twofold 1 analysis su0026p500 stock index illustrate selfreorganizing approach follow trajectory shift timevariant su0026p500 index 60 year 2 benchmark profile show fuzzyassociative approach yield comparable result fuzzyprecision model similar online objectivesn',\n",
       " 'Ulrich Ruckert Ralf Eickhoff neural network intend future nanoelectronic technology architecture robust malfunction element noise input parameter work robustness radial basis function network analyze order operate noisy unreliable environment furthermore upper bind mean square error noise contaminate parameter input determine network parameter constrain achieve robust neural network architecture fundamental method introduce identify sensitive parameter neuronsn',\n",
       " 'Yaohua Xiong N.B. Karayiannis paper introduce learn algorithm train reformulate radial basis function neural network rbfnns capable identify uncertainty data classification learn algorithm train special class reformulate rbfnns know cosine rbfnns update select adjustable parameter minimize classconditional variance output radial basis function rbfs experiment verify quantum neural network qnns cosine rbfnns train propose learn algorithm capable identify uncertainty data classification property share cosine rbfnns train original learn algorithm conventional feedforward neural network ffnns finally study lead simple classification strategy improve classification accuracy qnns cosine rbfnns reject ambiguous feature vector base responsesn',\n",
       " 'E C Rouchka R N Mahdi hyper basis function hyperbf network generalize radial basis function neural network activation function radial function weight distance generalization provide hyperbf network high capacity learn complex function turn susceptible overfitting poor generalization train hyperbf network demand weight center local scale factor optimize simultaneously case relatively large dataset large network structure optimization computationally challenge paper new regularization method perform soft local dimension reduction addition weight decay propose regularize hyperbf network show provide classification accuracy competitive support vector machine require significantly small network structure furthermore practical train construct hyperbf network present hierarchal cluster initialize neuron follow gradient optimization scale version rprop algorithm localize partial backtrack step experimental result seven datasets propose train provide fast smooth convergence regular rprop algorithmn',\n",
       " 'Licai Fang Jianhua Lu Defeng Huang Jinshu Chen Lu Xu paper present theoretical bindedness convergence analysis online gradient method train twolayer feedforward neural network wellknown linear difference equation extend apply general case linear nonlinear activation function base extend difference equation investigate bindedness convergence parameter sequence concern train finite train sample constant learn rate uniform upper bind parameter sequence important train procedure solution inequality bind verify case linear activation function solution exist parameter sequence uniformly upper binded case nonlinear activation function simple adjustment method train set activation function derive improve bindedness property convergence analysis show parameter sequence converge zone optimal solution error function attain global minimum size zone associate learn rate particularly case perfect model strong global convergence result parameter sequence converge optimal solution provedn',\n",
       " 'Changhua Yu P.L. Narasimha M.T. Manry Jiang Li present efficient feature selection algorithm general regression problem utilize piecewise linear orthonormal square ols procedure algorithm 1 determine appropriate piecewise linear network pln model give data set 2 apply ols procedure pln model 3 search useful feature subset float search algorithm float search prevent nest effect propose algorithm computationally efficient data pa require example give demonstrate effectiveness propose algorithmn',\n",
       " 'XiZhao Wang E.C.C. Tsang Defeng Wang W.W.Y. Ng D.S. Yeung generalization error bind current error model number effective parameter classifier number train sample usually loose bind intend entire input space support vector machine svm radial basis function neural network rbfnn multilayer perceptron neural network mlpnn local learn machine solve problem treat unseen sample near train sample important paper propose localize generalization error model bind generalization error neighborhood train sample stochastic sensitivity measure develop architecture selection technique classifier maximal coverage unseen sample specify generalization error threshold experiment 17 university california irvine uci data set comparison cross validation cv sequential learn ad hoc method technique consistently yield best test classification accuracy few hide neuron train timen',\n",
       " 'WenBing Lv MingHui Huang Chuang Zhou XinJiang Lu order accurately model timevarying nonlinear system propose regularize online sequential extreme learn machine adaptive regulation factor roselmarf construction new objective function allow online update model coefficient regulation factor negate influence cumulate error differ traditional regularize online sequential extreme learn machine reoselm update model coefficient development application twostep solve method determine optimal parameter optimal regulation factor derive propose fast online leaveoneout cross validation foloo method computational performance drastically improve propose foloo method compare exist leaveoneout cross validation loo method application propose method model practical case order demonstrate effectiveness experimental result indicate propose method provide accurate model conventional model method improve computational performancen',\n",
       " 'Zili Wang Junyou Shi Yiqian Cui radial basis function network rbfn model successfully apply different application scenario universal approximator simple architecture online train capability approximation capability rrbfn greatly dependent determination center radius radial basis function rbfs network structure statisticsbase center determination approach like kmeans fail capture preserve train data structure paper new unsupervised rbfn construction methodology call lazy quantum cluster induce radial basis function network lqcrbfn propose inherit advantage data structure learn show high robustness data distribution quantum cluster qc time control parameter determine arbitrarily requirement precise calibration minimum search specific train data set center radius select base potential function generate quantum assimilation network structure adaptively update incorporate center information series application study present verify effectiveness propose lqcrbfn model concept lazy quantum cluster lqc propose rbfn adaptationlocal minimum obtain potential surface arbitrary scale selectionlqcrbfn able approximately best center capture data structuresseveral different application study provide verify lqcrbfn performancesn',\n",
       " 'ZiJun Jia Xiucai Huang YongDuan Song xe2x80x9cuniversalxe2x80x9d approximatinglearning feature neural network nn widely extensively control design contingent critical condition satisfy render feature vanish paper condition literally link fundamental issue overlook exist nnbased control design unconsciously deliberately propose collective approach explicitly address issue establish strategy enable nn unit fully functional control loop entire process operation ensure reliable effective nnassociated control performance achieve incorporate control new structural nn unit consist group diversify neuron selfadjusting subneuron drivenstimulated input signal confine compact set continuity control signal boundedness closedloop signal ensure theoretical analysis numerical simulation validate effectiveness propose methodn',\n",
       " 'JunFei Qiao Ying Hou Wei Lu HongGui Han paper selforganizing radial basis function sorbf neural network design improve accuracy parsimony aid adaptive particle swarm optimization apso propose apso algorithm avoid trap local optimal value nonlinear regressive function develop adjust inertia weight furthermore apso algorithm optimize network size parameter rbf neural network simultaneously result propose apsosorbf neural network effectively generate network model compact structure high accuracy analysis convergence give guarantee successful application apsosorbf neural network finally multiple numerical example present illustrate effectiveness propose apsosorbf neural network result demonstrate propose method competitive solve nonlinear problem exist sorbf neural networksn',\n",
       " 'JunFei Qiao YaNan Guo HongGui Han paper investigate construct recurrent radial basis function neural network rrbfnn informationoriented algorithm ioa adjust parameter gradient algorithm simultaneously ioabase rrbfnn ioarrbfnn propose ioa calculate information process strength ip hide neuron independent component contribution hide neuron output neuron extract novel selforganizing strategy propose optimize structure rrbfnn base input ip output ip hide neuron gradient algorithm develop update parameter ioarrbfnn propose ioarrbfnn organize network structure adjust parameter improve performance finally example present illustrate effectiveness ioarrbfnn result demonstrate propose ioarrbfnn competitive solve nonlinear model problem compare exist methodsn',\n",
       " 'Peter Andras approximation highdimensional function challenge neural network curse dimensionality data approximate function define reside lowdimensional manifold principle approximation function manifold improve approximation performance project data manifold low dimensional space follow neural network approximation function space provide precise approximation function approximation function neural network original data space data volume large projection lowdimensional space base limit sample data investigate nature approximation error neural network train projection space neural network good approximation performance neural network train highdimensional data projection base relatively sparse sample data manifold preferable use uniformly distribute sparse sample data purpose generation lowdimensional projection illustrate result consider practical neural network approximation set function define highdimensional data include real world data welln',\n",
       " 'Yixin Yin Sen Zhang Haigang Zhang abstract sequential learn algorithm good choice learn data onebyone chunkbychunk liang et al propose oselm algorithm base ordinary elm algorithm produce good generalization performance famous sequential learn algorithm deficiency oselm observation weight equally regardless acquisition time train data timeliness real industrial application paper propose modify online sequential learn algorithm forget factor name woselm algorithm weight new observation convergence analysis present sure estimation output weight tend converge exponential speed arrive new observation determination value forget factor change forecast error automatically rid excessive human interference employ application simulation include timeseries predication timevariant identification weather forecast problem simulation result woselm accurate robust sequential learn algorithmn',\n",
       " 'Chee Peng Lim Sreenatha G. Anavatti Meng Joo Er Edwin Lughofer Mahardhika Pratama abstract metacognitive scaffold learn machine mcslm combine concept metacognitionxe2x80x94whattolearn howtolearn whentolearn scaffold theoryxe2x80x94a tutor theory learner learn complex task successfully develop enhance capability evolve intelligent system eis process nonstationary data stream issue uncertainty temporal behaviour unknown order uncharted exist mcslms mcslms literature design classification problem paper propose novel mcslm call recurrent intervalvalued metacognitive scaffold fuzzy neural network rivmcsfnn solve regression timeseries model problem data stream rivmcsfnn present novel recurrent network architecture cognitive constituent feature double local recurrent connection hide layer consequent layer new recurrent network architecture drive intervalvalued multivariate gaussian function hide node nonlinear wavelet function consequent node predecessor rivmcsfnn characterise open structure automatically grow prune adjust merge recall hide node select relevant data sample fly online active learn methodology rivmcsfnn equip online dimensionality reduction technique cope curse dimensionality learn mechanism carry singlepass local learn mode actualise plugandplay learn principle aim minimise use preandor posttraining step efficacy algorithm test numerous datadrive model problem comprehensive comparison counterpart rivmcsfnn demonstrate substantial improvement accuracy complexity exist variant mcslms eissn',\n",
       " 'Yang Yi Yuequan Yang Tianping Zhang Zhiqiang Cao Jiaming Zhu brief sufficient condition propose existence compact set neural network control point existence compact set classical neural network control scheme unsolved result incomplete simple case derive sufficient condition existence compact set neural network control firstorder system finally propose sufficient condition existence compact set neuralnetworkbased backstepping control highorder nonlinear system theoretic result illustrate simulation examplen',\n",
       " 'Jacek M. Zurada Xifeng Yang Chen Xu Jian Wang paper propose new variant backpropagation algorithm improve generalization ability feedforward neural network basic idea method stem group lasso concept deal variable selection problem group level main drawback group lasso penalty directly employ network train numerical oscillation theoretical challenge compute gradient origin overcome obstacle smooth function introduce approximate group lasso penalty numerical experiment classification regression problem demonstrate propose algorithm perform good classical penalization method weight decay weight elimination approximate smooth generalization prune efficiency addition detail simulation base specific data set perform compare common prune strategy verify advantage propose algorithm prune ability propose strategy investigate relatively large data set mnist term smooth approximation casesn',\n",
       " 'Modjtaba Rouhani Alaleh Maskooki Hadi Sadoghi Yazdi Mojtaba Nayyeri objective function propose literature adjust input parameter node constructive network furthermore researcher focus universal approximation capability network base exist objective function brief use correntropy measure base sigmoid kernel objective function adjust input parameter newly add node cascade network propose network show capable approximate continuous nonlinear map probability compact input sample space convergence guarantee performance method compare different objective function exist hide layer feedforward network real regression data set impulsive noise experimental result indicate benefit correntropy measure reduce root mean square error increase robustness noisen',\n",
       " 'Tanveer Choudhury Cameron Foale Richard Dazeley Peter Vamplew abstract work identify important previously unaddressed issue regression base neural network xe2x80x93 learn accurately approximate problem output function input number output require vary input space nonfunctional regression problem arise number application adequately handle exist neural network algorithm demonstrate benefit possible directly address nonfunctional regression paper propose neural algorithm xe2x80x93 extension resource allocate network run add additional output neuron network structure train new algorithm call resource allocate network vary output cardinality runvoc demonstrate capable learn perform nonfunctional regression artificially construct data realworld task specify parameter set plasmaspray process importantly runvoc show outperform original run algorithm best possible error rate achievable functional form regressionn',\n",
       " 'Alberto Fachechi Matteo Beccaria Adriano Barra abstract propose modification cost function hopfield model salient feature shine taylor expansion result pairwise interaction alternate sign suggest unify framework handle deep learn network prune analysis heavily rely hamiltonxe2x80x93jacobi correspondence relate statistical model mechanical picture model relativistic extension original hopfield model cost function quadratic form mattis magnetization mimic nonrelativistic counterpart socalled classical limit focus lowstorage regime solve model analytically take advantage mechanical analogy obtain complete characterization free energy associate selfconsistency equation thermodynamic limit numerical test performance proposal extensive monte carlo simulation show stability spurious state limit capability standard hebbian construction sensibly reduce presence unlearn contribution prune massivelyn',\n",
       " 'Weihua Gui Tingwen Huang Shiwen Xie Jinjing Yu Yongfang Xie abstract outlet ferrous ion concentration essential indicator manipulate goethite process zinc hydrometallurgy plant measure online lead delay feedback information study selfadjusting structure radial basis function neural network sasrbfnn develop predict outlet ferrous ion concentration online supervise cluster algorithm propose initialize rbfnn network structure adjust develop selfadjusting structure mechanism mechanism merge divide hide neuron accord distance cluster achieve adaptability rbfnn finally connection weight determine gradientbased algorithm convergence sasrbfnn analyze lyapunov criterion simulation benchmark problem show effectiveness propose network sasrbfnn apply predict outlet ferrous ion concentration goethite process result demonstrate network provide accurate prediction mathematical model fluctuate production conditionn',\n",
       " 'Meng Joo Er Rajasekar Venkatesan paper progressive learn technique multiclass classification propose newly develop learn technique independent number class constraint learn new class retain knowledge previous class new class nonnative knowledge learn far encounter neural network structure get remodel automatically facilitate new neuron interconnection parameter calculate way retain knowledge learn far technique suitable realworld application number class unknown online learn realtime data require consistency complexity progressive learn technique analyze standard datasets evaluate performance develop technique comparative study show develop technique superiorn',\n",
       " 'A.G. Loukianov E.N. Sanchez A.Y. Alanis paper deal adaptive track discretetime multipleinputmultipleoutput mimo nonlinear system presence bound disturbance paper highorder neural network honn structure approximate control law design backstepping technique apply block strict feedback form bsff paper include respective stability analysis basis lyapunov approach control include extend kalman filter ekfbased nn learn algorithm applicability scheme illustrate simulation discretetime nonlinear model electric induction motorn',\n",
       " 'Zhongke Shi Fuchun Sun Danwei Wang Bin Xu abstract paper direct adaptive neural controller investigate longitudinal dynamic generic hypersonic flight vehicle hfv objective controller altitude velocity follow give desire trajectory presence aerodynamic uncertainty base functional decomposition adaptive discretetime nonlinear controller develop feedback linearization neural approximation subsystem different backstepping design altitude subsystem transform explicit fourstep ahead prediction model prediction model controller propose virtual controller design furthermore direct neural network nn employ lump uncertainty approximation controller considerably simple one base backstepping scheme algorithm need nn parameter adjust online semiglobal uniform ultimate boundedness sguub stability investigate discretetime lyapunov analysis output track error neighborhood zero accordingly nn controller design velocity subsystem simulation present effectiveness propose control approachn',\n",
       " 'Dewen Hu Zhiqiang Zheng Shuzhi Sam Ge Jianjun Ma note adaptive neural network nn control investigate class uncertain nonlinear system asymmetric saturation actuator external disturbance handle effect nonsmooth asymmetric saturation nonlinearity gaussian error functionbased continuous differentiable asymmetric saturation model employ backstepping technique control design explosion complexity traditional backstepping design avoid dynamic surface control radial basis function nn adaptive control develop guarantee signal closedloop semiglobally uniformly ultimately bound track error converge small neighborhood origin appropriately choose design constant effectiveness propose control demonstrate simulation studyn',\n",
       " 'TsuTian Lee WeiYen Wang YihGuang Leu paper observerbase direct adaptive fuzzyneural control scheme present nonaffine nonlinear system presence unknown structure nonlinearities direct adaptive fuzzyneural controller class generalize nonlinear system call nonaffine nonlinear system instead indirect affine nonlinear system give leu et al implicit function theorem taylor series expansion observerbase control law weight update law fuzzyneural controller derive nonaffine nonlinear system base strictlypositivereal spr lyapunov theory stability closedloop verify overall adaptive scheme guarantee signal involve bound output closedloop asymptotically track desire output trajectory demonstrate effectiveness propose method simulation result illustrate papern',\n",
       " 'JengTze Huang exist adaptive neural controller ensure semiglobally uniform ultimately bound stability condition neural approximation remain valid time condition difficult verify result deterioration track performance instability occur real application common recourse activate extra robust controller outside neural active region pull transient approach restrict dynamic system match uncertainty extend strictfeedback system mismatch uncertainty multiswitchbased backstepping methodology virtual actual controller propose design switch adaptive neural controller robust controller switch algorithm sufficiently smooth able incorporate backstepping tool overall controller ensure globally uniform ultimate boundness simultaneously avoid possible control singularity simulation result demonstrate validity propose designsn',\n",
       " 'GwiTae Park SamJun Seo SeongHwan Kim SungHoe Huh JangHyun Park direct adaptive statefeedback controller propose highly nonlinear system consider uncertain illdefined nonaffine nonlinear system employ neural network nn flexible structure online variation number neuron nn approximate adaptively cancel unknown plant nonlinearity control law adaptive law weight hide layer output layer nn establish closedloop stable sense lyapunov track error guarantee uniformly asymptotically stable uas uniformly ultimately bound uub aid additional robustifying control term propose control algorithm relatively simple require restrictive condition design constant stability efficiency propose scheme show simulation simple nonaffine nonlinear systemn',\n",
       " 'Cong Wang S.S. Ge paper direct adaptive neuralnetwork nn control present class affine nonlinear system strictfeedback form unknown nonlinearities utilize special property affine term develop schemeavoids controller singularity problem completely signal close loop guarantee semiglobally uniformly ultimately bound output prove converge small neighborhood desire trajectory control performance closeloop guarantee suitably choose design parameter simulation result present effectiveness approachn',\n",
       " 'Maoguo Gong Jian Wu Shuzhi Sam Ge Weisheng Chen paper address problem globally stable direct adaptive backstepping neural network nn track control design class uncertain strictfeedback system assumption accuracy ultimate track error give priori contrast classical adaptive backstepping nn control scheme paper analyze convergence track error barbalatxe2x80x99s lemma nonnegative function positivedefinite lyapunov function accuracy ultimate track error determine adjust accurately priori closedloop guarantee globally uniformly ultimately bound main technical novelty construct new n thorder continuously differentiable function design control law virtual control variable adaptive law finally simulation example give illustrate effectiveness advantage propose control methodn',\n",
       " 'S. Jagannathan multilayer neuralnetwork nn controller design deliver desire track performance control class unknown nonlinear system discrete time nonlinearities satisfy match condition lyapunov approach uniform ultimate boundedness track error nn weight estimate show novel weight update rigorous procedure provide analysis select nn controller parameter result structure consist nn function approximation inner loop out proportional derivative track loop simulation result carry justify theoretical conclusion net result design development nn controller strictfeedback class nonlinear discretetime systemn',\n",
       " 'Chun Lung Philip Chen Xin Chen Yun Zhang Guanyu Lai Zhi Liu paper investigate fusion unknown direction hysteresis model adaptive neural control technique face timedelayed continuous time nonlinear system strictfeedback form compare previous work hysteresis phenomenon direction modify boucxe2x80x93wen hysteresis model investigate literature unknown reduce computation burden adaptation mechanism optimize adaptation method successfully apply control design base lyapunovxe2x80x93krasovskii method neuralnetworkbase adaptive control algorithm construct guarantee state adaptive parameter remain bound track error converge adjustable neighborhood origin final numerical example provide validate effectiveness propose control methodsn',\n",
       " 'F.L. Lewis C.M. Kwan present new robust control technique induction motor neural network nns method systematic robust parameter variation motivate backstepping design technique treat certain signal fictitious control input simple subsystem twolayer nn stage design fictitious controller apply second twolayer nn robustly realize fictitious nn signal design previous step new tune scheme propose guarantee boundedness track error weight update main advantage method require regression matrix preliminary dynamical analysis need salient feature nn approach offline learn phase need state feedback need implementation load torque rotor resistance unknown boundedn',\n",
       " 'J.A. Farrell Jin Young Choi paper extend application neurocontrol approach new class nonlinear system diffeomorphic output feedback nonlinear system unmeasured state neuralbased adaptive observer introduce state estimation identification output measurement online operation identification achieve online approximation priori unknown function controller design backstepping control design procedure leakage term adaptive law nonlinear damp term backstepping controller introduce prevent instability arise inherent approximation error primary benefit online function approximation reduction approximation error allow reduction observer controller gain semiglobal stability analysis propose approach provide feasibility investigate illustrative simulation examplen',\n",
       " 'Chun Lung Philip Chen Yun Zhang Guanyu Lai Zhi Liu paper address problem adaptive neural outputfeedback control class special nonlinear system hysteretic output mechanism unmeasure state modify boucxe2x80x93wen model employ capture output hysteresis phenomenon design procedure fusion neural network nussbaumtype function key lemma establish extend property model avoid bad performance cause output nonlinearity barrier lyapunov function technique introduce guarantee prescribe constraint track error addition robust filter method design cancel restriction state require measure base lyapunov synthesis new neural adaptive controller construct guarantee prescribe convergence track error semiglobal uniform ultimate boundedness signal closedloop simulation implement evaluate performance propose neural control algorithm papern',\n",
       " 'Gang Feng Junfei Qiao Wendong Zhou Honggui Han paper concern problem adaptive neural control class uncertain illdefined nonaffine nonlinear system selforganizing radial basis function neural network rbfnn direct selfconstructing neural controller dsnc design unknown nonlinearities approximate closedloop stable key feature propose dsnc design scheme summarize follow different exist result literature selforganizing rbfnn adaptive threshold construct online dsnc improve control performance second control law adaptive law weight rbfnn establish closedloop stable term lyapunov stability theory track error guarantee uniformly asymptotically converge zero aid additional robustifying control term example finally give demonstrate design procedure performance propose method simulation result reveal effectiveness propose methodn',\n",
       " 'Ali Zayed Kevin Warwick Amir Hussain Rudwan Abdullah paper present novel intelligent multiplecontroller framework incorporate fuzzylogicbased switch tune supervisor generalise learn model glm autonomous cruise control application propose methodology combine benefit conventional proportionalintegralderivative pid controller pid structurebased simultaneous zero pole placement controller switch decision nonlinear fix structure controller basis require performance measure fuzzylogicbased supervisor operate high level supervisor employ adaptively tune parameter multiple controller order achieve desire closedloop performance intelligent multiplecontroller framework apply autonomous cruise control problem order maintain desire vehicle speed control throttle plate angle electronic throttle control sample simulation result validate nonlinear vehicle model demonstrate effectiveness multiplecontroller respect adaptively track desire vehicle speed change achieve desire speed response whilst penalise excessive control actionn',\n",
       " 'Heidar Ali Talebi Farzaneh Abdollahi Kasra Esfandiari paper present track control methodology class uncertain nonlinear system subject input saturation constraint external disturbance unlike previous approach saturate system assume affine nonlinear system paper track control problem solve uncertain nonaffine nonlinear system input saturation deal saturation constraint auxiliary construct modify track error define employ implicit function theorem mean value theorem modify track error update rule derive base wellknown backpropagation bp algorithm prove relevant update rule control problem previous approach bp algorithm suffer lack stability analysis inject damp term standard bp algorithm uniformly ultimately boundedness signal closedloop ensure lyapunovxe2x80x99s direct method furthermore present approach employ nonlinear parameter neural network propose scheme applicable system high degree nonlinearity highgain observer reconstruct state output feedback controller present finally simulation result perform duffingholmes chaotic generalize pendulumtype numerical present demonstrate effectiveness suggest state output feedback control schemesn',\n",
       " 'A.J. Calise Nakwan Kim extension neural network nnbased adaptive output feedback control nonlinear system develop extension linearly nonlinearly parameterized nns give direct adaptive output feedback approach extension permit introduction emodification direct adaptive approach errorobserverbased approach give finally case nonaffine system eliminate fixedpoint assumption appear early workn',\n",
       " 'Le Liu Jianxiong Li Yiming Fang Ru Chang adaptive control scheme investigate class strictfeedback markovian jump nonlinear system unknow control gain unmodeled dynamic deal unmodeled dynamic available dynamic signal employ construct appropriate lyapunov function rbf neural network approximate unknow nonlinear function markovian switch approximation capability neural network combine backstepping technique avoid inherent problem controller complexity traditional backstepping design method prove signal closedloop uniformly ultimately bound probability track error signal converge small neighborhood origin choose suitable design parameter simulation result illustrate effectiveness propose scheme complex nonlinear markovian jump system unmodeled dynamic consideredadaptive neural control scheme extend complex nonlinear markovian jump systemassumption sign unknow control gain abandonedthe propose scheme effective markovian jump system know control gainonly parameter require adjust onlinen',\n",
       " 'ZhongPing Jiang PeiYaun Peng Youping Zhang propose adaptive control perspective neural controller class unknow minimum phase feedback linearizable nonlinear know relative degree control scheme base backstepping design technique conjunction linearly parametrized neuralnetwork structure result controller move complex mechanic involve typical backstepping design offline online appropriate choice network size neural basis function controller train online control different nonlinear plant relative degree semiglobal stability show simple lyapunov analysis controller preserve performance property standard backstepping controller simulation result show demonstrate property compare neural controller standard backstepping controllern',\n",
       " 'S. Ferrari advantage bring classical linear control theory conjunction neural approximators long recognize literature particular linear controller obtain start neural control design show key step successful development implementation adaptivecritic neural controller despite adaptive capability neural controller criticize provide performance stability guarantee classical linear design paper develop algebraic synthesis procedure design dynamic outputfeedback neural controller closedloop stable meet performance objective classical linear design performance synthesis problem address derive implicit modelfollowing algebraic relationship model matrix obtain classical design neural control parameter additional linear matrix inequality lmis condition closedloop exponential stability neural controller derive exist integral quadratic constraint iqcs operator repeat sloperestricted nonlinearities approach demonstrate design recurrent neural network controller highly maneuverable tailfincontrolled missile meet multiple design objective include pole placement transient tune h infin h 2 performance presence parameter uncertainty commandinput trackingn',\n",
       " 'Jun Zhao Lijun Long paper investigate problem adaptive neural track control outputfeedback class switch uncertain nonlinear system measurement state unknown control signal approximate directly neural network novel adaptive neural control technique problem study set exploit average dwell time method backstepping switch filter different update law design reduce conservativeness cause adoption common observer common update law subsystem propose controller subsystem guarantee closedloop signal remain bound class switch signal average dwell time output track error converge small neighborhood origin application propose design method adaptive output feedback neural track controller massspringdamper constructedn',\n",
       " 'Mou Chen Gang Tao Yanjun Zhang paper present new study adaptive neural networkbased control class noncanonical nonlinear system large parametric uncertainty unlike commonly study canonical form nonlinear system neural network approximation model explicit relative degree structure directly derive parameterized controller adaptation noncanonical form nonlinear system usually explicit relative degree approximation model noncanonical form wellknown adaptive control noncanonical form nonlinear system involve parameterization dynamic demonstrate paper case noncanonical neural network approximation model effective control system open research problem especially presence uncertain parameter paper show necessary reparameterize neural network model adaptive control design reparameterization realize relative degree formulation concept study general neural network model paper derive parameterized controller guarantee closedloop stability asymptotic output track noncanonical form neural network model illustrative example present simulation result demonstrate control design procedure verify effectiveness new design methodn',\n",
       " 'Honghong Wang Chong Lin Bing Chen Yumei Sun paper address adaptive neural control class nonstrictfeedback stochastic nonlinear system time delay important structural property radial basis function rbf neural network nns introduce overcome design difficulty nonstrictfeedback structure lyapunovkrasovskii functional control design stability analysis backsteppingbased adaptive neural control strategy propose suggest adaptive neural controller guarantee closedloop signal semiglobally uniformly ultimately bound sguub track error converge small neighborhood origin simulation result demonstrate effectiveness propose approachn',\n",
       " 'Jianhui Zhi Yong Chen Jianping Xue Xinmin Dong Chao Shi abstract paper address adaptive neural track control problem class uncertain nonaffine nonlinear nonaffine function semibound possibly nondifferentiable compare traditional control scheme propose scheme apply general class nonaffine nonlinear relax constraint condition follow firstly assumption nonaffine function differentiable cancel continuous condition nonaffine function require guarantee controllability consider secondly assumption nonaffine function completely bound relax nonaffine function constrain semibound condition bind unknown function adaptive neural track controller design base invariant set control design process minimal learn parameter mlp technique reduce number adaptive parameter smooth robust compensator employ circumvent influence approximation error external disturbance furthermore prove closedloop signal semiglobally uniformly ultimately bound finally simulation example provide demonstrate effectiveness design methodn',\n",
       " 'GuangHong Yang YanHui Jing abstract paper problem nnbased adaptive faulttolerant track control class uncertain nonlinear timevarying delay system output constraint infinite number actuator fault consider construct lyapunovxe2x80x93krasovskii function introduce bind estimation approach dynamic surface control technique novel adaptive faulttolerant control scheme design compensate actuator fault unknown timedelay uncertain function output constraint violate compare exist result propose controller implement easily furthermore lyapunov theory prove signal closedloop semiglobally uniformly ultimately binded finally illustrative example verify effectiveness propose approachn',\n",
       " 'Yifang Liu Tao Xue Renfu Li Lin Hu abstract paper neuroadaptive optimal nonlinear control approach algorithm propose track control airbreathing hypersonic aircraft consider uncertainty base reinforcement learn mechanism neuroadaptive control agent construct actorcritic architecture consist interact neural network optimal control protocol know actor nn policy evaluation know critic nn optimality condition adaptive controller derive discrete minimum principle parametric mismatch uncertainty unmodeled nonlinearity handle neural network name unn aid concept virtual plant output network virtual plant help estimate optimal control vary dynamic airbreathing hypersonic flight aircraft simulation result present verify effectiveness design method track control airbreathing hypersonic aircraft presence uncertaintyn',\n",
       " 'Huajun Gong Mou Chen Dawei Wu abstract novel adaptive neural network nn flight control law develop high angle attack aoa longitudinal motion control aircraft consider unsteady effect longitudinal dynamic model propose uncertain nonstrictfeedback nonlinear timevarying distribute delay combine variable separation technique lyapunovxe2x80x93krasovskii function method robust adaptive nn high aoa flight control scheme design direction control gain need develop controller reduce computational burden adaptive parameter need propose control law prove closedloop stable signal closedloop bound finally simulation give confirm effectiveness propose control lawn',\n",
       " 'George W. Irwin Haibo He Kang Li Long Zhang construction radial basis function rbf network involve determination model size hide node output weight squarebased subset selection method determine rbf model size parameter simultaneously method robust achieve optimal result alternatively gradient method widely optimize parameter drawback algorithm converge slowly treat hide node output weight separately ignore correlation paper new discretecontinuous algorithm propose construction rbf model orthogonal square olsbased forward stepwise selection construct initial model select model term candidate term pool new levenbergmarquardt lmbased parameter optimization propose optimize hide node output weight continuous space speed convergence propose parameter optimization method consider correlation hide node output weight achieve translate output weight dependent parameter ols method correlation previously propose continuous forward algorithm cfa unlike cfa new method optimize parameter simultaneously addition equivalent recursive sum square error derive reduce computation demand derivative lm method computational complexity give confirm new method computationally efficient cfa different numerical example present illustrate effectiveness propose method friedman statistical test 13 classification problem perform result demonstrate rbf network build new method competitive comparison popular classifiersn',\n",
       " 'Yan Liu Dakun Yang Jian Wang Jacek M. Zurada Qinwei Fan Wei Wu aim paper develop novel method prune feedforward neural network introduce l12 regularization term error function procedure force weight small train eventually remove train usual l12 regularization term involve absolute value differentiable origin typically cause oscillation gradient error function train key point paper modify usual l12 regularization term smooth origin approach offer follow advantage remove oscillation gradient value secondly give good prune final weight remove small produce usual l12 regularization thirdly make possible prove convergence train support numerical example providedn',\n",
       " 'Abdesselam Bouzerdoum Fok Hing Chi Tivive paper present scale rotation invariant face detection employ hierarchical neural network call siconnet process element govern nonlinear mechanism shunt inhibition neural network facenonface classifier handle inplane rotate pattern train network rotation invariant face classifier enhance bootstrap train technique develop prevent bias nonface class furthermore multiresolution process employ scale invariance image pyramid form subsampling face detection perform scale pyramid adaptive threshold evaluate benchmark cmu rotate face database propose face detection outperform exist rotation invariant face detector few false positive high detection accuracyn',\n",
       " 'A. Bouzerdoum Fok Hing Chi Tivive article present efficient train algorithm base firstorder secondorder conjugate gradient optimization method class convolutional neural network conn know shunt inhibitory convolution neural network furthermore new hybrid method propose derive principle quickprop rprop supersab square l experimental result new hybrid method perform levenbergmarquardt lm algorithm low computational cost memory storage comparison sake visual pattern recognition task facenonface discrimination choose classification problem evaluate performance train algorithm sixteen train algorithm implement different variant propose conn architecture binary toeplitz fully connect architecture implement algorithm train network architecture successfully convergence speed vary markedly particular combination l new hybrid method l lm method achieve best convergence rate term number train epoch addition classification accuracy architecture assess tenfold cross validation result binary toeplitzconnect architecture outperform slightly fully connect architecture low error rate train algorithm 195 toeplitzconnect 210 binaryconnect 220 fully connect network general modify broydenfletchergoldfarbshanno bfgs method variant lm algorithm new hybridl method perform consistently achieve error rate 3 average architecturen',\n",
       " 'Eugenius Kaszkurewicz Amit Bhaya Fernando A. Pazos paper present unify way design neural network characterize second order ordinary differential equation ode aim global minimum nonconvex scalar function neural network alternatively refer continuous time algorithm interpret dynamical close loop control system design base control liapunov function clf method nonconvex scalar function goal algorithm produce trajectory start arbitrarily choose initial guess stick local minimum increase chance converge global minimumn',\n",
       " 'Jan Jantzen Georgios Dounias Nikolaos Ampazis paper use highly efficient second order neural network train algorithm lmam levenbergmarquardt adaptive momentum olmam optimizedlevenbergmarquardt adaptive momentum construction efficient papsmear test classifier algorithm methodologically similar base iteration form employ levenbergmarquardt lm method nonlinear square problem inclusion additional adaptive momentum term arise formulation train task constrain optimization problem classification result obtain application algorithm standard benchmark papsmear data set reveal power method obtain excellent solution difficult classification problem standard computational intelligence technique achieve inferior performancesn',\n",
       " 'Shitong Wang Yizhang Jiang Xiaoqing Luo Zhaohong Deng Jun Wang train feedforward neural network fnns critical issue fnns study fnns train method directly apply large dataset high computational space complexity order tackle problem ccmeb centerconstrained minimum enclose ball problem hide feature space fnn discuss novel learn algorithm call hfsrgcvm hidefeaturespace regression generalize core vector machine develop accordingly hfsrgcvm novel learn criterion l2norm penaltybase e insensitive function formulate parameter hide node generate randomly independent train set learn parameter output layer prove equivalent special ccmeb problem fnn hide feature space ccmeb approximation base machine learn algorithm propose hfsrgcvm train algorithm follow merit maximal train time hfsrgcvm train linear size train dataset maximal space consumption independent size train dataset experiment regression task confirm conclusionsn',\n",
       " 'JunFei Qiao Shuo Zhang HongGui Han abstract train recurrent neural network rnns concern selection structure connection weight efficiently enhance generalization capability rnns recurrent selforganizing neural network rsonn adaptive grow prune algorithm agpa propose improve performance paper agpa selforganize structure rnns base information process ability competitiveness hide neuron learn process hide neuron rsonn add prune improve generalization performance furthermore adaptive secondorder algorithm adaptive learn rate employ adjust parameter rsonn convergence rsonn give computational efficiency demonstrate merit rsonn data model benchmark datasets real world application associate nonlinear system model problem examine comparison exist method experimental result propose rsonn effectively simplify network structure perform good exit methodn',\n",
       " 'Jian Wang Dakun Yang Qinwei Fan Wei Wu Yan Liu abstract popular feasible approach determine appropriate size neural network remove unnecessary connection oversized network advantage l 12 regularization recognize sparse model nonsmoothness l 12 regularization lead oscillation phenomenon approach smooth l 12 regularization propose paper takagixe2x80x93sugeno txe2x80x93s fuzzy model order improve learn efficiency promote sparsity model new smooth l 12 regularizer remove oscillation enable prove weak strong convergence result txe2x80x93s fuzzy neural network zeroorder furthermore relationship learn rate parameter penalty parameter give guarantee convergence simulation result provide support theoretical find superiority smooth l 12 regularization original l 12 regularizationn',\n",
       " 'JunFei Qiao XiaoLong Wu HongGui Han paper realtime model predictive control rtmpc base selforganizing radial basis function neural network sorbfnn propose nonlinear system rtmpc simplicity parallelism model predictive control design efficiency deal computational complexity sorbfnn concurrent structure parameter learn develop predictive model nonlinear system model performance significantly improve sorbfnn model error uniformly ultimately bound second fast gradient method gm enhance solution optimal control problem propose gm reduce computational cost suboptimize rtmpc online condition stability analysis steadystate performance closedloop system present finally numerical simulation reveal propose control give satisfactory track disturbance rejection performance experimental result demonstrate effectivenessn',\n",
       " 'JunFei Qiao Ying Hou Lu Zhang HongGui Han nonlinear model predictive control nmpc scheme develop paper base selforganizing recurrent radial basis function srrbf neural network structure parameter adjust concurrently train process propose srrbf neural network represent general nonlinear form predict future dynamic behavior nonlinear system improve model accuracy spikingbase grow prune algorithm adaptive learn algorithm develop tune structure parameter srrbf neural network respectively control problem improve gradient method utilize solution optimization problem nmpc stability result control prove base lyapunov stability theory finally propose srrbf neural networkbase nmpc srrbfnmpc control dissolve oxygen concentration wastewater treatment process wwtp comparison exist method demonstrate srrbfnmpc achieve considerably good model fit wwtp good control performance concentrationn',\n",
       " 'Kiri Wagstaff Dennis DeCoste u0026 u0027 0 1 2 34 65 78 9 8 b 3 c bd2e 5 7fg ihb jk u003e u0026 l mn o 8 l p u0026 qu0027 rtst u 8 o v w mx 8 u0026 5 5yhl z u0026 e 0 m8 2 2a 3 1 ab u0027 vcp vcp fb 5 fghc0c0db0jpmi 85 8 7 j ak 8 u003e chj u0026 f e 5u003emi q lrnmv e u0026 iht q o hn 3 d2 nmi b 5 0 u 8 o 58 t ml w 3 0p 2 u003c 5 p u0027 q aq khl u mk u0027 l 8 q r category subject descriptor m r rr r9s tuwvxzyh ihx7f xc2x80 xc2x81hubxc2x82huu0026xc2x83xc2x84ueu0026xxc2x80 xc2x86gxc2x87txc2x88k wxc2x89u m48 8 2 general term u0026 uu0027 l 3 e c u0026 54n',\n",
       " 'ChihJen Lin ChihWei Hsu support vector machine svms originally design binary classification effectively extend multiclass classification ongoing research issue method propose typically construct multiclass classifier combine binary classifier author propose method consider class computationally expensive solve multiclass problem comparison method largescale problem seriously conduct especially method solve multiclass svm step large optimization problem require experiment limit small data set paper decomposition implementation alltogether method compare performance method base binary classification oneagainstall oneagainstone direct acyclic graph svm dagsvm experiment indicate oneagainstone dag method suitable practical use method result large problem method consider data general need few support vectorsn',\n",
       " 'S.S. Keerthi paper discus implementation issue relate tune hyperparameters support vector machine svm lsub 2 soft margin radiusmargin bind take index minimize iterative technique employ compute radius margin implementation show feasible efficient large problem have 10000 support vectorsn',\n",
       " 'Francisco J MartinezEstudillo Cesar HervasMartinez Pedro Antonio Gutierrez paper propose hybrid multilogistic methodology name logistic regression initial radial basis function rbf covariates process obtain coefficient carry step evolutionary program ep algorithm apply order produce rbf neural network rbfnn reduce number rbf transformation simple structure possible initial attribute space commonly know logistic regression literature covariate space transform add nonlinear transformation input variable give rbfs best individual final generation finally maximum likelihood optimization method determine coefficient associate multilogistic regression model build augment covariate space final step different multilogistic regression algorithm apply consider initial rbf covariates multilogistic initialrbf regression incrementally construct model apply cross validation result automatic covariate selection simplelogistic initialrbf regression slirbf method include regularization parameter optimize methodology propose test 18 benchmark classification problem wellknow machine learn problem real agronomical problem result compare correspond multilogistic regression method apply initial covariate space rbfnns obtain ep algorithm probabilistic classifier include different rbfnn design method relax variable kernel density estimation support vector machine sparse classifier sparse multinomial logistic regression procedure similar slirbf product unit basis function slirbf model competitive compare correspond multilogistic regression method rbfep method measure statistical significance indicate slirbf reach state artn',\n",
       " 'Licheng Jiao Min Wang Shuyuan Yang multiscale property reception field human visual cortex illuminate research wavelet neural network wnn find neurophysiology indicate human visual specialize area visual cortex respond particular orientation word reception field visual cortex multiresolution property direction localization scale enlighten fact layer feedforward neural network fnn present employ ridgelet activation function hide layer rapid learn deal high dimensional sample propose efficient linear learn algorithm inspire traditional kernel smooth method low computation complexity proportional number dimension sample cost little degradation accuracy network achieve rapid learn simulation experiment function approximation take commonly regression way consider condition comparison result result propose linear ridgelet network overcome curse dimensionality train fnns exhibit good performance high dimension counterpart especially spatial inhomogeneity exist functionn',\n",
       " 'Zhenyuan Wang Kimberly Andrews Espy Honggang Wang Maria L. Rizzo Hua Fang paper propose new nonlinear classifier base generalize choquet integral sign fuzzy measure enhance classification accuracy power capture possible interaction attribute generalize approach develop address unsolved choquetintegral classification issue allow flexible location projection line ndimensional space automatic search misclassification rate base choquet distance penalty misclassified point special genetic algorithm design implement classification optimization fast convergence numerical experiment empirical case study generalize approach improve extend functionality choquet nonlinear classification realworld multiclass multidimensional situationsn',\n",
       " 'K.A. Faisal A.S. Hadi E.A. ElSebakhy paper propose unconstrained functional network new classifier deal pattern recognition problem methodology learn algorithm kind computational intelligence classifier iterative square optimization criterion derive performance new intelligent system scheme demonstrate examine realworld application comparative study common classification algorithm machine learn statistic community carry study achieve set secondorder linearly independent polynomial function approximate neuron function result new framework classifier reliable flexible stable achieve highquality performancen',\n",
       " 'Chris J. Harris Xia Hong Sheng Chen fundamental principle data model incorporate available priori information underlie data generate mechanism model process adopt principle consider greybox radial basis function rbf model capable incorporate prior knowledge specifically explicitly incorporate type prior knowledge underlie data generate mechanism exhibit know symmetric property ii underlie process obey set give boundary value constraint class efficient orthogonal square regression algorithm readily apply modification construct parsimonious greybox rbf model enhance generalisation capabilityn',\n",
       " 'Yuichi Motai kernel association ka statistical pattern recognition classification prediction recently emerge machine learn signal process context survey outline late trend innovation kernel framework big data analysis ka topic include offline learn distribute database online learn prediction structural presentation comprehensive list reference gear provide useful overview evolve field specialist relevant scholarsn',\n",
       " 'Decai Li Zhiping Liang Min Han novel sparse kernel density estimation method propose base sparse bayesian learn random iterative dictionary preprocessing empirical cumulative distribution function response vector sparse weight density estimation estimate sparse bayesian learn propose iterative dictionary learn algorithm reduce number kernel computation essential step sparse bayesian learn sparse kernel density estimation quadratic renyi entropy base normalize mutual information feature selection method propose simulation example demonstrate propose method comparable typical parzen kernel density estimation compare stateofart sparse kernel density estimation method show good performance number kernel require density estimation example friedman data house data property propose feature variable selection methodn',\n",
       " 'Ezequiel LopezRubio estimation multivariate probability density function traditionally carry mixture parametric density kernel density estimator present new nonparametric approach problem base integration multivariate histogram compute affine transformation train data proposal belong class average histogram density estimator inherent discontinuity histogram smooth low computational complexity retain provide formal proof convergence real probability density function number train sample grow demonstrate performance approach compare set standard probability density estimatorn',\n",
       " 'S. B. Ameneiro E. Cernadas J. Ribeiro M. FernandezDelgado parallel perceptrons pps simple efficient committee machine single layer perceptrons threshold activation function binary output majority vote decision scheme behave universal approximators parallel delta pdelta rule effective train algorithm follow idea statistical learn theory support vector machine svm raise generalization ability maximize difference perceptron activation train pattern activation threshold correspond separate hyperplane paper propose analytical closedform expression calculate ppsu0027 weight classification task method call direct parallel perceptrons dpps directly calculate iteration weight train pattern desire output search numeric function optimization calculate weight globally minimize error function simultaneously take account train error classification margin give analytical noniterative nature dpps computationally efficient relate approach pdelta svm computational complexity linear input dimensionality dpps appeal term time complexity memory consumption easy use highdimensional classification task real benchmark datasets multiple class dpps competitive svm approach allow online learn oppose tunable parametersn',\n",
       " 'SungKwun Oh Witold Pedrycz SuChong Joo SeokBeom Roh remarkably rich landscape fuzzy cluster ensue design procedure information granule nutshell fuzzy cluster cluster general lead directionfree construct mean clear distinction input output variable framework fuzzy model information granule development inputoutput map perspective beneficial consider aspect directionality construction information granule fuzzy set input space conditional fuzzy cmeans cluster come algorithmically viable alternative construct fuzzy set input space presence supervision come form structure data distribute output space paper present new cluster method use ambiguity index express boundary cluster design illustrate aid numeric example provide detail insight performance fuzzy model form manner highlight crucial design issuesn',\n",
       " 'Fang Liu Ronghua Shang Licheng Jiao Juanjuan Luo cluster learn classification learn major task pattern recognition traditional hybrid cluster classification algorithm handle sequential way simultaneous way fortunately multiobjective optimization provide way solve problem paper algorithm learn simultaneous cluster classification adaptively multiobjective evolutionary algorithm propose main idea paper optimize objective function represent fuzzy cluster connectedness classification error rate achieve goal simultaneous learn firstly adopt graph base representation scheme encode generate set solution different number cluster single run relationship cluster classification build bayesian theory optimization process quality cluster classification measure objective function feedback draw aspect guide mutation set nondominated solution generate final pareto optimal solution select adjust rand index result synthetic datasets reallife datasets demonstrate rationality effectiveness propose algorithm furthermore apply propose algorithm image segmentation include texture image synthetic aperture radar image experimental result superiority propose algorithm compare algorithm highlightsa simultaneous adaptive cluster classification learn moea proposenew cluster objective function design objective function complement otherthe number cluster determine adaptively learn processthe drawback clusterclassification learn guide searchwe extend texture image sar image segmentation effectivenessn',\n",
       " 'Yuichi Motai Ammar O. Hoori paper propose multicolumn rbf network mcrn method improve accuracy speed traditional radial basis function network rbfn rbfn fully connect artificial neural network ann suffer costly kernel innerproduct calculation use instance center hide unit issue critical small datasets add hide unit burden computation time large datasets rbfn require hide unit kernel computation generalize problem mcrn mechanism construct base divide dataset small subset kd tree algorithm n resultant subset consider separate train datasets train n individual rbfns small rbfns stack parallel bulge mcrn structure test mcrn consider welldeveloped easytouse parallel structure individual ann train subset completely separate anns parallelize structure reduce test time compare single large rbfn easily parallelize fully connect structure small informative subset provide mcrn regional experience specify problem instead generalize mcrn test benchmark datasets show good accuracy great improvement train test time compare single rbfn mcrn show good result compare machine learn technique support vector machine knearest neighborsn',\n",
       " 'HuiYuan Yeh YuYen Ou Edward Kien Yee Yapp QuangThai Ho Nguyen Quoc Khanh Le abstract electron transport chain series protein complex embed transport protein important process transfer electron macromolecule cell primary process extract energy redox reaction case oxidation sugar cellular respiration accord molecular function component electron transport chain form complex different electron carrier functional loss specific molecular function electron transport chain implicate variety human disease diabetes neurodegenerative disorder parkinson alzheimeru0027s disease create precise model identify function pertinent understand human disease design drug target previous bioinformatics study exclusively focus electron transport protein information complex present deepetc deep learn model u twodimensional convolutional neural network positionspecific score matrix profile classify electron transport protein complex deepetc classify electron transporter independent test accuracy 991 997 998 988 998 complex ii iii iv v respectively performance result significantly accurate stateoftheart traditional neural network typical measurement metric propose study provide effective tool investigate electron transport protein achievement promote use deep learn bioinformatics computational biology deepetc freely accessible httpwwwbiologydeepcomdeepetc n',\n",
       " 'Tingwen Huang Xiaoping Chen He Huang Xusheng Qian abstract paper investigate construction sparse radial basis function neural network rbfnns classification problem efficient twophase construction algorithm abbreviate tpclr 1 simplicity propose l 1 regularization phase improve maximum data coverage imdc algorithm present initialization rbf center width specialize orthantwise limitedmemory quasinewton sowlqn method employ perform simultaneous network prune parameter optimization second phase advantage tpclr 1 lie good generalization performance guarantee high model sparsity require storage space test time reduce regularization parameter maximum number function evaluation require prescribe entire construction procedure automatic learn algorithm verify classification benchmark different level complexity experimental result appropriate value regularization parameter easy costly cross validation propose tpclr 1 offer efficient procedure construct sparse rbfnn classifier good generalization performancen',\n",
       " 'George E. Tsekouras Antonios D. Niros propose twostaged fuzzy cluster algorithm train radial basis function neural network novelty contribution lie way handle input train data information stage algorithm backpropagation method employ optimize network parameter number hide node determine iterative implementation fuzzy cluster backpropagation simulation result methodology produce accurate model compare standard sophisticate technique report literaturen',\n",
       " 'B. Prieto O. Valenzuela I. Rojas J. Gonzalez H. Pomares A. Guillen design radial basis function neural network rbfnns remain difficult task apply classification regression problem difficulty arise parameter define rbfnn set number rbfs position center length radius issue face apply model real world application select variable rbfnn use input literature present methodology perform task separately intrinsic parallelism genetic algorithm parallel implementation allow algorithm propose paper evolve solution problem time parallelization algorithm consist evolution problem specialization crossover mutation operator order evolve different element optimize design rbfnns subjacent genetic algorithm nonsorting dominate genetic algorithm ii nsgaii help balance size network approximation accuracy order avoid overfitted network novelty propose algorithm incorporation local search algorithm stage algorithm initialization population evolution individual final optimization pareto initialization individual perform hybridize cluster technique mutual information mi theory select input variable experiment synergy different paradigm technique combine present algorithm allow obtain accurate model significant input variablen',\n",
       " 'Stefan Roth Alexander Gepperth present application multiobjective evolutionary optimization feedforward neural network nn real world problem car face classification possibly conflict requirement nns speed classification accuracy enhance embed system compare result outcome greedy optimization heuristic magnitudebased prune couple multiobjective performance evaluation car classification problem magnitudebased prune yield competitive result difficult face classification evolutionary approach nn design clearly preferablen',\n",
       " 'Antonio Padua Braga Illya Kokshenev modern multiobjective machine learn method base evolutionary optimization algorithm know global convergent usually deliver nondeterministic result work propose deterministic global solution multiobjective problem supervise learn methodology nonlinear program result propose multiobjective algorithm perform global search paretooptimal hypothesis space rbf network determine weight basis function combination akaike bayesian information criterion algorithm demonstrate high generalization efficiency synthetic realworld benchmark problemsn',\n",
       " 'Pedro Antonio Gutierrez Cesar Hervas Francisco Jose Martinez Juan Carlos Fernandez Caballero paper propose multiclassification algorithm multilayer perceptron neural network model try boost conflict main objective multiclassifiers high correct classification rate level high classification rate class objective usually optimize classification consider give need obtain high precision class real problem solve machine learn problem use paretobase multiobjective optimization methodology base memetic evolutionary algorithm consider memetic pareto evolutionary approach base nsga2 evolutionary algorithm mpensga2 pareto build strategy automatic individual selection best model accuracy best model sensitivity extreme pareto methodology apply solve 17 classification benchmark problem obtain university california irvine uci repository complex real classification problem model obtain high accuracy high classification rate classn',\n",
       " 'Yeung Yam Xiaolin Huang Shuning Wang smooth hinge hyperplane shh propose improvement wellknown hinge hyperplane hh fact retain useful feature hh overcome hhu0027s drawback nondifferentiability paper introduce formal characterization smooth hinge function shf generate shh neural network method general construction shf give furthermore work prof shh good hh functional approximation optimal error shh approximate general function small equal hh particularly case shf generate integration class sigmoidal function prove correspond shh 2m shf outperform neural network m sigmoidal function shf derive upper bind establish approximation error neural network m sigmoidal activation function translate shh m shf replace m m2 work include algorithm identification shh make use differentiability property simulation experiment present validate theoretical conclusion possible extentn',\n",
       " 'Marco Russo Giuseppe Patane cluster application cover field audio video data compression pattern recognition vision medical image recognition paper present new cluster algorithm call enhance lbg elbg belong hard kmeans vector quantization group derive directly simple lbg basic idea develop concept utility codeword powerful instrument overcome main drawback cluster algorithm generally result achieve good case bad choice initial codebook present experimental result show elbg able good codebooks previous cluster technique computational complexity virtually simple lbgn',\n",
       " 'A. Prieto J. Ortega H. Rojas J. Gonzalez date cluster technique orient solve classification pattern recognition problem author apply unchanged construct initial model function approximators classification function approximation problem present different objective necessary design new cluster algorithm specialize problem function approximation paper present new cluster technique specially design function approximation problem improve performance approximator obtain compare model derive traditional classification orient cluster algorithm inputoutput cluster techniquen',\n",
       " 'Risto Miikkulainen Nate Kohl evolution neural network neuroevolution successful approach lowlevel control problem pole balance vehicle control collision warn certain type problemuch involve strategic decisionmakinghave remain difficult neuroevolution solve paper evaluate hypothesis problem difficult fracture correct action vary discontinuously agent move state state method measure fracture concept function variation propose base concept method deal fracture examine neuron local receptive field refinement base cascade network architecture experiment benchmark domain perform evaluate different level fracture affect performance neuroevolution method demonstrate modification improve performance significantly result form promise start point expand neuroevolution strategic tasksn',\n",
       " 'Jie Yang Hua Yu n',\n",
       " 'A.C. Kak A.M. Martinez context appearancebase paradigm object recognition generally believe algorithm base lda linear discriminant analysis superior base pca principal component analysis communication case present case intuitively plausible argument show actual result face database overall conclusion train data set small pca outperform lda pca sensitive different train data setsn',\n",
       " 'M. Wilkes M. Neamtu H. Cevikalp pattern recognition task dimension sample space large number sample train set know small sample size problem linear discriminant analysis lda technique apply directly small sample size case small sample size problem encounter kernel approach recognition paper attempt answer question choose optimal projection vector feature extraction small sample size case base find propose new method call kernel discriminative common vector method method nonlinearly map original input space implicit high dimensional feature space data hop linearly separable optimal projection vector compute transform space propose method yield optimal solution maximize modify fisheru0027s linear discriminant criterion discuss paper certain condition 100 recognition rate guarantee train set sample experiment test data situation generalization performance propose method compare favorably kernel approachn',\n",
       " 'A.N. Venetsanopoulos K.N. Plataniotis Juwei Lu technique introduce lowdimensional feature representation enhance discriminatory power paramount importance face recognition fr system know distribution face image perceivable variation viewpoint illumination facial expression highly nonlinear complex surprise linear technique base principle component analysis pca linear discriminant analysis lda provide reliable robust solution fr problem complex face variation paper propose kernel machinebase discriminant analysis method deal nonlinearity face patternsu0027 distribution propose method effectively solve socalled small sample size s problem exist fr task new algorithm test term classification error rate performance multiview umist face database result indicate propose methodology able achieve excellent performance small set feature error rate approximately 34 48 commonly kernel fr approach kernelpca kpca generalize discriminant analysis gda respectivelyn',\n",
       " 'S.Z. Li A.N. Venetsanopoulos K.N. Plataniotis J. Lu paper propose novel ensemblebase approach boost performance traditional linear discriminant analysis ldabase method face recognition ensemblebase approach base recently emerge technique know boost generally believe boostlike learn rule suit strong stable learner lda break limitation novel weakness analysis theory develop theory attempt boost strong learner increase diversity classifier create learner expense decrease margin achieve tradeoff suggest recent boost study low generalization error addition novel distribution account pairwise class discriminant information introduce effective interaction booster ldabase learner integration methodology propose lead novel ensemblebase discriminant learn approach capable take advantage boost lda technique promise experimental result obtain difficult face recognition scenario demonstrate effectiveness propose approach believe work especially beneficial extend boost framework accommodate general strongweak learnersn',\n",
       " 'I. Pitas I. Buciu A. Tefas S. Zafeiriou paper supervise method enhance classification accuracy nonnegative matrix factorization nmf algorithm present idea extend nmf algorithm order extract feature enforce spatial locality separability class discriminant manner method employ discriminant analysis feature derive nmf way twophase discriminant feature extraction procedure implement nmf plus linear discriminant analysis lda second method incorporate discriminant constraint inside nmf decomposition decomposition face discriminant part obtain new update rule weight basis image derive introduce method apply problem frontal face verification wellknown xm2vts database method greatly enhance performance nmf frontal face verificationn',\n",
       " 'A.N. Venetsanopoulos K.N. Plataniotis Juwei Lu lowdimensional feature representation enhance discriminatory power paramount importance face recognition fr system traditional linear discriminant analysis ldabased method suffer disadvantage optimality criterion directly relate classification ability obtain feature representation classification accuracy affect small sample size s problem encounter fr task paper propose new algorithm deal shortcoming efficient cost effective manner propose method compare term classification accuracy commonly fr method face database result indicate performance propose method overall superior traditional fr approach eigenfaces fisherfaces dlda methodn',\n",
       " 'Hock Lye Toh Juwei Lu Shiqian Wu Meng Joo Er general efficient design approach radial basis function rbf neural classifier cope small train set high dimension problem frequently encounter face recognition present order avoid overfitting reduce computational burden face feature extract principal component analysis pca method result feature process fisheru0027s linear discriminant fld technique acquire lowerdimensional discriminant pattern novel paradigm propose data information encapsulate determine structure initial parameter rbf neural classifier learn take place hybrid learn algorithm train rbf neural network dimension search space drastically reduce gradient paradigm simulation result conduct orl database achieve excellent performance term error rate classification learn efficiencyn',\n",
       " 'YenTing Chen YiHung Liu paper present new classifier call total marginbased adaptive fuzzy support vector machine tafsvm deal problem occur support vector machine svms apply face recognition propose tafsvm solve overfitting problem result outlier approach fuzzification penalty correct skew optimal separate hyperplane imbalanced data set different cost algorithm addition introduce total margin algorithm replace conventional soft margin algorithm low generalization error bind obtain function embody traditional svm tafsvm propose reformulate linear nonlinear case database chung yuan christian university cycu multiview facial recognition technology feret face database kernel fisheru0027s discriminant analysis kfda algorithm extract discriminate face feature experimental result propose tafsvm superior svm term facerecognition accuracy result indicate propose tafsvm achieve small error variance svm number test good recognition stability obtainn',\n",
       " 'A.M. Martinez Zhu Manli success linear discriminant analysis lda simplicity formulation reduce simultaneous diagonalization symmetric matrix b fundamental drawback approach efficiently apply matrix singular small variance noise paper present factorization a1b correlationbased criterion readily employ solve problem provide detail derivation linear nonlinear classification problem usefulness propose approach demonstrate thoroughly large variety databasesn',\n",
       " 'He Helen Huang Haibo He Yuan Cao paper novel learn methodology face recognition learn test data lift framework propose consider face recognition problem feature inadequate train example availability vast test example aim explore useful information test data facilitate learn oneagainstall technique integrate learn recover label test data expand train population recover data paper neural network support vector machine base learn model furthermore integrate transductive method consistency method lrga method lift framework experimental result hypothesis test popular face benchmark illustrate effectiveness propose frameworkn',\n",
       " 'R. Kothari R. Lotlikar linear projection dimensionality reduction compute linear discriminant analysis lda commonly base optimization certain separability criterion output space result optimization problem linear separability criterion directly relate classification accuracy output space consequently trial error procedure invoke experiment different separability criterion differ weight function select perform best train set best weight function trial choice result poor classification data subspace short paper introduce concept fractional dimensionality develop incremental procedure call fractionalstep lda flda reduce dimensionality fractional step flda algorithm robust selection weight function give weight function find subspace classification accuracy high obtain ldan',\n",
       " 'T. Windeatt difficulty tune parameter multilayer perceptrons mlp classifier know paper measure describe capable predict number classifier train epoch achieve optimal performance ensemble mlp classifier measure compute pair pattern train data base spectral representation boolean function representation characterize map classifier decision target label allow accuracy diversity incorporate single measure result benchmark problem include olivetti research laboratory orl face database demonstrate measure correlate baseclassifier test error predict optimal number train epoch correlation ensemble test error strong show paper measure predict number epoch optimal ensemble performance technique applicable twoclass problem extend multiclass output cod outputcod technique random code matrix show good performance oneperclass code base classifier welltunedn',\n",
       " 'Xiaoyang Tan Songcan Chen Jun Liu face representation fr play typically important role face recognition method principal component analysis pca linear discriminant analysis lda receive wide attention recently despite achieve success fr method inevitably lead poor classification performance case great facial variation expression light occlusion fact image gray value matrix manipulate sensitive facial variation paper notice fact image matrix wellknown singular value decomposition svd regard composition set base image generate svd point lead base image correspond large singular value hand sensitive aforementioned facial variation hand dominate composition face image base observation subtly deflate weight facial variation sensitive base image parameter xcexb1 propose novel fractional order singular value decomposition representation fsvdr alleviate facial variation face recognition finally experimental result fsvdr 1 effectively alleviate facial variation 2 form intermediate representation fr method pca lda significantly improve classification performance case great facial variationn',\n",
       " 'Huiting He Hua Huang lowresolution lr face image significantly decrease performance face recognition address problem present superresolution method u nonlinear map infer coherent feature favor high recognition near neighbor nn classifier recognition single lr face image canonical correlation analysis apply establish coherent subspace principal component analysis pca base feature highresolution hr lr face image nonlinear map hrlr feature build radial basis function rbfs low regression error coherent feature space pca feature space compute superresolved coherent feature correspond input lr image accord train rbf model efficiently accurately face identity obtain feed superresolved feature simple nn classifier extensive experiment facial recognition technology university manchester institute science technology olivetti research laboratory database propose method outperform stateoftheart face recognition algorithm single lr image term recognition rate robustness facial variation pose expressionn',\n",
       " 'A. Bouzerdoum S.L. Phung paper propose new neural architecture classification visual pattern motivate concept image pyramid local receptive field new architecture call pyramidal neural network pyranet hierarchical structure type process layer pyramidal layer onedimensional 1d layer new network nonlinear twodimensional 2d neuron train perform image feature extraction dimensionality reduction present analyze train method pyranet gradient descent gd gradient descent momentum resilient backpropagation rprop polakribiere conjugate gradient cg levenbergmarquadrt lm choice error function meansquareerror mse crossentropy ce paper apply pyranet determine gender facial image compare performance standard facial recognition technology feret database classifier convolutional neural network nn knearest neighbor knn support vector machine svmn',\n",
       " 'Xueming Wang Haojie Li Fuming Sun smart phone feature dramatical performance include fast processor gps chip advance camera people use photography photo store personal device grow need photo tool empower user organize manage personal photograph target goal devote effort develop photo management cellphone photo 4w offer index mobile photography relate key technology mainly include face annotation scene classification face annotation implement interactive method face detection help user annotate face provide batch solution user allow multiselect group face assign predict candidate list scene classification utilize semiautomated method classify photo base semantic content experiment conduct real family album demonstrate propose approach effective efficient mobile photo managementn',\n",
       " 'Ali Aghagolzadeh Masoumeh P. Ghaemmaghami Saeed Dabbaghchian discrete cosine transform dct powerful transform extract proper feature face recognition apply dct entire face image coefficient select construct feature vector conventional approach select coefficient zigzag manner zonal mask case lowfrequency coefficient discard order compensate illumination variation discrimination power coefficient discriminant achieve high true recognition rate discriminant coefficient dc feature vector discrimination power analysis dpa statistical analysis base dct coefficient property discrimination concept search coefficient power discriminate different class good propose approach conventional approach datadependent able dc database simulation result coefficient selection c approach orl yale database confirm success propose approach dpabase approach achieve performance pcalda good complexity propose method implement feature selection problem dct coefficient new modification pca lda propose dpapca dpalda modification dc select dpa input transform simulation result dpapca dpalda orl yale database verify improvement result new modificationn',\n",
       " 'Jin Young Choi Myoung Soo Park paper present theoretical analysis novel supervise feature extraction method call classaugmented principal component analysis capca compose process encode class information augment encode information data extract feature classaugmented data apply pca combination process capca extract feature appropriate classification theoretical analysis aim clarify role process provide explanation capca extract good feature experimental result datasets provide order validity propose method real problem effect parameter quality extract feature investigate rule thumb determine appropriate parameter providen',\n",
       " 'Chengjun Liu Zhiming Liu paper present novel face recognition method mean fuse color local spatial global frequency information specifically propose method fuse multiple feature derive hybrid color space gabor image representation local binary pattern lbp discrete cosine transform dct input image novelty paper threefold hybrid color space rcrq color space construct combine r component image rgb color space chromatic component image cr q ycbcr yiq color space respectively rcrq hybrid color space component image posse complementary characteristic enhance discriminate power face recognition second effective image encode method propose component image rcrq hybrid color space extract feature patchbased gabor image representation r component image ii multiresolution lbp feature fusion scheme cr component image iii componentbased dct multiple face encode q component image finally decision level similarity matrix generate component image rcrq hybrid color space fuse weight sum rule experiment face recognition grand challenge frgc version 2 experiment 4 propose method improve face recognition performance significantly particular propose method achieve face verification rate roc iii curve 9243 false accept rate 01 compare frgc baseline performance 1186 face verification rate false accept raten',\n",
       " 'Washington Mio Xiuwen Liu Keith Haynes paper propose method achieve object classification high throughput accuracy rapid classification tree achieve decouple train test stage train stage learn optimal discriminatory feature train set train classifier high accuracy create classification tree node u lookup table store solution result high throughput test stage lookup table feasible application learn projection matrix stochastic optimization illustrate effectiveness propose method datasets result propose method achieve order magnitude improvement throughput maintain similar accuracyn',\n",
       " 'E.C. Tsang D.S. Yeung Defeng Wang support vector machine svm demonstrate effective classifier application performance limit data distribution information underutilized determine decision hyperplane exist kernel employ nonlinear svms measure similarity pair pattern image base euclidean inner product euclidean distance correspond input pattern ignore data distribution tendency make svm essentially ldquolocalrdquo classifier paper provide step paradigm kernel incorporate data specific knowledge exist kernel data structure class adaptively input space agglomerative hierarchical cluster ahc construct weight mahalanobis distance wmd kernel detect data distribution information wmd kernel similarity pattern image determine mahalanobis distance md correspond input pattern size cluster reside wmd kernel guarantee positive definite pd conditionally positive definite cpd satisfactory classification result achieve regularizers svms wmd kernel empirically positive pseudoeuclidean pe space experimental result synthetic realworld data set effectiveness ldquopluggingrdquo data structure exist kerneln',\n",
       " 'M. Paindavoine Fan Yang paper describe real time vision allow localize face video sequence verify identity process image process technique base radial basis function rbf neural network approach robustness evaluate quantitatively video sequence adapt model application face recognition olivetti research laboratory orl cambridge uk database compare performance system hardware implementation model embed system base field programmable gate array fpga zero instruction set zisc chip digital signal processor dsp tms320c62 respectively analyze algorithm complexity present result hardware implementation term resource process speed success rate face track identity verification 92 fpga 85 zisc 982 dsp respectively embed system process speed image size 288 spl time 352 14 images 25 images 48 images respectivelyn',\n",
       " 'Meijun Sun Yahong Han Jesse S. Jin Zheng Wang Wei Jiang abstract different western paint chinese inkwash paint iwps distinctive art style furthermore chinese iwps divide class gongbi traditional chinese realistic paint xieyi freehand style extraction chinese iwp feature good classification result challenge similar content paper present novel framework combine discrete cosine transformation dct convolutional neural network cnns framework cnn automatically extract chinese iwp feature small subset dct coefficient image instead raw pixel commonly good performance evaluate propose framework dataset include 1400 chinese iwps experimental result propose framework achieve competitive classification performance compare exist benchmark methodsn',\n",
       " 'Janez Demsar method compare learn algorithm single data set scrutinize time issue statistical test comparison algorithm multiple data set essential typical machine learn study ignore article review current practice theoretically empirically examine suitable test base recommend set simple safe robust nonparametric test statistical comparison classifier wilcoxon sign rank test comparison classifier friedman test correspond posthoc test comparison classifier multiple data set result neatly present newly introduce cd critical difference diagramsn',\n",
       " 'Witold Pedrycz Roberto Tagliaferri Antonino Staiano fuzzy cmeans base cluster technique develop tackle problem number area pattern recognition image analysis communication data mine common use class cluster algorithm train radial basis function neural network rbfnns paper novel approach fuzzy cluster organize data cluster basis input data u0027prototypeu0027 regression function build output space summation number linear local regression model methodology show effective train rbfnns lead improve performance respect cluster algorithmn',\n",
       " 'S. Suresh G. S. Babu paper present sequential projectionbased metacognitive learn algorithm radial basis function network pblmcrbfn classification problem algorithm inspire human metacognitive learn principle component cognitive component metacognitive component cognitive component singlehidelayer radial basis function network evolve architecture metacognitive component control learn process cognitive component choose best learn strategy current sample adapt learn strategy implement selfregulation addition sample overlap condition past knowledge sample form pseudosample proper initialization new hide neuron minimize misclassification parameter update strategy u projectionbased direct minimization hinge loss error interaction cognitive component metacognitive component address whattolearn whentolearn howtolearn human learn principle efficiently performance pblmcrbfn evaluate set benchmark classification problem university california irvine machine learn repository statistical performance evaluation problem prof superior performance pblmcrbfn classifier result report literature evaluate performance propose algorithm practical alzheimeru0027s disease detection problem performance result open access series image study alzheimeru0027s disease neuroimage initiative datasets obtain different demographic region clearly pblmcrbfn handle problem change distributionn',\n",
       " 'Yang Wang Mohamed Kamel Yanmin Sun classification data imbalance class distribution pose significant drawback performance attainable standard classifier learn algorithm assume relatively balance class distribution equal misclassification cost learn difficulty attract lot research interest effort concentrate biclass problem biclass scenario class imbalance problem prevail report solution biclass application applicable multiclass problem paper develop costensitive boost algorithm improve classification performance imbalance data involve multiple class barrier apply costensitive boost algorithm imbalance data cost matrix unavailable problem domain solve problem apply genetic algorithm search optimum cost setup class empirical test propose costensitive boost algorithm improve classification performance imbalance data set significantlyn',\n",
       " 'Gunther Palm Hans A. Kestler Friedhelm Schwenker abstract paper learn algorithm radial basis function rbf network discuss multilayer perceptrons mlp typically train backpropagation algorithm start train procedure random initialization mlpu0027s parameter rbf network train different way categorize rbf train method threephase learn scheme twophase rbf learn common learn scheme layer rbf network learn separately rbf layer train include adaptation center scale parameter weight output layer adapt rbf center train cluster vector quantization classification tree algorithm output layer supervise learn gradient descent pseudo inverse solution result numerical experiment rbf classifier train twophase learn present completely different pattern recognition application classification 3d visual object b recognition handwritten digit 2d object c categorization highresolution electrocardiogram give time series 1d object set feature extract time series application observe performance rbf classifier train twophase learn improve backpropagationlike train phase rbf network adapt set parameter rbf center scale parameter output layer weight simultaneously threephase learn rbf network practical advantage threephase learn rbf network possibility use unlabeled train data train phase support vector sv learn rbf network different learn approach sv learn consider context learn special type onephase learn output layer weight rbf network calculate rbf center restrict subset train data numerical experiment classifier scheme include knearestneighbor learn vector quantization rbf classifier train twophase threephase support vector learn give performance rbf classifier train sv learn threephase learn superior result twophase learn sv learn lead complex network structure number support vector small fraction total number data pointsn',\n",
       " 'B. Wilamowski P. Rozycki J. Hewlett Hao Yu Tiantian Xie paper propose improve second order iso algorithm train radial basis function rbf network traditional parameter include center width output weight input weight connection input layer hide layer adjust train process accurate result obtain increase variable dimension initial center choose train pattern parameter generate randomly limit range take advantage fast convergence powerful search ability second order algorithm propose iso algorithm normally reach small traintesting error number rbf unit computation process quasi hessian matrix gradient vector accumulate sum relate sub matrix vector respectively jacobian row store multiplication instead entire jacobian matrix storage multiplication memory reduction benefit computation speed allow train problem basically unlimit number pattern practical discrete continuous classification problem apply test property propose iso train algorithmn',\n",
       " 'SuChong Joo Witold Pedrycz WookDong Kim SungKwun Oh paper introduce advance architecture kmeans clusterbased polynomial radial basis function neural network prbf nns design aid particle swarm optimization pso differential evolution develop comprehensive design methodology support construction architecture prbf nns come result synergistic usage evolutionary optimizationdriven hybrid tool connection weight propose prbf nns certain functional character realize consider type polynomial order design optimize prbf nns prototype center value receptive field determine run kmeans cluster algorithm prototype spread correspond receptive field optimize run particle swarm optimization pso differential evolution weight square estimation wlse estimate coefficient polynomial serve functional connection network performance propose model comparative analysis involve model design aid pso present case nonlinear function machine learn ml datasetsn',\n",
       " 'Jianhui Xi Min Han paper study train new feedforward neural network radial basis perceptron rbp neural network distinguish different set rl rbp neural network base radial basis function rbf neural network perceptron neural network hide layer node fully connect use selective connection train algorithm correspond structure rbp network present adopt inputoutput cluster ioc method provide efficient powerful procedure construct rbp network generalize learn procedure rbp neural network adopt ioc method define number unit hide layer select center second width parameter center selfadjustable accord information include learn sample effectiveness network illustrate example take application component analysis civil build material simulation show rbp neural network predict component civil build material successfully get good generalization abilityn',\n",
       " 'M.M. RandolphGips N.B. Karayiannis present systematic approach construct reformulate radial basis function rbf neural network develop facilitate train supervise learn algorithm base gradient descent approach reduce construction radial basis function model selection admissible generator function selection generator function rely concept blind spot introduce paper paper introduce new family reformulate radial basis function neural network refer cosine radial basis function cosine radial basis function construct linear generator function special form use similarity measure radial basis function model justify geometric interpretation set experiment variety datasets indicate cosine radial basis function outperform considerably conventional radial basis function neural network gaussian radial basis function cosine radial basis function strong competitor exist reformulate radial basis function model train gradient descent feedforward neural network sigmoid hide unitsn',\n",
       " 'Majid Ahmadi Karim Faez Javad Haddadnia paper present fuzzy hybrid learn algorithm fhla radial basis function neural network rbfnn method determine number hide neuron rbfnn structure cluster validity index majority rule characteristic hide neuron initialize base advance fuzzy cluster fhla combine gradient method linear leastsquared method adjust rbf parameter neural network connection weight rbfnn propose fhla classifier face recognition input rbfnn feature vector obtain combine shape information principal component analysis design rbfnn propose fhla provide fast convergence train phase require hide layer few neuron sensitivity train test pattern efficiency propose method demonstrate orl yale face database comparison algorithm indicate fhla yield excellent recognition rate human face recognitionn',\n",
       " 'H.N. Koivo Z. Uykan paper present analyze new structure design radial basis function neural network rbfnn train phase input layer rbfnn augment desire output vector generalization phase involve follow step 1 identify cluster previously unseen input vector belong 2 augment input layer average target input vector identify cluster 3 use augment network estimate unknown target show reasonable assumption generalization error function admit upper bind term quantization error minimize determine center propose method train set difference train sample generalization sample deterministic set difference train generalization sample go zero upper bind arbitrarily small increase number hide neuron simulation verify effectiveness propose methodn',\n",
       " 'KeKun Huang ChuanXian Ren DaoQing Dai ZhaoRong Lai propose novel linear learn base peak price track ppt strategy portfolio selection p recently topic track control attract intensive attention novel model propose base backstepping method output track desire trajectory propose similar evolution transform function aggressively track increase power different asset result good perform asset receive investment propose ppt objective formulate fast backpropagation algorithm suitable largescale timelimited application highfrequency trade extensive experiment benchmark data set diverse real financial market ppt outperform stateoftheart system computational time cumulative wealth riskadjusted metric suggest ppt effective robust defensive system pn',\n",
       " 'Qi Tian Changsheng Li Jian Cheng Chunjie Zhang image classification method try learn classifier class train image interclass intraclass variation effective test image consideration classifier learn brief propose novel imagepecific classification method comb local global discrimination train image adaptively train classifier test image instead generate classifier class train image test image select k near neighbor train set correspond label local classifier train help model distinctive character test image use train image global discrimination model local global discrimination combine final classification way model specific character test image avoid local optimum jointly consider train image evaluate usefulness propose imagepecific classification local global discrimination isclg method conduct image classification experiment public image data set superior performance baseline method prove effectiveness propose isclg methodn',\n",
       " 'KeKun Huang ChuanXian Ren DaoQing Dai ZhaoRong Lai propose set novel radial basis function adaptive input composite trend representation aictr portfolio selection p trend representation asset price main information exploit p stateoftheart trend representationbased system exploit kind trend information lack effective mechanism construct composite trend representation propose exploit set rbfs multiple trend representation improve effectiveness robustness price prediction input rbfs automatically switch best trend representation accord recent invest performance different price prediction propose novel objective combine rbfs select portfolio extensive experiment benchmark data set include new challenge data set propose different realworld stock market indicate propose rbfs effectively combine different trend representation aictr achieve stateoftheart invest performance risk control aictr withstand reasonable transaction cost run fast applicable realworld financial environmentsn',\n",
       " 'S Selvakumar N G Bhuvaneswari Amma abstract nowadays denial service do attack security threat online service unavailable legitimate user do attack detection need protect online service malicious activity machine learn approach widely detect cyberattack lacuna exist machine learn base attack detection system learn time vanish gradient get stick local minimum selection random weight paper issue address deep radial intelligence deerai cumulative incarnation cui approach propose detect do attack propose deerai approach learn intelligence extract radial basis function different level abstraction propose cui optimize weight deerai network knowledge gain progress generation experiment conduct benchmark datasets propose approach compare exist classifier stateoftheart attack detection system see performance evaluation propose approach give promise result exist approach evident propose approach converge fast provide best weight compare exist optimization methodsn',\n",
       " 'Thomas Huang Yihong Gong Kai Yu Jianchao Yang recently svms spatial pyramid match spm kernel highly successful image classification despite popularity nonlinear svms complexity on2 n3 train test n train size imply nontrivial scaleup algorithm handle thousand train image paper develop extension spm method generalize vector quantization sparse cod follow multiscale spatial max pool propose linear spm kernel base sift sparse code new approach remarkably reduce complexity svms train constant test number image categorization experiment term classification accuracy suggest linear spm base sparse cod sift descriptor significantly outperform linear spm kernel histogram good nonlinear spm kernel lead stateoftheart performance benchmark single type descriptorn',\n",
       " 'Andrew Y. Ng Rajat Raina Alexis Battle Honglak Lee sparse cod provide class algorithm find succinct representation stimulus give unlabeled input data discover basis function capture higherlevel feature data find sparse code remain difficult computational problem paper present efficient sparse cod algorithm base iteratively solve convex optimization problem l1regularized square problem l2constrained square problem propose novel algorithm solve optimization problem algorithm result significant speedup sparse cod allow learn large sparse code possible previously describe algorithm apply algorithm natural image demonstrate infer sparse code exhibit endstopping nonclassical receptive field surround suppression provide partial explanation phenomenon v1 neuronsn',\n",
       " 'Olvi L. Mangasarian support vector machine utilize 1norm typically set linear program mangasarian 2000 bradley mangasarian 1998 formulate completely unconstrained minimization convex differentiable piecewisequadratic objective function dual space objective function lipschitz continuous gradient contain additional finite parameter minimize generalize newton method lead exact solution support vector machine problem approach base formulation general linear program unconstrained minimization problem application support vector machine classification problem present approach generalize mangasarian 2004 fung mangasarian 2004 apply nonlinear approximation minimal number nonlinear kernel function utilize approximate function give number function valuesn',\n",
       " 'Yi Ma Arvind Ganesh S. Shankar Sastry Allen Y. Yang provide comprehensive review representative l 1 minimization method gradient projection homotopy iterative shrinkagethresholding proximal gradient augment lagrange multiplier repository intend gap exist literature systematically benchmark performance algorithm consistent experimental set experiment focus application face recognition sparse representation framework recently develop recover human identity facial image affect illumination change occlusion facial disguise paper provide useful guideline practitioner work similar fieldsn',\n",
       " 'GuiFu Lu Ying Tai Jianjun Qian Jian Yang Lei Luo deal partial occlusion illumination challenge problem image representation classification problem characterization representation error play crucial role current approach error matrix need stretch vector element assume independently corrupt ignore dependence element error paper assume error image cause partial occlusion illumination change random matrix variate follow extend matrix variate power exponential distribution heavy tail region matrix pattern ltimes m dimensional observation independent paper reveal essence propose distribution actually alleviate correlation pixel error matrix e make e approximately gaussian basis distribution derive schatten p normbased matrix regression model lq regularization alternate direction method multiplier apply solve model closedform solution step algorithm singular value function thresholding operator introduce addition extend schatten p norm utilize characterize distance test sample class design classifier extensive experimental result image reconstruction classification structural noise demonstrate propose algorithm work robustly exist regressionbased methodsn',\n",
       " 'Dansong Cheng Junbin Gao Daming Shi Yongqiang Zhang learn robust regression model highdimensional corrupt data essential difficult problem practical application stateoftheart method study lowrank regression model robust typical noise like gaussian noise outsample sparse noise outlier regression model learn clean data lie underlie subspace exist lowrank regression method handle outliernoise lie sparsely corrupt disjoint subspace address issue propose lowranksparse subspace representation robust regression refer lrsrr paper main contribution include follow 1 unlike exist regression method propose approach phase lowranksparse subspace recovery regression optimization carry simultaneously2 apply linearize alternate direction method adaptive penalty solve formulate lrsrr problem prove convergence algorithm analyze complexity 3 demonstrate efficiency method highdimensional corrupt data synthetic data benchmark datasets stateoftheart robust methodn',\n",
       " 'ChiSing Leung ZiFa Han Ruibin Feng Hao Wang train stage radial basis function rbf network need select suitable rbf center exist center selection algorithm design faultfree situation brief develop fault tolerant algorithm train rbf network select rbf center simultaneously select input vector train set rbf center afterward define correspond fault tolerant objective function add ell 1 norm term objective function ell 1 norm term able force unimportant weight zero center selection achieve train stage ell 1 norm term nondifferentiable formulate original problem constrain optimization problem base alternate direction method multiplier framework develop algorithm solve constrain optimization problem convergence proof propose algorithm provide simulation result propose algorithm superior exist center selection algorithmn',\n",
       " 'Z. Zhang propose flexible technique easily calibrate camera require camera observe planar pattern show different orientation camera planar pattern freely move motion need know radial lens distortion model propose procedure consist closedform solution follow nonlinear refinement base maximum likelihood criterion simulation real data test propose technique good result obtain compare classical technique use expensive equipment orthogonal plane propose technique easy use flexible advance 3d vision step laboratory environment real world usen',\n",
       " 'P. Meer V. Ramesh D. Comaniciu new approach target representation localization central component visual track nonrigid object propose feature histogrambased target representation regularize spatial mask isotropic kernel mask induce spatiallysmooth similarity function suitable gradientbased optimization target localization problem formulate basin attraction local maximum employ metric derive bhattacharyya coefficient similarity measure use mean shift procedure perform optimization present track example new method successfully cop camera motion partial occlusion clutter target scale variation integration motion filter data association technique discuss potential application exploitation background information kalman track motion model face trackn',\n",
       " 'Nathan Jacobs Radu Paul Mihail Scott Workman rainbow natural cue calibrate outdoor imagery ephemeral provide unique calibration cue center exactly opposite sun out radius 42 degree work define geometry rainbow minimal set constraint sufficient estimate camera calibration present semiautomatic fully automatic method calibrate camera image rainbow demonstrate method collect large database rainbow image use evaluate calibration accuracy create empirical model rainbow appearance model edit rainbow appearance natural image rainbow geometry conjunction horizon line capture time provide estimate camera location focus rainbow geometric property algorithm present apply solarrefractive phenomenon parhelion call sun dog 22 degree solar halon',\n",
       " 'M. Shah Xiaochun Cao paper camera parameter light source orientation recover perspective view scene give vertical line cast shadow compare traditional calibration method involve image precisely machine calibration pattern method u new calibration object vertical object parallel shadow line common natural environment addition benefit increase accessibility calibration object propose method especially useful case limit information available demonstrate accuracy application propose algorithm present result synthetic real imagen',\n",
       " 'M. Werman Y. Caspi paper present method capture compute 3d parallax 3d parallax refer vertical offset grind plane height method base analyze shadow vertical pole tall buildingxc2x92s contour sweep object unlike exist beamscanning approach shadow structure light recover distance point camera approach measure height grind plane directly previous method compute distance camera triangulation ray outgo lightsource camera triangulation difficult object far camera require accurate knowledge light source position contrast approach intersect unknown plane generate separately cast object omit need precompute location light source furthermore allow move light source propose setup particularly useful camera directly face scene object far away camera good example urban scene capture single webcamn',\n",
       " 'R. Pless R. Speyer N. Roman S. Satkin N. Jacobs key problem widely distribute camera network locate camera paper consider scenario camera localization localize camera unknow environment add new camera region camera localize camera find correlation satellite imagery simple summary statistic time course principal component coefficient sufficient geolocate camera determine correspondence camera explicitly reason weather scene present result database image 538 camera collect course year camera remain stationary accurate image time tamp localize camera 50 mile know location addition demonstrate use distribute camera network construction map weather conditionsn',\n",
       " 'Karianto Leman Teck Wee Chua Li Shen local structure shadow boundary complex interaction image region remain largely unexploited previous shadow detection approach paper present novel learnbased framework shadow region recovery single image exploit local structure shadow edge structure cnn learn framework structure label information classification improve local consistency pixel label avoid spurious label propose formulate shadowbright measure model complex interaction image region shadow bright measure patch compute shadow edge detect propose cnn global interaction constraint patch formulate leastsquare optimization problem shadow recovery solve efficiently shadow recovery method achieve stateoftheart result major shadow benchmark database collect conditionsn',\n",
       " 'Xiaochun Cao Lin Wu position world pointu0027s solar shadow depend geographical location geometrical relationship orientation sunshine grind plane shadow cast paper investigate property solar shadow trajectory planar surface show camera parameter latitude longitude estimate observe shadow trajectory contribution use design analemmatic sundial shadow conic furthermore recover camerau0027s geographical location propose method require shadow cast object vertical object visible recovery camera calibration approach thoroughly validate synthetic real data test source error include noise number observationsn',\n",
       " 'Nathan Jacobs Scott Workman Menghua Zhai address problem singleimage geocalibration estimate geographic location view direction field view seek camera capture image dominant approach problem match feature query image color texture reference database nearby grind imagery fail imagery available propose overcome limitation match geographic database contain location know object house road body water unable onetoone correspondence image location object database model problem probabilistically base geometric configuration multiple weak correspondence propose markov chain monte carlo mcmc sample approach approximate underlie probability distribution geocalibration cameran',\n",
       " 'Dimitris Samaras Minh Hoai ChenPing Yu Le Hou Tomas F. Yago Vicente paper introduce train shadow detector largescale dataset paradigm previously impossible high cost precise shadow annotation instead advocate use quickly imperfectly label image novel label recovery method automatically correct portion erroneous annotation train classifier perform stateoftheart level apply method improve accuracy label new dataset 20 time large exist datasets contain large variety scene image type naturally large dataset appropriate train deep learn method propose semanticaware patch level convolutional neural network architecture efficiently train patch level shadow example incorporate image level semantic information mean detect shadow patch refine base image semantics propose pipeline useful baseline future advance shadow detectionn',\n",
       " 'Srinivasa G. Narasimhan Alexei A. Efros JeanFrancois Lalonde give single outdoor image present method estimate likely illumination condition scene particular compute probability distribution sun position visibility method rely combination weak cue extract different portion image sky vertical surface grind single cue reliably estimate illumination reinforce yield robust estimate combine datadriven prior compute dataset 6 million internet photo present quantitative result webcam dataset annotate sun position qualitative result consumergrade photograph download internet base estimate illumination realistically insert synthetic 3d object scenen',\n",
       " 'Dimitris Samaras Minh Hoai Maozheng Zhao Tomas F. Yago Vicente Vu Nguyen introduce scgin novel extension conditional generative adversarial network gin tailor challenge problem shadow detection image previous method shadow detection focus learn local appearance shadow region limit local context reason form pairwise potential conditional random field contrast propose adversarial approach able model high level relationship global scene characteristic train shadow detector correspond generator conditional gin augment shadow accuracy combine typical gin loss data loss term unbalance distribution shadow label use weight cross entropy standard gin architecture properly set weight cross entropy require train multiple gins computationally expensive grid procedure scgin introduce additional sensitivity parameter w generator propose approach effectively parameterizes loss train detector result shadow detector single network generate shadow map correspond different sensitivity level obviate need multiple model costly train procedure evaluate method largescale sbu ucf shadow datasets observe 17 error reduction respect previous stateoftheart methodn',\n",
       " 'PhengAnn Heng Jing Qin Xuemiao Xu ChiWing Fu Xiaowei Hu Zijun Deng Lei Zhu paper present network detect shadow explore combine global context deep layer local context shallow layer deep convolutional neural network cnn technical contribution network design formulate recurrent attention residual rar module combine context adjacent cnn layer learn attention map select residual refine context feature second develop bidirectional feature pyramid network bfpn aggregate shadow context span different cnn layer deploy series rar module network iteratively combine refine context feature series refine context feature deep shallow layer series shallow deep layer good suppress false detection enhance shadow detail time evaluate network common shadow detection benchmark datasets sbu ucf experimental result network outperform best exist method 3488 reduction sbu 3457 reduction ucf balance error raten',\n",
       " 'Jian Yang Xiang Li Jifeng Wang understand shadow single image consist type task previous study contain shadow detection shadow removal paper present multitask perspective embrace exist work jointly learn detection removal endtoend fashion aim enjoy mutually improve benefit framework base novel stack conditional generative adversarial network stcgan compose stack cgans generator discriminator specifically shadow image feed generator produce shadow detection mask shadow image concatenate predict mask go second generator order recover shadowfree image consequently addition correspond discriminator likely model high level relationship global scene characteristic detect shadow region reconstruction remove shadow respectively importantly multitask learn design stack paradigm provide novel view notably different commonly multibranch version fully evaluate performance propose framework construct largescale benchmark 1870 image triplet shadow image shadow mask image shadowfree image 135 scene extensive experimental result consistently advantage stcgan representative stateoftheart method largescale publicly available datasets newly release onen',\n",
       " 'PhengAnn Heng Jing Qin ChiWing Fu Lei Zhu Xiaowei Hu shadow detection fundamental challenge task require understand global image semantics background shadow paper present novel network shadow detection analyze image context directionaware manner achieve formulate directionaware attention mechanism spatial recurrent neural network rnn introduce attention weight aggregate spatial context feature rnn learn weight train recover directionaware spatial context dsc detect shadow design develop dsc module embed cnn learn dsc feature different level weight cross entropy loss design train effective employ common shadow detection benchmark datasets perform experiment evaluate network experimental result network outperform stateoftheart method achieve 97 accuracy 38 reduction balance error raten',\n",
       " 'Zhanyi Hu Xiaoqiao Meng inspire zhangxe2x80x99s work new easy technique calibrate camera base circular point propose propose technique require camera observe newly design planar calibration pattern refer model plane hereinafter include circle pencil line pass circlexe2x80x99s center different unknown orientation intrinsic parameter determine linearly main point new technique need know metric measurement model plane correspondence point model plane image one fully automatically propose technique particularly useful people familiar vision experiment simulate data real image new technique robust accuraten',\n",
       " 'Tomas Pajdla Michal Jancosek Jan Smisek analyze kinect 3d measure device experimentally investigate depth measurement resolution error property quantitative comparison kinect accuracy stereo reconstruction slr camera 3dtof camera propose kinect geometrical model calibration procedure provide accurate calibration kinect 3d measurement kinect camera demonstrate functionality kinect calibration integrate sfm pipeline 3d measurement move kinect transform common coordinate compute relative pose match color cameran',\n",
       " 'B. Bhanu J. Han paper propose new spatiotemporal gait representation call gait energy image gei characterize human walk property individual recognition gait address problem lack train template propose novel approach human recognition combine statistical gait feature real synthetic template directly compute real template train silhouette sequence generate synthetic template train sequence simulate silhouette distortion use statistical approach learn effective feature real synthetic template compare propose geibased gait recognition approach gait recognition approach usf humanid database experimental result propose gei effective efficient gait representation individual recognition propose approach achieve highly competitive performance respect publish gait recognition approachn',\n",
       " 'K.W. Bowyer P. Grother I.R. Vega Z. Liu P.J. Phillips S. Sarkar identification people analysis gait pattern extract video recently popular research problem condition problem solvable understand characterize provide mean measure progress characterize property gait recognition introduce humanld gait challenge problem challenge problem consist baseline algorithm set 12 experiment large data set baseline algorithm estimate silhouette background subtraction perform recognition temporal correlation silhouette 12 experiment increase difficulty measure baseline algorithm examine effect covariates performance covariates change view angle change shoe type change walk surface carry carry briefcase elapse time sequence compare identification rate 12 experiment range 78 percent easy experiment 3 percent hard covariates statistically significant effect performance walk surface time difference have great impact data set consist 1870 sequence 122 subject span covariates 12 gigabyte data infrastructure support development gait recognition algorithm additional experiment understand strength weakness new algorithm experimental result present detail possible metaanalysis great understand potential adoption challenge problem represent radical departure traditional vision research methodologyn',\n",
       " 'Larry S. Davis Ross Cutler Chiraz BenAbdelkader present parametric method automatically identify people monocular lowresolution video estimate height stride parameter walk gait stride parameter stride length cadence function body height weight gender previous work demonstrate effective use biometrics identification verification people paper performance significantly improve height additional discriminant feature height estimate robustly segment person background fit apparent height timedependent model method correspondencefree work lowresolution image people viewinvariant albeit performance optimal near frontoparallel configuration identification accuracy estimate 47 frontoparallel sequence 41 people 65 nonfrontoparallel sequence 17 people compare 18 51 respectively stride cadence usedn',\n",
       " 'A.Y. Johnson A.F. Bobick gaitrecognition technique recover static body stride parameter subject walk present approach example activityspecific biometric method extract identify property individual individualu0027s behavior applicable person perform specific action evaluate parameter derive expect confusion metric relate mutual information oppose report percent correct limit database metric predict give feature vector filter identity large population test utility variety body stride parameter recover different view condition database consist 15 20 subject walk angle frontalparallel view respect camera indoors analyze motioncapture data subject discover confusion parameter inherently physical visual measurement error propertyn',\n",
       " 'Robert Bergevin Alexandra Branzan Albu Frederic Jean paper propose approach compute viewnormalized body trajectory pedestrian walk potentially nonlinear path propose approach find application gait model gait biometrics medical gait analysis approach u 2d trajectory foot head extract track silhouette basis compute apparent walk sagittal plane detect gait halfcycle homography transformation compute walk plane appear walk observe frontoparallel view finally homography apply head foot trajectory correspond gait halfcycle view normalization make head foot trajectory appear see frontoparallel viewpoint assume optimal gait model purpose propose approach fully automatic require manual initialization camera calibration extensive experimental evaluation propose approach confirm validity normalization processn',\n",
       " 'R.I. Damper A. PrugelBennett Ziheng Zhou extract fullbody motion walk people monocular video sequence complex realworld environment important difficult problem go simple track satisfactory solution demand appropriate balance use prior knowledge learn data propose consistent bayesian framework introduce strong prior knowledge extract human gait work strong prior build simple articulate model have timeinvariant static timevariant dynamic parameter model easily modify cater situation walker wear clothe obscure limb statistic parameter learn highquality indoor laboratory data bayesian framework allow bootstrap accurate gait extraction noisy image typical clutter outdoor scene achieve automatic fit use hide markov model detect phase image walk cycle demonstrate approach silhouette extract frontoparallel sideways sequence walker highquality indoor noisy outdoor condition highquality data synthetic noise occlusion add test walker rucksack skirt trench coat result quantify term chamfer distance average pixel error automatically extract body point correspond handlabeled point novel overall framework make feasible extract gait poor quality image sequence hitherto confirm compare person identification gait method wellestablished baseline recognition algorithmn',\n",
       " 'S. R. Neves C. L. Pagliari E. A. B. da Silva A. Ellmauthaler work propose fusion framework base undecimated wavelet transform spectral factorization include information presence target infrared ir image fusion process purpose novel ir segmentation algorithm extract target lowcontrast environment suppress introduction spurious segmentation result introduce ensure relevant information ir image include fuse image lead accurate representation capture scene propose use novel hybrid fusion scheme combine pixeland regionlevel information guide fusion process turn fusion process robust possible segmentation error represent common source problem regionlevel image fusion combination technique lead novel fusion framework able improve fusion result pure pixellevel counterpart target extraction additionally traditional pixellevel fusion approach base stateoftheart transform nonsubsampled contourlet transform dualtree complexwavelet transform significantly outperform use propose set methodsn',\n",
       " 'E. A. B. da Silva C. L. Pagliari A. Ellmauthaler multiscale transform popular technique field pixellevel image fusion fusion performance method deteriorate image derive different sensor modality paper demonstrate image result improve novel undecimated wavelet transform uwtbased fusion scheme split image decomposition process successive filter operation spectral factorization analysis filter actual fusion take place convolution filter pair significantly small support size lead minimization unwanted spread coefficient value overlap image singularity usually complicate feature selection process lead introduction reconstruction error fuse image nonsubsampled nature uwt allow design nonorthogonal filter bank robust artifact introduce fusion additionally improve obtain result combination technique lead fusion framework provide clear advantage traditional multiscale fusion approach independent underlie fusion rule reduce unwanted effect ring artifact fuse reconstructionn',\n",
       " 'Chandra Kambhamettu Stephen Rhein Scott Sorensen Philip Saponaro calibration stereo camera important accurate 3d reconstruction standard color camera available tool algorithm accurate calibration detect corner chessboard pattern planar calibration board view thermal imagery chessboard pattern difficult detect uniform temperature white black square previous technique involve create custom calibration board multiple material abandon approach u print chessboard heat flood lamp result blurry hard detect corner visible short period time propose improvement approach reliable iterative preprocessing technique enhance contrast glaze finish ceramic tile back retain heat long present result calibration board retain heat reliably detect corner 10 minute method perform real calibration trialsn',\n",
       " 'Zhengyou Zhang Yuncai Liu Zijian Zhao planebase 2d camera calibration hot research topic recent year flexibility image point need view denote coplanar feature 2d camera calibration camera calibration calibration object point 1d camera calibration technique use setup collinear point know distance kind special condition calibration object setup general setupthree noncollinear point propose new camera calibration algorithm base calibration object noncollinear point experiment simulate data real image carry verify theoretical correctness numerical robustness result object noncollinear point special property camera calibration midway 1d 2d calibration object method actually new kind camera calibration algorithmn',\n",
       " 'Guangyou Xu HeungYeung Shum Sing Bing Kang Lei Wang selfcalibration pure rotation wellknown technique show reliable mean recover intrinsic camera parameter practice virtually impossible ensure camera motion type selfcalibration pure rotation paper present error analysis recover intrinsic camera parameter presence translation derive closedform error expression single pair image nondegenerate motion multiple rotation closedform solution analysis repeat experiment translationindependent solution exist certain practical conditionsn',\n",
       " 'J. Illingworth A.S. Aguado J.Y. Guillemaut majority camera calibration method include gold standard algorithm use pointbased information simultaneously estimate calibration parameter contrast propose novel calibration method exploit line orientation information decouple problem simple stage formulate problem minimization lateral displacement single project image line vanish point unlike previous vanish point method parallel line pair require additionally invariance property vanish point mean multiple image relate pure translation increase calibration data set size increase number estimate parameter compare method vanish point method gold standard algorithm demonstrate comparable performancen',\n",
       " 'Toshikazu Wada Haiyuan Wu Qian Chen paper novel camera calibration method estimate extrinsic parameter focal length camera single image coplanar circle arbitrary radiusn',\n",
       " 'Fuchao Wu Zhanyi Hu Haijiang Zhu Yihong Wu paper new camera calibration algorithm propose quasiaffine invariance parallel circle parallel circle mean circle plane parallel plane common lifen',\n",
       " 'T. Kanade S. Baker S. Narasimhan S. Yamazaki acquire 3d model intricate object like tree branch bicycle insect hard problem severe selfocclusion repeat structure surface discontinuity theory shapefromsilhouette sfs approach overcome difficulty use view reconstruct visual hull close actual shape practice sfs highly sensitive error silhouette contour calibration image suitable obtain reliable shape large number view present practical approach sfs novel technique call coplanar shadowgram image allow use dozen hundred view visual hull reconstruction point light source move object shadow silhouette cast single background plane observe characterize image term image projection reconstruction ambiguity epipolar geometry shape source recovery coplanarity shadowgrams yield novel geometric property possible traditional multiview camera base image system property allow derive robust automatic algorithm recover visual hull object 3d position light source simultaneously regardless complexity object demonstrate acquisition intricate shape severe occlusion structure 50 120 viewn',\n",
       " 'X. Maldague L. Rossi T. Toulouse M.A. Akhloufi paper propose new multimodal stereovision framework wildland fire detection analysis propose u near infrared visible image robustly segment fire extract threedimensional characteristic propagation u multiple multimodal stereovision system capture complementary view new registration approach propose u multisensory fusion base gnss imu data extract projection matrix permit representation 3d reconstruct common reference frame parameter extract 3d space propagation complete reconstruct obtain result efficiency propose wildland fire research firefighting decision support operational scenariosn',\n",
       " 'T. Pajdla B. Micusik paper present method fully automatic robust estimation twoview geometry autocalibration 3d metric reconstruction point correspondence image take camera wide circular field view focus camera 180deg field view standard perspective camera model sufficient camera equip circular fisheye lens nikon fce8 183deg sigma 8 mmf4ex 180deg curve conical mirror assume circular field view axially symmetric image projection autocalibrate camera wide field view camera model central projection follow nonlinear image map example abovementioned fisheye lens properly assemble catadioptric camera conical mirror epipolar geometry camera estimate small number correspondence solve polynomial eigenvalue problem allow use efficient ransac robust estimation image projection model epipolar geometry selection true point correspondence tentative correspondence contaminate mismatch real catadioptric camera slightly noncentral propose autocalibration approximate central model usually good correct point correspondence accurate noncentral model bundle adjustment obtain accurate 3d scene reconstruction noncentral camera model deal result show catadioptric camera parabolic spherical mirrorn',\n",
       " 'Konstantinos Daniilidis Christopher Geyer omnidirectional vision system provide panoramic alertness surveillance improve navigational capability produce panoramic image multimedia catadioptric realization omnidirectional vision combine reflective surface lens particular class central panoramic system preserve uniqueness projection viewpoint fact central projection include know perspective projection plane fall 62paper provide unify theory central catadioptric system isomorphic projective map sphere plane projection center perpendicular plane subcases stereographic projection equivalent parabolic projection central planar projection equivalent conventional camera define duality projection point line different 136unification novel significant impact 3d interpretation image present new invariance inherent parabolic projection unify calibration scheme view imply advantage catadioptric system explain image arise central catadioptric system contain information image conventional camera example intrinsic calibration single view possible parabolic catadioptric system give line example metric rectification affine information scenen',\n",
       " 'A.W. Fitzgibbon problem uncalibrate stereo reconstruction camera deviate pinhole model precalibrate order correct nonlinear lens distortion point correspondence attempt uncorrected image match constraint provide fundamental matrix set loose point match significantly hamper paper show linear estimation fundamental matrix twoview point correspondence augment include term radial lens distortion achieve 1 change standard radiallens model equivalent power take simple form homogeneous coordinate 2 express fundamental matrix estimation quadratic eigenvalue problem qep efficient algorithm know derive new estimator compare performance bundleadjusted calibrationgrid data new estimator fast include ransacbased match loop case match render possible use lens calibrate natural scene lack straight line preclude previous technique modification multiview relation planar homography trifocal tensor describedn',\n",
       " 'Srikumar Ramalingam Peter F. Sturm present theory algorithm generic calibration concept base follow recently introduce general image model image consider collection pixel pixel measure light travel half ray 3space associate pixel calibration determination common coordinate coordinate pixelxe2x80x99 ray model encompass projection model vision photogrammetry include perspective affine model optical distortion model stereo system catadioptric system xe2x80x93 central single viewpoint noncentral one propose concept calibrate general image model base view object know structure acquire unknow viewpoint allow principle calibrate camera type contain general image model algorithm develop theory algorithm general case noncentral camera observe 3d calibration object specialize case central camera use planar calibration object validity concept show experiment synthetic real datan',\n",
       " 'S.K. Nayar M.D. Grossberg linear perspective projection serve dominant image model vision recent development image sense perspective model highly restrictive paper present general image model represent arbitrary image observe image system perform map incoming scene ray photosensitive element image detector map conveniently describe set virtual sense element call raxels raxels include geometric radiometric optical property present novel calibration method u structure light pattern extract raxel parameter arbitrary image experimental result perspective ionperspective image system includedn',\n",
       " 'P. Meer D. Comaniciu general nonparametric technique propose analysis complex multimodal feature space delineate arbitrarily shape cluster basic computational module technique old pattern recognition procedure mean shift discrete data prove convergence recursive mean shift procedure near stationary point underlie density function utility detect mode density relation mean shift procedure nadarayawatson estimator kernel regression robust mestimators location establish algorithm lowlevel vision task discontinuitypreserving smooth image segmentation describe application algorithm userset parameter resolution analysis graylevel color image accept input extensive experimental result illustrate excellent performancen',\n",
       " 'Jean Ponce Yasutaka Furukawa article present novel method acquire highquality solid model complex 3d shape multiple calibrate photograph purely geometric constraint associate silhouette image construct coarse surface approximation form visual hull photoconsistency constraint enforce consecutive step 1 rim surface graz visual hull identify dynamic program 2 rim fix visual hull carve graph cut globally optimize photoconsistency surface recover main feature 3 iterative local refinement step finally recover fine surface detail propose approach implement experiment real data set present qualitative comparison stateoftheart imagebasedmodeling algorithmsn',\n",
       " 'A. Baumberg present robust method automatically match feature image correspond physical point object see arbitrary viewpoint unlike conventional stereo match approach assume prior knowledge relative camera position orientation fact application information wish determine image feature match feature detect image characterise affine texture invariant problem window effect explicitly address methodour feature characterisation invariant linear transformation image data include rotation stretch skew feature match process optimise structurefrommotion application wish ignore unreliable match expense reduce number feature matchn',\n",
       " 'Joos Vandewalle Bart De Moor Guido Dedene Jan Vanthienen Stijn Viaene Bart Baesens Johan A. K. Suykens Tony Van Gestel support vector machine svms solution classification problem characterize convex quadratic program qp problem modify version svms call square svm classifier lssvms square cost function propose obtain linear set equation dual space svm classifier large margin interpretation lssvm formulation relate paper ridge regression approach classification binary target fisheru0027s linear discriminant analysis feature space multiclass categorization problem represent set binary classifier different output cod scheme regularization control effective number parameter lssvm classifier sparseness property svms lose choice 2norm sparseness impose second stage gradually prune support value spectrum optimize hyperparameter sparse approximation procedure paper public domain benchmark datasets evaluate test set performance lssvm classifier linear polynomial radial basis function rbf kernel svm lssvm classifier rbf kernel combination standard crossvalidation procedure hyperparameter selection achieve comparable test set performance svm lssvm performance consistently good compare variety method describe literature include decision tree base algorithm statistical algorithm instance base learn method uci datasets lssvm sparse approximation procedure successfully appliedn',\n",
       " 'R. Cipolla T.W. Drummond A. Broadhurst paper introduce new probabilistic framework space carve framework voxel assign probability compute compare likelihood voxel exist exist new framework avoid difficulty associate original space carve algorithm specifically need global threshold parameter guarantee hole carve model paper propose voxelbased texture realistic efficient representation scene contain dominant plane algorithm test real synthetic data qualitative quantitative result presentedn',\n",
       " 'Nathalie Japkowicz Benjamin X. Wang real world data mine application address issue learn imbalanced data set problem occur number instance class greatly outnumber number instance class data set cause default classifier build skew vector space lack information common approach deal class imbalance problem involve modify data distribution modify classifier work choose use combination approach use support vector machine soft margin base classifier solve skew vector space problem counter excessive bias introduce approach boost algorithm ensemble svms make impressive improvement prediction performance majority class minority classn',\n",
       " 'R. Cipolla P.H.S. Torr G. Vogiatzis paper present novel formulation multiview scene reconstruction problem formulation benefit volumetric scene representation amenable computationally tractable global optimisation graphcuts algorithm propose u visual hull scene infer occlusion constraint topology scene photo consistencybased surface cost functional define discretised weight graph optimal surface discretised functional obtain minimum cut solution weight graph method provide viewpoint independent surface regularisation approximate handle occlusion tractable optimisation scheme promise experimental result real scene quantitative evaluation synthetic scene presentedn',\n",
       " 'Sclaroff Isidro iterative method reconstruct 3d polygonal mesh color texture map multiple view object present iteration method estimate texture map give current shape estimate texture map associate residual error image obtain maximum posteriori estimation reprojection multiple view texture space surface shape adjust minimize residual error texture space surface deform photometricallyconsistent solution series 1d epipolar search randomly select surface point texture space formulation improve computational complexity standard imagebased error approach allow computation reprojection error uncertainty point surface shape adjustment constrain recover modelu0027s silhouette match input image experiment real world imagery demonstrate validity approachn',\n",
       " 'JeanMarc Chassery Ioannis Pitas Adrian G. Bors William Puech paper present new approach reconstruct image map paint straight uniform generalize cylinder sugc set monocular image take dierent viewpoint order mosaiced represent entire scene expression sugcu0027s projection axis derive crosssections project image plane base ax derive sugc localization camera coordinate explain nd virtual image representation intersection ax match image center analyze perspective distortion attening scene map sugc evaluate low upper bind necessary number view order represent entire scene sugc consider distortion produce perspective projection region match base mosaicing method propose apply attened image order obtain complete scene mosaiced scene visualize new synthetic surface map procedure propose algorithm representation mural paint locate sugcs close crosssections circle column open crosssections ellipsis parabola vault 2001 pattern recognition society publish elsevier science right reservedn',\n",
       " 'K.Y.K. Wong Chen Liang paper address problem reconstruct object surface silhouette previous work author show base principle duality surface point recover theoretically dual tangent plane space object practice identification tangent basis tangent plane space trivial give set discretely sample data problem complicate existence bitangents object surface key contribution paper introduction epipolar parameterization identify welldefined local tangent basis extend applicability exist dual space reconstruction method fairly complicate shape make explicit assumption object topology verify approach synthetic realworld data compare qualitatively quantitatively popular reconstruction algorithm experimental result demonstrate propose approach produce accurate estimation maintain reasonable robustness shape complex topologiesn',\n",
       " 'D.B. Cooper J.P. Tarel Kongbin Kang Shubao Liu paper duality differential form develop 3d primal surface dual manifold form surfaceu0027s tangent plane tangent plane primal surface represent fourdimensional vector constitute point dual manifold iterate dual theorem show tangent plane dual manifold correspond point original 3d surface dual dual go primal theorem directly reconstruct 3d surface image edge estimate dual manifold edge paper develop work original conference paper result robust differential dual operator argue operator make good use information available image data point intensity discontinuity edge direction provide simple physical interpretation abstract algorithm actually estimate make sense term estimation accuracy algorithm operate edge image include silhouette edge self occlusion edge texture edge distinguish type result improve accuracy handle locally concave surface estimation texture edge present algorithm automatically handle degeneracy algorithm incorporate new methodology implement require operation appropriately relate edge pair image evaluate algorithmu0027s sensitivity noise determine accuracy estimate 3d point experiment synthetic real image demonstrate operator accurate robust degeneracy noise general reconstruct freeform object occlude edge texture edge detect calibrate image video sequencesn',\n",
       " 'Adrian G. Bors Matthew Grum paper propose enforce consistency segment contour model scene multiple object multiview image certain rough initialization 3d scene assume available case multiple object inconsistency expect propose shapefromcontour approach image segment backprojections segment contour enforce consistency segment contour 3d object scene provide study physical requirement detect occlusion reconstruct 3d scene multiple objectn',\n",
       " 'Antonio RoblesKelly Surya Prakash paper present semisupervised approach space carve cast recovery volumetric data multiple view evidence combine set method present statistical nature employ start point manually obtain contour make use userprovided information obtain probabilistic silhouette successive image silhouette provide prior distribution compute probability voxel carve evidence combine set allow use background pixel information result method combine advantage shapefromsilhouette technique statistical space carve approach carve process propose new voxelated space propose space projective provide colour map object voxels consistent term pixel coverage projection image plane imagery consideration provide quantitative result illustrate utility method realworld imageryn',\n",
       " 'Adrian G. Bors Nikolaos Nasios paper introduce new nonparametric estimation approach inspire quantum mechanic kernel density estimation associate function data sample classical kernel estimation theory probability density function calculate sum kernel propose approach assume data sample associate quantum physic particle radial activation field schrodinger differential equation quantum mechanic define location particle give observe energy level approach consider know location data sample model correspond probability density function analogy quantum potential function kernel scale estimate distribution knearest neighbour statistic order apply propose algorithm pattern classification use local hessian detect mode quantum potential hypersurface mode assimilate nonparametric class define mean region grow algorithm apply propose algorithm artificial data topography segmentation radar image terrainn',\n",
       " 'A.G. Bors M. Grum paper analysis reconstruction 3d scene multiple image propose approach u voxel representation initialize implicit radial basis function rbf model 3d scene voxel representation produce space carve algorithm disparity error identify inconsistency 3d patch scene surface pixel block correspondent image match algorithm estimate disparity error align pixel block region epipolar line correspond 3d patch scene surface algorithm derive update rbf center location disparity error experiment perform different set image numerical result provide compare result 3d scene grind truth give laser scannern',\n",
       " 'F. Sillion Long Quan S. Paris Gang Zeng introduce new surface representation method call patchwork extend threedimensional surface reconstruction capability multiple image patchwork combination patch build design potentially allow reconstruction object arbitrarily large dimension preserve fine level formally demonstrate strategy lead spatial complexity independent dimension reconstruct object time complexity linear respect object area property ensure run storage mean reconstruct object reasonable time addition patchwork representation handle equivalently open close surface exist approach limit specific scenario open close surface patchwork concept orthogonal method choose surface optimization exist optimization technique cast framework illustrate possibility offer approach propose application demonstrate method dramatically extend recent accurate graph technique base minimal cut revisit popular carve technique result wellposed reconstruction problem enjoy tractability voxel space advantageously combine imagedriven criterion achieve finely detail geometry surface propagation example demonstrate versatility flexibility patchwork reconstruction underscore property inherit patchwork representation mincut method difficulty handle complex shape complex topology naturally manipulate geometry patchwork representation preserve intrinsic quality property patchwork representation reconstruction demonstrate real image sequencesn',\n",
       " 'G. Slabaugh G. Turk Huong Quynh Dinh present new method surface reconstruction generate smooth seamless model sparse noisy nonuniform low resolution range data data acquisition technique vision stereo range image space carve produce 3d point set imprecise nonuniform compare laser optical range scanner traditional reconstruction algorithm design dense precise data produce smooth reconstruction apply visionbased data set method construct 3d implicit surface formulate sum weight radial basis function achieve primary advantage exist algorithm 1 implicit function construct estimate surface region little data 2 reconstruct surface insensitive noise data acquisition allow surface approximate exactly interpolate data 3 reconstruct surface locally detail globally smooth use radial basis function achieve multiple order smoothnessn',\n",
       " 'Fons J. Verbeek Herman P. Spaink Wouter J. Veneman Yuanhao Guo good estimation volume surface area important biological system measurement paper develop 3d reconstruction evenly sample axial view order enable volume surface area measurement develop high throughput application zebrafish model vast bioimager specifically develop purpose axial view produce silhouette derive axial sequence shape prior directly solve camera calibration problem require accurate 3d reconstruction nonlinear optimisation algorithm show suitable development reconstruction problem method propose paper include measurement pipeline kind high throughput application zebrafish field 3d reconstruction feature derive contribute phenotyping zebrafishn',\n",
       " 'Linyuan Zhang Zhiwei Qiao Yuhua Qian Zhiguo Hu Tao Yan abstract multifocus image fusion technique primarily emphasize human vision machine perception evaluate image ignore depth information contain focus region paper novel 3d shape reconstruction algorithm base nonsubsampled shearlet transform nsst microscopic multifocus image fusion method propose 3d depth information fusion process shiftinvariant property nsst guarantee spatial correspond relationship image sequence highfrequency subbands highfrequency component image represent focus level image new multidirectional modify laplacian mdml focus measure map highfrequency subbands image level depth initial 3d reconstruction result obtain optimal level selection strategy base summation multiscale laplace response exploit depth map finally iterative edge repair method implement refine reconstruction result experimental result propose method good performance especially source image lowcontrast regionn',\n",
       " 'H. J. Zhu Z. Y. Hu F. C. Wu paper rotate 1d calibrate object literature essence equivalent familiar 2d planar calibration object addition 1d object undergo planar motion rotate fix point equivalence hold traditional way fail handle experiment carry verify theoretical correctness numerical robustness resultsn',\n",
       " 'Zhengyou Zhang camera calibration study extensively vision photogrammetry propose technique literature include 3d apparatus plane orthogonal plane undergo pure translation 2d object planar pattern undergo unknown motion 0d feature selfcalibration unknown scene point paper propose new calibration technique 1d object point align line fill miss dimension calibration particular camera calibration possible freemoving 1d object solve point fix closedform solution develop observation 1d object high accuracy nonlinear technique base maximum likelihood criterion refine estimate singularity study theoretical aspect propose technique important practice especially calibrate multiple camera mount apart calibration object require visible simultaneouslyn',\n",
       " 'Elaine G. Alves Maria B. de M. Franca Marcelo R. Stemmer Jose A. de Franca recent year camera calibration 1d pattern study improve researcher world progress area mainly sense reduce restriction 1d pattern movement hand methodu0027s accuracy demand improvement present paper original technique propose zhang revisit demonstrate methodu0027s accuracy significantly improve simply analyze reformulate problem numerical condition improve simple data normalization perform furthermore nonlinear solution base partition levenbergmarquardt algorithm propose solution take advantage problemu0027s particular structure reduce computational complexity original method improve accuracy test synthetic real image demonstrate calibration method 1d pattern apply practice accuracy comparable traditional methodsn',\n",
       " 'A. Heyden P. Sturm P. Hammarstedt camera calibration onedimensional object base algebraic constraint image absolute conic alternative derivation constraint allow geometrical interpretation derive degenerate case critical motion calibration algorithm fail constraint intrinsic parameter lead simplify closedform solution reduce set critical motion simulation real data experiment perform evaluate accuracy calibration result motion close criticaln',\n",
       " 'Ling Li En Peng camera model calibration require application coordinate conversion twodimensional image real threedimensional world selfcalibration method usually choose camera calibration uncontrol environment scene geometry unknow reliable feature correspondence establish camera static relation majority scene selfcalibration method fail work hand objectbase calibration method reliable selfcalibration method existence object know geometry objectbase calibration method unable work uncontrol environment require geometric knowledge calibration object past year simple geometry require calibration object reduce 1d object point easy object uncontrol environment mention additional metricmotion requirement exist method easy 1d object end point scene worthwhile investigate objectbase method base simple object possible calibrate camera selfcalibration exist objectbase calibration fail work propose new camera calibration method require object end point simple geometry extract reallife object observation 1d object different positionsorientations plane fix relation camera intrinsic focal length extrinsic rotation angle translation camera parameter calibrate propose method propose method test simulate data real data control uncontrol environment include situation explicit 1d calibration object available human walk sequence accurate camera calibration result achieve propose methodn',\n",
       " 'K e Lu Fuqing Duan Liang Wang abstract camera calibration necessary step extract 3d metric information 2d image 1d objectbased camera calibration recently receive attention selfocclusion problem 1d object paper adaptively weight 1d calibration algorithm high accuracy present innovation contribution algorithm twofold data normalization improve estimation accuracy relative depth marker point 1d calibration object weight coefficient adaptively assign constraint camera parameter analyze data error involve constraint experiment synthetic real image data accuracy propose algorithm high classical 1d calibration case outperform nonlinear optimal 1d algorithm comparable normalize 2d 3d calibrationn',\n",
       " 'John Illingworth JeanYves Guillemaut present novel technique calibrate zoom camera base invariance property normalise image absolute conic niac camera parameter independent position orientation zoom determine uniquely niac exploit invariance property develop stratify calibration method decouple calibration parameter method organise step computation niac ii computation focal length image iii computation orientation position camera method require minimum view single planar grid experiment synthetic real data suggest method competitive stateoftheart planebase zoom calibration method scenario consideredn',\n",
       " 'Fuqing Duan Chao Shen Weiwei Wang Liang Wang accurate calibration prerequisite application multicamera onedimensional calibration method highly suitable multicamera system onedimensional object easy construct selfocclusions progress onedimensional calibration primarily focus reduce constraint motion onedimensional object calibration accuracy need improve paper accurate algorithm multicamera calibration onedimensional object base convex relaxation technique propose constraint absolute conic camera construct optimal solution absolute conic convex relaxation intrinsic parameter camera compute finally extrinsic parameter multicamera compute simple matrix operation compare exist algorithm propose algorithm accurate converge fast experiment synthetic real image data validate propose algorithmn',\n",
       " 'D. Nister efficient algorithmic solution classical fivepoint relative pose problem present problem possible solution relative camera pose calibrate view give correspond point algorithm consist compute coefficient tenth degree polynomial close form subsequently find root algorithm wellsuited numerical implementation correspond inherent complexity problem investigate numerical precision algorithm study performance noise minimal overdetermined case performance compare wellknown 8 7point method 6point scheme algorithm robust hypothesizeandtest framework estimate structure motion realtime low delay realtime u solely visual input demonstrate major conferencesn',\n",
       " 'M. Pollefeys S. Thirthala study multiview geometry 1d radial camera broad class central noncentral camera fisheye catadioptric camera reduce 1d radial camera assumption know center radial distortion camera general configuration introduce quadrifocal tensor compute linearly 15 feature see view tensor metric reconstruction 1d camera observe feature obtain second phase reconstruction calibration object estimate nonparametric noncentral model camera study degenerate case include pure rotation case purely rotate camera obtain trifocal tensor estimate linearly 7 point view allow obtain metric reconstruction plane infinity use plane infinity calibration device nonparametrically estimate radial distortion demonstrate result approach real synthetic imagesn',\n",
       " 'P. Sturm S. Ramalingam generic image model refer nonparametric camera model camera treat set unconstrained projection ray calibration simply method map projection ray image pixel map compute plane base calibration grid exist algorithm generic calibration use point correspondence theoretical minimum wellestablished nonminimal solution calibration structurefrommotion algorithm generally noiseprone compare minimal solution work derive minimal solution generic calibration algorithm algorithm generally central camera use 4 point correspondence calibration grid compute motion grid simulation minimal solution robust noise compare nonminimal solution accurate distortion correction result fisheye imagesn',\n",
       " 'Joan Batlle Jordi Pages Joaquim Salvi cod structure light consider reliable technique recover surface object technique base project light pattern view illuminate scene point view pattern cod correspondence image point point project pattern easily decod point triangulate 3d information obtain present overview exist technique new definitive classification pattern structure light sensor implement set representative technique field present comparative result advantage constraint different pattern discussedn',\n",
       " 'T. Pajdla B. Micusik generalize method simultaneous linear estimation multiple view geometry lens distortion introduce fitzgibbon cvpr 2001 omnidirectional angle view large 180spl deg camera perspective camera replace linear camera spherical retina nonlinear map sphere image plane unlike previous distortionbased model new camera model capable camera angle view large 180spl deg cost introduce extra parameter suitable linearization camera model epipolar constraint develop order arrive quadratic eigenvalue problem efficient algorithm know lens calibration automatically establish image correspondence rigidity assumption scene presence calibration object demonstrate method experiment nikon fce8 fisheye converter coolpix digital camera practical situation propose method allow incorporate new omnidirectional camera model ransac robust estimation techniquen',\n",
       " 'E. Mjolsness G.D. Hager C.P. Lu determine rigid transformation relate 2d image know 3d geometry classical problem photogrammetry vision heretofore best method solve problem rely iterative optimization method prove converge andor effectively account orthonormal structure rotation matrix pose estimation problem formulate minimize error metric base collinearity object oppose image space object space collinearity error derive iterative algorithm directly compute orthogonal rotation matrix globally convergent experimentally method computationally efficient accurate best currently employ optimization method outperform test method robustness outliersn',\n",
       " 'P. Sturm present method estimation relative pose plane camera base projection set coplanar feature image method exist simple case especially plane see view aim paper propose solution multiplane multiview situation possibly little overlap propose factorizationbase method general case n plane see m view mechanism compute miss data plane visible image describe experimental result real image shownn',\n",
       " 'Peter Sturm Srikumar Ramalingam paper propose unify theory calibrate wide variety camera model pinhole fisheye catadioptric multicamera network model camera set image pixel associate camera ray space pixel measure light travel half ray 3space associate pixel definition calibration simply refer computation map pixel associate 3d ray map compute image calibration grid object know 3d geometry take unknow position general camera model allow represent noncentral camera consider special subclass central axial camera central camera ray intersect single point ray completely arbitrary noncentral axial camera intermediate case camera ray intersect single line work theory calibrate central axial noncentral model calibration grid threedimensional planarn',\n",
       " 'Z.Y. Hu F.C. Wu L. Wang know order calibrate single camera onedimensional 1d calibration object object undertake constrain motion word impossible calibrate single camera object motion general multicamera setup number camera camera calibrate 1d object general motion work prove camera calibrate calibration algorithm propose experimentally test contrast multicamera calibration method calibrate base camera need addition multicamera case minimum condition calibration critical motion similar calibrate single camera 1d calibration objectn',\n",
       " 'Dongcheng Hu Yupin Luo Qihe Li Fei Qi camera calibration onedimensional 1d rigid object arrest attention researcher easytoconstruct geometrical structure apparatus paper extend motion pattern applicable calibration motion 1d object 1d object marker rotate marker move plane provide constraint equation camera intrinsic parameter stick move gravity force act perform motion simulate test feasibility numerical robustness methodn',\n",
       " 'Shuda Li Dirk Schnieders KwanYee K. Wong paper introduce novel method recover light direction camera pose single sphere traditional method estimate light direction sphere assume radius center sphere know precisely depend multiple calibrate view recover parameter show paper light direction uniquely determine specular highlight observe single view sphere know recover exact radius center sphere sphere observe multiple camera image uniquely define translation vector camera common world origin center sphere center show relative rotation camera recover light direction estimate view close form solution recover light direction camera pose present experimental result synthetic real data practicality propose methodn',\n",
       " 'D. Demirdijian G. Csurka R. Horaud method calibrate stereo pair camera general planar motion method consist upgrade 3d projective representation affine euclidean knowledge motion parameter 3d layout investigate algebraic property relate projective representation plane infinity intrinsic camera parameter camera pair consider move rigid body computation carry standard linear resolution technique error analysis reveal relative importance step calibration process projectivetoaffine affinetometric upgrade extensive experiment perform calibrate natural data confirm error analysis sensitivity study perform simulate datan',\n",
       " 'Qi Zheng Can Zhong Fan Zhou fundamental matrix estimation study extensively area vision previously propose technique include use feature point study propose new technique calculate fundamental matrix combine feature line base epipolar geometry horizontal vertical feature line method parameterizing fundamental matrix introduce camera orientation element relative orientation element parameter fundamental matrix equivalent relationship deduce base horizontal vertical feature line feature line interior point ransac algorithm search optimal feature point subset determine weight factor mestimators algorithm build unify adjustment model estimate fundamental matrix experimental result obtain simulate image real image demonstrate propose approach feasible practice greatly reduce dependency feature point traditional method introduction feature line improve accuracy stability result extentn',\n",
       " 'Peter James Vial Prashan Premaratne Brendan Halloran abstract calibration localisation camera sensor network essential requirement higherlevel vision task map track additionally distribute algorithm increasingly create scalable network robust node failure propose distribute calibration localisation algorithm base multiview onedimensional calibration alternate direction method multiplier gaussian belief propagation algorithm build exist calibration algorithm improve numerical condition nonlinear refinement adapt distribute network bring local estimate camera node global consensus simulation experimental result algorithm perform high accuracy compare calibration technique centralise distribute network suit practical applicationsn',\n",
       " 'Manolis I. A. Lourakis Theodore Papadopoulo singular value decomposition svd matrix linear algebra tool successfully apply wide variety domain present paper concern problem estimate jacobian svd component matrix respect matrix exact analytic technique develop facilitate estimation jacobian calculation base simple linear algebra knowledge jacobian svd useful certain application involve multivariate regression computation uncertainty relate estimate obtain svd usefulness generality propose technique demonstrate apply estimation uncertainty different vision problem selfcalibration epipole computation rigid motion estimationn',\n",
       " 'S.K. Nayar R. Swaminathan image take wideangle camera tend severe distortion pull point optical center paper propose simple method recover distortion parameter use calibration object distortion cause straight line scene appear curve image algorithm seek distortion parameter map image curve straight line user select small set point image curve recovery distortion parameter formulate minimization objective function design explicitly account noise select image point experimental result present synthetic data real image present idea polycamera define tightly pack camera cluster possible configuration propose capture large field view camera cluster tend nonsingle viewpoint provide analysis minimum work distance cluster finally present result polycamera consist wideangle sensor have minimum work distance 4 m undistorting acquire image propose technique create realtime high resolution panoramasn',\n",
       " 'Hongbin Zha Ganwen Wang Sen Yang Xiang Mei Xianghua Ying hartleykangu0027s paper 7 directly treat planar calibration pattern image construct image pair radial distort image planar calibration pattern propose efficient method determine center radial distortion estimate epipole radial distort image determine center radial distortion square method utilize recover radial distortion function monotonicity constraint paper present convex optimization method recover radial distortion function constraint require hartleykangu0027s method method obtain good result radial distortion correction experiment validate approachn',\n",
       " 'Sing Bing Kang R. Hartley propose method simultaneously calibrate radial distortion function camera internal calibration parameter method rely use planar alternatively nonplanar calibration grid capture image way determination radial distortion easy addon popular calibration method propose method entirely noniterative extremely rapid immune problem local minimum method determine radial distortion parameterfree way rely particular radial distortion model make applicable large range camera narrowangle fisheye lens method compute center radial distortion argue important obtain optimal result experiment point significantly displace center image principal point cameran',\n",
       " 'Edward Jones Martin Glavin Patrick Denny Ciaran Hughes paper method photogrammetrically estimate intrinsic extrinsic parameter fisheye camera property equidistance perspective particularly vanish point estimation aim provide rectify image scene view application estimate intrinsic parameter optical center fisheye lensing parameter extrinsic parameter rotation world ax relative checkerboard calibration diagramn',\n",
       " 'Stefano Tubaro Augusto Sarti Marco Marcon n',\n",
       " 'Andrey Bushnevskiy Lorenzo Sorgi n',\n",
       " 'Bodo Rosenhahn Lorenzo Sorgi Andrey Bushnevskiy camera calibration denote task estimate projective map 3d world camera image plane modern camera multiple image video acquisition mode differ image resolution field view aspect ratio mode treat independent device independently calibrate straightforward solution imply acquisition calibration dataset execution calibration routine camera mode timeconsuming errorprone task especially multicamera scenario paper propose multimode camera calibration approach noticeably relief task calibration model camera mode easily transfer mode need multiple calibration evaluation test perform offtheshelf consumer camera show propose method reduce require user interaction allow calibration accuracy improvementn',\n",
       " 'Mahdi Nezamabadi Manuel Martinello Evan Levine main challenge multiview camera calibration precise pose estimation camera especially field view little overlap work propose accurate multiview camera calibration method require camera share theirs field view rotate stage calibration target know trajectory consist pure rotation field view camera calibration parameter camera pose axis rotation extract novel highly constrain optimization procedure experiment 3d scan demonstrate propose method achieve high degree accuracy compare conventional 2d plane base calibrationn',\n",
       " 'Andreas Luber Kristian Manthey Dominik Ruess Ralf Reulke utilization camera system surveillance task e g traffic monitor standard procedure use 20 year camera operate locally data analyze manually locally mean limit field view image sequence process independently camera enlargement observation area avoid occlusion nonaccessible area multiple camera system overlap nonoverlap camera joint process image sequence multicamera scientific technical challenge process divide traditionally camera calibration object detection track interpretation fusion information different camera carry world coordinate reduce network load distribute process concept 135detection track fundamental image process task scene evaluation situation assessment base mainly characteristic local movement pattern direction speed trajectory derive possible recognize atypical movement pattern detect object compare local property trajectory interaction different object predict additional classification 197presentation discus trajectory base recognition algorithm atypical event 135detection multi object scene obtain area base type information map speed pattern trajectory curvature erratic movement show twodimensional areal data analysis move object multiple camera offer new possibility situational analysisn',\n",
       " 'Janne Heikkila Sami S. Brandt Juho Kannala paper propose generic selfcalibration method central camera method require twoview point correspondence estimate internal external camera parameter minimize angular error minimization use generic camera model suitable central camera different kind radial distortion model propose method apply large range camera narrowangle fisheye lens catadioptric camera camera parameter estimate minimize angular error depend 3d coordinate point correspondence error local minimum order avoid propose multistep optimization approach demonstrate method experiment synthetic real datan',\n",
       " 'Zoltan Kato Robert Frohlich Levente Tamas paper present novel approach extrinsic parameter estimation omnidirectional camera respect 3d lidar coordinate frame method work specific setup calibration target pair 2d3d data pose estimation formulate 2d3d nonlinear shape registration task solve point correspondence complex similarity metric rely set correspond region pose parameter obtain solve small nonlinear equation efficiency robustness propose method confirm synthetic real data urban environmentn',\n",
       " 'Luciano Sanchez Jose Otero use stereo vision 3d data gather affect constraint position camera quality optical element numerical algorithm calibration match wide agreement best procedure bound 3d error uncertainty volume work problem solve implement set computation include calibration triangulation interval data contrast previous work rely direct linear transform dlt camera model good real lens aberration local iterative modification propose provide ondemand set calibration parameter 3d point comprise near 3d space way estimate camera parameter closely relate camera aberration lens area 3d point image reduce triangulation uncertainty volume soft compute approach propose represent 3d point uncertainty cloud crisp point compatible intervalvalued calibration datareal data previous work relate research area judge new approach improve precision accuracy crisp intervalvalued estimation degrade precision conclude new technique able significantly improve uncertainty volumesn',\n",
       " 'Z. Tang J.M. Morel P. Monasse R. Grompone von Gioi paper point attempt remedy discrepancy result obtain global calibration method reprojection error render small method optical distortion correction far accurate discrepancy explain internal error compensation global method leave undetected inadequacy distortion model fact lead design modelfree distortion correction method distortion image domain diffeomorphism obtain precision compare favorably distortion give state art global calibration reach rmse 008 pixel nonetheless accuracy improvedn',\n",
       " 'T. Aach D. Friedrich M. Muehlich image analysis problem feature track edge detection image enhancement texture analysis require detection multioriented pattern appear arbitrary orientation direct rotate match filter feature detection computationally expensive speed steerable filter far steerable filter approach limit direction important lowlevel image feature characterize single orientation present framework efficiently detect specific multioriented pattern arbitrary orientation grayscale image core idea construct multisteerable filter appropriate combination singlesteerable filter exploit steerable filter close addition multiplication allow derive design guide multisteerable filter mean multivariate polynomial furthermore efficient implementation scheme discus use weight function reduce angular oscillation application camera calibration junction analysis image plant root discrimination l t xjunctions demonstrate potential approachn',\n",
       " 'K. Voss C. BrauerBurchardt use superwide angle fisheye lens cause strong distortion result image methodology correction distortion case single image linearity image object present contrary algorithm algorithm discuss depend information real world coordinate match point reference point determination camera calibration require case algorithm base circle fit require possibility extraction distort image point straight line 3d scene actual distortion approximately fit choose distortion model fisheye lens appropriate distortion correction result obtainedn',\n",
       " 'A.W. Fitzgibbon D. Claus introduce new rational function rf model radial lens distortion wideangle catadioptric lens allow simultaneous linear estimation motion lens geometry uncalibrated view 3d scene contrast exist model admit linear estimate new model specialize particular lens geometry sufficiently general model variety extreme distortion key step define map image pixel coordinate 3d ray camera coordinate linear combination nonlinear function image coordinate like kernel trick allow linear algorithm estimate nonlinear model particular offer simple solution estimation nonlinear image distortion model yield explicit form epipolar curve allow correspondence search efficiently guide epipolar geometry result implementation rf model estimate geometry real camera lens uncalibrated footage compare estimate obtain calibration gridn',\n",
       " 'K. Daniilidis J.P. Barreto deploy heterogeneous camera network use cheap zoom camera like cellphone practical impossible offline calibrate radial distortion camera reference object desirable automatic procedure strong assumption scene paper present new algorithm estimate epipolar geometry view view radially distort different distortion factor algorithm literature solve case different distortion leave right view linearly assume existence line scene point projective plane lift quadric threedimensional projective space radial distortion projective plane result matrix transformation space lift coordinate new epipolar constraint depend linearly 4 spl time 4 radial fundamental matrix 9 degree freedom complete algorithm present test real imageryn',\n",
       " 'D. Scaramuzza A. Censi paper present new intrinsic calibration method allow calibrate generic singleview point camera wave video sequence obtain camera undergo random motion compute pairwise time correlation luminance signal subset pixel camera undergo random uniform motion pairwise correlation pixel pair function distance pixel direction visual sphere lead formalize calibration problem metric embed nonmetric measurement want disposition pixel visual sphere similarity unknown function distance problem generalization multidimensional scale md far resist comprehensive observability analysis reconstruct metrically accurate embed solid generic solution observability depend local geometric property curvature global topological property connectedness target manifold contrast euclidean case sphere recover scale point distribution obtain metrically accurate solution nonmetric measurement algorithm robust manifold recover metrically accurate solution metric information observable demonstrate performance algorithm camera pinhole fisheye omnidirectional obtain result comparable calibration classical method additional synthetic benchmark algorithm perform theoretically predict corner case observability analysisn',\n",
       " 'S. Roy M. Trudeau P. Sturm J.P. Tardif present algorithm planebase calibration general radially distort camera understand camera distortion center optical axis projection ray pixel lie circle center distortion center form right view cone center optical axis camera say single viewpoint svp view cone apex optical center speak nsvp case model encompass classical radial distortion model fisheyes central noncentral catadioptric camera calibration consist estimation distortion center open angle view cone optical center present approach compute calibration dense correspondence single multiple plane know euclidean structure base geometric constraint link view cone intersection calibration plane conic section second approach homographybase method experiment simulate broad variety real camera great stability furthermore provide comparison hartleykangu0027s algorithm handle broad variety camera configuration show similar performancen',\n",
       " 'Andrea Torsello Emanuele Rodola Andrea Albarelli Filippo Bergamasco traditional camera model result compromise ability account nonlinearities image formation model need feasible number degree freedom estimation process consideration lead definition ad hoc model best adapt different image device range pinhole camera radial distortion complex catadioptric polydioptric optic paper propose use unconstrained model standard central camera set dominate pinhole model introduce novel calibration approach deal effectively huge number free parameter associate result high precision calibration possible standard pinhole model correction radial distortion effectively extend use general model set traditionally rulead parametric approach practical consideration benefit unconstrained model quasipinhole central camera support extensive experimental validationn',\n",
       " 'H. Araujo P. Miraldo generic image model represent camera current generic model discrete define map pixel image straight line 3d space paper present modification generic camera model allow simplification calibration procedure requirement coordinate 3d project line relate function vary smoothly space model obtain modify general image model radial basis function rbfs interpolate image coordinate 3d line allow increase resolution continuous nature compact representation variation general image model develop calibration procedure procedure require 3d point match pixel addition pixel need calibrate result complexity procedure significantly decrease normalization apply coordinate image 3d point increase accuracy calibration result synthetic real datasets model calibration procedure easily applicable provide accurate calibration resultn',\n",
       " 'M. Lhuillier automatic reconstruction 3d model image sequence active field research exist method design give camera model new ambitious challenge 3d model method exploitable kind camera similar approach recently suggest structurefrommotion thank use generic camera model paper introduce geometric tool design 3d scene model generic camera model tool solve issue match error wide range point depth depth discontinuity viewpoint selection reconstruction experiment provide perspective catadioptric camerasn',\n",
       " 'Andre Kaup Jurgen Seiler Michel Batz Andrea Eichenseer n',\n",
       " 'Jianmin Jiang KwokWai Hung Qinglong Chang abstract recent development virtual reality application accelerate usage camera wideangle telephotomacro lens produce nonlinear radial lens distortion barrel distortion pincushion distortion reason resolution image nonlinear lens distortion limit paper address image superresolution sr image nonlinear lens distortion deep convolutional neutral network residual learn significantly improve image quality camera calibration propose deep learn network train hundred simulate image test real camera fisheye macro lens experimental result propose image sr method outperform stateoftheart sr method degree radialbased barrel pincushion distortionn',\n",
       " 'Andrea Torsello Andrea Albarelli Andrea Gasparetto Luca Cosmo Filippo Bergamasco core vision application stand need define mathematical model describe image process end pinhole model radial distortion probably commonly balance low complexity precision sufficient application hand unconstrain nonparametric model despite originally propose handle specialty camera show outperform pinhole model simple setup notwithstanding high accuracy inability describe image model simple linear projective operator severely limit use standard algorithm unconstrain model paper propose parameterfree camera model image ray constrain common optical center force camera central model easily calibrate practical procedure provide convenient undistortion map obtain virtual pinhole camera propose method calibrate stereo rig displacement map simultaneously provide stereo rectification correct lens distortionn',\n",
       " 'In So Kweon YuWing Tai Yunsu Bok Jinsun Park Gyeongmin Choe Jaesik Park HaeGon Jeon core application light field image depth estimation acquire depth map exist approach apply single photoconsistency measure entire light field optimal choice nonuniform light field degradation produce limitation hardware design paper introduce pipeline automatically determine best configuration photoconsistency measure lead reliable depth label light field analyze practical factor affect degradation lenslet light field camera design learn base framework retrieve best cost measure optimal depth label enhance reliability method augment exist light field benchmark simulate realistic source dependent noise aberration vignetting artifact augment dataset train validation propose approach method competitive stateoftheart method benchmark realworld light field datasetsn',\n",
       " 'R. Cipolla P.R.S. Mendonca K.Y.K. Wong paper address problem calibrate pinhole camera image surface revolution camera calibration process determine intrinsic internal parameter aspect ratio focal length principal point camera important motion estimation metric reconstruction 3d model paper novel simple calibration technique introduce base exploit symmetry image surface revolution traditional technique camera calibration involve take image precisely machine calibration pattern calibration grid use surface revolution commonly daily life bowl va make process easy result reduce cost increase accessibility calibration object paper show image surface revolution provide information determine aspect ratio focal length principal point camera fix intrinsic parameter algorithm present paper implement test synthetic real data experimental result camera calibration method present practical accuraten',\n",
       " 'Rene Payrissat Alain Crouzil Pierre Gurdjos planebase calibration consist recover internal parameter camera view planar pattern know geometric structure exist direct algorithm use problem formulation base property basis vector minimize algebraic distance require u0027goodu0027 choice normalization contribution problem intuitive geometric framework solution obtain intersect circle call centre circle parameter compute worldtoimage homographies centre circle camera centre locus planar figure perpective correspondence accordance ponceletu0027s theorem interest aspect formulation centre circle constraint easily transform cost function sum square euclidean distance simulation synthetic data application real image confirm strong point methodn',\n",
       " 'P. Sturm P. Gurdjos consider problem camera selfcalibration image planar object unknown euclidean structure general case possibly vary focal length address problem nonlinear general contribution nonlinear approach make abstraction possibly vary focal length result computationally efficient algorithm addition require good initial estimate focal length unlike previous approach initialization parameter propose practical approach simply require take image roughly frontoparallel position closedform solution configuration unknown intrinsic parameter provide method evaluate compare previous approach simulate real image practical contribution provide detail geometrical interpretation principle underlie approachn',\n",
       " 'I. Reid A. Zisserman J. Knight planar scene appear ideally suit selfcalibration eliminate problem occlusion parallax high accuracy twoview relationship calculate restrict motion pure rotation unfortunately monocular solution far devise involve costly nonlinear minimization initialize educate guess calibration parameter far problem circumvent stereo know calibration object work control motion camera fast linear solution available restriction camera undergo motion planenormal rotation axis typify instance motion plane scene complex eigenvectors planeinduced homography coincident circular point motion homographies provide sufficient information solve image absolute conic iac calibration parameter require situation arise commonly camera view grind plane move rotate vertical axis demonstrate number useful application algorithm simple fast accuraten',\n",
       " 'Hongbin Zha Xianghua Ying spherical object introduce camera calibration year utilize property image conic projection occlude contour sphere perspective image literature algebraic interpretation present relation image absolute conic sphere image paper propose geometric interpretation relation novel camera calibration method sphere image derive geometric interpretationn',\n",
       " 'J. Kybic address problem estimate uncertainty pixel base image registration algorithm give image register case grind truth data available novel method u bootstrap resampling general applicable registration method base minimize pixelbase similarity criterion demonstrate ssd sad correlation mutual information criterion experimentally bootstrap method provide good estimate registration accuracy stateoftheart cramerrao bind method additionally evaluate fast registration accuracy estimation frae method base quadratic sensitivity analysis idea negligible computational overhead frae work good cramerrao bind method outperform bootstrap methodn',\n",
       " 'JenHui Chuang JainShing Liu abstract choose specific crossratios 2d projective coordinate vision application reasonable error analysis model usually require investigation adopt assumption normal distribution position error point feature image formulate error variance crossratios base geometrybase error analysis straightforward way identify crossratios minimum error variance propose simulation result propose approach simplify alternative yield good estimation minimum error variance term accuracy cost stability compare method base rule give georis et al ieee trans pattern anal mach intell 20 4 1998 366 cause performance difference estimation explain special configuration point featuren',\n",
       " 'H. Foroosh I.N. Junejo paper solar shadow trajectory robust camera calibration expand previous work calibration solar shadow relax condition shadow cast object visible image enable work shadow nonvertical object important observation work shadow trajectory form interest geometry grind plane property cast shadow horizon line line infinity grind plane robustly estimate lead polepolar constraint image absolute conic iac decompose estimate camera parameter method perform presence large noise perform experiment synthetic data real data capture live webcam demonstrate encourage resultsn',\n",
       " 'Robert Pless Kylia Miskell Austin Abrams shadow encode powerful geometric cue pixel cast shadow pixel colinear light direction give image light direction constraint leverage recover depth scene single viewpoint outdoor scene solar illumination term episolar constraint provide convex optimization solve sparse depth scene shadow correspondence method reduce search space find shadow correspondence method geometrically calibrate camera shadow constraint method construct dense network nonlocal constraint complement recent work outdoor photometric stereo cloud base cue 3d demonstrate result variety timelapse sequence web cam wildn',\n",
       " 'Jean Meunier Rafik Gouiaa paper propose multiinfrared light human posture recognition u input combination body silhouette cast shadow precisely experimental setup instal 4 infrared light different upper corner room project shadow person camera infrared filter ceiling capture image simple electronic device turn light turn cast shadow frame illustrate feasibility approach consider simple scene shadow project directly grind feature recognition base distance transform combination body silhouette cast shadow weight majority vote scheme decide correspond posture approach validate new data set capture laboratory compare traditional monocamera systemn',\n",
       " 'Andrew Blake Alex Kipman Richard Moore Mark Finocchio Toby Sharp Mat Cook Andrew Fitzgibbon Jamie Shotton propose new method quickly accurately predict 3d position body joint single depth image temporal information object recognition approach design intermediate body part representation map difficult pose estimation problem simple perpixel classification problem large highly vary train dataset allow classifier estimate body part invariant pose body shape clothe finally generate confidencescored 3d proposal body joint reprojecting classification result find local mode run 200 frame second consumer hardware evaluation show high accuracy synthetic real test set investigate effect train parameter achieve state art accuracy comparison relate work demonstrate improve generalization exact wholeskeleton near neighbor matchingn',\n",
       " 'J. Davis Ruigang Yang Liang Wang Jiejie Zhu timeofflight range sensor error characteristic complementary passive stereo provide real time depth estimate condition passive stereo work white wall contrast sensor noisy perform poorly textured scene stereo excel introduce method combine result method perform good depth probability distribution function method calculate merge addition stereo method long global method belief propagation graph cut improve result apply method sensor timeofflight device primarily individual sensor typically poorly calibrate introduce method substantially improve manufacturerpsilas calibration technique lead improve accuracy robustnessn',\n",
       " 'Christian Theobalt Sebastian Thrun Derek Chan Sebastian Schuon Yan Cui method 3d object scan align depth scan take object timeofflight camera tof camera measure depth scan video rate comparably simple technology bear potential low cost production big volume easytouse costeffective scan solution base sensor 3d scan technology accessible everyday user algorithmic challenge face sensoru0027s level random noise substantial nontrivial systematic bias paper surprise result 3d scan reasonable quality obtain sensor low data quality establish filter scan alignment technique literature fail achieve goal contrast algorithm base new combination 3d superresolution method probabilistic scan alignment approach explicitly take account sensoru0027s noise characteristicsn',\n",
       " 'Vladlen Koltun Stephen Miller QianYi Zhou present approach reconstruction detail scene geometry range video range data produce commodity handheld camera suffer highfrequency error lowfrequency distortion approach deal source error reconstruct locally smooth scene fragment let fragment deform order align develop volumetric registration formulation leverage smoothness deformation optimization practical large scene experimental result demonstrate approach substantially increase fidelity complex scene geometry reconstruct commodity handheld cameran',\n",
       " 'Zoltan Kato Levente Tamas novel method propose calibration camera 3d lidar pair use special calibration pattern point correspondence propose method specific assumption data source plain depth information expect lidar scan simple perspective camera 2d image calibration solve 2d3d registration problem minimum extrinsic intrinsicextrinsic planar region visible camera registration trace solution nonlinear equation directly provide calibration parameter base sensor method test large set synthetic lidarcamera image pair real data acquire outdoor environmentn',\n",
       " 'John W. Fisher John J. Leonard Oren Freifeld Guy Rosman Julian Straub object structure manmade environment typically exhibit high degree organization form orthogonal parallel plane traditional approach scene representation exploit phenomenon somewhat restrictive assumption plane perpendicular ax single coordinate know manhattanworld model assumption widely vision robotics complexity realworld scene necessitate flexible model propose novel probabilistic model describe world mixture manhattan frame frame define different orthogonal coordinate result expressive model exploit orthogonality constraint propose adaptive markovchain montecarlo sample algorithm metropolishastings splitmerge move utilize geometry unit sphere demonstrate versatility mixtureofmanhattanframe model describe complex scene depth image indoor scene aeriallidar measurement urban center additionally model lend focallength calibration depth camera plane segmentationn',\n",
       " 'Bodo Rosenhahn Alina Kuznetsova timeofflight tof camera popular vision application 3d information deliver tof camera important know cameraxe2x80x99s extrinsic intrinsic parameter precise depth information straightforward algorithm calibrate tof camera use standard color camera calibration procedure 12 amplitude image depth information deliver tof camera know contain complex bias error source 6 additionally desirable case determine pose tof camera relative sensor usedn',\n",
       " 'S. Thrun C. Theobalt D. Chan Young Min Kim paper describe design calibration enable simultaneous record dynamic scene multiple highresolution video lowresolution swissranger timeofflight tof depth camera shall serve testbed development new algorithm highquality multiview dynamic scene reconstruction 3d video paper provide detail analysis random systematic depth camera noise important reliable fusion video depth data finally paper describe compensate systematic depth error calibrate dynamic depth video data common framen',\n",
       " 'Ian Reid Carl Yuheng Ren paper present unify energy minimization framework model fit pose recovery problem depth camera 3d levelset embed function represent object model implicitly novel 3d chamfer match base energy function minimize adjust generic projection matrix parameterized differently accord specific application propose energy function take advantage gradient 3d levelset embed function efficiently solve gradientsbase optimization method realworld application include realtime 3d track depth simultaneous calibration track 3d point cloud model perform experiment real data synthetic data superior performance method application aboven',\n",
       " 'In So Kweon Yekeun Jeong JoonYoung Lee Jiyoung Jung present calibration method timeofflight tof sensor color camera pair align 3d measurement color image correctly design 25d pattern board irregularly place hole accurately detect low resolution depth image tof camera high resolution color image order improve accuracy 3d measurement tof camera propose perform ray correction range bias correction reset transformation tof sensor transform radial distance scene depth cartesian coordinate ray correction capture planar scene different depth correct distance error show dependent distance pixel location range error profile calibrate distance classify accord wiggle shape cluster profile similar shape separately estimate bspline function standard deviation remain random noise record uncertainty information distance measurement performance calibration method quantitatively qualitatively datasets validate impact method demonstrate rgbd shape refinement applicationn',\n",
       " 'Jaime S. Cardoso Paulo Aguiar Helder P. Oliveira Joao P. Monteiro animal behavior assessment play important role basic clinical neuroscience assess high functional level nervous possible behavioral test extremely complex design analyze animalu0027s response evaluate manually make subjective extremely time consume poorly reproducible potentially fallible main goal present work evaluate use consumer depth camera microsoftu0027s kinect detection behavioral pattern mouse hypothesis depth information enable feasible robust method automatic behavior recognition introduce depthmap base approach comprise mouse segmentation bodylike perframe feature extraction perframe classification give temporal context prove usability methodologyn',\n",
       " 'Pietro Zanuttigh Giampaolo Pagnutti scene segmentation challenge problem color information sufficient recently introduction consumer depth camera open way novel approach exploit depth data paper propose novel segmentation scheme exploit joint usage color depth data 3d surface estimation scheme firstly set multidimensional vector build color geometry information normalize cut spectral cluster apply order coarsely segment scene nurbs model fit compute segment accuracy fit measure plausibility segment represent single surface object segment represent single surface split small region process iterate optimal segmentation obtain experimental result propose method allow obtain accurate reliable scene segmentationn',\n",
       " 'Roger Olsson Marten Sjostrom Sebastian Schwarz acquire scenery depth fundamental task vision application manufacture surveillance robotics rely accurate scenery information timeofflight camera provide depth information realtime overcome shortcoming traditional stereo analysis provide limit spatial resolution sophisticate upscaling algorithm seek paper present sensor fusion approach timeofflight super resolution base combination depth texture source unlike texture guide approach interpret depth upscaling process weight energy optimization problem different weight introduce employ different available sensor data individual weight address object boundary depth depth sensor noise temporal consistency apply consecutive order form weight strategy timeofflight super resolution objective evaluation advantage depth accuracy depth image base render compare stateoftheart depth upscaling subjective view synthesis evaluation show significant increase viewer preference factor stereoscopic view condition best knowledge extensive subjective test perform timeofflight depth upscaling objective subjective result proof suitability approach timeofflight super resolution approach depth scenery capturen',\n",
       " 'In So Kweon Michael S. Brown YuWing Tai Hyeongwoo Kim Jaesik Park paper describe application framework perform highquality upsampling completion noisy depth map framework target complementary setup consist depth camera couple rgb camera inspire recent work u nonlocal structure regularization regularize depth map order maintain fine detail structure extend regularization combine additional highresolution rgb input upsampling lowresolution depth map weight scheme favor structure detail technique able repair large hole depth map consideration structure discontinuity utilize edge information rgb input quantitative qualitative result method outperform exist approach depth map upsampling completion complete process include device calibration scene warp input alignment framework extend video depthmap completion consideration temporal coherencen',\n",
       " 'BinhSon Hua Tian Tsong Ng Minh N. Do Ramanpreet Singh Pahwa depth sense device create new application scientific commercial research advent microsoft kinect pmd photon mix device camera application require depth camera precalibrated traditional calibration method checkerboard work depth camera low image resolution paper propose depth calibration scheme excel estimate camera calibration parameter handful corner calibration image available exploit noise property pmd device denoise depth measurement perform camera calibration denoised depth additional set measurement synthetic real experiment depth denoising depth base calibration scheme provide significantly good result traditional calibration methodn',\n",
       " 'Hongan Wang Liang Chang Feng Tian Jie Liu Xiaoming Deng automatic motion estimation multiple depth camera remain challenge topic vision reliance image correspondence problem paper spherical object employ estimate motion parameter multiple depth camera sphere time common view depth camera fit spherical point cloud sphere center depth camera introduce factorization base approach estimate motion depth camera simulate real experiment robustness effectiveness methodn',\n",
       " 'Olga Bellon Luciano Silva Dmitry Goldgof Sudeep Sarkar Mauricio Pamplona Segundo present continuous 3d face authentication u rgbd camera monitor access user ensure allow user u protect best knowledge u 3d face image accomplish objective depth image reduce user cooperation require previous continuous authentication work literature evaluate 40 minute long video variation facial expression occlusion pose equal error rate 08 achievedn',\n",
       " 'G. Hirzinger S. Fuchs recently tofcameras attract attention ability generate 2 12d depth image video frame rate tofcameras suitable realtime 3d task track visual servoing object pose estimation usability system mainly depend accurate camera calibration work calibration process tofcameras respect intrinsic parameter depth measurement distortion pose camera relative robotpsilas end effector describe calibration process base monochromatic image camera u depth value generate chequerboard pattern robustness precision present method assess apply randomly select shoot compare calibrate measurement grind truth obtain laser scannern',\n",
       " 'Vladlen Koltun QianYi Zhou approach simultaneous localization calibration stream range image approach jointly optimize camera trajectory calibration function correct camerau0027s unknown nonlinear distortion experiment realworld benchmark data synthetic data approach increase accuracy camera trajectory geometric model estimate range video produce consumergrade camerasn',\n",
       " 'Jose Jesus Guerrero Gonzalo LopezNicolas Alejandro PerezYus consumer rgbd camera useful year field view narrow certain application propose new hybrid camera compose conventional rgbd fisheye camera extend field view 180circ region hemispherical image depth certainty color data periphery extend structural information scene develop new method generate scale layout hypothesis relevant corner combine extraction line fisheye image depth information experiment real image different scenario validate layout recovery method advantage camera able overcome severe occlusion result obtain scale 3d model expand original depth information wide scene reconstruction proposal expand successfully depth map time single shotn',\n",
       " 'Chunping Hou Nam Ling Feng Wu Huanjing Yue Lele Li Jianjun Lei accurate highquality depth map require lot 3d application multiview render 3d reconstruction 3dtv resolution capture depth image low correspond color image affect application performance paper propose novel depth map superresolution sr method take view synthesis quality account propose approach mainly include technical contribution capture lowresolution lr depth map corrupt noise occlusion propose credibility base multiview depth map fusion strategy consider view synthesis quality interview correlation refine lr depth map second propose view synthesis quality base trilateral depthmap upsampling method consider depth smoothness texture similarity view synthesis quality upsampling filter experimental result demonstrate propose method outperform stateoftheart depth sr method superresolved depth map synthesize view furthermore propose method robust noise achieve promise result noisecorruption conditionsn',\n",
       " 'Gerasimos Potamianos Petros Daras Georgios Th. Papadopoulos Spyridon Thermos wellestablished cognitive neuroscience human perception object constitute complex process object appearance information combine evidence socalled object affordances type action human typically perform interact fact recently motivate sensorimotor approach challenge task automatic object recognition information source fuse improve robustness work aforementioned paradigm adopt surpass current limitation sensorimotor object recognition research specifically deep learn paradigm introduce problem time develop number novel neurobiologically neurophysiologically inspire architecture utilize stateoftheart neural network fuse available information source multiple way propose method evaluate large rgbd corpus specifically collect task sensorimotor object recognition publicly available experimental result demonstrate utility affordance information object recognition achieve 29 relative error reduction inclusionn',\n",
       " 'Long Quan Guang Jiang Huan Lei paper present robust global approach point cloud registration uniformly sample point base eigenvalue normal compute multiple scale design fast descriptor extract local structure point eigenvaluebase descriptor effective find seed match low precision near neighbor search generally recover transformation match low precision challenge introduce mechanism name correspondence propagation aggregate seed match set numerous match set match multiple transformation point cloud compute quality function formulate distance error identify best transformation fulfill coarse alignment point cloud finally refine alignment result trim iterative close point algorithm propose approach apply register point cloud significant limit overlap small large transformation encouragingly efficient robust noise comparison traditional descriptorbase method global algorithm demonstrate fine performance propose approach promise application largescale reconstruction scan real scene addition propose approach register lowresolution point cloud capture kinect welln',\n",
       " 'Masatoshi Okutomi Masayuki Tanaka Takashi Shibata powerful joint filter crossmodal image pair propose exist joint filter generate severe artifact misalignment target guidance image goal generate artifactfree output image misalign target guidance image propose novel misalignmentrobust joint filter base weightvolumebase image composition jointfilter cost volume propose method generate set translate guidance jointfilter cost volume set filter image compute target image set translate guidance weight volume obtain jointfilter cost volume consider spatial smoothness labelsparseness final output image compose fuse set filter image weight volume filter image key generate final output image directly set filter image weight average weight volume obtain jointfilter cost volume propose framework widely applicable involve kind joint filter experimental result propose method effective application include image denosing image upsampling haze removal depth map interpolationn',\n",
       " 'Debin Zhao Xianming Liu Deming Zhai Rong Chen depth information widely realworld task 3dtv 3d scene reconstruction multiview render capture depth map practice usually suffer quality degradation include lowresolution noise corruption limit application noiseaware superresolution depth map challenge task receive increasingly attention recent year paper propose novel method base plugandplay scheme cast powerful graphbase toolsthe graph laplacian regularizer 3d graph fourier transforminto unify admm optimization framework perform iterative manner easily treatable convex optimization subproblems experiment result demonstrate method achieve superior performance compare stateoftheart work respect objective subjective quality evaluationsn',\n",
       " 'Wen Gao Debin Zhao Xiangyang Ji Rong Chen Deming Zhai Xianming Liu paper propose novel depth restoration algorithm rgbd data combine characteristic local nonlocal manifold provide lowdimensional parameterizations local nonlocal geometry depth map specifically hand local manifold model define favor local neighbor relationship pixel depth accord manifold regularization introduce promote smooth manifold structure hand nonlocal characteristic patchbased manifold build highly dataadaptive orthogonal base extract elongate image pattern account selfsimilar structure manifold define manifold thresholding operator 3d adaptive orthogonal spectral basexe2x80x94eigenvectors discrete laplacian local nonlocal manifoldxe2x80x94to retain low graph frequency depth map restoration finally propose unify alternate direction method multiplier optimization framework elegantly cast adaptive manifold regularization thresholding jointly regularize inverse problem depth map recovery experimental result demonstrate method achieve superior performance compare stateoftheart work respect objective subjective quality evaluationsn',\n",
       " 'Wen Gao Debin Zhao Xiangyang Ji Rong Chen Deming Zhai Xianming Liu depth information widely realworld application limitation depth sense technology capture depth map practice usually low resolution color image counterpart paper propose combine internal smoothness prior external gradient consistency constraint graph domain depth superresolution hand new graph laplacian regularizer propose preserve inherent piecewise smooth characteristic depth desirable filter property specific weight matrix respect graph define use information depth correspond guidance image hand inspire observation gradient depth small edge separate region introduce graph gradient consistency constraint enforce graph gradient depth close thresholded gradient guidance reinterpret gradient thresholding model variational optimization sparsity constraint way remedy problem structure discrepancy depth guidance finally internal external regularization cast unify optimization framework efficiently address admm experimental result demonstrate method outperform stateoftheart respect objective subjective quality evaluationsn',\n",
       " 'JeanThierry Lapreste Gerard Rives JeanMarc Lavest article deal optical law consider underwater camera theoretical experimental point view describe show relationship air water calibration foundn',\n",
       " 'T.W. Parks K. Hirakawa costeffective digital camera u singleimage sensor apply alternate pattern red green blue color filter pixel location way reconstruct threecolor representation color image estimate miss pixel component color plane call demosaicing algorithm paper present inherent problem associate demosaicing algorithm incorporate twodimensional 2d directional interpolation misguidance color artifact interpolation color artifact aliasing level misguidance color artifact present image compare metric neighborhood model propose demosaicing algorithm estimate miss pixel interpolate direction few color artifact aliasing problem address apply filterbank technique 2d directional interpolation interpolation artifact reduce nonlinear iterative procedure experimental result digital image confirm effectiveness approachn',\n",
       " 'Joan Batlle Xavier Armangue Joaquim Salvi camera calibrate crucial problem metric scene measurement technique study concern calibration present year difficult detail determine calibrate technique compare accuracy respect method principally problem emerge lack standardize notation existence method accuracy evaluation choose article present detail review calibrate technique principal idea present notation furthermore technique survey test accuracy evaluate comparative result show discuss article code result available internetn',\n",
       " 'Kin Hong Wong Zhe Zhang paper propose novel geometric approach solve camera calibration efficiently camera calibration procedure implement planar object rectangle image planar object take different view corner feature extract geometric relationship rectangle corner utilize reduce number extrinsic unknown parameter process reduce complexity optimization space greatly enable approach accurate efficient geometric approach experimental result synthetic image real image witness advantage approach stateoftheart approachn',\n",
       " 'M. Pollefeys JM. Frahm A. Ilie R.K. Kumar calibrate network camera nonoverlapping view important challenge problem vision paper present novel technique camera calibration planar mirror overcome need camera common calibration object directly allow mirror use fact mirror view generate family mirror camera pose uniquely real camera pose method consist follow step 1 standard calibration method internal external parameter set mirror camera pose 2 estimate external parameter real camera mirror pose formulate constraint demonstrate method real synthetic data camera cluster small overlap view nonoverlapping viewn',\n",
       " 'InSo Kweon JunSik Kim P. Gurdjos planebased calibration popular procedure flexibility key step consist detect set coplanar feature euclidean structure correspond 3d plane compute suggest use confocal conic calibration target offer undeniable advantage one point line term detection estimation especially presence partial occlusion introduce important projective euclidean property linear family conic confocal conic range span confocal conic particular rely fact circular pointenvelope rank2 conic encode 2d euclidean structure degenerate member confocal conic range allow closedform solution case conic know focus confocal conic know product ratio semi ax unknow confocal conic performance propose algorithm consist line matlablike code high accuracy intrinsic extrinsic camera parameter addition experiment synthetic data video sequence process show confocal conic calibration target augment reality purposesn',\n",
       " 'Rama Chellappa Feng Guo paper present video metrology approach uncalibrated single camera stationary planar motion theoretically simple measure length line segment give video difficult problem exist technique task extension single imagebase technique achieve desire accuracy especially noisy environment contrast propose algorithm move line segment reference plane share common endpoint vanish line information follow fit multiple concentric circle image plane fully automate realtime base algorithm develop measure vehicle wheelbase uncalibrated stationary camera estimate vanish line invariant length reference plane multiple frame give parallel line exist video extend camera undergo planar motion automatically select frame similar vanish line video experimental result measurement result accurate classify move vehicle base sizen',\n",
       " 'Zhihu Chen Guoqiang Zhang KwanYee Kenneth Wong paper propose stratify approach camera calibration sphere previous work exploit epipolar tangent locate frontier point sphere estimate epipolar geometry show paper frontier point additional point feature obtain consider bitangent envelope pair sphere simple method locate image point feature sphere center present algorithm recover fundamental matrix plane plus parallax representation recover image point epipolar tangent sphere develop new formulation absolute dual quadric cone tangent dual sphere plane infinity vertex derive allow recovery absolute dual quadric upgrade weak calibration calibration experimental result synthetic real data present demonstrate feasibility high precision achieve propose algorithmn',\n",
       " 'V. Charvillat P. Gurdjos L. Calvet c2tag refer set concentric circle different radius c2tag recently introduce vision particular camera calibration offer highly interest photometric geometric property compare classical xe2x80x9ccheckerboardxe2x80x9d tag work propose general paradigm camera track base planar marker consist c2tag involve step describe detection identification 2d reconstruction calibration 3d reconstruction contribution introduce follow miss step require deal long video sequence time constraint keyframe selection bundle adjustment intermediate pose refinementn',\n",
       " 'Manolis I.A. Lourakis metric rectification important application topic singleview metrology camera calibration optical character recognition texture extraction synthesis paper propose method estimate rectification homography world plane pertain single view contain image circle general position method presuppose circle radius camerau0027s intrinsics planeu0027s vanish line know simultaneously accommodate multiple circle comparative experimental result demonstrate approachu0027s efficacyn',\n",
       " 'Nathan Jacobs Ryan Baltenberger Menghua Zhai Connor Greenwell Scott Workman n',\n",
       " 'R. Nevatia Tao Zhao Fengjun Lv selfcalibration method estimate camerau0027s intrinsic extrinsic parameter vertical line segment height present algorithm obtain need line segment detect head foot position walk human legcrossing phase describe experimental result method accurate robust respect view angle subjectsn',\n",
       " 'Tomita Ueshiba new calibration algorithm multicamera system planar reference pattern propose algorithm extension sturmmaybankzhang style planebase calibration technique use multiple camera rigid displacement camera recover intrinsic parameter capture camera model plane know reference point place location algorithm yield simple calibration mean stereo vision system arbitrary number camera maintain handiness flexibility original method algorithm base factorization homography matrix model image plane camera plane parameter compensate indetermination scale factor homography matrix rescale double eigenvalue planar homology define view model plane obtain parameter finally refine nonlinear maximum likelihood estimation mle process validity propose technique verify simulation experiment real datan',\n",
       " 'Fuqing Duan Liang Wang camera calibration necessary step extract 3d information 2d image 1d object easy construct selfocclusion 1d calibration propose zhang receive attention progress 1d calibration mainly focus reduce restriction 1d objectu0027s movement calibration accuracy demand improvement paper computational model 1d calibration reformulate noise 1d calibration analyze model heteroscedastic errorinvariables modelbased 1d calibration algorithm propose comparison exit algorithm propose algorithm advantage high accuracy small number measurement rapid convergence weak insensitivity initial condition experiment synthetic real image data validate propose algorithmn',\n",
       " 'H. Foroosh Xiaochun Cao paper propose novel method camera calibration image mirror symmetric object assume unit aspect ratio zero skew interimage homographies express function principal point minimize symmetric transfer error obtain accurate solution camera parameter extend approach calibration technique image 1d object fix pivot point unlike exist method rely orthogonality polepolar relationship approach utilize new interimage constraint require knowledge 3d coordinate feature point demonstrate effectiveness approach present result synthetic real imagen',\n",
       " 'Robert Pless Ian Schillebeeckx consider problem camera pose estimation scenario camera continuous unknown change focal length understand frame frame change camera focal length vital accurately estimate camera pose vital accurately render virtual object scene correct perspective approach camera calibration require geometric constraint frame observation 3d calibration object xe2x80x94 feasible augment reality set paper introduce calibration object base flat lenticular array create color cod lightfield observe color change depend angle view derive approach estimate focal length camera relative pose object single image characterize performance camera calibration focal length camera model demonstrate advantage focal length estimation render virtual object video constant zoomingn',\n",
       " 'A. Shashua S. Avidan consider problem reconstruct 3d coordinate move point see monocular move camera reconstruct move object lineofsight measurement task feasible constraint place shape trajectory move point coin family task trajectory triangulation investigate solution point move straightline conicsection trajectory point move straight line parameter line 3d position point time instant uniquely recover linear method view case conicshaped trajectory generally view sufficient unique reconstruction move point few view conic know type like circle 3d euclidean space seven view sufficient paradigm trajectory triangulation general push envelope process dynamic scene forward static scene particular case general task reconstruct scene rich move object object single pointn',\n",
       " 'Zijian Zhao focus recover 2d euclidean structure view projection n parallel conic paper work denote conic dual absolute point general form conic dual circular point encode euclidean structure recover circular pointenvelope useful information euclidean structure rely fact line infinity symmetric axis recover provide solution recover line deduce constraint recover conic dual circular point apply camera calibration work relax problem condition give general framework past experiment simulate real data carry validity propose algorithm especially method apply endoscope operation calibrate camera track surgical tool main interestpoint pay attention ton',\n",
       " 'James H. Elder Eduardo R. CorralSoto typical method camera calibration image rectification single view assume existence straight parallel line vanish point compute orthogonal structure know exist scene practical situation assumption apply single family parallel line grind plane insufficient information recover complete rectification study generalization method scene know contain parallel curve method base establish association pair correspond point lie image projection curve method compute leastsquares estimate focal length camera pose tangent line associate point allow complete rectification image evaluate method highway sport track imagery demonstrate accuracy relative stateoftheart vanish point methodn',\n",
       " 'Rama Chellappa Feng Guo paper present method video mensuration single stationary camera problem address simple mensuration arbitrary line segment reference plane multiple frame minimal calibration unlike previous solution base planar rectification approach base fit image multiple concentric circle plane propose method aim minimize error mensuration calculate mensuration line segment lie reference plane algorithm detect track wheel automobile implement fully automatic wheel base mensuration mensuration result accurate determine vehicle class furthermore measure line segment point vehicle plot viewsn',\n",
       " 'Yiuming Cheung Hui Zhang Haifei Huang projective geometry common selfpolar triangle discus position relationship planar conic research property common selfpolar triangle especially planar conic special conic paper explore property common selfpolar triangle conic happen concentric circle exist infinite common selfpolar triangle concentric circle provide method locate vertex triangle investigate triangle encode important property triangle share common vertex opposite common vertex lie line circle center line infinity support plane second triangle right triangle base property image circle center varnish line support plane recover simultaneously conjugate pair vanish line obtain allow induce good constraint image absolute conic evaluate calibration algorithm accurate result achieve main contribution paper initiate new perspective look circlebase camera calibration problem believe calibration method different circle pattern benefit perspective especially pattern involve circlen',\n",
       " 'Hongbin Zha Ren Ren Kun Peng Xianghua Ying catadioptric consist pinhole camera planar mirror deeply investigate paper mirror combine form corner facetoface pinhole relative pose unknown object reflect mirror corner onetime multipletimes pinhole image contain object reflection simultaneously image multiple view object single camera discover 3d point reflection lie circle point set compose 3d point reflection reflection point group rpg circle relate rpg call rpg circle rpg circle parallel furthermore rpg partition separate subgroup shape form point subgroup invariant respect location 3d point geometric property calibration approach utilize base parallel circle 2d homographies invariant shape experiment validate approachn',\n",
       " 'K. Ikeuchi P. Vasseur C. Demonceaux In So Kweon Hongdong Li J. Bazin data correspondencegrouping unknown parametric model fundamental topic vision find feature correspondence image probably popular application research field main motivation work key ingredient wide range vision task include threedimensional reconstruction object recognition exist feature correspondence method base local appearance similarity global geometric consistency combination heuristic manner method fully satisfactory especially presence repetitive image texture mismatch paper present new algorithm combine benefit appearancebase geometrybase method mathematically guarantee global optimization algorithm accept set feature extract image input output feature correspondence large number inliers verify appearance similarity geometric constraint specifically formulate problem mix integer program solve efficiently series linear program branchandbound procedure subsequently generalize framework context data correspondencegrouping unknown parametric model apply certain class vision problem algorithm validate successfully synthesize data challenge real imagen',\n",
       " 'Hongbin Zha Jing Kong Sheng Guan Yongbo Hou Kun Peng Xianghua Ying object interreflect planar mirror image contain object multiple reflection simultaneously image multiple view object single pinhole camera paper emphasize problem recover intrinsic extrinsic parameter camera multiple silhouette single image view pair view single image divide kind relationship view pair reflect mirror real virtual circular motion epipoles kind pair easily determine intersection common tangent line silhouette base projective property epipoles efficient method propose recover image circular point include angle mirror epipoles second kind pair recover simultaneously projection intersection line mirror solve simple 1d optimization problem consistency constraint epipolar tangent line fundamental matrix view single image recover estimate intrinsic extrinsic parameter camera euclidean reconstruction obtain experiment validate propose approachn',\n",
       " 'Long Quan HungTat Tsui Guang Jiang circular motion single axis motion widely vision graphic 3d model acquisition paper describe new simple method recover geometry uncalibrated circular motion minimal set point image problem previously solve nonminimal data compute fundamental matrix trifocal tensor image fit conic track point image establish set track point different image circular motion distinct space point relate homography compute plane homography minimal point image unique pair complex conjugate eigenvectors homography image circular point parallel plane circular motion subsequently motion structure parameter compute homography straightforward manner experiment real image sequence demonstrate simplicity accuracy robustness new methodn',\n",
       " 'YiuMing Cheung Hui Zhang Haifei Huang avoid ambiguity challenge problem conicbase homography estimation paper address problem homography estimation separate ellipsis ellipsis unique common selfpolar triangle provide line correspondence furthermore investigate location feature common selfpolar triangle vertex triangle lie outside ellipsis vertex lie inside ellipsis separately accordingly line correspondence obtain intersection conic common selfpolar triangle line correspondence obtain base common selfpolar triangle provide constraint homography estimation main contribution paper include 1 new discovery location feature common selfpolar triangle separate ellipsis 2 novel approach homography estimation simulate experiment real experiment conduct demonstrate feasibility accuracy approachn',\n",
       " 'Yiuming Cheung Hui Zhang Haifei Huang paper investigate property common selfpolar triangle separate coplanar circle apply camera calibration separate circle unique common selfpolar triangle particular vertex common selfpolar triangle lie line infinity give separate circle line infinity recover vertex common selfpolar triangle accordingly vanish line support plane obtain image allow recover image circular point provide good constraint image absolute conic compare previous calibration method separate circle approach avoid solve quartic equation cause numerical instability application test calibration algorithm accurate result achievedn',\n",
       " 'A. Farag M. Ahmed paper address problem calibrate camera lens distortion significant medium wide angle lens approach base analysis distort image straight line derive new distortion measure optimize nonlinear search technique best distortion parameter straighten line unlike exist approach use measure fast closedform solution distortion coefficient experiment evaluate performance approach synthetic real data reportedn',\n",
       " 'In So Kweon KukJin Yoon present new windowbase method correspondence search vary supportweights adjust supportweights pixel give support window base color similarity geometric proximity reduce image ambiguity method outperform local method standard stereo benchmarksn',\n",
       " 'Margrit Gelautz Carsten Rother Michael Bleyer Asmaa Hosni Christoph Rhemann vision task formulate label problem desire solution spatially smooth label label transition align color edge input image solution efficiently achieve smooth label cost fast edge preserve filter paper propose generic simple framework comprise step construct cost volume ii fast cost volume filter iii winnertakeall label selection main contribution simple framework stateoftheart result achieve vision application particular achieve disparity map realtime quality exceed fast local approach middlebury stereo benchmark ii optical flow field fine structure large displacement demonstrate robustness parameter framework set nearly identical value application competitive result interactive image segmentation present work hope inspire researcher leverage framework application areasn',\n",
       " 'Terence Sim Shaojie Zhuo paper address challenge problem recover defocus map single image present simple effective approach estimate spatially vary defocus blur edge location input defocused image reblurred gaussian kernel defocus blur obtain ratio gradient input reblurred image propagate blur edge location entire image defocus map obtain experimental result synthetic real image demonstrate effectiveness method provide reliable estimation defocus mapn',\n",
       " 'David Gallup Fisher Yu discover 3d reconstruction achieve asingle photographic capture accidental motion thephotographer attempt hold camera motion result little baseline high depth uncertainty theory combine measurement duration capture process second achieve usable depth estimate wepresent novel 3d reconstruction tailor problemthat produce depth map short video sequence standard cameraswithout need multilens optic active sensor intentionalmotion photographer result lead possibilitythat depth map sufficient quality rgbd photography application likeperspective change simulate aperture object segmentation cancome free significant fraction photographsunder reasonable conditionsn',\n",
       " 'Anselm Grundhofer Thabo Beeler Mahdi Abbaspour Tehrani calibrate intrinsic property camera fundamental task require variety vision image process task precise measurement focal length location principal point distortion parameter lens crucial example 3d reconstruction 27 variety method exist achieve goal cumbersome carry require substantial manual interaction expert knowledge significant operate volume propose novel calibration method base usage directionally encode light ray estimate intrinsic parameter enable fully automatic calibration small device mount close lens element enable accuracy comparable standard method lens focus infinity method overcome mention limitation guarantee accurate calibration human intervention require limit space approach allow estimate distance focal plane size aperture demonstrate advantage propose method evaluate cameralens configuration prototypical devicesn',\n",
       " 'Rafael Grompone Gioi Pierre Gurdjos Viorica PazTrazUcean propose combine line segment elliptical arc detector formally guarantee control number false positive require parameter tune accuracy detect elliptical feature improve novel noniterative ellipse fit technique merge algebraic distance gradient orientation performance detector evaluate computergenerated image natural imagen',\n",
       " 'M K Leung D Rajan S Rahardja A YS Chia novel ellipse detector base edge follow propose paper detector model edge connectivity line segment exploit line segment construct set ellipticalarcs disconnect ellipticalarcs ellipse identify group incrementally find optimal pair ellipticalarcs extract hypothetical ellipsis image fit ellipse ellipticalarcs group finally feedback loop develop sieve low confidence hypothetical ellipsis regenerate good set hypothetical ellipsis aspect propose algorithm perform selfcorrection home xe2x80x9cdifficultxe2x80x9d ellipsis detail evaluation synthetic image show algorithm outperform exist method substantially term recall precision score scenario image clutter saltandpepper noise partial occlusion additionally apply detector set challenge realworld image successful detection ellipsis present image demonstrate aware work detect ellipsis difficult image work present significant contribution ellipse detectionn',\n",
       " 'G. Randall J.M. Morel J. Jakubowicz R.G. von Gioi propose lineartime line segment detector give accurate result control number false detection require parameter tune algorithm test compare stateoftheart algorithm wide set natural imagesn',\n",
       " 'Pushmeet Kohli Victor Lempitsky Olga Barinova detect multiple object method base hough transform use nonmaxima supression mode seek order locate distinguish peak hough image postprocessing require tune extra parameter fragile especially object tend closely locate paper develop new probabilistic framework way relate hough transform share simplicity wide applicability time framework bypass problem multiple peak identification hough image permit detection multiple object invoke nonmaximum suppression heuristic result experiment demonstrate significant improvement detection accuracy classical task straight line detection modern categorylevel pedestrian detection problemn',\n",
       " 'SiuYeung Cho Maylor K. H. Leung Dilip K. Prasad paper propose novel ellipse detection method real image propose method u information edge curvature convexity relation edge contour clue identify edge contour group search region compute edge contour contain edge contour eligible group current edge contour twodimensional hough transform perform intermediate step use new u0027relationship scoreu0027 rank edge contour group instead conventional histogram count score selective efficient addition use novel saliency criterion nonheuristic consider aspect quantify goodness detect elliptic hypothesis finally select good elliptic hypothesis threshold selection elliptic hypothesis determine detect hypothesis selection free human intervention method require second case suitable practical application performance propose ellipse detection method test dataset contain 1200 synthetic image caltech 256 dataset contain real image case result propose ellipse detection method perform far good exist method close ideal result precision recall fmeasure close 1 method robust increase complexity image overlap ellipsis occlude ellipsis performance contemporary method deteriorate significantlyn',\n",
       " 'R. Dahyot standard hough transform popular method image process traditionally estimate histogram density model histogram high dimensional space andor observation sparse highly demand memory paper propose extend formulation continuous kernel estimate second dependency variable take account estimate density robust noise insensitive choice origin spatial coordinate finally new statistical framework unsupervised need parameter automatically estimate flexible prior easily attach observation experimentally new model encode good alignment content imagesn',\n",
       " 'J. Zerubia X. Descombes G. Perrin work present framework extract tree crown remotely sense data especially plantation image stochastic geometry aim find tree position tree crown diameter distribution approach consist consider image realization mark point process model tree plantation configuration unknown number ellipsis bayesian energy define contain prior energy incorporate prior knowledge plantation geometric property likelihood fit object data eventually estimate global minimum energy reversible jump markov chain monte carlo dynamic simulate anneal scheme present result optical aerial image poplar provide ifnn',\n",
       " 'IMing Chen Dilip K. Prasad Huixu Dong abstract accurate ellipse detection image stream realtime execution open challenge present novel fast robust ellipse detection method method adopt arc selection smart group repeat utilization gradient information significantly reduce computation need compromise detection effectiveness geometric property calculable computation arc smoothness relative placement curve region confidence ellipse centre utilize purpose exhaustive sensitivity analysis methodu0027s control parameter perform reveal range value support consistent performance diverse challenge datasets complex background multiple differently size ellipsis occlude overlap ellipsis methodu0027s performance compare stateoftheart detector diverse datasets test method propose method demonstrate best balance detection effectiveness best second best fmeasure score computation time u003e40xe2x80xafhz datasetsn',\n",
       " 'Sebastien Roy Peter Sturm JeanPhilippe Tardif present new approach selfcalibrate distortion function distortion center camera general radially symmetric distortion contrast current model propose model encompass fisheye lens catadioptric camera view angle large 45representing distortion image displacement model vary focal length function distance distortion center function discretized act general model represent polynomial present 88two flexible approach calibrate distortion function plumblinetype method image line pattern formulate linear constraint distortion function parameter linear solve unknown scale factor global focal length sufficient image rectification second approach base perform selfcalibration image textured planar object unknown structure restrict camera motion selfcalibration possible image completely unknown nonplanar 183analysis rectify image obtain compute distortion function show good result compare approach model rely nonlinear optimizationn',\n",
       " 'Aly A. Farag Moumen Taha ElMelegy paper address problem calibrate camera lens distortion significant medium wide angle lens exist nonmetric distortion calibration method need user involvement form anotherwe present approach distortion calibration base robust theleastmedianofsquares lmeds estimator approach able proceed ful lyautomatic manner sensitive erroneous input data image curve mistakenly consider projection 3d linear segment approach uniquely u fast closedform solution distortion coefficient serve initial point nonlinear optimization algorithm straighten image line propose method distortion model selection base geometrical inferencesuccessful experiment evaluate performance approach synthetic real data reportedn',\n",
       " 'T. Hanning S. Graf camera calibration algorithm consider parameter define lens distortion map usual way determine parameter minimize nonlinear optimization problem final step calibration algorithm neglect fact give projection matrix calibration data parameter radial distortion determine analytically article problem solve close form present algorithm compare classic approachesn',\n",
       " 'M. Fiala fiducial marker system consist pattern mount environment automatically detect digital camera image accompany detection algorithm useful augment reality ar robot navigation general application relative pose camera object require important parameter marker system false detection rate false positive rate intermarker confusion rate minimal detection size pixel immunity light variation artag marker u digital cod theory low false positive intermarker confusion rate small require marker size employ edge link method robust light variation immunity artag marker bitonal planar pattern contain unique id number encode robust digital technique checksum forward error correction fec propose new artag low numerically quantifiable error rate require grey scale threshold marker system encode 2002 different unique idu0027s need store pattern experimental result show validate systemn',\n",
       " 'C. Schmid K. Mikolajczyk paper compare performance descriptor compute local region example extract harrisaffine detector mikolajczyk k schmid c 2004 different descriptor propose literature unclear descriptor appropriate performance depend region detector descriptor distinctive time robust change view condition error detector evaluation u criterion recall respect precision carry different image transformation compare shape context belongie s et al april 2002 steerable filter freeman w adelson e setp 1991 pcasift ke y sukthankar r 2004 differential invariant koenderink j van doorn 1987 spin image lazebnik s et al 2003 sift lowe d g 1999 complex filter schaffalitzky f zisserman 2002 moment invariant van gool l et al 1996 crosscorrelation different type region propose extension sift descriptor outperform original method furthermore observe rank descriptor independent region detector siftbased descriptor perform best moment steerable filter best performance low dimensional descriptorn',\n",
       " 'Jihong Pei Xia Li Fanyang Meng feature point match fundamental challenge problem vision application paper robust feature point match algorithm name spatial order constraint bilateralneighbor vote socbv propose remove outlier set match include outlier image direct k near neighbor knn graph match set generate problem feature point match formulate binary discrimination problem discrimination process class label matrix build spatial order constraint define edge connect point knn posterior inlier class probability match estimate knn density estimation spatial order constraint vote match determine average posterior class probability originate associative inliers set remove outlier algorithm iteratively remove outlier direct graph recomputes vote stop condition satisfy compare popular algorithm ransac rsoc gtm soc wgtm experiment test data set demonstrate strong robustness propose algorithmn',\n",
       " 'P. Saeedi M. Izadi paper present automatic point match algorithm establish accurate match correspondence image propose algorithm utilize group feature point explore geometrical relationship graph arrangement algorithm start set match include outlier image set nondirectional graph generate feature k near match choose initial set angular distance edge connect feature point k near neighbor graph algorithm find graph second image similar graph case graph include outlier algorithm remove outlier accord strength graph reevaluate angle graph match discard simple intuitive robust algorithm inspire previous work experimental result demonstrate superior performance algorithm condition rigid nonrigid transformation ambiguity partial occlusion match correspondence multiplicity scale large view variationn',\n",
       " 'M.J. MarinJimenez F.J. MadridCuevas R. MunozSalinas S. GarridoJurado paper present fiducial marker specially appropriate camera pose estimation application augment reality robot localization main contribution present propose algorithm generate configurable marker dictionary size number bitee follow criterion maximize intermarker distance number bite transition process derive maximum theoretical intermarker distance dictionary square binary marker second method automatically detect marker correct possible error propose solution occlusion problem augment reality application show aim multiple marker combine occlusion mask calculate color segmentation experiment conduct proposal obtain dictionary high intermarker distance low false negative rate stateoftheart system provide effective solution occlusion problem highlightswe propose algorithm generate configurable marker dictionarywe derive maximum theoretical intermarker distancea method automatically detect marker correct error proposea solution occlusion problem augment reality application shown',\n",
       " 'Mark Fiala fiducial marker artificial landmark add scene facilitate locate point correspondence image image know model reliable fiducials solve point detection match problem add marker convenient proper design fiducials associate vision algorithm detect enable accurate pose detection application range augment reality input device hci robot navigation marker system typically stage hypothesis generation unique image feature verificationidentification set criterion high robustness practical use identify optimize produce artag fiducial marker edgebased method robust light partial occlusion hypothesis stage reliable digital cod identification verification stage design criterion large gain performance achieve artag conventional ad hoc designsn',\n",
       " 'Xiantong Zhen Ling Shao Jun Tang find correspondence relate feature point set basic task vision pattern recognition paper present novel method point pattern match spectral graph analysis particular aim render spectral match algorithm robust positional jitter outlier local structural descriptor spectral context propose attribute domain point set fundamentally different previous method furthermore approximate distance order define employ metric geometric consistency neighbor point work combine novel ingredient formulate feature point set match optimization problem onetoone constraint correspondence obtain maximize give objective function technique probabilistic relaxation comparative experiment conduct synthetic real data demonstrate effectiveness propose method especially presence positional jitter outlier novel spectral method propose problem point pattern matchwe propose local structural descriptor spectral graph analysiswe define metric geometric consistency quantity ordern',\n",
       " 'C. P. Winlove James Bell Richard Everson Jacqueline Christmas introduce novel bayesian inexact point pattern match model assume linear transformation relate set point match problem inexact lack onetoone correspondence point set presence noise algorithm inexact use variational bayesian approximation estimate posterior distribution face problematic evidence term method turn similar structure iterative close point algorithmn',\n",
       " 'ZhiYong Liu Hong Qiao Xu Yang correspondence point set fundamental problem pattern recognition formulate solve graph match paper propose solve correspondence problem new order graph match algorithm compare previous hypergraph match algorithm propose achieve considerable memory reduction applicable undirect direct graph specifically correspondence formulate match adjacency tensor encode order structural information graph transform tractable matrix form type gradient base optimization method graduate nonconvexity concavity procedure gnccp graduate assignment ga algorithm generalize solve problem comparative experiment stateoftheart algorithm synthetic real data witness effectiveness propose method highlightsan adjacency tensor base order graph match algorithm proposeit enjoy low storage complexity affinity tensor base high order algorithminstead spectral decomposition base optimization adopt gradient base optimizationexperiment synthetic realworld data witness stateoftheart performancen',\n",
       " 'R. MedinaCarnicer Enrique YeguasBolivar Manuel J. MarinJimenez Rafael MunozSalinas abstract square planar marker popular tool fast accurate robust camera localization use frequently limit single marker small set relative pose know map localization large set planar marker scarcely treat problem favour keypointbased approach keypoint detector robust rapid motion large change viewpoint significant change appearance fiducial marker robustly detect wide range condition paper propose novel method simultaneously solve problem map localization set square planar marker quiver pairwise relative marker pose create initial pose graph obtain pose graph contain small pairwise pose error propagate lead large error distribute rotational translational error basis cycle graph obtain correct pose graph finally perform global pose optimization minimize reprojection error planar marker observe frame experiment conduct method perform good structure motion visual slam techniquesn',\n",
       " 'Hongdong Li paper present simple practical solution 6point 2view focallength estimation problem base hiddenvariable technique derive 15th degree polynomial unknown focallength course simple constructive algorithm establish use multiple redundant measurement select best solution suggest kernelvoting scheme algorithm test synthetic data real image satisfactory result obtain case reference purpose include matlab implementation paper concise consist 20 line code result paper small useful module vision systemsn',\n",
       " 'K. Astrom K. Josephson M. Byrod paper present technique improve numerical stability grobner basis solver polynomial equation recently grobner basis method successfully solve polynomial equation arise global optimization view triangulation important minimal case structure motion method work extremely problem reasonably low degree involve variable currently limit factor method large demand problem numerical difficulty paper change basis quotient space rxi propose strategy select basis improve condition crucial elimination step ii use technique devise grobner basis improve precision iii solve eigenvalue instead eigenvectors improve precision retain speed study method late report u grobner basis method demonstrate dramatically improve numerical precision new technique make possible solve large class problem previouslyn',\n",
       " 'Gerd Hirzinger Klaus H. Strobl paper present novel approach camera calibration improve final accuracy respect standard method precision planar target inaccurate unmeasured roughly planar target work build recent trend camera calibration concurrent optimization scene structure intrinsic camera parameter 4 8 1 novel formulation present allow maximum likelihood estimation case inaccurate target extend camera extrinsic parameter tight parametrization scene structure furthermore observe special characteristic multiview perspective projection planar target natural extension stereo camera calibration handeye calibration present experiment demonstrate improvement parametrization camera model eventual stereo reconstructionn',\n",
       " 'W.F. Sze H. Zhong Y.S. Hung F. Mai paper present hierarchical approach fast robust ellipse extraction image low level image describe set edge pixel line segment extract line segment potential candidate elliptic arc link form arc segment accord connectivity curvature relation arc segment belong ellipse group finally robust statistical method ransac apply fit ellipsis method need high dimensional parameter space like hough transform base algorithm reduce computation memory requirement experiment synthetic real image demonstrate propose method excellent performance handle occlusion overlap ellipsisn',\n",
       " 'Fugen Zhou Changming Sun Xiangzhi Bai new touch cell split algorithm base concave point ellipse fit propose paper algorithm include part contour preprocess ellipse process purpose contour preprocess smooth fluctuation contour concave point contour divide contour different segment concave point purpose ellipse process process different segment contour possible single cell property fit ellipsis concave point divide contour touch cell different segment different segment single cell similar property ellipse process separate touch cell ellipse fit paper demonstrate new way ellipse fit split binary contour touch cell experimental result algorithm efficientn',\n",
       " 'Michael Kemp Richard Yi Da Xu paper seek fit model specify term connect ellipsis image silhouette algorithm attempt problem sensitive initial guess converge wrong solution attempt minimize objective function entire ellipse structure step present algorithm overcome issue step temporarily ignore connection refine initial guess unconstrained expectationmaximization em mixture gaussian density ellipsis reconnect linearly lastly apply levenbergmarquardt algorithm finetune ellipse shape best align contour fit achieve hierarchical manner base joint model experiment algorithm robustly fit complex ellipse structure correspond shape applicationsn',\n",
       " 'Simone Gasparini Carsten Griwodz Pierre Gurdjos Lilian Calvet fiducial marker ensure reliable detection identification planar feature image fiducials wide range application especially reliable visual reference need track camera clutter textureless environment marker design application robust partial occlusion vary distance angle view fast camera motion paper present robust highly accurate fiducial marker consist concentric ring theoretical foundation rely projective property allow robustly localize image marker accurately detect position image common circle center demonstrate detect accurately localize circular fiducials challenge condition experimental result reveal outperform recent fiducial systemsn',\n",
       " 'Tie Qiu Lianbo Song Zhongxuan Luo Xin Fan Qi Jia detect elliptical object image central task robot navigation industrial diagnosis detection time critical issue exist method hardly applicable realtime scenario limit hardware resource huge number fragment candidate edge arc fit ellipse equation paper present fast algorithm detect ellipsis high accuracy algorithm leverage newly develop projective invariant significantly prune undesired candidate pick elliptical one invariant able reflect intrinsic geometry planar curve give value xe2x88x921 collinear point 1 point ellipse apply prune pick simply compare binary value calculation invariant involve determinant 3times 3 matrix extensive experiment challenge data set 648 image demonstrate detector run 20xe2x80x9350 fast stateoftheart algorithm comparable high precisionn',\n",
       " 'Yun Fu Ming Shao Wanming Huang Siyu Xia Changsheng Lu circle detection fundamental object detection high accuracy localization visual control system propose novel method circle detection analyse refine arcsupport line segment key idea use line segment detector extract arcsupport line segment likely circle instead line segment couple line segment analyze form valid pair follow generate initial circle set mean shift cluster circle candidate generate verify base geometric attribute circle edge finally twice circle fit apply increase accuracy circle locate radius measure experimental result demonstrate propose method perform good know approach circle incomplete occlude blurry overillumination method show significant improvement accuracy robustness efficiency industrial print circuit board pcb image synthesize natural complicate imagen',\n",
       " 'Timotius A. Lagaunne James Tory Cobb Gbenga O. Omotara Huy N. Trinh Alina Zare Chao Chen topic model probabilistic latent semantic analysis latent dirichlet allocation lda supervise lda widely segment imagery model confine crisp segmentation force visual word image patch belong topic image region assign crisp categorical label transition region foggy sky grind sand water beach case visual word best represent partial membership multiple topic address present partial membership lda pmlda model associate parameter estimation algorithm model useful imagery visual word mixture multiple topic experimental result visual sonar imagery pmlda produce crisp soft semantic image segmentation capability previous topic model method haven',\n",
       " 'J. Malik D. Tal C. Fowlkes D. Martin paper present database contain u0027ground truthu0027 segmentation produce human image wide variety natural scene define error measure quantify consistency segmentation differ granularity different human segmentation image highly consistent use dataset demonstrate application 1 evaluate performance segmentation algorithm 2 measure probability distribution associate gestalt group factor statistic image region propertiesn',\n",
       " 'I. Pitas M. Krinidis paper present new approach segmentation color textured image base novel energy function propose energy function express local smoothness image area derive exploit intermediate step modal analysis utilize order analyze deformation 3d deformable surface model external force attract 3d deformable surface model combine intensity image pixel spatial information local image region propose image segmentation algorithm step color quantization scheme base node displacement deformable surface model utilize order decrease number color image propose energy function criterion region grow algorithm final segmentation image derive region merge approach propose method apply berkeley segmentation database obtain result good segmentation robustness compare state art image segmentation algorithmsn',\n",
       " 'King Tai Leung Wing Yan Lee Kwok Wai Yu Shu Yan Lam Siu Kai Choy fuzzy generalize gaussian density ggd segmentation model proposea ggdbase agglomerative fuzzy algorithm developeda twostage fuzzy ggd segmentation algorithm propose paper propose novel fuzzy modelbase unsupervised learn algorithm boundary correction image segmentation propose fuzzy generalize gaussian density ggd segmentation model ggdbase agglomerative fuzzy algorithm group image pixel merit algorithm sensitive initial parameter number group estimate validation technique minimize objective function model define dissimilarity measure base kullbackleibler divergence ggds compute discrepancy ggds space generalize probability distribution effectively segment image texture propose twostage fuzzy ggd segmentation algorithm stage adopt propose fuzzy algorithm obtain initial segmentation second stage improve initial segmentation image boundary correction experimental result propose method promise performance compare exist approachesn',\n",
       " 'Yuan Yan Tang Zhaowei Shang Anyong Qin Taiping Zhang Jinyu Tian paper propose new cluster method image call distribution preserve index dpi aim low dimensional semantic space approximate original image space sense preserve distribution data theory intrinsic structure data cluster describe distribution data effectively cluster structure data low dimensional semantic space derive dpi clear unlike distancebased cluster method reveal intrinsic euclidean structure data method attempt discover intrinsic cluster structure data space actually union submanifolds propose revise kernel density estimator case highdimensional data crucial step dpi addition provide theoretical analysis bind method finally extensive experiment compare algorithm coil20 cbcl mnist demonstrate effectiveness propose approachn',\n",
       " 'Yuchen Zeng Jixiang Du Wentao Fan Can Hu abstract paper novel bayian statistical approach propose tackle problem natural image segmentation propose approach base finite dirichlet mixture model contextual proportion probability class label model spatial smoothness constraint major merit approach summarize follow firstly exploit dirichlet mixture model obtain good statistical performance commonly mixture model gaussian mixture model especially proportional data normalize histogram secondly explicitly model mix contextual proportion probability vector simultaneously integrate spatial relationship pixel dirichlet mixture model result robust framework image segmentation finally develop variational bay learn method update parameter closedform expression effectiveness propose approach compare mixture modelingbase image segmentation approach extensive experiment involve simulate natural color imagesn',\n",
       " 'Na Li Rong Lan Chang Wen Chen Jiulun Fan Hanqiang Liu Feng Zhao abstract intuitionistic fuzzy set useful tool handle uncertainty data order deal uncertainty image overcome sensitivity image noise multiobjective evolutionary intuitionistic fuzzy cluster algorithm multiple image spatial information moeifcmsi propose perform image segmentation paper important innovation method list follow 1 intuitionistic fuzzy set image construct generalize fuzzy complement function 2 intuitionistic fuzzy set image utilize compute fitness function fuzzy evaluation index select optimal solution 3 kind complementary image spatial information introduce fitness function fuzzy evaluation index propose method robust image noise 4 realcoded variable string length technique utilize encode cluster center automatically determine number cluster experimental result synthetic berkeley magnetic resonance mr image propose method outperform stateoftheart method noise robustness segmentation performancen',\n",
       " 'Feiping Nie Kai Xiong Junwei Han Jinglin Xu partitionbased cluster algorithm like kmeans fuzzy kmeans widely successfully data mine past decade paper present robust sparse fuzzy kmeans cluster algorithm extension standard fuzzy kmeans algorithm incorporate robust function square data fit term handle outlier importantly combine concept sparseness new algorithm introduce penalty term objectclusters membership sample suitable sparseness experimental result benchmark datasets demonstrate propose algorithm ensure robustness soft cluster algorithm real world application avoid performance degradation consider membership sparsityn',\n",
       " 'Feiping Nie Deanna Needell Yanni Xiao Yicang Zhou Tong Wu abstract paper firstly propose novel strategy fuse kmeans fuzzy cmeans objective function multiobjective problem unify model extreme difficulty choose optimal level cluster fuzziness avoid level cluster fuzziness vanish accordingly fuse kmeans fuzzy cmeans cluster fkmfcm problem achieve brand new form base propose fkmfcm novel degree membership derive close form additionally penalty problem introduce propose fkmfcm fuzzy cluster centroid modify segregate consequently fkmfcm modify cluster centroid fkmfcmmcc problem represent biobjective optimization efficient cluster address propose fkmfcmmcc problem original characteristic function introduce correspond algorithm converge global optimum propose fkmfcmmcc problem eventually theoretical analysis empirical result provide validate effectiveness propose fkmfcmmcc approachn',\n",
       " 'Yake Zhang Junhu Xie Yuanhao Cui Puhua Chen Rongfang Wang Shuyuan Yang Fang Liu Licheng Jiao Jing Gu new random subspace base joint sparse representation model proposethe sparse representation multiple subspace integrate ensemble sparse representationthe spectral cluster algorithm random subspace base ensemble sparse representation shownthe semisupervised classification method random subspace base ensemble sparse representation describe paper new random subspace base ensemble sparse representation rsesr algorithm propose random subspace introduce sparse representation model highdimensional data random subspace method reduce dimension data use effective information data like traditional dimensionality reduction method lose information original data additionally joint sparse representation model emloyed obtain sparse representation sample set low dimensional random subspace sparse representation multiple random subspace integrate ensemble sparse representation obtain rsesr apply classical cluster semisupervised classification experimental result different realworld data set superiority rsesr traditional methodn',\n",
       " 'Aimin Wang Yun Zhang Guoying Liu fuzzy cmeans fcm cluster spatial constraint attract great attention field image segmentation popular technique fail resolve misclassification problem inaccuracy spatial model paper present new unsupervised fcmbased image segmentation method pay close attention selection local information method regionlevel local information incorporate fuzzy cluster procedure adaptively control range strength interactive pixel novel dissimilarity function establish combine regionbased pixelbased distance function order enhance relationship pixel similar local characteristic second novel prior probability function develop integrate difference neighbor region mean template fuzzy membership function adaptively select local spatial constraint tradeoff weight depend pixel belong homogeneous region incorporate regionbased information spatial constraint propose method strengthen interaction pixel region prevent smooth region boundary experimental result synthetic noise image natural color image synthetic aperture radar image propose method achieve accurate segmentation result compare stateoftheart image segmentation methodsn',\n",
       " 'Thanh Minh Nguyen Q. M. Jonathan Wu Hui Zhang fuzzy cmeans fcm consider effective algorithm image segmentation lack sufficient robustness image noise paper propose simple effective method traditional fcm robust noise help generalize mean traditional fcm consider linear combination membership distance function expression mathematical formula propose generalize fcm gfcm generate apply generalize mean item impose generalize mean membership incorporate local spatial information cluster information distance function incorporate local spatial information observation information image intensity value gfcm robust image noise spatial constraint generalize mean performance propose algorithm compare stateoftheart technology include modify fcm hmrf hybrid model demonstrate improve robustness effectivenessn',\n",
       " 'Jiulun Fan Xiaobin Zhi Haiyan Yu fuzzy partition entropybase method effective way image segmentation paper segmentation method base weak fuzzy partition present firstly propose method construct generalize fuzzy complement construct generalize fuzzy complement operator nice property parameter optimization real application onedimensional 1d weak fuzzy partition twodimensional 2d weak fuzzy partition obtain cartesian product 1d fuzzy partition define propose generalize fuzzy complement concept weak fuzzy partition entropybase image segmentation method propose method describe 1d 2d case model 1d 2d histogram 2d approach allow ensure spatial regularity fuzzy classification finally nest optimization method develop base improve uniformity measure search optimal threshold image segmentation method empirical result propose weak fuzzy partition entropybase method capable achieve good segmentation result stateoftheart method base base fuzzy entropy propose 2d weak fuzzy partition entropybase method especially effective noisy imagesn',\n",
       " 'Jingjing Ma Zhiqiang Zhou Maoguo Gong paper present unsupervised distributionfree change detection approach synthetic aperture radar sar image base image fusion strategy novel fuzzy cluster algorithm image fusion technique introduce generate difference image complementary information meanratio image logratio image order restrain background information enhance information change region fuse difference image wavelet fusion rule base average operator minimum local area energy choose fuse wavelet coefficient lowfrequency band highfrequency band respectively reformulate fuzzy localinformation cmeans cluster algorithm propose classify change unchange region fuse difference image incorporate information spatial context novel fuzzy way purpose enhance change information reduce effect speckle noise experiment real sar image image fusion strategy integrate advantage logratio operator meanratio operator gain good performance change detection result obtain improve fuzzy cluster algorithm exhibit low error preexistencesn',\n",
       " 'Asmatullah Chaudhry Asifullah Khan Mehdi Hassan Jan Alam abstract paper robust image segmentation intelligent decision make carotid artery ultrasound image propose medical image type inherent degradation image equipment operate environment instance carotid artery ultrasound image affect low resolution speckle noise wave interference robust medical image cluster technique inevitable obtain accurate result subsequent stage context robust fuzzy radial basis function network rfrbfn technique propose propose technique modify fuzzy rbf algorithm incorporate spatial information smooth parameter objective function consequently propose technique able cope noise relate variation as effectiveness rfrbfn technique apply segment carotid artery ultrasound image performance evaluate impulse gaussian noise intensity performance comparison exist method show propose rfrbfn outperform exist fuzzy base c mean rbf technique case noisy noisefree image experiment 200 real carotid artery ultrasound image reveal propose technique offer effective segmentation result finally intimamedia thickness measure obtain segment image multilayer backpropagation neural network employ classify segment image normal diseased subject propose intelligent decision make secondary observer identification plaque carotid arteryn',\n",
       " 'Yi Liu Xiaofeng Zhang Caiming Zhang Fuhua Zheng conventional fuzzy cmeans fcm algorithm consider spatial information cluster make sensitive noise inefficient order overcome problem propose fast antinoise fcm algorithm image segmentation construct new spatial function combine pixel gray value similarity membership spatial function update membership turn obtain cluster center iteratively propose algorithm achieve desirable segmentation result iteration reduce effect noise effectively experimental result propose algorithm outperform conventional fcm extend fcm algorithmsn',\n",
       " 'Qi Feng Fang Liu Guangpu Shao Tianjiang Wang Yucheng Shu paper present novel image descriptor robust variety photometric geometric image transformation specifically robust differential circle pattern rdcp propose encode continuous intensity change circularshaped structure pixel compare pixelwise feature compute scheme rdcp capable describe relatively large local structure image descriptor construct stage propose fuzzy membershippooling algorithm capture local structure region achieve rotation invariance inherently experimental result popular datasets oxford dataset patch dataset ukbench dataset demonstrate superiority propose method stateoftheart algorithm image transformation rotation scale change viewpoint change image blur jpeg compression illumination change image noisen',\n",
       " 'Feng Zhao limitation local spatial information image fuzzy cmeans cluster algorithm local spatial information obtain satisfy segmentation performance image heavily contaminate noise order compensate drawback local spatial information effective kind nonlocal spatial information extract image paper acquisition nonlocal spatial information filter degree parameter h crucial parameter need set appropriately instead single h value pixel calculation adaptive parameter h pixel study statistical characteristic search window nonlocal spatial information obtain adaptive h value pixel call selftuning nonlocal spatial information paper novel fuzzy cluster algorithm selftuning nonlocal spatial information propose algorithm framework spatial constraint term utilize selftuning nonlocal spatial information pixel define introduce objective function fcm algorithm call fuzzy cmeans cluster algorithm selftuning nonlocal spatial information fcmsnls second algorithm framework novel gray level histogram construct selftuning nonlocal spatial information pixel cluster perform gray level histogram algorithm call fast fuzzy cmeans cluster algorithm selftuning nonlocal spatial information ffcmsnls experimental result propose method effective fuzzy cluster algorithm local spatial information noise suppression edge preservation robust fuzzy cluster algorithm nonlocal spatial informationn',\n",
       " 'Caiming Zhang Shixiang Jia semantically group pixel local neighborhood superpixel capture image redundancy significantly improve performance postprocessing algorithm paper investigate application superpixel fcm framework propose modify fcm algorithm spfcm utilize superpixel cluster object instead pixel superpixel neighborhood increase cluster granularity allow compute objective function naturally adaptive domain fix window algorithm use spatial information robust noise compact image representation base superpixel computational complexity method drastically reduce experimental result synthetic real image demonstrate effectiveness efficiency algorithmn',\n",
       " 'Hwee Kuan Lee T. Celik recent paper krinidis chatzis propose variation fuzzy cmeans algorithm image cluster local spatial graylevel information incorporate fuzzy way energy function local minimizers design energy function obtain fuzzy membership pixel cluster center propose paper show local minimizers krinidis chatzis obtain fuzzy membership cluster center iterative manner exclusively solution true local minimizers design energy function local minimizers krinidis chatzis converge correct local minimum design energy function tackle local minimum design energy functionn',\n",
       " 'Jingjing Ma Wenping Ma Jiao Shi Yan Liang Maoguo Gong paper present improve fuzzy cmeans fcm algorithm image segmentation introduce tradeoff weight fuzzy factor kernel metric tradeoff weight fuzzy factor depend space distance neighbor pixel graylevel difference simultaneously factor new algorithm accurately estimate damp extent neighbor pixel order enhance robustness noise outlier introduce kernel distance measure objective function new algorithm adaptively determine kernel parameter fast bandwidth selection rule base distance variance data point collection furthermore tradeoff weight fuzzy factor kernel distance measure parameter free experimental result synthetic real image new algorithm effective efficient relatively independent type noisen',\n",
       " 'Qiang Chen Quansen Sun Guo Cao Jinyao Liu Zexuan Ji abstract objective accurate brain tissue segmentation magnetic resonance mr image essential step quantitative brain image analysis attract extensive research attention existence noise intensity inhomogeneity brain mr image segmentation algorithm suffer limit robustness outlier oversmoothness segmentation limit segmentation accuracy image detail improve accuracy brain mr image segmentation robust spatially constrain fuzzy cmeans rscfcm algorithm propose paper method firstly novel spatial factor propose overcome impact noise image incorporate spatial information neighborhood pixel propose spatial factor construct base posterior probability prior probability take spatial direction account play role linear filter smooth restore image corrupt noise propose spatial factor fast easy implement preserve detail secondly negative logposterior utilize dissimilarity function take prior probability account improve ability identify class pixel finally overcome impact intensity inhomogeneity approximate bias field pixelbypixel level linear combination orthogonal polynomial fuzzy objective function integrate bias field estimation model overcome intensity inhomogeneity image segment brain mr image simultaneously result demonstrate performance propose algorithm image withwithout skull strip group experiment carry clinical 3tweighted brain mr image contain intensity inhomogeneity noise quantitatively compare algorithm stateoftheart segmentation approach jaccard similarity benchmark image obtain ibsr brainweb different level noise intensity inhomogeneity comparison result demonstrate propose algorithm produce high accuracy segmentation strong ability denoising especially area abundant texture detail conclusion paper rscfcm algorithm propose utilize negative logposterior dissimilarity function introduce novel factor integrate bias field estimation model fuzzy objective function algorithm successfully overcome drawback exist fcmtype cluster scheme emtype mixture model statistical result mean standard deviation jaccard similarity tissue synthetic clinical image propose algorithm overcome difficulty cause noise bias field capable improve 5 segmentation accuracy compare stateoftheart algorithmn',\n",
       " 'Chengan Guo Ailing De image segmentation view unsupervised cluster process pixel image process exist segmentation algorithm common use single pixel process unit segment image mainly base gray value information image pixel spatially structural information pixel provide important information image content order effectively exploit gray value spatial information pixel paper present adaptive image segmentation approach base vector quantization vq technique method image segment divide small subblocks subblock constitute vector vector cluster vq method implement segmentation selforganizing map som neural network adopt realize vq algorithm adaptively order resolve problem determine codebook size segment number sombase vq segmentation approach adaptive search algorithm estimate optimum codebook size develop minimize ratio withinclass scatter betweenclass scatter segmentation process experiment conduct work real brain mri image internet brain segmentation repository ibsr database comparison study algorithm stateoftheart perform experimental result evaluate subjective comparison human vision quantitative evaluation term average overlap metric show propose method outperform exist algorithmn',\n",
       " 'Sandor M. Szilagyi Laszlo Szilagyi intend achieve algorithm characterize quick convergence hard cmeans hcm fine partition fuzzy cmeans fcm suppress fuzzy cmeans sfcm cluster design augment gap high low value fuzzy membership function suppression produce modify fcm iteration create competition cluster input vector low degree membership proportionally reduce multiply previously set constant suppression rate large fuzzy membership grow maintain probabilistic constraint far treat optimal algorithm employ series application report accurate efficient cluster problem paper introduce generalize formulation suppression rule lead infinite number new cluster algorithm identify close relation sfcm cluster model socalled fcm algorithm generalize improve partition gifpfcm finally reveal constraint generalize sfcm cluster model minimize objective function gifpfcm allow suppress cluster model optimal base large numerical test perform multidimensional environment generalize form suppression prove accurate partition early solution need significantly iteration conventional fcmn',\n",
       " 'Daoqiang Zhang Songcan Chen Weiling Cai fuzzy cmeans fcm algorithm spatial constraint fcms prove effective image segmentation follow disadvantage 1 introduction local spatial information correspond objective function enhance insensitiveness noise extent lack robustness noise outlier especially absence prior knowledge noise 2 objective function exist crucial parameter balance robustness noise effectiveness preserve detail image select generally experience 3 time segment image dependent image size large size image segmentation time paper incorporate local spatial gray information novel fast robust fcm framework image segmentation fast generalize fuzzy cmeans fgfcm cluster algorithm propose fgfcm mitigate disadvantage fcms time enhance cluster performance furthermore fgfcm include exist algorithm fast fcm enhance fcm special case derive new algorithm fgfcms1 fgfcms2 propose rest paper major characteristic fgfcm 1 use new factor sij local spatial gray similarity measure aim guarantee noiseimmunity detailpreserve image remove empiricallyadjusted parameter 2 fast cluster segment image segment time dependent number graylevels q size nq image consequently computational complexity reduce onci1 oqci2 c number cluster i1 i2n',\n",
       " 'D.L. Pham novel approach fuzzy cluster image segmentation describe fuzzy cmeans objective function generalize include spatial penalty membership function penalty term lead iterative algorithm slightly different original fuzzy cmeans algorithm allow estimation spatially smooth membership function determine strength penalty term criterion base crossvalidation employ new algorithm apply simulate real magnetic resonance image show robust noise artifact standard algorithmn',\n",
       " 'BaiLin Li ShaoJie Chen Biao He Qiang Wang XiaoLiang Jiang accurate image segmentation challenge task image analysis understand fuzzy cmeans cluster fcm spatial constraint fcms effective algorithm suitable challenge fcms high computational complexity lack robustness noise outlier limit usefulness overcome difficulty local correntropybase fuzzy cmeans cluster algorithm spatial constraint lcfcms simplify model lcfcms1 propose paper utilize correntropy criterion cluster algorithm efficiently emphasize weight sample close correspond cluster center propose cluster algorithm incorporate variational level set formulation level set regularization term finally iteratively reweighted algorithm adopt solve lcfcms lcfcms1 base level set method experimental result synthetic real image superiority method term accuracy robustness segment image intensity inhomogeneity noise compare stateoftheart approachesn',\n",
       " 'Q.M. Jonathan Wu Byeungwoo Jeon Yuhui Zheng Hui Zhang Yunjie Chen accurate segmentation magnetic resonance mr image essential step quantitative brain image analysis existence intensity inhomogeneity name bias field noise mr image segmentation method suffer limit robustness hard accurate result paper propose improve anisotropic multivariate student tdistribution base hierarchical fuzzy cmeans method iamthfcm firstly improve anisotropic spatial information define neighborhood pixel propose overcome effect noise preserve information especially point repetitive pattern corner end point secondly improve anisotropic spatially information utilize negative multivariate student tdistribution base logposterior dissimilarity function improve robustness accuracy thirdly use hierarchical strategy construct flexible objective function consider improve dissimilarity function subfcm method robust accurate outlier weak edge finally intensity inhomogeneity model linear combination set orthogonal basis function parameterized coefficient orthogonal basis function objective function integrate bias field estimation make propose method estimate bias field segment image segmentation bias field estimation obtain benefit statistical result synthetic clinical image propose method overcome difficulty cause noise bias field obtain accurate result reduce effect noise preserve detailsusing multivariate tdistribution improve robustnessusing hierarchical fcm reduce effect outlierthe method obtain accurate segment result estimate bias fieldn',\n",
       " 'Shuang Wang Biao Hou Xiangrong Zhang Hongying Liu Licheng Jiao Yaoguo Zheng abstract paper novel unsupervised saliencyguided synthetic aperture radar sar image change detection method propose salient area image discriminative different area easily notice strong visual contrast local area make saliency suitable guide change detection sar image exist difference image apply saliency extraction initial difference map obtain log ratio operator saliency map obtain change area include false change pixel raise speckle noise neglect simultaneously thresholding saliency map region preserve extract region initial sar image generate difference image principal component analysis pca method extract feature local patch incorporate spatial information reduce influence isolate pixel finally k mean cluster employ obtain change map extract feature cluster class change area unchange area experimental result real simulate sar image data set demonstrate effectiveness propose methodn',\n",
       " 'Long JianWu Lv Yingda Shen XuanJing Chen Haipeng abstract field data cluster find cluster number automatically generate reliable cluster give dataset fundamental challenge task recently cluster analysis algorithm automatic identification cluster number present achieve accurate cluster result datasets complex structure unfortunately algorithm utilize hard partition approach process integration use membership information fuzzy cmeans fcm cluster result scheme integrate fcm cluster result require iteration process iterative graph partition address problem automatic fuzzy cluster algorithm propose paper combine soft partition method membership information fcm cluster result finally extensive experiment perform premise obtain accurate cluster result simultaneously propose algorithm effectively decrease number fcm cluster result process integration compare original algorithm furthermore number iteration propose scheme iterative graph partition process half original approachn',\n",
       " 'Hailun Yang Jia Liu Mingyang Zhang Puzhao Zhang Maoguo Gong Linzhi Su paper mainly introduce novel deep learn map dlm framework orient ternary change detection task information unbalance image different traditional intensitybase method available dlm framework base operation feature extract image excellent performance deep learn information representation feature learn network stack denoising autoencoder image serve feature extractor sample selection process stack map network employ obtain map function establish relationship feature class finally comparison feature final ternary map generate cluster comparison result work highlight aspect firstly previous work focus image similar property dlm framework base image different property usually encounter case secondly dlm framework base analysis feature instead superficial intensity avoid corruption unbalance information large extent parameter test datasets provide appropriate parameter set correspond experimental result demonstrate robustness effectiveness term accuracy time complexity highlightsthe problem ternary change detection information unbalance image raisedthe analysis image base inner feature intensity pixelthe stack denoising autoencoder stack map network usedn',\n",
       " 'Xi Li Xin Xu Li Chen Xiaowei Fu Chengzhen Guo microstructural information acquire image analysis cell model order obtain precise solid oxide fuel cell sofc microstructure parameter adaptive fuzzy approach develop threephase identification yszni anode optical microscopic om image new quantuminspired clique potential markov random field mrf function propose considerate spatial information fuzzy logic model space distance base weight introduce reflect influence neighborhood pixel simulate image real sofc anode om image compare effectiveness practicability propose algorithm experiment result demonstrate propose method accurately separate therephase sofc om image lay foundation subsequent microstructural parameter extractingn',\n",
       " 'Biao Hou Wenping Ma Shuyuan Yang Xu Tang Licheng Jiao Fang Liu detect change area multitemporal polarimetric synthetic aperture radar sar image paper present novel version convolutional neural network cnn name local restrict cnn lrcnn cnn convolutional layer employ change detection lrcnn form impose spatial constraint call local restriction output layer cnn train cnnlrcnn polarimetric property sar image fully instead manual label pixel preparation similarity measure polarimetric sar data propose layer difference image ldis polarimetric sar image produce ldis transform discriminative enhance ldis deldis cnnlrcnn train model deldis regression pretrain classification finetuning conduct pseudolabel pixel obtain deldis finally change detection result show change area directly generate output train cnnlrcnn relation lrcnn traditional way change detection discuss illustrate method overall point view test simulate data set real data set effectiveness lrcnn certify outperform traditional algorithm fact experimental result demonstrate propose lrcnn change detection recognize different type changeunchange data ensure noise insensitivity lose detail change arean',\n",
       " 'Biao Hou Shuyuan Yang Fang Liu Miaomiao Liang Licheng Jiao Huan Chen abstract paper fast unsupervised deep fusion framework change detection multitemporal synthetic aperture radar sar image present mainly aim generate difference image di feature learn procedure stack autoencoders saes stack autoencoders kind deep neural network learn feature map retain structural information suppress noise sar image beneficial di generation compare shallow network propose framework extract available feature favorable get good change result different common deep neural network propose method need label data train network addition subset entire sample appropriately represent dataset speed train deep neural network underfitting design fusion network structure combine ratio operator base method ensure representation high layer good low one summarize main contribution work lie deep fusion network generation di fast unsupervised way experiment real sar image confirm network perform good traditional ratio method convolutional neural networkn',\n",
       " 'Pradipta Maji Shaswati Roy abstract segmentation brain mr volume different meaningful tissue class essential prerequisite clinical analysis intensity inhomogeneity bias field present mr volume considerably degrade quality segmentation regard paper present new segmentation algorithm term color herent lo cal intensity r ough s egmentation brain mr volume corrupt bias field artifact judiciously integrate merit coherent local intensity cluster theory rough set simultaneous segmentation bias field correction brain mr volume propose algorithm partition entire image space number small overlap neighborhood region bias neighborhood region assume constant individual region objective function define coherent local intensity rough segmentation voxels near center point similar influence local objective function addition small distance center neighbor voxels yield contribution voxel propose algorithm u dualregion concept represent neighborhood structure efficiently make possible separate model voxels neighborhood accord location region consider tissue class tissue class consist core region overlap region segmentation fuzzy approximation space provide effective mean brain mr volume analysis handle overlap partition address vagueness tissue class definition effectiveness propose algorithm comparison exist approach demonstrate publicly available brain mr datan',\n",
       " 'Roee Diamant Avi Abu recent boost undersea operation lead development highresolution sonar system mount autonomous vehicle vehicle scan seafloor search different object sink ship archaeological site submerge mine important detection operation segmentation sonar image objectxe2x80x99s highlight shadow distinguish seabed background paper focus automatic segmentation sonar image present enhance fuzzybased kernel metric enfk algorithm segmentation sonar image attempt improve segmentation accuracy introduce new fuzzy term local spatial statistical information algorithm include preliminary denoising algorithm original image fee segmentation procedure avoid trap local minimum improve convergence result segmentation procedure specifically suit intensity inhomogeneity complex seabed texture sonar image test approach simulate image real sonar image sonar image create different sea experiment multibeam sonar synthetic aperture sonar result accurate segmentation performance far stateoftheart resultn',\n",
       " 'Christopher Leckie Masud Moshtaghi Mahsa Salehi Sutharshan Rajasegarar Minh Tuan Doan pedestrian movement major impact dynamic city provide valuable guidance city planner paper model normal behaviour pedestrian flow detect anomalous event pedestrian count data city melbourne data span extend period pedestrian activity change intermittently activity winter v summer apply ensemble switch model dynamic anomaly detection technique accommodate system switch different state result compare produce static cluster model hycarce crossvalidated know event result ensemble switch model valid accurate hycarcen',\n",
       " 'James Bailey Julien Epps Nguyen Xuan Vinh information theoretic base measure form fundamental class similarity measure compare cluster class paircounting base setmatching base measure paper discus necessity correction chance information theoretic base measure cluster comparison observe baseline measure average value random partition data set constant value tend large variation ratio number data point number cluster small effect similar noninformation theoretic base measure wellknown rand index assume hypergeometric model randomness derive analytical formula expect mutual information value pair cluster propose adjust version popular information theoretic base measure example give demonstrate need usefulness adjust measuren',\n",
       " 'D. Wunsch Rui Xu data analysis play indispensable role understand phenomenon cluster analysis primitive exploration little prior knowledge consist research develop wide variety community diversity hand equip tool hand profusion option cause confusion survey cluster algorithm data set appear statistic science machine learn illustrate application benchmark data set travel salesman problem bioinformatics new field attract intensive effort tightly relate topic proximity measure cluster validation discussedn',\n",
       " 'Jiawei Han R.T. Ng spatial data mine discovery interest relationship characteristic exist implicitly spatial database end paper main contribution propose new cluster method call clarans aim identify spatial structure present data experimental result indicate compare exist cluster method clarans efficient effective second paper investigate clarans handle point object polygon object efficiently method consider call irapproximation efficient cluster convex nonconvex polygon object build clarans paper develop spatial data mine algorithm aim discover relationship spatial nonspatial attribute algorithm discover knowledge difficult exist spatial data mine algorithmn',\n",
       " 'Amparo AlonsoBetanzos Oscar FontenlaRomero Enrique Castillo David MartinezRego technique adjust minimum volume set cover ellipsoid technique elaborate solution problem potential application oneclass classification cluster problem main original feature 1 avoid direct evaluation determinant diagonalization property involve matrix 2 identify remove outlier estimation process 3 avoid binary variable result combinatorial character assignment problem replace continuous variable range 0 1 4 problem solve bilevel algorithm level determine ellipsoid second level reassign data point ellipsoid identify outlier base algorithm force karushkuhntucker condition satisfy theorem provide rigorous base propose method finally set example application different field give illustrate power method practical performancen',\n",
       " 'Olli Virmajoki Pasi Franti agglomerative cluster generate partition hierarchically sequence merge operation propose alternative mergebased approach remove cluster iteratively desire number cluster reach apply local optimization strategy remove cluster increase distortion data structure update strategy consider propose algorithm apply crossover method genetic algorithm compare best exist cluster algorithm propose method provide best performance term minimize intracluster variancen',\n",
       " 'Marimuthu Palaniswami Christopher Leckie Masud Moshtaghi Michele Nati Muhammad Ali Imran Alexander Gluhak Sutharshan Rajasegarar anomaly detection resource constrain wireless network important challenge task intrusion detection quality assurance event monitor application challenge detect interest event anomaly timely manner minimise energy consumption network propose distribute anomaly detection architecture u multiple hyperellipsoidal cluster model data sensor node identify global local anomaly network particular novel anomaly score method propose provide score hyperellipsoidal model base remote ellipsoid relative neighbour demonstrate synthetic real datasets propose scheme achieve high detection performance significant reduction communication overhead network compare centralise exist schemesn',\n",
       " 'Sebti Foufou Rachid Hadjidj Ahmed Ben Said paper present novel cluster approach base classic fuzzy cmeans algorithm approach inspire concept interaction object physic data point regard particle specific weight associate data particle depend interaction particle interaction induce attraction force pair particle escape velocity particle classification experiment data set uci repository demonstrate outperformance propose approach cluster algorithm addition result demonstrate effectiveness propose scheme segmentation multispectral face imagesn',\n",
       " 'MiinShen Yang KuoLung Wu abstract paper propose new metric replace euclidean norm cmeans cluster procedure basis robust statistic influence function claim propose new metric robust euclidean norm create new cluster method call alternative hard cmeans ahcm alternative fuzzy cmeans afcm cluster algorithm alternative type cmeans cluster robustness cmeans cluster numerical result ahcm good performance hcm afcm good fcm recommend afcm use cluster analysis recently afcm algorithm successfully segment magnetic resonance image ophthalmology differentiate abnormal tissue normal tissuen',\n",
       " 'Francesco Corona Guilherme De A. Barreto Amauri H. Souza Junior abstract global model consist fit single regression model available data set input output observation spectrum stand local model approach input space segment small partition specialize regression model fit partition paper propose novel approach call regional model rm stand global local model one proposal extend twolevel cluster approach vesanto alhoniemi 2000 1 regression problem specifically identification regard partition input space selforganizing map som perform cluster prototype train som finally regional regression model build cluster region som prototype som prototype local model propose framework build regional linear nonlinear regression model linear case use autoregressive model exogenous arx parameter estimate ordinary leastsquares ols method regional nonlinear arx narx model build extreme learn machine network additionally develop robust variant propose regional model use m estimation statistical framework handle outlier ols highly sensitive comprehensive performance evaluation propose model synthetic realworld datasets carry result compare achieve standard global local modeln',\n",
       " 'Ah Chung Tsoi D. Ralph M. Palaniswami A. Shilton propose new algorithm incremental train support vector machine svms suitable problem sequentially arrive data fast constraint parameter variation method involve warmstart algorithm train svms allow advantage natural incremental property standard active set approach linearly constrain optimization problem incremental train involve quickly retrain support vector machine add small number additional train vector train set exist train support vector machine similarly problem fast constraint parameter variation involve quickly retrain exist support vector machine train set different constraint parameter case demonstrate computational superiority incremental train usual batch retrain methodn',\n",
       " 'Yingchun Bo Zhaozhao Zhang Junfei Qiao propose online selfadaptive modular neural network osamnn timevarying system start zero subnetworks osamnn u singlepass subtractive cluster algorithm update center radialbasis function rbf neuron learn input space partition osamnn structure capable grow merge subnetworks maintain suitable model complexity center rbf neuron dynamically adjust accord change data environment fuzzy strategy apply select suitable subnetworks learn current sample method yield improve learn efficiency accuracy osamnn adapt architecture realize online model timevarying nonlinear inputoutput map result experiment benchmark realworld timevarying system support propose techniquesn',\n",
       " 'H. P. Lee Xiu Ju Fu U. Periyathamby J. Q. Zhang ChongJin Ong S. S. Keerthi L. J. Cao sequential minimal optimization smo popular algorithm train support vector machine svm require large computation time solve large size problem paper propose parallel implementation smo train svm parallel smo develop message pass interface mpi specifically parallel smo partition entire train data set small subset simultaneously run multiple cpu processor deal partition data set experiment great speedup adult data set mix national institute standard technology mnist data set processor satisfactory result web data setn',\n",
       " 'Liqun Cong W. Pedrycz Wei Wang Quanli Liu Jun Zhao energy important part steel industry reasonable operation exhibit critical impact manufacture cost energy security natural environment respect operation optimization problem coke oven gas twophase datadriven base forecast optimize adjust method propose gaussian processbase echo state network establish predict gas realtime flow gasholder level prediction phase predict gas flow gasholder level develop certain heuristic quantify useru0027s optimal gas adjustment propose operation measure verify effective experiment realworld online energy data set come shanghai baosteel corporation china present schedule software develop propose model ensue algorithm apply production practice baosteel application effect indicate software largely improve realtime prediction accuracy gas unit provide optimize gas balance direction energy optimizationn',\n",
       " 'S. Jakubek C. Hametner local model network approximate nonlinear multiple local model fit partition space main advantage approach identification complex nonlinear process alleviate integration structure knowledge process paper extend concept integration quantitative process knowledge identification procedure quantitative knowledge describe explicit dependence input output integrate parameter estimation process mean equality constraint purpose constrain generalize total square algorithm local parameter estimation present furthermore problem proper integration constraint partition process treat expectationmaximization procedure combine constrain parameter estimation benefit applicability propose concept demonstrate mean illustrative example practical application real measurement datan',\n",
       " 'A.F. Murray P.J. Edwards G. Papadopoulos feedforward neural network particularly multilayer perceptrons widely regression classification task reliable practical measure prediction confidence essential work alternative approach prediction confidence estimation present compare method maximum likelihood approximate bayesian bootstrap technique consider prediction uncertainty owe data noise model parameter misspecification method test number control artificial problem real industrial regression application prediction paper curl confidence estimation performance assess calculate mean standard deviation prediction interval coverage probability treat data noise variance function input appropriate curl prediction task mean coverage probability gauge confidence estimation performance average input space global performance standard deviation coverage unreliable measure local performance approximate bayesian approach perform good term global performancen',\n",
       " 'M.A. Motter D. Erdogmus J.C. Principe Jeongho Cho generation aircraft dynamic vary considerably operate regime single controller difficulty meet design specification paper selforganizing map sombased local linear model scheme unman aerial vehicle uav develop design set inverse controller som select operate regime depend embed output space information avoid normalization input data local linear model associate linear controller easy design switch controller synchronously active local linear model track different operate condition propose multiple model control strategy successfully test simulator model loflyte uavn',\n",
       " 'David Pal Ulrike von Luxburg Shai BenDavid stability common tool verify validity sample base algorithm cluster widely tune parameter algorithm number k cluster spite popularity stability practical application little theoretical analysis notion paper provide formal definition stability analyze basic property surprisingly conclusion analysis large sample size stability fully determine behavior objective function cluster algorithm aim minimize objective function unique global minimizer algorithm stable unstable particular conclude stability wellsuited tool determine number cluster xe2x80x93 determine symmetry data unrelated cluster parameter prove result centerbase cluster spectral cluster support conclusion example behavior stability counterintuitiven',\n",
       " 'Ujjwal Maulik Sanghamitra Bandyopadhyay Malay Kumar Pakhira abstract article cluster validity index fuzzification describe provide measure goodness cluster different partition data set maximum value index call pbm index hierarchy provide best partition index define product factor maximization ensure formation small number compact cluster large separation cluster kmeans expectation maximization algorithm underlie crisp cluster technique fuzzy cluster utilize wellknown fuzzy cmeans algorithm result demonstrate superiority pbm index appropriately determine number cluster compare wellknown measure daviesxe2x80x93bouldin index dunnu0027s index xiexe2x80x93beni index provide artificial reallife data setsn',\n",
       " 'Pasi Franti Mohammad Rezaei compare cluster result data set challenge task cluster analysis external validity measure propose literature good measure invariant change data size cluster size number cluster overview exist set match index analyze property set match measure base match cluster cluster analyze measure part 1 cluster similarity 2 match 3 overall measurement correction chance investigate prove normalize mutual information variation information intrinsically correct propose new scheme experiment base synthetic data evaluation external validity index accordingly popular external index evaluate compare apply cluster different data size cluster size number cluster experiment set match measure clearly good test base analytical comparison introduce new index call pair set index psin',\n",
       " 'C. Lucas S. M. Fakhraie H. R. Mahdiani reliability identify important challenge future nanoscale large scale integration vlsi implementation technology development complex integrate system normally fault tolerance ft conventional achieve increase redundancy imply high implementation cost low performance make infeasible contrast custom approach new class application categorize paper inherently capable absorb degree vulnerability provide ft base natural property neural network good indicator imprecisiontolerant application propose new class ft technique call relax faulttolerant rft technique develop vlsi implementation imprecisiontolerant application main advantage rft technique respect traditional ft solution exploit inherent ft different application reduce implementation cost improve performance applicability efficiency rft method experimental result implementation facerecognition computationally intensive neural network correspond rft realization present paper result demonstrate promise high performance artificial neural network vlsi solution complex application faulty nanoscale implementation environmentsn',\n",
       " 'Keechul Jung KyoungSu Oh abstract graphic process unit gpu fast artificial neural network implement matrix multiplication neural network enhance time performance text detection preliminary result produce 20fold performance enhancement ati radeon 9700 pro board parallelism gpu fully utilize accumulate lot input feature vector weight vector convert innerproduct operation matrix operation research area include benchmarking performance hardware gpuaware learn algorithmsn',\n",
       " 'P. Niyogi J.C. Principe D. Erdogmus D. Deodhare S. Rakshit V. Sindhwani paper present feature selection algorithm multilayer perceptrons mlps multiclass support vector machine svms mutual information class label classifier output objective function objective function involve inexpensive computation information measure discrete variable provide immunity prior class probability bracket probability error classifier maximum output information moi algorithm employ function feature subset selection greedy elimination direct search output moi algorithm feature subset userdefined size associate train classifier mlpsvm algorithm compare favorably number method term performance artificial realworld data setsn',\n",
       " 'HuaLiang Wei XiaoFeng Yang Tingwen Huang YuZhu Guo WeiGang Cui Yang Li new parametric approach propose nonlinear nonstationary identification base timevarying nonlinear autoregressive exogenous input tvnarx model tv coefficient tvnarx model expand multiwavelet basis function model transform timeinvariant regression problem ultraorthogonal forward regression uofr algorithm aid mutual information mi design identify parsimonious model structure estimate associate model parameter uofrmi algorithm u observe data weak derivative signal powerful model structure detection propose approach combine advantage basis function expansion method uofrmi algorithm prove capable track change tv parameter effectively numerical simulation real eeg datan',\n",
       " 'D. Huang T.W.S. Chow novel feature selection method concept mutual information mi propose paper mi base feature selection method effective efficient estimation highdimensional mi crucial paper prune parzen window estimator quadratic mutual information qmi combine address problem result propose approach estimate mi effective efficient way contribution novel feature selection method develop identify salient feature appropriate feature subset classification reliably estimate propose methodology thoroughly test different classification application number feature range 10 15000 present result promise corroborate contribution propose feature selection methodologyn',\n",
       " 'L. Z. Guo Y. F. Zhao S. A. Billings H. L. Wei start basic concept couple map lattice new family adaptive wavelet neural network awnn introduce spatiotemporal identification combine efficient wavelet representation couple map lattice model new orthogonal projection pursuit opp method couple particle swarm optimization pso algorithm propose augment propose network novel twostage hybrid train scheme develop construct parsimonious network model stage apply orthogonal projection pursuit algorithm significant wavelet neuron adaptively successively recruit network adjustable parameter associate wavelet neuron optimize particle swarm optimizer resultant network model obtain stage redundant second stage orthogonal square algorithm apply refine improve initially train network remove redundant wavelet neuron network propose twostage hybrid train procedure generally produce parsimonious network model rank list wavelet neuron accord capability neuron represent total variance output signal produce spatiotemporal identification example present demonstrate performance propose new model frameworkn',\n",
       " 'Shunichi Amari Yuanqing Li sparse representation important sparse solution 0norm 1norm solution receive attention 0norm solution sparse easy obtain 1norm solution sparse easily obtain linear program method case 0norm solution obtain find 1norm solution discussion exist equivalence sparse solution paper analyze condition equivalence sparse solution condition necessary sufficient difficult verify second necessary sufficient easy verify paper analyze second condition stochastic framework propose variant prove equivalence sparse solution hold high probability variant second condition furthermore limit case 0norm solution extremely sparse second condition sufficient condition probability 1n',\n",
       " 'Ke Li MeiLin Luo Yang Li efficient multiwaveletbased timevarying model scheme propose timefrequency analysis tfa electroencephalogram eeg data new multiwaveletbased parametric model framework timedependent parameter timevarying model locally represent novel multiwavelet decomposition scheme effective orthogonal square ols algorithm aid mutual information criterion apply sparse model selection parameter estimation resultant estimation timedependent spectral density signal simultaneously achieve high resolution time frequency powerful tfa technique nonstationary biomedical signal include eeg example artificial eeg signal real eeg signal include effectiveness applicability new propose approach simulation study application real eeg data elucidate propose wavelet approach capable achieve high timefrequency representation nonstationary processesn',\n",
       " 'Ke Li Yang Li SiRui Tan efficient timevarying autoregressive tvar model scheme multiresolution wavelet method propose model nonstationary signal application timefrequency analysis tfa timevarying signal new parametric model framework timedependent parameter tvar model locally represent novel multiresolution wavelet decomposition scheme wavelet coefficient estimate effective orthogonal square ols algorithm resultant estimation timedependent spectral density signal simultaneously achieve high resolution time frequency powerful tfa technique nonstationary signal artificial eeg signal include effectiveness new propose approach experimental result elucidate multiresolution wavelet approach capable achieve accurate timefrequency representation nonstationary signaln',\n",
       " 'Shitong Wang FuLai Chung KupSze Choi Yizhang Jiang Zhaohong Deng classical fuzzy model method consider current scene train data assume fully collectable data available current scene insufficient fuzzy system train incomplete datasets suffer weak generalization capability prediction scene order overcome problem knowledgeleveragebased fuzzy klfs study paper perspective transfer learn klfs intend use data current scene learn procedure effectively leverage exist knowledge reference scene specifically knowledgeleveragebased takagisugenokangtype fuzzy kltskfs propose integrate correspond knowledgeleverage mechanism new fuzzy model technique evaluate experiment synthetic realworld datasets result demonstrate kltskfs good performance adaptability traditional fuzzy model method scene insufficient datan',\n",
       " 'M. Abdollahzade A. Miranian local model approach owe ability model different operate regime nonlinear system process independent local model appeal model identification prediction application paper propose local neurofuzzy lnf approach base leastsquares support vector machine lssvms propose lnf approach employ lssvms powerful model predict time series local model u hierarchical binary tree hbt learn algorithm fast efficient estimation parameter hbt algorithm heuristically partition input space small subdomains axisorthogonal split partition validity function automatically form unity partition normalization effect reactivation prevent integration lssvms lnf network local model hbt learn algorithm yield highperformance approach model prediction complex nonlinear time series propose approach apply model prediction different nonlinear chaotic realworld handdesigned system time series analysis prediction result comparison recent old study demonstrate promise performance propose lnf approach hbt learn algorithm model prediction nonlinear chaotic system time seriesn',\n",
       " 'JunFei Qiao LuMing Ge HongGui Han paper adaptive second order algorithm asoa develop train fuzzy neural network fnn achieve fast robust convergence nonlinear model different recent study asoabased fnn asoafnn quasi hessian matrix gradient vector accumulate sum relate sub matrix vector respectively learn rate asoafnn design accelerate learn speed addition convergence propose asoafnn prove fix learn rate phase adaptive learn rate phase finally comparison realize show propose asoafnn fast convergence speed accurate result exist methodsn',\n",
       " 'Fadhel M. Ghannouchi Mohamed Helaoui Mohsin Aziz Dongming Wang digital predistorter model augment realvalued timedelay neural network arvtdnn propose suitable mitigate nonlinear distortion power amplifier pa modulator imperfection wideband directconversion transmitter input signal propose arvtdnn consist cartesian inphase quadrature phase iq component envelopedependent term theoretical analysis show propose model able produce rich basis function contain desire odd evenorder term result improve model capability distortion mitigation actual performance validate extensive simulation experiment result compensation hardware impairment mitigation capability arvtdnn superior exist stateoftheart realvalued focus timedelay neural network rvftdnn 3xe2x80x934 db adjacent channel power ratio 2xe2x80x933 db term normalize mean square error important feature propose model reduce complexity term number parameter floatingpoint operation improve numerical stability compare rvftdnn modeln',\n",
       " 'Dakun Yang Long Li Yan Liu abstract paper investigate split complex gradient descent base neurofuzzy algorithm selfadaptive momentum l2 regularizer train tsk takagixe2x80x93sugenoxe2x80x93kang fuzzy inference model major threat dispose complex data fuzzy contradiction boundedness analyticity complex domain express liouvillexe2x80x99s theorem propose algorithm operate couple realvalued function split complex variable real imaginary dynamical momentum include learn mechanism promote learn speed l2 regularizer add control magnitude weight parameter furthermore detail convergence analysis propose algorithm fully study monotonic decrease property error function convergence weight sequence guarantee plus mild condition strong convergence weight sequence deduce finally simulation result demonstrate verify theoretical analysis resultn',\n",
       " 'J M Benitez J L Aznarte soft compute sc emerge integrate framework number technique complement artificial neural network fuzzy system evolutionary algorithm probabilistic reason inception distinctive goal dig deep relationship component paper consider wide family sc model hand regimeswitching autoregressive paradigm recent development statistical time series model include set model closely relate artificial neural network hand consider fuzzy rulebase system framework time series analysis paper disclose original result establish functional equivalence model class open door productive line research result technique area apply consequence equivalence present paper prove asymptotic stationarity class fuzzy rulebase system simulation base information criterion importance selection proper membership functionn',\n",
       " 'C.L.P. Chen HongXing Li demonstrate fuzzy logic system feedforward neural network equivalent essence introduce concept interpolation representation fuzzy logic system important conclusion define mathematical model rectangular wave neural network nonlinear neural network definition prove nonlinear neural network represent rectangular wave neural network base result prove equivalence fuzzy logic system feedforward neural network result provide useful guideline perform theoretical research application fuzzy logic system neural network neurofuzzy systemn',\n",
       " 'H.J. Tang K.C. Tan letter present new dynamical optimal learn dol algorithm threelayer linear neural network investigate generalization ability optimal learn rate fully determine train process mean square error mse guarantee stably decrease learn sensitive initial parameter set simulation result illustrate propose dol algorithm give good generalization performance fast convergence compare standard error propagation algorithmn',\n",
       " 'ShieJue Lee W. R. Jeng ChiYuan Yeh propose novel approach build type2 neuralfuzzy give set inputoutput train data selfconstructing fuzzy cluster method partition train dataset cluster inputsimilarity outputsimilarity test membership function associate cluster define mean deviation data point include cluster type2 fuzzy takagisugenokang ifthen rule derive cluster form fuzzy rule base fuzzy neural network construct accordingly associate parameter refine hybrid learn algorithm incorporate particle swarm optimization square estimation new input correspond crisp output obtain combine infer result rule type2 fuzzy set defuzzified apply refine type reduction algorithm experimental result present demonstrate effectiveness propose approachn',\n",
       " 'Hong Wang Xinkai Chen Jing Sun Tianyou Chai Yajun Zhang n',\n",
       " 'Jun Wang Zheng Yan paper present neural network approach robust model predictive control mpc constrain discretetime nonlinear system unmodel dynamic affect bound uncertainty exact nonlinear model underlie process precisely know partially know nominal model available partially know nonlinear model decompose affine term plus unknow highorder term jacobian linearization linearization residue combine unmodel dynamic model extreme learn machine supervise learn minimax methodology exploit deal bound uncertainty minimax optimization problem reformulate convex minimization problem iteratively solve twolayer recurrent neural network propose neurodynamic approach nonlinear mpc improve computational efficiency shed light realtime implementability mpc technology simulation result provide substantiate effectiveness characteristic propose approachn',\n",
       " 'Chris J. Harris Sheng Chen Xuemin Tian Xiaogang Deng industrial process contain linear nonlinear part kernel principal component analysis kpca widely nonlinear process monitor offer effective mean deal nonlinear process paper propose new hybrid linearnonlinear statistical model approach nonlinear process monitor closely integrate linear principal component analysis pca nonlinear kpca serial model structure refer serial pca spca specifically pca apply extract pc linear feature decompose data pc subspace residual subspace r kpca perform r extract nonlinear pc nonlinear feature monitor statistic construct fault detection base linear nonlinear feature extract propose spca effectively perform fault identification fault detect spca similarity factor method build fault recognition fuse linear nonlinear feature unlike pca kpca propose method take account linear nonlinear pc simultaneously good exploit underlie processxe2x80x99s structure enhance fault diagnosis performance case study involve simulate nonlinear process benchmark tennessee eastman process demonstrate propose spca approach effective exist stateoftheart approach base kpca term nonlinear process fault detection identificationn',\n",
       " 'Li Cheng Xingjian Jing Hanwen Ning identification nonlinear spatiotemporal system significance engineer practice provide useful insight underlie nonlinear mechanism physical characteristic study paper nonlinear spatiotemporal model transform class multiinputmultioutput mimo partially linear system plss effective online identification algorithm propose prune error minimization principle square support vector machine show benchmark physical engineer system transform mimoplss important physical spatiotemporal relationship helpful identification analysis underlie compare exist method advantage propose method use prior structural information physical model realize online estimation dynamic achieve accurate characterization important nonlinear physical characteristic provide important basis state estimation control optimal analysis design nonlinear distribute parameter system propose algorithm apply identification problem stochastic spatiotemporal dynamical system numeral example comparison give demonstrate resultsn',\n",
       " 'YiHu Wu HanXiong Li Hua Deng new feedbacklinearizationbase neural network nn adaptive control propose unknown nonaffine nonlinear discretetime system equivalent model afflnelike form derive original nonaffine discretetime system feedback linearization method implement system feedback linearization adaptive control implement base affinelike equivalent model identify neural network pretraining require weight neural network adaptive control directly update online base inputoutput measurement deadzone technique remove requirement persistence excitation adaptation propose neural network adaptive control stability performance closedloop rigorously establish illustrate example provide validate theoretical findingsn',\n",
       " 'WeiDong Luo Xuan Han WenFang Xie ZhiJun Fu paper deal adaptive nonlinear identification trajectory track dynamic multilayer neural network nn different timescales nn identifier propose nonlinear system identification dynamic nns different timescales include fast slow phenomenon nn identifier u output signal actual identification second nn identifier output signal nonlinear replace state variable nns online identification algorithm nn identifier parameter propose lyapunov function singularly perturb technique identify nn model indirect adaptive nn controller nonlinear system contain slow fast dynamic process develop develop adaptive nn controller trajectory error analyze stability system prove simulation result controller base second identifier good performance identifiern',\n",
       " 'Bin Jiang Gang Tao Mou Chen paper dynamic surface control dsc scheme propose class uncertain strictfeedback nonlinear system presence input saturation unknow external disturbance radial basis function neural network rbfnn employ approximate unknow function efficiently tackle unknow external disturbance nonlinear disturbance observer ndo develop develop ndo relax know boundary requirement unknow disturbance guarantee disturbance estimation error converge bound compact set ndo rbfnn dsc scheme develop uncertain nonlinear system base backstepping method dsc technique problem explosion complexity inherent conventional backstepping method avoid specially important design neural network approximation propose dsc scheme ultimately bound convergence closedloop signal guarantee lyapunov analysis simulation result give effectiveness propose dsc design ndo rbfnnn',\n",
       " 'Youxian Sun Qinmin Yang Zaiyue Yang brief consider asymptotic track problem class highorder nonaffine nonlinear dynamical system nonsmooth actuator nonlinearities novel transformation approach propose able systematically transfer original nonaffine nonlinear equivalent affine deal unknown dynamic unknown control coefficient contain affine online approximator nussbaum gain technique utilize controller design prove rigorously asymptotic convergence track error ultimate uniform boundedness signal guarantee propose control method control feasibility verify numerical simulationsn',\n",
       " 'Hong Wang Tianyou Chai Yajun Zhang paper present novel nonlinear control strategy class uncertain singleinput singleoutput discretetime nonlinear system unstable zerodynamic propose method combine adaptivenetworkbased fuzzy inference anfis multiple model linear robust controller anfisbased nonlinear controller switch mechanism integrate multiple model technique show linear controller ensure boundedness input output signal nonlinear controller improve dynamic performance close loop show use switch mechanism simultaneously guarantee close loop stability improve performance result controller follow outstanding feature compare exist control strategy method relax assumption commonlyused uniform boundedness unmodeled dynamic enhance applicability second anfis estimate compensate effect cause unmodeled dynamic convergence rate neural network learn increase xe2x80x9conetoone mappingxe2x80x9d technique adapt guarantee universal approximation property anfis propose controller apply numerical example pulverize process alumina sinter respectively effectiveness justifiedn',\n",
       " 'C. L. Philip Chen Dan Wang Zifu Li Tieshan Li paper present adaptive outputfeedback neural network nn control scheme class stochastic nonlinear timevarying delay system unknown control direction controller design feasible unknown control coefficient group original transform new linear state transformation technique nussbaum function technique incorporate backstepping recursive design technique solve problem unknown control direction furthermore assumption timevarying delay exist output nn employ compensate unknown nonlinear term depend delay output estimate maximum nn parameter instead parameter nn parameter estimate greatly decrease online learn time dramatically decrease show signal closedloop bound probability effectiveness propose scheme demonstrate simulation resultsn',\n",
       " 'Dongbin Zhao Amir Hussain Frank L. Lewis Zhongsheng Hou Tianyou Chai 21 paper special section focus databased control model optimizationn',\n",
       " 'Jing Sun ChunYi Su Hong Wang Yajun Zhang Tianyou Chai complex industrial multivariable nonlinear nature generally difficult impossible obtain accurate model especially model structure unknown control class complex system difficult handle traditional controller design operate point paper explore concept controllerdriven model virtual unmodeled dynamic propose new design framework design consist controller distinct function input output data selftuning controller construct base linear controllerdriven model output signal controllerdriven model compare true output produce socalled virtual unmodeled dynamic base compensator virtual unmodeled dynamic second controller base nonlinear controllerdriven model propose controller integrate adaptive switch control algorithm advantage complementary feature offer stabilization function provide improve performance condition stability convergence closedloop analyze simulation experimental test heavily couple nonlinear twintank carry confirm effectiveness propose methodn',\n",
       " 'K.S. Narendra Lingji Chen paper present unify theoretical framework identification control nonlinear discretetime dynamical nonlinear represent explicitly sum linearize component residual nonlinear component refer high order function representation substantially simplify procedure apply implicit function theorem derive local property nonlinear reveal role play linearize transparent form assumption linearize controllable observable show 1 nonlinear controllable observable local domain 2 feedback law exist stabilize nonlinear locally 3 nonlinear exactly track constant periodic sequence locally linearize additional assumption nonlinear show welldefined relative degree delay zerodynamics zerodynamics linearize asymptotically stable nonlinear case control law exist nonlinear asymptotically track arbitrary reference signal exactly neighborhood equilibrium state track achieve state vector feedback input output case nonlinear autoregressive movingaverage narma model establish utilize result important understand use neural network identifier controller general nonlinear discretetime dynamical systemsn',\n",
       " 'Roman Zajdel Maciej Kusy paper propose new method choice adaptation smooth parameter probabilistic neural network pnn method base reinforcement learn algorithm q0 learn qlambda learn stateless q learn regard type pnn classifier model u single smooth parameter network model utilize single smooth parameter data attribute model posse matrix smooth parameter different data variable data class reinforcement learn apply method find value smooth parameter ensure maximization prediction ability pnn model smooth parameter compute accord propose algorithm test database calculate test error use cross validation procedure result compare stateoftheart method pnn train publish literature date additionally pnn sigma determine mean conjugate gradient approach result demonstrate propose approach alternative pnn train proceduresn',\n",
       " 'Jian Xu Hongbin Fang Zuolin Liu abstract selflocking origami structure characterize piecewise linear constitutive relation force deformation practice completely opaque unmeasurable number piecewise segment position nonsmooth point linear parameter segment unknown priori acquire information fundamental importance understand origami structurexe2x80x99s dynamic fold process predict dynamic behavior arouse adopt dynamical identification process determine model estimate parameter research base piecewise linear assumption physicallyinterpretable neuralfuzzy network build correlate measure input output data unlike conventional approach construct neural network posse specific physical mean component number neuron relate number piecewise segment coefficient local linear model relate parameter constitutive relation validity function relate position nonsmooth point address example different background networkxe2x80x99s underlie data train method illustrate include local linear optimization linear parameter nest optimization nonlinear partition local linear model tree optimization model selection note tackle origami problem hold strong universality term unknown piecewise characteristic propose approach provide effective generic physically significant mean handle piecewise linear dynamical system bring fresh vitality artificial neural network researchn',\n",
       " 'John Y. Goulermas Sophia Ananiadou Tingting Mu Austin J. Brockmeier descriptive cluster consist automatically organize data instance cluster generate descriptive summary cluster description inform user content cluster examination specific instance enable user rapidly scan relevant cluster selection description rely heuristic criterion model descriptive cluster autoencoder network predict feature cluster assignment predict cluster assignment subset feature subset feature predict cluster serf description text document occurrence count word phrase attribute provide sparse feature representation interpretable feature label propose network cluster prediction logistic regression model feature prediction rely logistic multinomial regression model optimize model lead completely selftuned descriptive cluster approach automatically select number cluster number feature cluster apply methodology variety short text document show select cluster evidence select feature subset associate meaningful topical organizationn',\n",
       " 'Trevor Darrell Chang Huang Yangqing Jia paper examine effect receptive field design classification accuracy commonly adopt pipeline image classification exist algorithm usually use manually define spatial region pool learn adaptive receptive field increase performance significantly small codebook size cod layer learn optimal pool parameter adopt idea overcompleteness start large number receptive field candidate train classifier structure sparsity use sparse subset feature efficient algorithm base incremental feature selection retrain propose fast learn method achieve best publish performance cifar10 dataset low dimensional feature space previous methodsn',\n",
       " 'Deng Cai Guang Qiu Lijun Zhang Can Wang Chun Chen Jiajun Bu Miao Zheng sparse cod receive increase recent year unsupervised learn algorithm find basis set capture highlevel semantics data learn sparse coordinate term basis set originally apply model human visual cortex sparse cod show useful application exist approach sparse cod fail consider geometrical structure data space real application data likely reside lowdimensional submanifold embed highdimensional ambient space show geometrical information data important discrimination paper propose graph base algorithm call graph regularize sparse cod learn sparse representation explicitly account local manifold structure data graph laplacian smooth operator obtain sparse representation vary smoothly geodesic data manifold extensive experimental result image classification cluster demonstrate effectiveness propose algorithmn',\n",
       " 'ShihFu Chang John Wright Yadong Mu traditional localitysensitive hash lsh technique aim tackle curse explosive data scale guarantee similar sample project proximal hash bucket despite success lsh numerous vision task like image retrieval object match potential largescale optimization realize recently paper advance nascent area identify common operation know computational bottleneck numerous optimization algorithm largescale set minmax inner product propose hash scheme accelerate minmax inner product exploit property order statistic statistically correlate random vector compare scheme algorithm exhibit improve recall low computational cost effectiveness efficiency propose method corroborate theoretic analysis important application especially use propose hash scheme perform approximate l1 regularize square dictionary million element scale capability currently know exact solver nonetheless highlight focus paper new hash scheme approximate near neighbor problem exploit new application hash technique propose general framework accelerate large variety optimization procedure visionn',\n",
       " 'Mohand Said Allili Ouiza Ouyed multinominal kernel logistic regression mklr supervise classification method design separate class nonlinear boundary rely assumption feature equally important decrease classification performance deal highdimensional noisy data propose approach embed feature relevance multinomial kernel logistic regression approach coin frmklr generalize mklr introduce feature weight scheme gaussian kernel socalled l 0 xe2x80x9dnormxe2x80x9d sparsitypromoting regularization contribution feature tune accord relevance classification lead generalizable interpretable sparse model classification application approach standard datasets video action recognition provide promise result compare methodsn',\n",
       " 'Edgar Korner HorstMichael Gross Heiko Wersing Stephan Kirstein present new method capable learn multiple category interactive lifelong learn fashion approach u0027u0027stabilityplasticity dilemmau0027u0027 problem incremental learn multiple category largely unsolved especially true domain cognitive robotics require realtime interactive learn achieve lifelong learn ability cognitive propose new learn vector quantization approach combine categoryspecific feature selection method allow metrical u0027u0027viewsu0027u0027 representation space individual vector quantization node categoryspecific feature incrementally collect learn process balance correction wrong representation stability acquire knowledge achieve demonstrate approach difficult visual categorization task learn apply complexshaped object rotate depthn',\n",
       " 'HeungYeung Shum Xiaoou Tang Lin Liang Xiao Zhang paper present l1 regularize projection pursuit algorithm additive model learn new algorithm develop regression classification respectively sparse projection pursuit regression sparse jensenshannon boost introduce l1 regularize projection pursuit encourage sparse solution new algorithm robust overfitting present good generalization ability especially set irrelevant input feature noisy data optimization l1 regularization efficient develop ldquoinformative feature firstrdquo sequential optimization algorithm extensive experiment demonstrate effectiveness propose approachn',\n",
       " 'Chris Dyer Stefan Riezler Patrick Simianer exception discriminative train statistical machine translation smt content tune weight large feature set small development data evidence machine learn indicate increase train sample size result good prediction goal paper common wisdom bring bear smt deploy local feature scfgbased smt read rule runtime present learn algorithm apply l1l2 regularization joint feature selection distribute stochastic learn process present experiment learn 15 million train sentence significant improvement tune discriminative model small development setn',\n",
       " 'Huan Liu Jiliang Tang Xia Hu Jundong Li explosive growth social medium site bring massive amount highdimensional data feature selection effective prepare highdimensional data data analytics characteristic social medium present novel challenge feature selection social medium data fully structure feature usually predefined generate dynamically example twitter slang word feature create everyday quickly popular short period time hard directly apply traditional batchmode feature selection method feature second give nature social medium label information costly collect exacerbate problem feature selection know feature relevance hand opportunity unequivocally present additional data source example link information ubiquitous social medium helpful select relevant feature paper study novel problem conduct unsupervised stream feature selection social medium data investigate exploit link information stream feature selection result novel unsupervised stream feature selection framework usfs experimental result realworld social medium datasets effectiveness efficiency propose framework compare stateoftheart unsupervised feature selection algorithmsn',\n",
       " 'ChihJen Lin ChoJui Hsieh KaiWei Chang GuoXun Yuan largescale linear classification widely area l1regularized form apply feature selection nondifferentiability cause difficulty train optimization method propose recent year compare suitably paper broadly review exist method discus stateoftheart software package propose efficient implementation extensive comparison indicate carefully implement coordinate descent method suitable train large document datan',\n",
       " 'Kenji Yamanishi Shin Matsushima Taito Lee n',\n",
       " 'Philip S. Yu Chengqi Zhang Xingquan Zhu Jia Wu Shirui Pan graph classification aim learn model classify structure data date exist graph classification method design target single learn task require large number label sample learn good classification model reality realworld task limit number label sample multiple similar learn task provide useful knowledge benefit task paper formulate new multitask graph classification mtg problem multiple graph classification task jointly regularize discriminative subgraphs share task learn niche mtg stem fact limit number train sample subgraph feature select single graph classification task tend overfit train data additional task evaluation set mtg jointly regularize multiple task explore high quality subgraph feature graph classification achieve goal formulate objective function combine multiple graph classification task evaluate informativeness score subgraph feature iterative subgraph feature exploration multitask learn process propose incrementally select subgraph feature graph classification experiment realworld multitask graph classification dataset demonstrate significant performance gainn',\n",
       " 'Qiang Yang Yu Zhang Ying Wei Bo Liu deep neural network dnn achieve breakthrough application large sample size face high dimension low sample size hdl data phenotype prediction problem genetic data bioinformatics dnn suffer overfitting highvariance gradient paper propose dnn model tailor hdl data name deep neural pursuit dnp dnp select subset high dimensional feature alleviation overfitting take average multiple dropout calculate gradient low variance dnn method apply hdl data dnp enjoy advantage high nonlinearity robustness high dimensionality capability learn small number sample stability feature selection endtoend train demonstrate advantage dnp empirical result synthetic realworld biological datasetsn',\n",
       " 'Jinseog Kim Yongdai Kim lasso absolute shrinkage selection operator useful tool achieve shrinkage variable selection simultaneously lasso u l 1 penalty optimization rely quadratic program qp general nonlinear program know computational intensive paper propose gradient descent algorithm lasso final result slightly accurate propose algorithm computationally simple qp nonlinear program apply large size problem provide convergence rate algorithm illustrate simulate model real data setsn',\n",
       " 'Shirish Shevade Sellamanickam Sundararajan Dinesh Garg paper develop game theoretic approach cluster feature learn problem feature cluster serve important preprocessing step problem feature selection dimensionality reduction approach view feature rational player coalitional game form coalition cluster order maximize individual payoff nash stable partition nsp know concept coalitional game theory provide natural way cluster feature approach obtain desirable property cluster choose appropriate payoff function small number feature nsp base cluster solve integer linear program ilp large number feature ilp base approach scale propose hierarchical approach interestingly key result prove equivalence ksize nsp coalitional game minimum kcut appropriately construct graph come handy large scale problem paper use feature selection problem classification set run example illustrate approach conduct experiment illustrate efficacy approachn',\n",
       " 'Babak Nadjar Araabi Majid Nili Ahmadabadi Mohammad Hassan Zokaei Ashtiani abstract work propose method local feature subset selection simultaneously partition sample space locality select feature partition correspond local feature represent novel notion feature tree problem find appropriate feature tree formulate reinforcement learn problem valuebased monte carlo tree search correspond credit assignment policy devise learn nearoptimal feature tree furthermore monte carlo tree search enhance way applicable large number action feature objective achieve take account banditbased explorative policy have soft exploitive estimation policy result synthetic datasets local feature present data propose method outperform feature selection method furthermore result microarray classification method obtain result comparable state art simple knn classifiern',\n",
       " 'William W. Cohen Yandong Liu Liu Xinwang Jun Zhu Ni Lao markov network mn incorporate arbitrarily complex feature model relational data flexibility come sharp price train exponentially complex model address challenge propose novel relational learn approach consist restrict class relational mn rmn call relation treebased rmn treermn efficient hide variable detection algorithm call contrastive variable induction cvi hand restrict treermn consider simple unary pairwise feature relational data achieve computational efficiency hand cvi algorithm efficiently detect hide variable capture long range dependency resultant approach highly efficient sacrifice expressive power empirical result real datasets propose relational learn method achieve similar prediction quality stateoftheart approach significantly efficient train induce hide variable semantically meaningful crucial improve train speed prediction quality treermnn',\n",
       " 'Carsten Wiuf Georgiana Ifrim present framework discriminative sequence classification linear classifier work directly explicit highdimensional predictor space subsequence train set oppose kernelinduced space feasible employ gradientbounded coordinatedescent algorithm efficiently select discriminative subsequence have expand space framework apply wide range loss function include binomial loglikelihood loss logistic regression square hinge loss support vector machine apply protein remote homology detection remote fold recognition framework achieve comparable performance stateoftheart kernel support vector machine contrast stateoftheart sequence classifier model simply list weight discriminative subsequence interpret relate biological problem crucial requirement bioinformatics medical communitiesn',\n",
       " 'I. D. Melamed Benjamin Wellington Joseph Turian parse translate natural language view problem predict tree structure machine learn approach prediction diversity high dimensionality structure involve mandate large train set paper present purely discriminative learn method scale problem size accuracy good comparable method standard parse task knowledge purely discriminative learn algorithm translation treestructured model unlike popular method method require great deal feature engineer priori perform feature selection compound feature space learn experiment demonstrate methodu0027s versatility accuracy efficiency relevant software freely available httpnlpcsnyueduparser httpnlpcsnyuedugenparn',\n",
       " 'Pinar Duygulu Eren Golge attack problem build classifier public face web image collect query search result noisy face detection irrelevant face correspond people photograph take wild large variety pose expression propose novel method face association model evolution fame able prune data iterative way model associate evolve idea base capture discriminative representative property instance eliminate outlier final model classify face novel datasets different characteristic benchmark datasets result comparable good stateoftheart study task face identificationn',\n",
       " 'Robert A. Jacobs Manu Chhabra computational complexity arise motor control ameliorate use library motor synergy present new model refer greedy additive regression gar model learn library torque sequence learn coefficient linear combination sequence minimize cost function perspective numerical optimization gar model interest create library local featureseach sequence library solution single train taskand learn combine sequence local optimization procedure additive regression speculate learner local representational primitive local optimization procedure good performance nonlinear task gar model interest perspective motor control outperform compete model result simulate twojoint arm suggest gar model consistently show excellent performance sense rapidly learn perform novel complex motor task library overcomplete sparse mean small fraction store torque sequence learn new movement library robust sense initial train period nearly novel movement learn additive combination sequence library sense show good generalization armu0027s dynamic alter train test condition payload add arm lastly gar model work regardless motor task specify joint space cartesian space conclude learn technique local primitive optimization procedure viable potentially important method motor control possibly domain technique deserve examination artificial intelligence cognitive science communitiesn',\n",
       " 'Jinho D. Choi introduce novel technique call dynamic feature induction keep induce high dimensional feature automatically feature space xe2x80x98morexe2x80x99 linearly separable dynamic feature induction search feature combination strong clue distinguish certain label pair generate joint feature combination induce feature train primitive low dimensional feature approach evaluate core nlp task partofspeech tag name entity recognition show stateoftheart result task achieve accuracy 9764 f1score 9100 respectively 25 increase feature spacen',\n",
       " 'Yuji Matsumoto Kaoru Yamamoto Taku Kudo paper present japanese morphological analysis base conditional random field crfs previous work crfs assume observation sequence word boundary fix word boundary clear japanese straightforward application crfs possible crfs apply situation word boundary ambiguity exist crfs offer solution longstanding problem corpusbase statistical japanese morphological analysis flexible feature design hierarchical tagsets possible second influence label length bias minimize experiment crfs standard testbed corpus japanese morphological analysis evaluate result experimental dataset hmms memms previously report task result confirm crfs solve longstanding problem improve performance hmms memmsn',\n",
       " 'Eric P. Xing Ni Lao Jun Zhu feature selection important task order achieve good generalizability high dimensional learn structure learn markov random field mrfs automatically discover inherent structure underlie complex data problem cast solve l1norm regularize parameter estimation problem exist graft method avoid inference dense graph structure learn incrementally select new feature graft perform greedy step optimize free parameter new feature include greedy strategy result low efficiency parameter learn nontrivial mrfs parameter learn depend expensive subroutine calculate gradient complexity calculate gradient mrfs typically exponential size maximal clique paper present fast algorithm call graftlight solve l1norm regularize maximum likelihood estimation mrfs efficient feature selection structure learn graftlight iteratively perform onestep orthantwise gradient descent free parameter select new feature lazy strategy guarantee converge global optimum effectively select significant feature synthetic real data set graftlight efficient graft feature selection structure learn perform comparably optimal batch method directly optimize feature feature selection efficient accurate structure learn mrfsn',\n",
       " 'Manuela M. Veloso Douglas L. Vail multirobot set activity recognition allow robot respond intelligently robot environment conditional random field temporal model suit activity recognition robustly incorporate rich nonindependent feature compute sensory data work explore feature selection conditional random field activity recognition choose feature include final model compare feature selection method graft greedy forwardselection strategy l1 regularization simultaneously smooth model select subset feature use robot data record game small size league robocupu002707 robot soccer world championship empirically compare performance feature selection algorithm term accuracy final model number feature select final model time require train final modeln',\n",
       " 'Bernt Schiele Stefan Roth Mario Fritz Paul Schnitzspan variety flexible model propose detect object challenge real world scene motivate successful technique propose hierarchical multifeature representation automatically learn flexible hierarchical object model wide variety object class end rely automatic selection relevant individual feature previous work automatically select model complex longrange feature couple model achieve generality flexibility work combine structure learn conditional random field discriminative parameter learn classifier hierarchical feature adopt efficient gradient base heuristic model selection carry forward discriminative multidimensional selection feature couple improve detection performance experimentally consistently outperform currently lead method 20 class pascal voc 2007 challenge achieve best publish result 16 20 classn',\n",
       " 'Masaru Kitsuregawa Naoki Yoshinaga paper propose efficient online method train classifier conjunctive feature employ kernel computation call kernel slice explicitly consider conjunction frequent feature compute polynomial kernel combine merit linear kernelbased train improve scalability train reuse temporal margin partial feature vector terminate unnecessary margin computation experiment dependency parse hyponymyrelation extraction demonstrate method train classifier order magnitude fast kernelbased online learn retain space efficiencyn',\n",
       " 'Chen Goldberg Eyal Krupka Dan Levi Aharon BarHillel introduce new approach learn partbased object detection feature synthesis method consist iterative process feature generation prune feature generation procedure present basic partbased feature develop feature hierarchy operator localization refine combination feature prune new feature selection algorithm linear svm term predictive feature selection pfs govern weight prediction algorithm make possible choose o106 feature efficient accurate manner analyze validity behavior pfs empirically demonstrate speed accuracy advantage relevant competitor present empirical evaluation method human detection datasets include current defacto benchmark inria caltech pedestrian datasets new challenge dataset child image difficult pose evaluation suggest approach par best current method advance stateoftheart caltech pedestrian train datasetn',\n",
       " 'Alice X. Zheng Kilian Q. Weinberger Gao Huang Zhixiang Xu feature selection algorithm ideally satisfy condition reliably extract relevant feature able identify nonlinear feature interaction scale linearly number feature dimension allow incorporation know sparsity structure work propose novel feature selection algorithm gradient boost feature selection gbfs satisfy requirement algorithm flexible scalable surprisingly straightforward implement base modification gradient boost tree evaluate gbfs real world data set match outperform state art feature selection algorithm scale large data set size naturally allow domainspecific informationn',\n",
       " 'I. Dan Melamed Joseph Turian present work advance accuracy train speed discriminative parse discriminative parse method generative component surpass generative baseline constituent parse minimal linguistic cleverness model incorporate arbitrary feature input parse state perform feature selection incrementally exponential feature space train demonstrate flexibility approach test parse strategy feature set implementation freely available httpnlpcsnyueduparsern',\n",
       " 'Gertjan van Noord Barbara Plank Daniel de Kok attractive property attributevalue grammar reversibility attributevalue grammar usually couple separate statistical component parse selection fluency rank propose reversible stochastic attributevalue grammar single statistical model employ parse selection fluency rankn',\n",
       " 'Jeff A Bilmes Koji Tsuda Kiyohito Nagano Yoshinobu Kawahara key problem machine learn feature selection active learn formulate submodular set function maximization present novel algorithm maximize submodular set function cardinality constraint xe2x80x94 algorithm base cuttingplane meethod implement iterative smallscale binaryinteger linear program procedure know problem nphard approximation factor achieve greedy algorithm theoretical limit polynomial time nonpolynomial time exact algorithm perform reasonably practice little literature problem important application algorithm guarantee exact solution finitely iteration converge fast practice efficiency cuttingplane mechanism provide meethod produce successively decrease upperbounds optimal solution algorithm provide successively increase lowerbounds accuracy current solution estimate point algorithm stop early desire degree tolerance meet evaluate algorithm sensor placement feature selection application show good performancen',\n",
       " 'Sophia Ananiadou Junichi Tsujii Yoshimasa Tsuruoka paper present chunkbased discriminative approach parse convert task parse series chunk task apply conditional random field crf model level chunk probability entire parse tree compute product probability individual chunk result parse perform bottomup manner best derivation efficiently obtain depthfirst search algorithm experimental result demonstrate simple parse framework produce fast reasonably accurate parsern',\n",
       " 'Daphne Koller Varun Ganapathi Suin Lee markov network commonly wide variety application range vision natural language computational biology current application rely heavily learn model structure markov network construct hand lack effective algorithm learn markov network structure data paper provide computationally efficient method learn markov network structure data method base use l1 regularization weight loglinear model effect bias model solution parameter zero formulation convert markov network learn problem convex optimization problem continuous space solve efficient gradient method key issue set unavoidable use approximate inference lead error gradient computation network structure dense explore use different feature introduction scheme compare performance provide result method synthetic data real world data set pixel value mnist data genetic sequence variation human hapmap data l1 base method achieve considerably high generalization performance standard l2base method gaussian parameter prior pure maximumlikelihood learn learn mrf network structure computational cost great learn parameter demonstrate existence feasible method important problemn',\n",
       " 'Alexander Vasserman Stefan Riezler present approach bound constraintrelaxation entropy maximization correspond doubleexponential prior 1 regularizer likelihood maximization loglinear model combine incremental feature selection regularization method establish maximum entropy model natural incorporation regularizer gradientbased feature selection follow perkins et al 2003 provide efficient alternative standard 1 regularization feature set mathematical justification thresholding technique likelihoodbased feature selection motivate extension nbest feature selection linguistic feature set moderate redundancy present experimental result show advantage 0 1best 1 2 regularization standard incremental feature selection task maximumentropy parsing1n',\n",
       " 'Andre Elisseeff Isabelle Guyon variable feature selection focus research area application datasets ten hundred thousand variable available area include text process internet document gene expression array analysis combinatorial chemistry objective variable selection threefold improve prediction performance predictor provide fast costeffective predictor provide good understand underlie process generate data contribution special issue cover wide range aspect problem provide good definition objective function feature construction feature rank multivariate feature selection efficient search method feature validity assessment methodn',\n",
       " 'Inderjit S. Dhillon ChoJui Hsieh nonnegative matrix factorization nmf effective dimension reduction method nonnegative dyadic data prove useful area text mine bioinformatics image process nmf usually formulate constrain nonconvex optimization problem algorithm develop solve recently coordinate descent method call fasthals propose solve square nmf regard stateoftheart technique problem paper fasthals inefficiency u cyclic coordinate descent scheme perform unneeded descent step unimportant variable present variable selection scheme u gradient objective function arrive new coordinate descent method new method considerably fast practice theoretical convergence guarantee solution sparse case real application new method benefit select important variable update result high speed example text dataset rcv1 method 7 time fast fasthals 15 time fast sparsity increase add l1 penalty develop new coordinate descent method error nmf measure kldivergence apply newton method solve onevariable subproblems experiment indicate algorithm minimize kldivergence fast lee u0026 seung multiplicative rule factor 10 cbcl image datasetn',\n",
       " 'Vladimir Vapnik Tomaso Poggio Massimiliano Pontil Olivier Chapelle Sayan Mukherjee Jason Weston introduce method feature selection support vector machine method base find feature minimize bind leaveoneout error search efficiently perform gradient descent result algorithm show superior standard feature selection algorithm toy data reallife problem face recognition pedestrian detection analyze dna microarray datan',\n",
       " 'Eric P. Xing Yiming Yang Fan Li lasso regression tend assign zero weight irrelevant redundant feature promise technique feature selection limitation offer solution linear model kernel machine feature scale technique study feature selection nonlinear model approach require solve hard nonconvex optimization problem paper propose new approach name feature vector machine fvm reformulate standard lasso regression form isomorphic svm form easily extend feature selection nonlinear model introduce kernel define feature vector fvm generate sparse solution nonlinear feature space tractable compare feature scale kernel machine experiment fvm simulate data encourage result identify small number dominate feature nonlinearly correlate response task standard lasso fail completen',\n",
       " 'Yiming Yang Fan Li learn structure large undirected graph thousand node data open challenge paper use graphical gaussian model ggm underlie model propose novel ard style wishart prior precision matrix ggm encode graph structure want learn prior map estimation precision matrix solve modify version lasso regression achieve sparse solution use approach learn genetic regulatory network genomewide expression microarray data proteinbinding location analysis data evaluate basis consistency annotation experiment approach good performance clusteringbased approach bn learn approach discover gene regulatory modulesn',\n",
       " 'Gideon Dror Eytan Ruppin Shay Cohen present study contributionselection algorithm csa novel algorithm feature selection algorithm base multiperturbation shapley analysis framework rely game theory estimate usefulness algorithm iteratively estimate usefulness feature select accordingly forward selection backward elimination empirical comparison exist feature selection method show backward eliminatination variant csa lead accurate classification result array datasetsn',\n",
       " 'Tommi Jaakkola Tony Jebara incorporate feature selection classification regression method carry number advantage paper formalize feature selection specifically discriminative perspective improve classification regression accuracy feature selection method develop extension recently propose maximum entropy discrimination med framework med flexible bayesian regularization approach subsume support vector classification regression exponential family model brevity restrict primarily feature selection context linear classificationregression method demonstrate propose approach carry substantial improvement practice discus develop extension feature selection include problem deal example specific unobserved degree freedom alignment invariantsn',\n",
       " 'Sanmay Das paper examine advantage disadvantage filter wrapper method feature selection propose new hybrid algorithm u boost incorporate feature wrapper method fast filter method feature selection empirical result report realworld datasets uci repository show hybrid algorithm competitive wrapper method fast scale datasets thousand featuren',\n",
       " 'James Theiler Simon Perkins standard feature selection problem give fix set candidate feature use learn problem select subset train model good possible accord criterion paper present interest useful variant online feature selection problem instead feature available start feature arrive time learneru0027s task select subset feature return correspond model time step good possible give feature see far argue exist feature selection method perform scenario promise alternative method base stagewise gradient descent technique graftingn',\n",
       " 'Daming Shi Paul Wing Hing Kwan Junbin Gao kernelized lasso absolute selection shrinkage operator investigate separate recent paper gao et al 2008 wang et al 2007 paper concern learn kernel lasso formula tion adopt generative bayesian learn inference approach new robust learn algorithm propose produce sparse kernel model capability learn regularize parameter kernel hyperparameter comparison stateoftheart method construct sparse regression model relevance vector machine rvm local regularization assist orthogonal square regression lrols give new algorithm demonstrate posse considerable computational advantagesn',\n",
       " 'Shigeo Abe Masamichi Ashihara Tsuneyoshi Ishii twoclass problem propose feature selection criterion base kernel discriminant analysis kda objective function kernel discriminant analysis call kda criterion kda criterion monotonic deletion feature ensure stable feature selection second recognition rate obtain kda classifier call kdabase recognition rate define onedimensional space obtain kda conditional probability datum give class calculate datum classify class maximum conditional probability ensure stable feature selection evaluate kdabase recognition rate crossvalidation experiment compare criterion twoclass problem recognition rate support vector machine svm evaluate crossvalidation call svmbase recognition rate selection performance kda criterion kdabase recognition rate comparable good svmbase recognition raten',\n",
       " 'Rong Yu Yong Tang Yan Pan HanJiang Lai recent year grow learn rank introduction feature selection different learn problem prove effective fact motivate investigate problem feature selection learn rank propose joint convex optimization formulation minimize rank error simultaneously conduct feature selection optimization formulation provide flexible framework easily incorporate importance measure similarity measure feature solve optimization problem use nesterovu0027s approach derive accelerate gradient algorithm fast convergence rate o1t2 develop generalization bind propose optimization problem rademacher complexity extensive experimental evaluation conduct public letor benchmark datasets result demonstrate propose method show 1 significant rank performance gain compare feature selection baseline rank 2 competitive performance compare stateoftheart learntorank algorithmsn',\n",
       " 'Thierry Denoeux Su Ruan Chunfeng Lian paper investigate way learn efficiently uncertain data belief function order extract knowledge imperfect insufficient information improve classification accuracy propose supervise learn method compose feature selection procedure twostep classification strategy train information propose feature selection procedure automatically determine informative feature subset minimize objective function propose twostep classification strategy improve decisionmaking accuracy complementary information obtain classification process performance propose method evaluate synthetic real datasets comparison classification method present highlightsa classifier base belief function tackle uncertain datathe classifier compose feature selection twostep classificationa new combination rule good represent data uncertaintya new feature selection base minimize uncertainty sparse constrainttwostep classification improve accuracy decision makingn',\n",
       " 'Changyin Sun Wankou Yang Liang Wang Yongzhen Huang Feng Liu spatial information important cue visual object analysis study field conduct rigid fragile efficiently utilize information paper propose model distribution object local appearance pattern cooccurrence different spatial location order represent distribution propose flexible framework call spatial feature copooling relation pattern discover final representation result framework high dimensionality propose semigreedy sg graft algorithm select discriminative feature experimental result cifar 10 uiuc sport voc 2007 datasets method effective comparable stateofart algorithmsn',\n",
       " 'Robert Tibshirani Holger Hofling consider problem estimate parameter structure binaryvalued markov network maximize penalize loglikelihood implement approximate procedure base pseudolikelihood besag 1975 generalize fast exact algorithm exact algorithm start pseudolikelihood solution adjust pseudolikelihood criterion additional iteration move close exact solution result procedure fast compete exact method propose lee ganapathi koller 2006a approximate pseudolikelihood approach wainwright et al 2006 implement coordinate descent procedure friedman hastie tibshirani 2008b fast exact method slightly accuraten',\n",
       " 'Jieping Ye Jianhui Chen Jun Liu logistic regression wellknow classification method widely application data mine machine learn vision bioinformatics sparse logistic regression embed feature selection classification framework l1norm regularization attractive application involve highdimensional data paper propose lassplore solve largescale sparse logistic regression specifically formulate problem l1ball constrain smooth convex optimization propose solve problem nesterovu0027s method optimal firstorder blackbox method smooth convex optimization critical issue use nesterovu0027s method estimation step size optimization iteration previous approach apply constant step size assume lipschitz gradient know advance require sequence decrease step size lead slow convergence practice paper propose adaptive line search scheme allow tune step size adaptively guarantee optimal convergence rate empirical comparison stateoftheart algorithm demonstrate efficiency propose lassplore algorithm largescale problemsn',\n",
       " 'O.H. Elgawi paper aim contribute merit online ensemble learn classification problem end induce random forest algorithm online mode estimate importance variable incrementally base correlation rank cr test method ldquoincremental hill climbingrdquo algorithm feature greedily add ldquoforwardrdquo step f remove ldquobackwardrdquo step resort implementation combine cr f implementation corrf corrbe respectively evaluation base public uci database demonstrate method achieve comparable performance classifier construct batch train addition framework allow fair comparison batch mode feature selection approach gini index relieff gain ration',\n",
       " 'James Theiler Don Hush Damian Eads Reid Porter investigate stack filter function class like weight order statistic apply classification problem lead new design criterion linear classifier input binaryvalued weight positive present rankbased measure margin directly optimize standard linear program investigate relationship regularization approach robustly combine large number base hypothesis similar performance type regularizationn',\n",
       " 'James Theiler Damian Eads Karen Glocer online feature selection ofs provide efficient way sort large space feature particularly scenario feature space large feature significant memory store image process operator especially combination image process operator provide rich space potential feature use machine learn image process task expensive generate store paper apply ofs problem edge detection grayscale imagery use standard data set compare result obtain traditional edge detector result obtain recently statistical edge detection compare different ofs approach include hill climb best search graftingn',\n",
       " 'Marc Toussaint Nikolay Jetchev trajectory plan optimization fundamental problem articulate robotics algorithm typically problem compute optimal trajectory scratch new situation effect extensive data accumulate contain situation respective optimize trajectory data practice hardly exploit aim paper learn data give new situation want predict suitable trajectory need minor refinement conventional optimizer approach essential ingredient generalize previous situation new one need appropriate situation descriptor propose sparse feature selection approach wellgeneralizing feature situation second transfer previously optimize trajectory new situation joint angle space propose efficient task space transfer old trajectory new situation experiment simulate humanoid reach problem predict reasonable motion prototype new situation refinement fast optimization scratchn',\n",
       " 'Hideki Isozaki Jun Suzuki Taku Kudo paper introduce new application boost parse reranking parser propose utilize allsubtrees representation tree kernel data orient parse paper argue allsubtrees representation extremely redundant comparable accuracy achieve small set subtrees boost algorithm apply allsubtrees representation select small relevant feature set efficiently experiment parse reranking method achieve comparable good performance kernel method improve test efficiencyn',\n",
       " 'Tommy W. S. Chow Zhaohui Gan D. Huang paper focus enhance effectiveness filter feature selection model aspect featuresearching engine modify base optimization theory second point injection strategy design improve regularization capability feature selection second topic important overfitting usually experience evaluate propose strategy implement strategy modify classic filter feature selection model model base sequential forward search scheme employ genetic algorithm ga feature selection compare original modify model synthetic real data contribution modification shownn',\n",
       " 'Dacheng Tao Chang Xu Jun Yu Liu Liu Maoying Qiao multipleinstance learn mil popular topic study pattern recognition year usefulness task drug activity prediction imagetext classification typical mil set bag contain baglevel label instancepattern bridge instancelevel representation baglevel label key step achieve satisfactory classification accuracy result paper present supervise learn method diversify dictionary mil address problem approach hand exploit baglevel label information train classspecific dictionary hand introduce diversity regularizer classspecific dictionary avoid ambiguity best knowledge time diversity prior introduce solve mil problem experiment conduct benchmark drug activity imagetext annotation datasets propose method compare favorably stateoftheart method highlightsthis paper present supervise diversify dictionary mil address problem bridge instancelevel representation baglevel labelthe propose method exploit baglevel label information train classspecific dictionarythe propose method introduce diversity regulariser classspecific dictionary avoid ambiguity themto best knowledge time diversity prior introduce solve mil problemn',\n",
       " 'Christopher Re Alexander Ratner Bryan He Stephen H. Bach curating label train data primary bottleneck machine learn recent framework address bottleneck generative model synthesize label scale weak supervision source generative modelu0027s dependency structure directly affect quality estimate label select structure automatically label data distinct challenge propose structure estimation method maximize l1regularized marginal pseudolikelihood observe data analysis show unlabel data require identify true structure scale sublinearly number possible dependency broad class model simulation method 100x fast maximum likelihood approach select 14 extraneous dependency method provide average 15 f1 point improvement exist userdeveloped information extraction application realworld data pubmed journal abstractsn',\n",
       " 'Kumiko TanakaIshii Kotaro Kitagawa relational lasso method incorporate feature relation machine learn automatically obtain noisy relation feature relational lasso learn additional penalty parameter feature incorporate term regularizer target optimization function relational lasso test different task text categorization polarity estimation parse compare conventional lasso adaptive lasso zou 2006 multiclass logistic regression optimization method relational lasso outperform lasso method testsn',\n",
       " 'Amaury Lendasse Feng Zhang Guangzhi Qu Paula Lauren abstract unstructured nature clinical narrative make complex automatically extract information feature learn important precursor document classification subdiscipline natural language process nlp nlp word document embeddings effective approach generate word document representation vector lowdimensional space paper u skipgram paragraph vectordistributed bag word pvdbow multiple discriminant analysis mda arrive discriminant document embeddings kernelbased extreme learn machine elm map clinical text medical code experimental result clinical text indicate overall improvement especially minority classesn',\n",
       " 'Zhenzhen Sun Yuanlong Yu supervise learn algorithm extreme learn machine elm propose singlehidelayer feedforward neural network slfn show great generalization performance elm randomly assign weight bias input hide layer train weight hide output layer physiological research show neuron layer laterally inhibit output layer type sparse cod difficult accommodate lateral inhibition directly random feature map elm paper propose sparse cod elm scelm algorithm map input feature vector sparse representation map feature sparse propose scelm algorithm unsupervise way sparse cod sense dictionary randomly assign learn gradient projection gp base method sparse cod output weight train supervise way elm present experimental result benchmark database show propose scelm algorithm outperform stateofthe art method term classification accuracyn',\n",
       " 'Hong Li Hongkai Zhao Hongfeng Li paper propose novel simple multilayer feature learn method image classification employ extreme learn machine elm propose algorithm compose stage multilayer elm mlelm feature map stage elm learn stage mlelm feature map stage recursively build alternate feature map construction maximum pool operation particular input weight construct feature map randomly generate need train tune make algorithm highly efficient maximum pool operation enable algorithm invariant certain transformation elm learn stage elasticnet regularization propose learn output weight elasticnet regularization help learn compact meaningful output weight addition preprocess input data dense scaleinvariant feature transform operation improve robustness invariance algorithm evaluate effectiveness propose method experiment conduct challenge database compare conventional deep learn method relate one propose method achieve best classification result high computational efficiencyn',\n",
       " 'Xiangfei Kong Ke Li Xiaodong Li Ran Wang Sam Kwong Jingjing Cao compare conventional weight vote method classspecific soft vote cssv advantage hand deal soft class probability output refine weight classifier class hand classspecific weight improve combinative performance increase computational load paper propose weight optimization base ensemble method cssvelm spacssvelm framework cssv scheme multiple extreme learn machine elm design model term accuracy sparsity aspect respectively firstly cssvelm take advantage condition number matrix reveal stability linear equation determine weight base elm classifier model reduce unreliability induce randomly input parameter single elm solve illconditioned problem cause linear structure elm simultaneously secondly sparse ensemble method low memory requirement speed classification process classifierpecific weight level spacssvelm method propose transform weight optimization problem sparse cod problem u sparse representation technique maintain classification performance nonzero weight coefficient experiment carry uci data set finance event series data experimental result superior performance cssv base elm algorithm compare stateoftheart algorithmn',\n",
       " 'Ioannis Pitas Anastasios Tefas Alexandros Iosifidis paper propose extension extreme learn machine algorithm singlehidden layer feedforward neural network train incorporate dropout dropconnect regularization optimization process type regularization lead solution network output weight calculation adopt propose dropelm network propose algorithm able exploit dropout dropconnect regularization computationally intensive iterative weight tune adoption regularization approach lead good solution network output weight incorporate propose regularization approach recently propose elm algorithm performance enhance require additional computational costn',\n",
       " 'BaoLiang Lu Xianzhong Long Suhang Wang Yong Peng extreme learn machine elm propose new algorithm train single hide layer fee forward neural network main merit elm lie fact input weight hide layer bias randomly generate output weight obtain analytically overcome drawback incur gradientbase train algorithm local optimum improper learn rate low learn speed base consistency property data enforce similar sample share similar property propose discriminative graph regularize extreme learn machine gelm enhance classification performance paper propose gelm model label information train sample construct adjacent graph correspondingly graph regularization term formulate constrain output weight learn similar output sample class propose gelm model close form solution standard elm output weight obtain efficiently experiment widely face database propose gelm achieve performance gain standard elm regularize elm gelm perform compare stateoftheart classification method face recognitionn',\n",
       " 'BaoLiang Lu Yong Peng extreme learn machine elm u noniterative method train singlehidelayer feedforward network slfns prove efficient effective learn model classification regression main advantage elm lie input weight hide layer bias randomly generate contribute analytical solution output weight paper propose discriminative manifold elm dmelm simultaneously consider discriminative information geometric structure data specifically exploit discriminative information local neighborhood data point end graph regularizer base newly design graph laplacian characterize property formulate incorporate elm objective dmelm output weight obtain analytical form extensive experiment conduct image eeg signal classification evaluate effectiveness dmelm result dmelm consistently achieve good performance original elm yield promise result comparison stateoftheart algorithm suggest discriminative manifold information beneficial classificationn',\n",
       " 'BaoLiang Lu Yong Peng abstract order seek nonpropagation method train generalize singlehide layer fee forward neural network extreme learn machine propose prove effective efficient model multiclass classification regression different exist study consider extreme learn machine classifier improvement let feature extraction model paper specifically discriminative extreme learn machine supervise sparsity preserve spelm model propose hide layer output layer spelm perform subspace learn method consider discriminative sparsity information data sparsity information data identify solve supervise sparse representation objective experiment conduct widely image benchmark data set classification result demonstrate effectiveness propose spelm modeln',\n",
       " 'GuangBin Huang Xiaofan Jia Jichao Chen Yue Li Yijie Zeng abstract dictionary learn widely adopt approach image classification exist method focus find dictionary produce discriminative sparse representation enforce prior best dataset distribution case dataset size small large intraclass variability nondiscriminative feature space work propose simple effective framework call elmddl address issue specifically represent input feature extreme learn machine elm orthogonal output projection enable diverse representation nonlinear hide space task specific feature learn output space embeddings regularize maximum margin criterion mmc maximize interclass variance minimize intraclass variance dictionary learn design novel weight class specific l 1 2 norm regularize sparse cod vector promote uniformity sparse pattern sample belong class suppress support overlap different class regularization robust discriminative easy optimize propose method combine sparse representation classifier src evaluate benchmark datasets result approach achieve stateoftheart performance compare dictionary learn methodn',\n",
       " 'JeanPhilippe Urban Philippe Smagghe JeanLuc Buessler paper describe structure image receptive field neural network irfnn introduce recently team structure extend simplify learn introduce extreme learn machine reservoir compute technique field image neuron organize single hide layer feedforward network architecture original organization emailxc2xa0protecteds input weight represent color image efficiently prior feature extraction weight value link neuron determine 2d gaussian function activation neuron image present property nonlinear localize receptive field parameterized small number degree freedom network compose large number neuron associate randomly initialize constant receptive field induce remarkable representation image supervise train determine output weight network extremely fast retropropagation iteration adapt large set image network easy implement present excellent generalization performance classification application allow detection unknown input efficiency technique illustrate benchmark photo video datasetn',\n",
       " 'Wei Jiang Weijie Mao Xiaodong Li extreme learn machine elm new method singlehidden layer feedforward network slfns simple train method conventional extreme learn machine base train test data distribution reality desirable learn accurate model tiny new data large old data transfer learn tl aim solve relate different target domain problem plenty label source domain data task new domain come new domain sample relabel costly waste discard old domain data algorithm call tlelm base elm algorithm propose u small target domain tag data large number source domain old data build highquality classification model method inherit advantage elm make defect traditional elm transfer knowledge experimental result indicate performance propose method superior comparable exist benchmarking method addition novel domain adaptation kernel extreme learn machine tldakelm base kernel extreme learn machine propose respect tlelm experimental result effectiveness propose algorithmn',\n",
       " 'Saeid Nahavandi Thanh Nguyen Abbas Khosravi Syed Moshfeq Salaken succinctly concisely summarize current work extreme learn base transfer learn taskswork start point future opportunity domainprovide recommendation future experiment extreme learn machine elm increasingly popular field transfer learn tl simplicity train speed ease use online sequential learn process paper critically examine transfer learn algorithm formulate elm technique provide state art knowledge expedite learn process elm base tl algorithm article discus available elm base tl algorithm provide holistic overview current literature serf start point new researcher elm base tl algorithm facilitate identification future research direction concise mannern',\n",
       " 'Dan Xiao Ning Lu AhHwee Tan paper present neural architecture learn category node encode map multimodal pattern involve sensory input action reward integrate adaptive resonance theory art temporal difference td method propose neural model call td fusion architecture learn cognition navigation tdfalcon enable autonomous agent adapt function dynamic environment immediate delay evaluative feedback reinforcement signal tdfalcon learn value function stateaction space estimate onpolicy offpolicy td learn method specifically stateactionrewardtateaction sarsa qlearn learn value function determine optimal action base action selection policy develop tdfalcon system td learn strategy compare performance term task completion learn speed time space efficiency experiment base minefield navigation task show tdfalcon system able learn effectively immediate delay reinforcement achieve stable performance pace fast standard gradientdescentbase reinforcement learn systemn',\n",
       " 'Chunxia Zhang Guang Shi Nannan Ji Jiangshe Zhang Fang Du abstract deep learn successfully apply multimodal representation learn similar single modal deep learn method multimodal deep learn method consist greedy layerwise feedforward propagation backpropagation bp finetune conduct diverse target model drawback time consume extreme learn machine elm fast learn algorithm single hide layer feedforward neural network previous work show effectiveness elm base hierarchical framework multilayer perceptron paper introduce elm base hierarchical framework multimodal data propose architecture consist main component 1 selftaught feature extraction specific modality elmbase sparse autoencoder 2 fuse representation learn base feature learn previous step 3 supervise feature classification base fuse representation exact feedforward framework layer establish weight fix finetuning good learn efficiency gradient base multimodal deep learn method conduct experiment mnist xrmb nu datasets propose algorithm obtain fast convergence achieve good classification performance compare exist multimodal deep learn modeln',\n",
       " 'BaoLiang Lu WeiLong Zheng Yong Peng extreme learn machine elm initially propose train singlelayer feedforward network slfns provide unify efficient effective framework regression multiclass classification elm variant propose recent year focus supervise learn scenario little effort extend unsupervise learn paradigm great significance elm learn task unlabeled data popular approach mine knowledge unlabeled data base manifold assumption exploit geometrical structure data assume nearby point close transformation space consider manifold information insufficient discriminative task paper propose improve unsupervise discriminative elm udelm model main advantage combine local manifold learn global discriminative learn udelm efficiently optimize solve generalize eigenvalue decomposition problem extensive comparison stateoftheart model cluster image emotional eeg data demonstrate efficacy udelmn',\n",
       " 'Zhiping Lin GuangBin Huang Chamara Kasun Liyanaarachchi Lekamalage Tianchi Liu abstract cluster generic data data specific particular field challenge problem diverse complex structure original feature space traditional approach address problem complement cluster feature learn method capture intrinsic structure data represent data cluster good reveal paper propose approach refer extreme learn machine joint embed cluster elmjec incorporate desirable property type feature learn method time specifically 1 preserve manifold structure data original space 2 maximize class separability data embed space type method improve cluster performance case motivation integrate desirable property improve accuracy robustness cluster additional notable feature elmjec provide nonlinear feature map achieve feature learn cluster formulation propose approach implement alternate optimization cluster performance compare favorably stateoftheart method realworld benchmark datasetsn',\n",
       " 'EamKhwang Teoh JianGang Wang PhyoKyaw Sai face image base age estimation approach classify face image predefined agegroup challenge facial age variation specific give individual determine personu0027s gene external factor exposure weather gender live style age estimation multiclass problem number class predict large surely facial age trend face close age range similar facial age feature difficult distinct facial age feature age facial age feature overlap nearby age group age life continuous nature paper emphasise work age range estimation predefined class apply fast efficient machine learn method extreme learn machine solve age categorization problem local gabor binary pattern biologically inspire feature gabor adopt represent face image age estimation perform different age datasets experimental result report demonstrate effectiveness robustnessn',\n",
       " 'GuangBin Huang Weiwei Zong abstract extreme learn machine elm efficient learn algorithm generalize single hide layer feedforward network slfns perform regression classification application recently show optimization point view elm support vector machine svm equivalent elm stringent optimization constraint mild optimization constraint elm easy implementation usually obtain good generalization performance paper study performance oneagainstall oaa oneagainstone oao elm classification multilabel face recognition application performance verify benchmarking face image data setsn',\n",
       " 'Erik Cambria Newton Howard Amir Hussain Haiyun Peng Soujanya Poria abstract advent social web enable internet connection easily create share idea opinion content million people world pace global deluge video billion computer smartphones tablet university projector security camera multimodal content web grow exponentially come need decode information useful knowledge paper multimodal affective data analysis framework propose extract user opinion emotion video content particular multiple kernel learn combine visual audio textual modality propose framework outperform stateoftheart model multimodal sentiment analysis research margin 10xe2x80x9313 3xe2x80x935 accuracy polarity detection emotion recognition respectively paper propose extensive study decisionlevel fusionn',\n",
       " 'G. Monfardini M. Hagenbuchner Ah Chung Tsoi M. Gori F. Scarselli underlie relationship data area science engineer vision molecular chemistry molecular biology pattern recognition data mine represent term graph paper propose new neural network model call graph neural network gnn model extend exist neural network method process data represent graph domain gnn model directly process practically useful type graph acyclic cyclic direct undirect implement function taugn isin irm map graph g node n mdimensional euclidean space supervise learn algorithm derive estimate parameter propose gnn model computational cost propose algorithm consider experimental result show validate propose learn algorithm demonstrate generalization capabilitiesn',\n",
       " 'Euntai Kim Andrew Beng Jin Teoh Peyman Hosseinzadeh Kassani abstract extreme learn machine elm popular analytic single hide layer feedforward neural network rapid learn capacity vanilla dense elm affect overfitting problem number hide neuron high direct consequence density decrease train prediction speed study propose incremental method sparsifying elm newly devise indicator drive condition number elm design matrix sparse pseudoinverse incrementalelm spielm spielm exhibit good generalization performance low runtime complexity compare elm sparsification process negatively affect learn speed spielm introduce iterative matrix decomposition algorithm address issue demonstrate useful relationship condition number elm design matrix number hide neuron relationship help understand random weight nonlinear activation function elm evaluate spielm method base 20 benchmark data set university california irvine repository realworld database vision domainn',\n",
       " 'Gian Luca Foresti Christian Micheloni Claudio Piciarelli Niki Martinel food recognition emerge vision topic problem characterize absence rigid structure food large intraclass variation exist approach tackle problem design adhoc feature representation base priori knowledge problem differently propose committeebase recognition choose optimal feature exist plethora available one color texture committee member extreme learn machine train classify food plate basis single feature type single member classification consider structural support vector machine produce final rank possible match achieve filter irrelevant featureclassifiers consider relevant one experimental result propose outperform stateoftheart work publicly available benchmark datasetsn',\n",
       " 'Yaoming Cai Zhihua Cai Xiaobo Liu Yongshan Zhang Yu Wu abstract extreme learn machine elm popular machine learn method widely apply realworld problem fast train speed good generalization performance elm randomly assign input weight hide bias usually degrade generalization performance furthermore elm consider empirical risk minimization model easily lead overfitting dataset exist outlier paper propose novel algorithm name multiobjective optimizationbased sparse extreme learn machine moselm parameter optimization structure learn integrate learn process simultaneously enhance generalization performance alleviate overfitting problem moselm train error connect sparsity take conflict objective multiobjective model aim sparse connect structure optimal weight bias hybrid encodingbased moead optimize multiobjective model addition ensemble learn embed algorithm decision multiobjective optimization experimental result classification regression application demonstrate effectiveness propose moselmn',\n",
       " 'Wellington P. dos Santos Abel G. da Silva Filho Michel M. dos Santos abstract recent year deep convolutional neural network model increasingly vision task like plate number recognition object recognition automatic digit recognition medical application support diagnosis signal image disadvantage network long train time day adjust weight iterative method base gradient descent obstacle application need frequent train real time fast convolutional network avoid gradientbase method efficiently define filter feature extraction weight classification issue set convolutional filter bank learn backpropagation gradient work propose deep fast convolutional neural network base extreme learn machine fix bank filter demonstrate model feasible costeffective nonspecialized hardware perform train task fast model run gpus result generate emnist dataset represent widely study problem digit recognition provide deep convolutional extreme learn machine celm feature extraction stage combination select filter propose network empirical generalization error explain error model base theorem rahimi retch comparison stateoftheart propose network result superior accuracy competitive train time relation approach employ process gpusn',\n",
       " 'GuangBin Huang Chenwei Deng Jiexiong Tang extreme learn machine elm emerge learn algorithm generalize single hide layer feedforward neural network hide node parameter randomly generate output weight analytically compute shallow architecture feature learn elm effective natural signal imagesvideos large number hide node address issue paper new elmbased hierarchical learn framework propose multilayer perceptron propose architecture divide main component 1 selftaught feature extraction follow supervise feature classification 2 bridge random initialize hide weight novelty paper follow 1 unsupervise multilayer encode conduct feature extraction elmbased sparse autoencoder develop l 1 constraint achieve compact meaningful feature representation original elm 2 exploit advantage elm random feature map hierarchically encode output randomly project final decision make lead good generalization fast learn speed 3 unlike greedy layerwise train deep learn dl hide layer propose framework train forward manner previous layer establish weight current layer fix finetuning good learn efficiency dl extensive experiment widely classification data set propose algorithm achieve good fast convergence exist stateoftheart hierarchical learn method furthermore multiple application vision confirm generality capability propose learn schemen',\n",
       " 'Zhengyou Zhang GuangBin Huang Yan Yang Liyanaarachchi Lekamalage Chamara Kasun data contain noise irrelevant information negatively affect generalization capability machine learn algorithm objective dimension reduction algorithm principal component analysis pca nonnegative matrix factorization nmf random projection rp autoencoder ae reduce noise irrelevant information data feature pca eigenvectors linear ae able represent data part nose face image hand nmf nonlinear ae maim slow learn speed rp represent subspace original data paper introduce dimension reduction framework extend represent data part fast learn speed learn betweenclass scatter subspace end paper investigate linear nonlinear dimension reduction framework refer extreme learn machine ae elmae sparse elmae selmae contrast tie weight ae hide neuron elmae selmae need tune parameter input weight additive neuron initialize orthogonal sparse random weight respectively experimental result usps handwritten digit recognition data set cifar10 object recognition norb object recognition data set efficacy linear nonlinear elmae selmae term discriminative capability sparsity train time normalize mean square errorn',\n",
       " 'Ken Liang Dongshun Cui GuangBin Huang Kang Song Chamara Kasun Liyanaarachchi Lekamalage fully connect multi layer neural network deep boltzmann machine dbm perform good fully connect single layer neural network image classification task small number hide layer neuron extreme learn machine elm base fully connect multi layer neural network multi layer elm mlelm hierarchical elm helm mlelm helm small train time dbm paper introduce fully connect multi layer neural network refer multi layer multi objective extreme learn machine mlmoelm u multi objective formulation pa label nonlinear information order learn network model similar number hide layer parameter dbm small train time dbm experimental result mlmoelm outperform dbm mlelm helm ocr norb datasetsn',\n",
       " 'Sante Carloni Annalisa Riccardi Francisco FernandezNavarro ordinal regression important branch supervise learn multiclass classification regression paper traditional classification scheme neural network adapt learn ordinal rank model propose impose monotonicity constraint weight connect hide layer output layer weight transcribe pad variable reformulation lead socalled inequality constrain square icls problem numerical solution obtain iterative method example trust region line search algorithm proposal optimum determine analytically accord closedform solution icls problem estimate karushxe2x80x93kuhnxe2x80x93tucker condition furthermore follow guideline extreme learn machine framework weight connect input hide layer randomly generate final model estimate parameter iterative tune model propose achieve competitive performance compare stateoftheart neural network method orn',\n",
       " 'Yuhai Zhao Peizhen Gong Xin Bi Guoren Wang Xiangguo Zhao abstract paper xml document classification framework base extreme learn machine elm basis structure link vector model slvm optimize reduce structure vector space model rsvsm propose incorporate structural information feature vector efficiently optimize computation document similarity apply elm xml document classification achieve good performance extremely high speed compare conventional learn machine support vector machine voteelm algorithm propose improve accuracy elm classifier revote equal vote rev method revote confuse class rcc method propose postprocess vote result velm improve performance experiment conduct real world classification problem demonstrate voteelm classifier present paper achieve good performance elm algorithm respect precision recall fmeasuren',\n",
       " 'Xin Wang Lin Chen Shiguo Lian QingHua Zheng WanYu Deng recently new fast learn algorithm call extreme learn machine elm develop singlehidden layer feedforward network slfns gb huang qy zhu ck siew u0027u0027extreme learn machine theory applicationsu0027u0027 neurocomputing 70 2006 489501 elm successfully apply classification regression problem paper elm algorithm study ordinal regression problem name orelm firstly propose encodebase framework ordinal regression include encode scheme single multioutput classifier multiple binaryclassifications oneagainstall oaa decomposition method oneagainstone oao method slfn redesign ordinal regression problem base propose framework algorithm train extreme learn machine input weight assign randomly output weight decide analytically lastly widely experiment kind datasets carry test propose algorithm comparative result traditional method gaussian process ordinal regression orgp support vector ordinal regression orsvm orelm obtain extremely rapid train speed good generalization ability especially data setu0027s scalability increase advantage orelm apparent additionally orelm follow advantage include capability learn online batch mode handle nonlinear datan',\n",
       " 'Baiyou Qiao Xin Bi Xiangguo Zhao paper present novel solution base extreme learn machine elm multiclass xml document classification elm generalize singlehidden layer feedforward network slfn extremely fast learn capacity improve vector model dsvm distribution base structure vector model propose represent xml document structural information precise semantic information xml document classifier conduct base pvelm probablity base vote elm postprocessing method rcc revote confuse class refine vote result evaluate overall performance solution series experiment conduct real datasets news fee online experimental result dsvm represent xml document effectively pvelm rcc achieve high accuracy original elm algorithm multiclass classificationn',\n",
       " 'Wei Jiang Weijie Mao Xiaodong Li introduce fast sparse approximation scheme extreme learn machine elm name fsaelm extreme learn machine elm algorithm compel feature low complexity sparse solution experiment benchmark data set propose algorithm obtain sparse classifier low complexity sacrifice generalization performance validate simulation result fsaelm tend good scalability achieve similar good generalization performance fast learn speed traditional elm algorithmn',\n",
       " 'Aurelio Uncini Michele Scarpiniti Danilo Comminiello Simone Scardapane extreme learn machine elm recently propose unify framework different family learn algorithm classical elm model consist linear combination fix number nonlinear expansion input vector learn elm equivalent find optimal weight minimize error dataset update work batch mode explicit feature map implicit map define kernel online version propose work point efficient learn algorithm online kernelbased elm exist remain open problem explicate connection nonlinear adaptive filter elm theory brief present algorithm task particular propose straightforward extension wellknown kernel recursive leastsquares belong kernel adaptive filter kaf family elm framework result algorithm kernel online sequential elm koselm consider different criterion kaf field obtain sparse filter extend context koselm integration result highly efficient algorithm term obtain generalization error train time empirical evaluation demonstrate interest result benchmarking datasetsn',\n",
       " 'HongBin Shen YongXian Fan pupylation important posttranslational modification prokaryotic protein play key role regulate wild range biological process prokaryotic ubiquitinlike protein attach specific lysine residue substrate protein form isopeptide bond selective degradation protein mycobacterium tuberculosis order comprehensively understand pupylationrelated biological process identification pupylation site substrate protein sequence step traditional wetlab experimental approach laborious timeconsuming timely effectively discover pupylation site face avalanche new protein sequence emerge postgenomic era novel computational predictor call pup pupylation site predictor propose pup construct pseudoamino acid composition train extreme learn machine jackknife crossvalidation result train dataset area roc curve auc value 06483 pup auc 06779 obtain independent set result demonstrate elm complementary algorithm construct ensemble classifier generate good result pup software package available httpwwwcsbiosjtueducnbioinfpupn',\n",
       " 'Yiqiang Chen GuangBin Huang Weiwei Zong extreme learn machine elm competitive machine learn technique simple theory fast implementation network type u0027u0027generalizeu0027u0027 single hide layer feedforward network diversify form variety feature map function kernel deal data imbalance class distribution weight elm propose able generalize balance data propose method maintain advantage original elm 1 simple theory convenient implementation 2 wide type feature map function kernel available propose framework 3 propose method apply directly multiclass classification task addition integrate weight scheme 1 weight elm able deal data imbalance class distribution maintain good performance balance data unweight elm 2 assign different weight example accord usersu0027 need weight elm generalize cost sensitive learnn',\n",
       " 'Hongyu Zhao Ye Chen Zhelong Wang Donghui Wu balance dataset utilize previous human activity recognition algorithm train classifier imbalance dataset ubiquitous human activity recognition especially case abnormal activity detection class imbalance problem exist universal phenomenon human activity recognition research mention problem solve order reduce influence imbalance datasets problem mixkernel base weight extreme learn machine mkwelm propose paper consider performance extreme learn machine elm greatly influence choice kernel mix kernel method propose elm order deal imbalance problem cost sensitive method utilize main idea cost sensitive method cost minority class increase misclassification rate consider cost sensitive function mix kernel method mkwelm construct compare elm weight elm method experimental result different human activity datasets demonstrate effectiveness propose methodn',\n",
       " 'Weidong Yang Sen Zhang Yanjiao Li Jie Zhang Wendong Xiao abstract fast speed good generalization performance extreme learn machine elm attract attention effective learn approach elm rarely involve strategy imbalanced data distribution exist field exist approach imbalance learn consider effect number class sample ignore dispersion degree data lead suboptimal learn result paper propose novel elm classspecific cost regulation extreme learn machine ccrelm kernel base extension binary multiclass classification problem imbalanced data distribution ccrelm introduce classspecific regulation cost misclassification class performance index tradeoff structural risk empirical risk performance ccrelm verify number benchmark datasets real blast furnace status diagnosis problem experimental result ccrelm achieve good performance classification problem imbalanced data distribution original elm exist elm imbalance learn approach kernel base ccrelm improve performance furthern',\n",
       " 'Rodolfo Zunino Federica Bisio Paolo Gastaldo Erik Cambria dawn internet year 2003 dozen exabyte information web today information create weekly opportunity capture opinion general public social event political movement company strategy market campaign product preference raise increase scientific community excite open challenge business world remarkable fallout market financial prediction keep evergrowing unstructured information web formidable task require fast efficient model opinion mine paper explore high generalization performance low computational complexity fast learn speed extreme learn machine exploit perform analogical reason vector space model affective commonsense knowledge particular enable fast reconfiguration vector space extreme learn machine allow polarity associate natural language concept calculate dynamic accurate way perform good conceptlevel sentiment analysisn',\n",
       " 'Jianping Yin Chengzhang Zhu Siqi Wang Qiang Liu Xinwang Liu Sihang Zhou paper propose novel algorithm term random fourier extreme learn machine 2 1 norm regularization improve robustness compactness widely extreme learn machine specific firstly introduce random fourier map activation function approximate gaussian kernel aim improve extendibility powerful kernel elm algorithm adopt 2 1 norm eliminate potential irrelevant neuron result compact discriminative hide layer propose efficient algorithm prove convergence solve resultant optimization problem extensive experiment conduct 30 benchmark data set compare propose algorithm popular extreme learn algorithm observe algorithm outperform enumerate hide layer reinforcement algorithm addition significantly improve computational efficiency gaussian kernel extreme learn machine comparable classification regression performance large scale learn scenariosn',\n",
       " 'Dong Sun Park Bo Liu Feilong Cao work new image classification method propose base extreme kmeans ekm effective extreme learn machine eelm propose method image decomposition curvelet transform reduce dimensionality discriminative locality alignment dla generate set distinctive feature ekm classification eelm ekm good cluster performance kmeans eelm good accuracy elm propose ekmeelm algorithm significant improvement classification rate extensive experiment perform challenge database result compare state art technique experimental result propose method superior performance classification rate traditional method image classificationn',\n",
       " 'QingHua Zheng Puay Siew Tan YewSoon Ong WanYu Deng paper present online sequential reduce kernel extreme learn machine osrkelm osrkelm small instance original train sample employ train kernel neuron output weight attain analytically similar online sequential extreme learn machine oselm osrkelm learn data sample chunkbychunk onebyone mode require archival data sample learn osrkelm contain control parameter avoid need cumbersome finetuning algorithm osrkelm support widespread type kernel hide neuron capable address singular problem arise initial train sample small neuron size comprehensive performance evaluation osrkelm stateoftheart sequential learn algorithm include oselm largescale active support vector machine lasvm budget stochastic gradient descent support vector machine bsgd popular time series regression classification benchmark conduct experimental result obtain indicate propose osrkelm showcase improve prediction accuracy efficiency oselm lasvm bsgd casesn',\n",
       " 'GeokChoo Tan KarAnn Toh structural resemblance exist classifier motivate investigate underlie relationship explore map solution classifier link simple feature data scale word key relationship lie replica feature data scale find lead directly exploration novel classifier exist set base extensive empirical evaluation propose formulation facilitate tune capability exist set classifier generalization highlightsestablishment relationship binary classifierexploration random projection network classifier learningproposal novel classifier exploit establish relationshipobserve performance novel classifier performance measuresn',\n",
       " 'QingHua Zheng GuangBin Huang Zuo Bai WanYu Deng abstract big dimensional data grow trend emerge real world context extend web mine gene expression analysis proteinxe2x80x93protein interaction highfrequency financial data nowadays grow consensus increase dimensionality pose impede effect performance classifier term xe2x80x9cpeaking phenomenonxe2x80x9d field machine intelligence address issue dimensionality reduction commonly employ preprocessing step big dimensional data build classifier paper propose extreme learn machine elm approach largescale data analytic contrast exist approach embed hide node design singular value decomposition svd classical elm svd node hide layer show capture underlie characteristic big dimensional data exhibit excellent generalization performance drawback svd entire dataset high computational complexity involve address fast divide conquer approximation scheme introduce maintain computational tractability high volume data resultant algorithm propose label fast singular value decompositionhidenode base extreme learn machine fsvdhelm short fsvdhelm instead identify svd hide node directly entire dataset svd hide node derive multiple random subset data sample original dataset comprehensive experiment comparison conduct as fsvdhelm stateoftheart algorithm result obtain demonstrate superior generalization performance efficiency fsvdhelmn',\n",
       " 'Zhisong Pan Yi Du Dong Li Yibing Wang machine learn algorithm widely traffic classification anomaly detection nowadays fast accurately classify flow remain extremely challengeable paper propose extreme learn machine elm base algorithm call l1norm minimization elm fully inherit merit elm exhibit sparsityinduced characteristic reduce complexity learn model evaluation stage preprocessed raw data trace transpacific backbone link japan unite state generate 248 feature datasets empirical study show l1elm achieve good generalization performance evaluation datasets preserve fast learn little human intervene advantage elm hasn',\n",
       " 'Jaihie Kim Andrew Beng Jin Teoh KarAnn Toh BeomSeok Oh aim extract robust facial feature pose variation paper present directional projection correspond extraction vertical horizontal local face image feature match score compute horizontal vertical feature subsequently fuse score level extreme learn machine optimize total error rate performance enhancement order benchmark performance feature extraction fusion result compare popular face recognition method principal component analysis linear discriminant analysis term equal error rate cpu time empirical experiment data set encourage result considerable horizontal pose variationn',\n",
       " 'Yong Zhou FanRong Meng ShiXiong Xia Bing Liu traditional manifold learn algorithm locally linear embed isomap laplacian eigenmap provide embed result train sample extension approach try solve outofsample extension problem computation avoid eigendecomposition dense matrix expensive time memory solve problem spectral regression sr cast problem learn embed function regression framework motivate effectiveness extreme learn machine elm paper solve outofsample extension problem seek embed function elm feature space extreme spectral regression esr algorithm propose speed kernelbased sr ksr addition prove esr approximation ksr similar sr propose esr algorithm perform supervise unsupervise semisupervise situation experimental result classification semisupervise classification demonstrate effectiveness efficiency algorithmn',\n",
       " 'Ioannis Pitas Anastasios Tefas Alexandros Iosifidis paper propose approximation scheme kernel extreme learn machine algorithm singlehidden layer feedforward neural network train large scale classification problem approximate kernel extreme learn machine able scale computational cost memory achieve good generalization performance regularize version extension order exploit total withinclass variance train data feature space propose extensive experimental evaluation mediumscale largescale classification problem denote propose approach able operate extremely fast train test phase provide satisfactory performance outperform relate classification schemesn',\n",
       " 'Amaury Lendasse Maite Termenon Philippe du Jardin Eric Severin KajMikael Bjork David Veganzones Anton Akusok Yoan Miche paper present new cluster technique base extreme learn machine elm cluster technique incorporate priori knowledge expert define optimal structure cluster number point cluster elm propose cluster problem formulation rewrite travel salesman problem solve heuristic optimization method second propose cluster problem formulation include priori knowledge selforganization base predefined map string cluster method successfully test 5 toy example 2 real datasetsn',\n",
       " 'Yuhong Feng Zhiqi Shen Chunyan Miao Haoming Zhong corporate credit rat key problem credit risk management attract research attention credit crisis 2007 scorecard widely approach corporate credit rat nowadays heavy dependency involvement user ai technology artificial neural network anns support vector machine svms demonstrate remarkable performance automatic corporate credit rat corporate credit rat involve rat model output scale multiple level application inherent complexity give rise requirement high demand effectiveness learn algorithm accuracy overfitness error distribution output distribution research work svms good performance anns accuracy paper carry comprehensive experimental comparison study effectiveness learn algorithm bp elm ielm svm data set consist real financial data corporate credit rat result present discuss papern',\n",
       " 'Yide Ma Yunsong Li Weiying Xie paper present novel computeraided diagnosis cad diagnosis breast cancer base extreme learn machine elm view mammographic image eliminate interference preprocessing stage preprocessed image segment level set model propose subsequently model multidimensional feature vector build feature vector contribute improvement performance feature selection combination support vector machine svm extreme learn machine elm finally optimal subset feature vector inputted classifier distinguish malignant mass benign one compare breast mass classification approach base elm stateoftheart classification model result propose cad good performance term specificity sensitivity accuracy achieve significant reduction train time compare svm particle swarm optimizationsupport vector machine psosvm ultimately achieve good performance average accuracy 9602 indicate propose segmentation model utilization select feature vector effective classifier elm provide satisfactory systemn',\n",
       " 'Zhongtang Zhao Mingjie Liu Yiqiang Chen Junfa Liu abstract indoor location estimation base wifi attract attention research industry field bring significant challenge require vast label calibration data realtime train test location estimation task traditional machine learn method high performance aspect paper propose novel semisupervised learn method selm semisupervised extreme learn machine apply sparse calibrate location estimation advantage propose selm employ graph laplacian regularization import large number unlabel sample dramatically reduce label calibration sample second inherit good property elm extreme train test speed comparative experiment number label sample method outperform original elm propagation bp network especially case calibration data sparsen',\n",
       " 'NaiYang Deng YongCui Wang YaFen Ye YuanHai Shao paper propose solve new machine learn problem call extensive semiquantitative regression information target value incomplete know low bind andor upper bind instead exact value employ information efficiently extensive semiquantitative regression introduce local graph capture geometric structure sample exact target value target bind construct graphbased support vector regressor call esqsvr efficiency esqsvr support result preliminary experiment conduct artificial real world datasets highlightswe propose new problem call extensive semiquantitative regressiona graphbased support vector regressor solve problemour approach capture geometric structure sampleit bound target value value rangesthe efficiency approach support result experimentn',\n",
       " 'Hao Dai Sanyang Liu Jin Xie abstract paper aim propose distribute semisupervised learn dssl algorithm solve dssl problem train sample extremely largescale locate distribute node communication network train data node consist label unlabel sample output value label unknown node communicate distribute way node access data exchange local information neighbor node scenario distribute data process centrally result dssl problem centrally solve traditional semisupervised learn ssl algorithm stateoftheart dssl algorithm denote distribute laplacian regularization square dlaprls kernel base algorithm essential dlaprls algorithm estimate global euclidian distance matrix edm respect total sample timeconsuming especially scale train data large order solve dssl problem overcome common drawback kernel base dssl algorithm propose novel manifold regularization mr base dssl algorithm wavelet neural network wnn zerogradientsum zgs distribute optimization strategy accordingly node assign individual wnn basis function order initialize propose dssl algorithm propose centralize mr base ssl algorithm wnn denote propose ssl dssl algorithm laplacian wnn lapwnn distribute lapwnn dlapwnn respectively dlapwnn algorithm work fully distribute fashion zgs strategy convergence guarantee lyapunov method learn process node exchange local coefficient neighbor raw data mean dlapwnn algorithm privacy preserve method illustrative simulation present efficiency advantage propose algorithmn',\n",
       " 'Hao Dai Sanyang Liu Jin Xie abstract paper aim propose distribute semisupervised learn ssl algorithm extreme learn machine elm timevarying communication network topology change time fix distribute ssl problem train data include label unlabel sample separately store node communication network centrally process order solve problem propose algorithm combine semisupervised elm sselm algorithm zerogradientsum zgs distribute optimization strategy sselm algorithm base manifold regularization mr framework approximate map sample node communication network zgs strategy train globally optimal coefficient single layer feedforward neural network slfnn correspond sselm algorithm denote propose algorithm distribute sselm dsselm algorithm train process node communication network exchange update coefficient raw data neighbor node mean dsselm algorithm fully distribute privacypreserving algorithm convergence propose dsselm guarantee lyapunov method simulation present efficiency propose algorithmn',\n",
       " 'Baoxian Wang GuangBin Huang Baojun Zhao Chenwei Deng Shuigen Wang increase number demand consumer digital multimedia application boost noreference nr image quality assessment iqa paper propose perceptual nr blur evaluation method new machine learn technique extreme learn machine elm propose metric blind image blur quality evaluator bibe exploit scene statistic gradient magnitude model property blur image underlie blur feature derive fit gradient magnitude distribution resultant feature finally map associate quality score elm subjective evaluation score human be integrate train machine learn technique predict image quality accurately traditional method compare learn technique support vector machine svm elm good learn performance fast learn speed experimental result public database propose bibe correlate human perceive blurriness outperform stateoftheart specific nr blur evaluation metric generic nr iqa method application automatic focus digital camera confirm capability biben',\n",
       " 'Sangyoun Lee Jaihie Kim KarAnn Toh receiver operate characteristic roc curve extensively performance evaluation multimodal biometrics fusion process fusion classifier design final roc performance evaluation usually conduct separately inevitable roc take error count point view wellposed structure link fusion classifier work propose optimize roc performance directly accord fusion classifier design area roc curve auc optimization objective provide good representation roc performance piecewise cumulative structure auc smooth approximate formulation propose enable direct optimization auc respect classifier parameter fusion classifier linear parameter computation solution optimize quadratic auc approximation surprisingly simple effective empirical experiment biometrics fusion strong evidence potential propose methodn',\n",
       " 'Jaihie Kim Andrew Beng Jin Teoh KarAnn Toh Kwontaeg Choi SeIn Jang abstract paper present visual object track tolerant external image factor illumination scale rotation occlusion background change specifically integration online version totalerrorrate minimization base projection network observation model particle filter propose effectively distinguish target object background reweighting technique propose stabilize sample particle filter stochastic propagation selfadaptation automatic update scheme extraction train sample propose adjust change online qualitative quantitative experiment 16 public video sequence convince performance term track accuracy computational efficiency compete stateoftheart algorithmsn',\n",
       " 'Ton Engbersen Yusuf Leblebici Valentin Cristea Stanislaw Wozniak AdelaDiana Almasi review provide highlevel synthesis significant recent advance artificial neural network research multidisciplinary concept connect farreaching goal obtain intelligent system assume global outlook interconnect field benefit researcher provide alternative viewpoint present different network neuron model discus model parameter mean obtain draw quick outline information encode proceed overview relevant learn mechanism range establish approach novel idea specifically focus compare classical artificial model biologicallyfeasible spike neuron comparison discusion biological plausibility learn approachn',\n",
       " 'Anhua Wan Xia Liu paper concern universal consistency extreme learn machine elm radial basis function network rbfns estimator construct elm rbfns learn approximate arbitrary regression function accuracy long number train sample sufficiently large furthermore condition kernel function correspond elmrbfns estimator strongly universal consistency result underlie feasibility elm rbfns case provide guidance practical selection kernel function elm applicationn',\n",
       " 'Yongquan Zhang Guisheng Liao Hongbing Ji Wenbo Zhang extreme learn machine elm competitive machine learn technique efficient usually lead good generalization performance compare traditional classifier order improve performance propose novel elm call elm introduce privilege information traditional elm method privilege information ignore classical elm exist human teach learn optimize train stage construct set correct function demonstrate performance elm datasets uci machine learn repository mackeyglass time series radar emitter recognition present comparison svm elm svm experimental result indicate validity advantage methodn',\n",
       " 'Amaury Lendasse KajMikael Bjork Yoan Miche Anton Akusok Alexandre Savio Manuel Grana Maite Termenon aim paper build tool able extract region brain magnetic resonance image discriminate healthy control subject probable dementia alzheimer type propose use extreme learn machine method select discriminant region perform final classification accord majority vote decision base strategy select optimal number vote require subject class alzheimer maximize global accuracy minimize number false positive discriminative region select case study locate hippocampus amygdala thalamus putamen location closely relate alzheimer disease accord medical literaturen',\n",
       " 'Amaury Lendasse KajMikael Bjork Yue Shen Bo He Rui Nian Weiguo Wang Qian Wang paper propose fast manifold learn strategy estimate underlie geometrical distribution develop relevant mathematical criterion basis extreme learn machine elm highdimensional space local tangent space alignment ltsa method perform manifold production single hide layer feedforward network slfn establish elm simulate lowdimensional representation process scheme elm ensemble combine individual slfn model selection manifold regularization mechanism bring elm preserve local geometrical structure ltsa development evaluate inherent representation embed elm learn simulation result show excellent performance accuracy efficiency develop approachn',\n",
       " 'Amaury Lendasse Maarit Kapyla Yoan Miche Alexander Grigorievskiy paper consider theory practical implementation singular value decomposition svd update algorithm update mean previously compute svd compute svd matrix augment column row compare standard svd algorithm term computational complexity accuracy svd update algorithm scale good work fast svd compute scratch addition analyze error singular value consecutive update verify reasonable bind finally apply svd update speed opelm algorithm propose new algorithm incoplem conclusion believe svd update apply computational intelligence method improve computational time scalingn',\n",
       " 'Weihui Dai Zhaohui Wu Min Yao Zhixin Xu extreme learn machine elm propose huang et al 2 novel algorithm single hide layer feedforward neural network slfns extremely fast learn speed good generalization performance new hide node add exist network retrain network time consume emelm 13 propose calculate output weight incrementally issue emelm initial hide layer output matrix rank deficient computation loss accuracy second emelm good generalization performance overfitting propose improve version emelm base regularization method call incremental regularize extreme learn machine irelm new hide node add irelm update output weight recursively fast way enhancement irelm eirelm selection hide node add network introduce paper empirical study benchmark data set regression classification problem show irelm eirelm get good generalization performance emelm similar train timen',\n",
       " 'Huisheng Zhang Tania Stathaki Yicheng Jiang Xiaohui Zhao investigate problem multiview human gait recognition straight walk path observe gait appearance change view change certain correlate information exist different view take advantage type correlation multiview gait recognition method propose paper estimate view angle monitor equipment term probe subject end method consider classification problem classification signal view angle classification feature element transformation matrix estimate transformation invariant lowrank texture tilt algorithm gallery gait appearance convert view probe subject propose appearance conversion machine acm gait feature spatially neighbour pixel gait feature consider correlate information view end similarity measurement apply convert gait appearance test gait appearance experiment casiab multiview gait database propose gait recognition method outperform stateoftheart viewn',\n",
       " 'Nagendra Vikash Chaudhary Neelam Dabas Ram Pal Singh protect secure information digital medium crucial illegal reproduction modification medium acute problem copyright protection day discrete wavelet transform dwt domain base robust watermarking scheme extreme learn machine elm online sequential extreme learn machine oselm weight extreme learn machine welm implement different color image propose scheme combine dwt elm oselm welm machine learn method watermark tag sequence embed ownership information experimental result demonstrate propose watermarking scheme imperceptibletransparent robust image process attack blur crop jpeg noise addition rotation scale scalecrop sharpen performance efficacy algorithm watermarking scheme determine measure peak signal noise ratio psnr bite error rate ber similarity parameter sim x x calibrate result compare exist machine learn method watermark detector machine learn technique learn neighbor relationship pixel natural image high relevance neighbor relationship predict neighbor machine learn method watermark image extract detect ownership verifiedn',\n",
       " 'Yuanwu Lei Yong Dou Kai Xu Zhige Xie Yueqing Wang 3d shape feature play crucial role graphic application 3d shape match recognition retrieval 3d shape descriptor develop decade exist descriptor handcraft feature laborintensively design extract discriminative information large set data paper propose rapid 3d feature learn method convolutional autoencoder extreme learn machine caeelm combine advantage convolutional neuron network autoencoder extreme learn machine elm method perform good fast method addition define novel architecture base caeelm architecture accept type 3d shape representation voxel data sign distance field data sdf input extract global local feature 3d shape voxel data structural information sdf data contain detail 3d shape propose caeelm practical graphic application 3d shape completion experiment feature extract caeelm superior exist handcraft feature deep learn method elm model classification accuracy propose architecture superior method modelnet10 914 modelnet40 8435 train process run fast exist deep learn method approximately order magnituden',\n",
       " 'Zongben Xu Jian Fang Shaobo Lin Xia Liu extreme learn machine elm feedforward neural network fnn like learn connection output neuron adjustable connection hide neuron randomly fix numerous application demonstrate feasibility high efficiency elmlike system open true general application twopart paper conduct comprehensive feasibility analysis elm provide answer question theoretically justify follow 1 suitable activation function polynomial nadarayawatson sigmoid function elmlike system attain theoretical generalization bind fnns connection adjust degrade generalization capability fnns connection hide neuron randomly fix 2 number hide neuron need elmlike achieve theoretical bind estimate 3 activation function take polynomial deduce hide layer output matrix columnrank generalize inverse technique efficiently apply yield solution elmlike furthermore nonpolynomial case tikhonov regularization apply guarantee weak regularity sacrifice generalization capability ii reveal different aspect feasibility elm exist activation function make correspond elm degrade generalization capability obtain result underlie feasibility efficiency elmlike system yield generalization improvement system welln',\n",
       " 'Amaury Lendasse Eric Severin AnneMari Ventela Yoan Miche Alexander Grigorievskiy paper optimally prune extreme learn machine opelm apply problem longterm time series prediction know strategy longterm time series prediction recursive direct dirrec consider combination opelm compare baseline linear square model leastsquare support vector machine lssvm strategy dirrec time consume usage nonlinear model like lssvm hyperparameters need adjust lead relatively heavy computation show opelm nonlinear model allow reasonable computational time dirrec strategy experiment opelm dirrec strategy outperform linear model strategy contrast propose algorithm lssvm behave unstably variable selection show superior strategy opelm best addition prediction accuracy ensemble opelm study show average prediction ensemble improve accuracy mean square error dramaticallyn',\n",
       " 'Yali Hao Aihua Li Jianbin Wang Liwei Tang Bing Liu reconstruction defect profile base ultrasonic guide wave mean acquisition defect profile parameter ultrasonic guide wave inspection signal key inversion ultrasonic guide wave method reconstruction 2d profile base kernelized extreme learn machine elm present quantum genetic algorithm qga adopt optimize cost parameter c kernel parameter c kernelized elm input data set kernelized elm defect echo signal output data set 2d profile parameter map defect echo signal 2d profile establish sample database achieve practical experiment numerical simulation 2d profile reconstruction artificial defect ultrasonic guide wave test implement qgakernelized elm compare generalization performance reconstruction result reconstruction model base lssvm design simultaneously kernel finally experimental result indicate propose method posse fast speed low computational complexity good generalization performance feasible effective approach reconstruct 2d defect profilen',\n",
       " 'XiZhao Wang HuiMin Feng generalization ability elm improve fuse number individual elm paper propose new scheme fuse elm base upper integral differ exist fuzzy integral model classifier fusion new scheme u upper integral reasonably assign test sample different elm maximize classification efficiency solve optimization problem upper integral obtain proportion assign sample different elm combination definition upper integral guarantee conclusion classification accuracy fuse elm individual elm theoretically numerical simulation demonstrate exist fusion methodology bag boost improve upper integral modeln',\n",
       " 'Guoren Wang Luxuan Qu Zhiqiong Wang Junchang Xin extreme learn machine elm variant widely application fast convergence good generalization performance distribute elm base mapreduce framework handle large scale train dataset big data application cope rapidly update challenge task paper novel elastic extreme learn machine base mapreduce framework name elastic elm e2lm propose cover shortage elm learn ability weak update largescale train dataset firstly analyze property elm adequately computationexpensive matrix multiplication incrementally decrementally correctionally calculate elastic elm base mapreduce framework develop calculate intermediate matrix multiplication update train data subset update matrix multiplication modify old matrix multiplication intermediate one correspond new output weight vector obtain centralize compute update matrix multiplication efficient learn rapidly update massive train dataset realize effectively finally conduct extensive experiment synthetic data verify effectiveness efficiency propose e2lm learn massive rapidly update train dataset experimental settingsn',\n",
       " 'YeBo Li KangKang Wang YongPing Zhao recently parsimonious algorithm propose sparsify extreme learn machine elm constructive parsimonious elm cpelm destructive parsimonious elm dpelm paper idea cpelm dpelm extend regularize elm relm obtain cprelm dprelm cprelmdprelm scheme realize viz cprelmi cprelmiidprelmi dprelmii generally speak cprelmiidprelmii outperform cprelmidprelmi term parsimoniousness nearly generalization compare cpelmdpelm cprelmiidprelmii usually need few hide node addition different cpelm dpelm cprelm dprelm number candidate hide node large number train sample assist selection good hide node construct compact network finally benchmark data set divide group utilize experiment usefulness propose algorithm reportedn',\n",
       " 'Francisca LopezGranados Jose M. PenaBarragan Pedro A. Gutierrez Cesar HervasMartinez Francisco FernandezNavarro classification problem decisionmaking task researcher study number technique propose perform binary classification neural network artificial intelligence technique successful result apply problem proposal use qgaussian radial basis function neural network qgaussian rbfnns basis function include supplementary degree freedom order adapt model distribution data hybrid algorithm ha search suitable architecture qgaussian rbfnn use type flexible kernel greatly improve discriminative power rbfnns order test performance rbfnn qgaussian basis function compare rbfnns gaussian cauchy inverse multiquadratic rbfs recent neural network approach experimental study present 11 binaryclassification datasets take uci repository aerial imagery take midmay midjune midjuly evaluate potential methodology propose discriminate ridolfia segetum patch dominant harmful weed sunflower crop naturally infest field southern spainn',\n",
       " 'Ningjie Dai Zhenxing Qian Guorui Feng paper attempt construct novel framework reversible watermarking work base differenceimage histogram shift decorrelation core high capacity datahiding histogramshift technique sake high payload choose downsample pattern reference set layer prediction point obtain term point reference set fullresolution image quality reconstruct determine reversible watermarking performance exist prior knowledge effective regression method name extreme learn machine utilize estimate miss pixel yield highquality recovery image compare good algorithm state art propose method achieve high capacity gain watermarked image similar distortionn',\n",
       " 'HongXing Li WenYan Song DeGang Wang paper utilize generalize bernstein polynomial construct fuzzy different traditional bernstein polynomial partition interval input variable choose nonequidistant division prove generalize bernstein fuzzy system universal approximators give continuous function highorder derivative elm method tune parameter generalize bernstein fuzzy spline fuzzy prove elmspline fuzzy approximate function derivative simulation example propose elmbernstein fuzzy elmspline fuzzy achieve high approximation capability nonlinear modeln',\n",
       " 'JunFei Qiao LiDan Wang HongGui Han approach name extend extreme learn machine elm propose train weight class hierarchical feedforward neural network hfnn unlike conventional singlehidelayer feedforward network slfns hierarchical elm helm base hierarchical structure capable hierarchical learn sequential information online simply choose hide layer need adjust output weight link hide layer output layer helm implementation extend elm provide good generalization performance learn process propose extend elm method efficient hfnns sigmoid hide node hfnns radial basis function rbf hide node finally helm apply activate sludge wastewater treatment process wwtps predict water quality experimental result performance comparison demonstrate effectiveness propose helmn',\n",
       " 'Suiyang Khoo Zhenwei Cao Dianhui Wang Kevin Lee Zhihong Man paper new robust singlehide layer feedforward network slfnbased pattern classifier develop show frequency spectrum desire feature vector specify term discrete fourier transform dft technique input weight slfn optimize regularization theory error frequency component desire feature vector one feature vector extract output hide layer minimize linearly separable input pattern hide layer slfn play role remove effect disturbance noisy input data provide linearly separable feature vector accurate classification nonlinearly separable input pattern hide layer capable assign dfts feature vector desire position frequencydomain separability nonlinearly separable pattern maximize addition output weight slfn optimally design empirical structural risk balance minimize noisy environment simulation example present excellent performance effectiveness propose classification schemen',\n",
       " 'Jose Antonio RamosHernanz Borja FernandezGauna Jose Manuel LopezGuede autonomous task learn link multicomponent robotic system lmcrs open research issue pilot study apply reinforcement learn rl single robot hose transport srht task need extensive simulation lmcrs involve task geometrically exact dynamic spline geds simulator accurate simulation dynamic overall time expensive process infeasible carry extensive learn experiment base paper address problem learn dynamic lmcrs encapsulate geds simulator extreme learn machine elm approach profit adaptability flexibility elm formalize problem learn hose geometry multivariate regression problem empirical evaluation strategy achieve remarkable accurate approximation resultsn',\n",
       " 'Min Han Meng Joo Er Ning Wang paper traditional singlehide layer feedforward network slfn extend novel generalize slfn gslfn employ polynomial function input output weight connect randomly generate hide unit correspond output node significant contribution paper follow 1 primal gslfn pgslfn implement randomly generate hide node polynomial output weight regression matrix augment partial input variable polynomial coefficient estimate 2 simplify gslfn sgslfn realize decompose polynomial output weight pgslfn randomly generate polynomial node tunable output weight 3 p sgslfn able achieve universal approximation output weight tune ridge regression estimator 4 virtue develop batch online sequential ridge elm brelm osrelm learn algorithm high performance propose gslfns term generalization learn speed guarantee comprehensive simulation study comparison standard slfns carry realworld regression benchmark data set simulation result demonstrate innovative gslfns brelm osrelm superior standard slfns term accuracy train speed structure compactnessn',\n",
       " 'Li Shang Hong Sun Feng Wang JinLin Ding regularize extreme learn machine relm ideal algorithm regression classification fast train speed good generalization performance obtain suitable number hide node challenge task order solve problem new incremental algorithm base cholesky factorization square root propose paper call improve incremental relm iirelm method automatically determine optimal network structure gradually add new hide node achieve computational cost good accuracy update output weight finally neural network generalize inverse nngi base iirelm apply twomotor synchronous decouple control simulation indicate propose algorithm excellent performance prediction control realize decouple control velocity tensionn',\n",
       " 'YewSoon Ong Amaury Lendasse MengHiot Lim Yoan Miche advancement game technology serve enrich playeru0027s game experience substantial way nowadays common blockbuster quality game realistic graphic engage story despite progress incorporate artificial intelligence slow realistic humanlike intelligence game hardly attempt use machine learn game attempt end impractical affect player enjoyment constrain factor paper meme war proofofconcept practical usage machine learn game introduce extreme learn machine elm approach achieve good experience employ machine learn game advantage elm multilayer perceptron mlp present term elm offer practical point view playern',\n",
       " 'Monther Alhamdoosh Dianhui Wang ensemble learn aim improve generalization power reliability learner model sample optimization technique show ensemble construct selective collection base learner outperform favorably effective implementation ensemble give learner pool open problem paper present evolutionary approach constitute extreme learn machine elm ensemble propose algorithm employ model diversity fitness function direct selection base learner produce optimal solution ensemble size control comprehensive comparison carry basic elm generate set neural network 12 benchmarked regression datasets employ simulation report result demonstrate propose method outperform ensembling technique include simple average bag adaboost term effectiveness efficiencyn',\n",
       " 'Zhongzhi Shi Qing He Fuzhen Zhuang Wenchao Yu extreme learn machine elm emerge technology achieve exceptional performance largescale set suit binary multiclass classification regression task exist elm variant predominantly employ single hide layer feedforward network leave popular potentially powerful stack generalization principle unexploited seek predictive deep representation input data deep architecture higherlevel representation potentially capture relevant higherlevel abstraction current deep learn method require solve difficult nonconvex optimization problem paper propose stack model drelm learn deep representation extreme learn machine accord stack generalization philosophy propose model utilize elm base build block incorporate random shift kernelization stack element specifically layer drelm integrate random projection prediction obtain elm original feature apply kernel function generate resultant feature verify classification regression performance drelm conduct experiment synthetic realworld data set experimental result drelm outperform elm kernel elm appear demonstrate drelm yield predictive feature suitable prediction task performance deep model stack autoencoder comparable utilization elm drelm easy learn fast testingn',\n",
       " 'Zichen Zheng Wen Yao Ying Ma Jun Yang Weitao Lu Min Xia classification scheme base extreme learn machine k near neighbor propose cloud classification work 21 characteristic parameter texture feature color feature shape feature select different sky condition cumulus stratus cirrus clear sky classification result new scheme texture feature color feature shape feature good performance feature 21 feature classification accurate identification rate cumulus stratus cirrus clear sky 8456 7806 7667 10000 respectively average 8482 propose model benefit merit knear neighbor extreme learn machine novel structure high robustness particularly cloud classification simulation result demonstrate propose model work practical cloud classification outperform extreme learn machine elm model artificial neural network ann knear neighbor knn hybrid method base knn ann knn ann support vector machine svmn',\n",
       " 'S. Baskar N. S. Marimuthu V. Malathi paper propose approach base wavelet transformsupport vector machine wtsvm wavelet transformextreme learn machine wtelm transmission line protection method u fault current sample half cycle inception fault feature line current extract level decomposition current sample discrete wavelet transform dwt extract feature apply input svm elm fault phase detection fault classification location discrimination fault switch transient condition feasibility propose method test 240kv 225km transmission line 10 type fault matlab simulink test 9600 fault case vary fault resistance fault inception angle fault distance prefault power level source impedance performance propose method promise performance propose method compare term classification accuracy fault location error result indicate svm base approach accurate compare elm base approach fault classification fault location maximum error svm elm mean error svm slightly high elmn',\n",
       " 'Junseok Lim paper propose algorithm entitle u0027u0027partitioned oselmu0027u0027 poselm partition large data matrix small matrix apply rls recursive square scheme small submatrix assemble estimation vector concatenation subvectors rls output submatrix consequently algorithm complex conventional oselm maintain compatible estimation performancen',\n",
       " 'Chunyan Miao Zhenwei Cao Dianhui Wang Kevin Lee Zhihong Man abstract robust train algorithm class singlehide layer feedforward neural network slfns linear node input tappeddelayline memory develop paper see order remove effect input disturbance reduce structural empirical risk slfn input weight slfn assign hide layer slfn perform preprocessor output weight train minimize weight sum output error square weight sum output weight square performance slfnbased signal classifier train propose robust algorithm study simulation section effectiveness efficiency new schemen',\n",
       " 'H. J. Wortche G. Iacca A. Liotta H. H. W. J. Bosman anomaly detection key factor process large amount sensor data wireless sensor network wsn efficient anomaly detection algorithm devise perform online nodelocal computation reduce communication overhead improve use limit hardware resource work introduce fixpoint embed implementation online sequential extreme learn machine oselm online learn algorithm single layer fee forward neural network slfn overcome stability issue introduce fix precision apply correction mechanism previously propose recursive square rls propose implementation test extensively generate realworld datasets compare rls linear square estimation rulebased method benchmark method evaluate prediction accuracy detection anomaly experimental result demonstrate fixpoint oselm successfully implement resourcelimit embed system guarantee numerical stability furthermore detection accuracy fixpoint oselm show good generalization property comparison instance fixpoint rlsn',\n",
       " 'Haicheng Wang Xiaoxia Mu Shengjie Zhao Wentao Mao important branch neural network extreme learn machine elm attract wide interest field pattern classification regression estimation face learn problem multidimensional output name multidimensional regression conventional elm generally satisfactory result incapable exploit relatedness output efficiently solve problem new regularize elm firstly propose paper introduce hyperspherical loss function regularizer regularization form loss function solve directly solution iterative procedure present improve learn performance algorithm propose reformulate identify inner group structure hide output assume group structure determine different linear combination small number latent basis neuron achieve mix integer program finally alternate minimization method present solve problem experiment multidimensional data set toy problem reallife dynamical cylindrical vibration data set conduct result demonstrate effectiveness propose algorithmn',\n",
       " 'Amaury Lendasse Olli Simula Patrick Bas Mark van Heeswijk Yoan Miche paper improvement optimally prune extreme learn machine opelm form l2 regularization penalty apply opelm propose opelm originally propose wrapper methodology extreme learn machine elm mean reduce sensitivity elm irrelevant variable obtain parsimonious model thank neuron prune propose modification opelm u cascade regularization penalty l1 penalty rank neuron hide layer follow l2 penalty regression weight regression hide layer output layer numerical stability efficient prune neuron new methodology test state art method support vector machine gaussian process original elm opelm 11 different data set systematically outperform opelm average 27 good mean square error provide reliable result xe2x80x93 term standard deviation result xe2x80x93 remain order magnitude slow opelmn',\n",
       " 'Qingling Zhang Bing Chen Huaguang Zhang Lili Cui paper novel control scheme propose track problem mechanical system presence external vibration friction acclerometer instal load frame mechanical system detect external vibration neural network nn compensator accelerometer signal input adopt feedforward compensation external vibration nn employ compensate unknown dynamic compensate friction eliminate residual reconstruction error come nn approximation robust integral sign error rise feedback term integrate control scheme lyapunov stability analysis perform propose control scheme yield semiglobal asymptotic track result uniformly ultimately bound uub result derive typical nnbased controller particular exact model plant external disturbance accelerometer need comparative study performance conduct twolink robot manipulator propose control scheme main conventional control method show satisfactory track performance propose control schemen',\n",
       " 'GuangShe Zhao Sundaram Suresh HaiJun Rong abstract paper present indirect adaptive neural control scheme general highorder nonlinear continuous propose scheme neural controller construct base singlehide layer feedforward network slfn approximate unknown nonlinearities dynamic system slide mode controller incorporate compensate model error slfn parameter slfn modify recently propose neural algorithm name extreme learn machine elm parameter hide node assign randomly different original elm algorithm output weight update base lyapunov synthesis approach guarantee stability overall control presence model error offset slide mode controller finally propose adaptive neural controller apply control invert pendulum different reference trajectory simulation result demonstrate good track performance achieve propose control schemen',\n",
       " 'Michel Verleysen Benoit Frenay support vector regression svr stateoftheart method regression u exe2x80x90sensitive loss produce sparse model nonlinear svr difficult tune additional kernel parameter paper new parameterinsensitive kernel inspire extreme learn nonlinear svr practitioner metaparameters optimise propose approach reduce significantly computational complexity experiment yield performance close stateoftheart unlike previous work rely montecarlo approximation estimate kernel work show propose kernel analytic form computationally easy evaluaten',\n",
       " 'Jacek M. Zurada Jian Wang Jan Chorowski paper present commonly machine learn classifier analyze common framework convex optimization classifier model support vector machine svm leastsquares svm lssvm extreme learn machine elm margin loss elm mlelm discuss demonstrate specific parametrizations general problem statement affect classifier design performance idea different classifier mix furthermore 21 public domain benchmark datasets experimentally evaluate performance metric model corroborate theoretical analysis comparison classification accuracy nest crossvalidation evaluation show exception model perform similarly evaluate datasets classifier command different amount computational resource test train requirement directly link formulation different convex optimization problemsn',\n",
       " 'Feilong Cao Yuguang Wang Yubo Yuan abstract extreme learn machine elm popular important learn algorithm come singlehiddenlayer feedforward neural network prove elm achieve good performance support vector machine svm regression classification paper mathematically regression problem step 3 elm study equation h xcexb2 t reformulate optimal model optimality necessary condition optimal solution present equation h xcexb2 t replace h t h xcexb2 h t t prove solution second optimal approximation solution discuss case h column rank row rank column row rank case rank1 rank2 method optimal approximation solution theory paper present good algorithm elm n',\n",
       " 'Bijaya Ketan Panigrahi Nitin Anand Shrivastava grow number country world switch deregulate market structure electricity sector view enhance productivity efficiency low price bar case deregulate structure country persistent issue plague involve party producer trader retailer uncertainty prevail number know unknow factor electricity price exhibit fluctuate characteristic difficult control predict forecast technique develop successfully implement exist market world comparable performance uncertainty aspect point forecast analyze significantly work attempt quantify uncertainty exist market statistical technique like prediction interval hybrid model neural network extreme learn machine wavelet preprocessors develop apply point prediction interval forecast ontario electricity market pjm dayahead real time marketn',\n",
       " 'Zhai JunHai Miao Qing Shao QingYan Wang XiZhao initial localize generalization error model lgem aim upper bind error target function radial basis function neural network rbfnn neighborhood train sample contribution lgem briefly describe generalization error equal summation term train error stochastic sensitivity measure ssm constant paper extend initial lgem new lgem model singlehidden layer feedforward neural network slfns train extreme learn machine elm type new train algorithm iteration development extend lgem provide useful guideline improve generalization ability slfns train elm algorithm architecture selection slfns propose base extend lgem experimental result number benchmark data set approximately optimal architecture term number neuron slfn method furthermore experimental result uci data set propose method effective efficientn',\n",
       " 'Yubo Yuan Feilong Cao Yuguang Wang extreme learn machine elm propose huang et al show promise learn algorithm singlehide layer feedforward neural network slfns random choice input weight bias elm algorithm make hide layer output matrix h slfn column rank low effectiveness elm paper discus effectiveness elm propose improve algorithm call eelm make proper selection input weight bias calculate output weight ensure column rank h theory improve extend learn rate test accuracy prediction accuracy learn time robustness property network experimental result base benchmark function approximation realworld problem include classification regression application good performance eelmn',\n",
       " 'Zhongzhi Shi Fuzhen Zhuang Changying Du Xin Jin Qing He extreme learn machine elm u0027u0027generalizedu0027u0027 singlehiddenlayer feedforward network slfns unify learn platform use widespread type feature map theory elm approximate target continuous function classify disjoint region application experiment result demonstrate good performance elm view good property elm feature map cluster problem elm feature map technique study paper experiment propose elm kmeans algorithm elm nmf nonnegative matrix factorization cluster good cluster result correspond mercer kernel base method traditional algorithm original data propose method advantage convenient implementation computation elm feature map simple mercer kernel function base feature map methodn',\n",
       " 'XiaoJun Zeng Syed Shabbir Haider unlike conventional fully connect feedforward multilayer neural network approximate function continuous input space paper investigate simplify neural network use common linear function hide layer approximate function discrete input space develop correspond learn algorithm test different data set show compare conventional multilayer neural network approximate function discrete input space propose simplify neural network architecture algorithm achieve similar good approximation accuracy especially deal high dimensionallow sample case simple architecture parametersn',\n",
       " 'Min Liu Xiaofeng Lin Q. M. Jonathan Wu Yaonan Wang Yimin Yang important property neural network nns universal approximation capability nns widely application property generally prove continuous system industrial system hybrid system piecewise continuous significant limitation real application recently identification method propose hybrid approximation method operate linear hybrid system paper progressive learn machinexe2x80x94a new learn algorithm base multinnsxe2x80x94is propose general hybrid nonlinearlinear approximation algorithm classify hybrid system continuous system approximate hybrid zero output error performance propose learn method demonstrate numerical example experimental data real applicationn',\n",
       " 'Di Wang Yi Zhao Guoren Wang paper propose extreme learn machine elm base protein secondary structure prediction framework provide good performance extremely fast speed achieve good performance framework secondary structure independently predict binary elm classifier ii probability base combination pbc method propose combine binary prediction result expect threeclassification result iii helix postprocessing hpp method finally propose improve overall performance framework base biological feature experiment conduct real data set cb513 rs126 demonstrate algorithm achieve good prediction accuracy popular method fast learn speedn',\n",
       " 'Amaury Lendasse Erkki Oja Yoan Miche Mark van Heeswijk abstract paper present approach perform regression large data set reasonable time ensemble extreme learn machine elm main purpose contribution paper explore evaluation ensemble elm accelerate distinct way 1 train model structure selection individual elm accelerate perform step graphic process unit gpu instead processor cpu 2 train elm perform way compute result reuse model structure selection make train plus model structure selection efficient 3 modularity ensemble model exploit process model train model structure selection parallelize multiple gpu cpu core multiple model build time experiment competitive performance obtain regression task gpuaccelerate parallelize elm ensemble achieve attractive speedup single cpu furthermore propose approach limit specific type elm employ large variety elmn',\n",
       " 'ZhiHong Man WenJian Cai Lei Zhao paper singlehidden layer feedforward neural network slfn model dynamic vapor compression cycle refrigeration airconditioning system base extreme learn machine elm show assignment random input weight slfn greatly reduce train time regularization base optimization output weight slfn ensure high accuracy model dynamic vapor compression cycle robustness slfn high frequency disturbance new slfn model test real experimental data compare one train propagation bp support vector regression svr radial basis function neural network rbf respectively result high degree prediction accuracy strong robustness input disturbance achievedn',\n",
       " 'Jianping Yin Jian Zhang GuangBin Huang Lei Wang Xinwang Liu extreme learn machine elm important research topic decade high efficiency easyimplementation unification classification regression unification binary multiclass learn task integrate advantage exist elm algorithm pay little attention optimize choice kernel crucial performance elm application importantly lack general framework elm integrate multiple heterogeneous data source classification paper propose general learn framework term multiple kernel extreme learn machine mkelm address issue propose mkelm optimal kernel combination weight structural parameter elm jointly optimize follow recent research support vector machine svm base mkl algorithm design sparse mkelm algorithm impose 1norm constraint kernel combination weight extend nonsparse scenario substitute 1norm constraint pnorm p 1 constraint radiusincorporated mkelm algorithm incorporate radius minimum enclose ball meb introduce efficient optimization algorithm propose solve correspond kernel learn problem comprehensive experiment conduct protein oxford flower17 caltech101 alzheimeru0027s disease data set evaluate performance propose algorithm term classification accuracy computational efficiency experimental result indicate propose algorithm achieve comparable good classification performance stateoftheart mkl algorithm incur computational costn',\n",
       " 'Yongchao Liu Huaibao Wang Peifeng Niu Guoqiang Li paper present novel artificial neural network fast learn speed weight bias determine twice square method call square fast learn network lsfln addition difference conventional neural network output neuron lsfln receive information hide layer neuron receive external information directly input neuron order test validity lsfln apply 6 classical regression application employ build functional relation combustion efficiency operate parameter 300wm coalfired boiler experimental result compare method lsfln hide neuron achieve good regression precision generalization ability fast learn speedn',\n",
       " 'Khamron Sunat Sirapat Chiewchanwattana Punyaphol Horata online sequential extreme learn machine oselm train retrain elm chunk data receive oselm affect improper number hide node set reduce generalization oselm paper address problem oselm new structural tolerance oselm stoselm base householder block exact inverse qrd recursive square algorithm have numerical robustness propose experimental result conduct regression classification problem show stoselm handle situation network construct improper number hide node accordingly propose stoselm easily apply size hide layer elm roughly approximate chunk data receive update exist network have worry proper number give hide node furthermore accuracy network train stoselm comparable batch elm network configuration stoselm apply ensemble version estoselm stability stoselm improve ensemble technique result estoselm stable accurate original oselm eoselm especially classification problemn',\n",
       " 'Amaury Lendasse Eric SeVerin Mark Van Heeswijk Emil Eirola Yoan Miche Qi Yu paper propose method advance modification original extreme learn machine new tool solve miss data problem u cascade l1 penalty lars l2 penalty tikhonov regularization elm tropelm regularize matrix computation make mse computation reliable hand estimate expect pairwise distance sample directly incomplete data offer elm solution solve miss data issue accord experiment data set method show significant advantage fast computational speed parameter need tune appear stable reliable generalization performance penalty complete elm new tool solve miss data problem half train data miss extreme casen',\n",
       " 'Yuhai Zhao Guoren Wang Linlin Ding Chen Chen Zhiqiong Wang Junchang Xin extreme learn machine elm widely field text classification image recognition bioinformatics provide good generalization performance extremely fast learn speed data volume realworld application large large traditional centralize elm learn massive data efficiently paper propose novel distribute extreme learn machine base mapreduce framework name elm cover shortage traditional elm learn ability weak huge dataset firstly adequately analyze property traditional elm expensive computation matrix moorepenrose generalize inverse operator output weight vector calculation matrix multiplication operator matrix multiplication operator decomposable distribute extreme learn machine elm base mapreduce framework develop calculate matrix multiplication effectively mapreduce parallel calculate correspond output weight vector centralize compute learn massive data effectively finally conduct extensive experiment synthetic data verify effectiveness efficiency propose elm learn massive data experimental settingsn',\n",
       " 'CheeKheong Siew Lei Chen MingBin Li GuangBin Huang huang et al universal approximation incremental constructive feedforward network random hide node ieee trans neural network 174 2006 879892 recently propose incremental extreme learn machine ielm randomly add hide node incrementally analytically determine output weight hide node generate randomly network construct ielm remain universal approximator paper extend ielm real domain complex domain long hide layer activation function complex continuous discriminatory complex bound nonlinear piecewise continuous ielm approximate target function complex domain universal capability ielm complex domain verify function approximation channel equalization problemsn',\n",
       " 'Amaury Lendasse Yoan Miche Qi Yu Mark Van Heeswijk Bing Zheng Bo He Rui Nian paper present dynamic model hypothesis perform fish trajectory track fish ethology research develop relevant mathematical criterion basis extreme learn machine elm show propose scheme conduct nonlinear non gaussian track process multiple historical cue current prediction state vector motion color distribution appearance recognition extract singlehidden layer feedforward neural network slfn diverse level elm strategy hierarchical hybrid elm ensemble combine individual slfn track cue performance improvement simulation result show excellent performance robustness accuracy develop approachn',\n",
       " 'Chang Feng Shizhong Liao extreme lean machine elm simply randomly assign input weight bias ineluctably lead certain stochastic behavior reduce generalization performance paper propose metalearn model elm call metaelm metaelm architecture consist base elm elm metaelm learn proceed stage base elm train subset train data elm learn base elm hide node theoretical analysis experimental result artificial benchmark regression datasets propose metaelm model feasible effectiven',\n",
       " 'QunXiong Zhu Yuan Xu ZhiQiang Geng YanLin He extreme learn machine elm competitive machine learn technique singlehiddenlayer feedforward neural network slfnns simple theory fast implementation deal highdimensional data noise elm hierarchical structure helm propose paper propose helm consist part group subnets main net subnets base welltrained autoassociative neural network aanns reduce dimension filter noise main net base traditional elm additionally perspective data attribute space da difficulty design subnets avoid method data attribute extension classification daec experiment highdimensional datasets noise carry examine helm model experimental result helm high accuracy few neuron main net elmn',\n",
       " 'Jochen J. Steil Klaus Neumann extreme learn machine randomly initialize singlehide layer feedforward neural network train restrict output weight order achieve fast learn good performance contribution show batch intrinsic plasticity novel efficient scheme input specific tune nonlinear transfer function ridge regression combine optimize extreme learn machine search suitable hide layer size scheme achieve excellent performance number standard regression task regression application roboticsn',\n",
       " 'YanCheng Liu Meng Joo Er Ning Wang JingChao Sun paper extreme learn control elc scheme singlehide layer feedforward network slfn track surface vehicle unknown dynamic external disturbance propose slide surface define incorporate track error derivative unknown dynamic include uncertainty external disturbance capsulate lump nonlinearity identify online slfn approximator random hide node generate elm technique consequence slfn approximator require priori information unknown dynamic avoid curse dimensionality predefining hide node high dimension track accuracy approximation ability enhance adaptive compensator approximation error addition adaptive output weight slfn derive lyapunov synthesis contribute global asymptotic stability term track error derivative entire closedloop simulation result comparative study demonstrate elc scheme achieve high accuracy track approximationn',\n",
       " 'Zhongzhi Shi Fuzhen Zhuang Tianfeng Shang Qing He regression basic problem data mine regression problem extreme learn machine elm good generalization performance fast learn speed enlarge volume datasets make regression elm large scale datasets challenge task analyze mechanism elm algorithm efficient parallel elm regression design implement base mapreduce framework simple powerful parallel program technique currently experimental result demonstrate propose parallel elm regression efficiently handle large datasets commodity hardware good performance different evaluation criterion include speedup scaleup sizeupn',\n",
       " 'Shitong Wang Longbing Cao KupSze Choi Zhaohong Deng challenge model type2 fuzzy logic system development efficient learn algorithm cope increase size realworld data set paper extreme learn strategy introduce develop fast train algorithm interval type2 takagisugenokang fuzzy logic system propose algorithm call type2 fuzzy extreme learn algorithm t2fela distinctive characteristic parameter antecedent randomly generate parameter consequents obtain fast learn method accord extreme learn mechanism addition obtain parameter optimal sense minimize norm result fuzzy system exhibit good generalization performance experimental result clearly demonstrate train speed propose t2fela algorithm superior exist stateoftheart algorithm propose algorithm show competitive performance generalization abilitiesn',\n",
       " 'Bogdan M. Wilamowski Philip D. Reiner abstract artificial neural network efficient universal approximators single layer feedforward network slfn popular easy train neuron network use sigmoidal function radial basis function rbf activation function function show work efficiently sigmoidal network describe literature paper focus construction slfn architecture rbf neuron algorithm construct train network solve function approximation problem paper algorithm modification incremental extreme learn machine ielm family algorithm propose propose algorithm eliminate randomness learn process respect center position width rbf neuron input high error magnitude save error calculation center incrementally add neuron radius new neuron iteratively choose nelderxe2x80x93meadxd7xb3s simplex method allow universal approximation property ielm preserve greatly reduce size train rbf networkn',\n",
       " 'QunXiong Zhu ZhiQiang Geng Yanlin He extreme learn machine elm effective learn algorithm singlehidelayer feedforward neural network slfnns easiness theory implementation elm widely field order enhance generalization performance elm positive negative correlation input attribute orient subnets base double parallel extreme learn machine pcnciaosdpelm propose paper salient feature pniaosdpelm special subnets subnets input attribute positive correlation output subnet input attribute negative correlation output kind input attribute obtain separate input attribute category correlation coefficient analysis accord category subnets establish subnets base welltrained autoassociative neural network aanns extract nonlinear information input attribute remove redundant information advantage pniaosdpelm proper number node hide layer determine test validity pniaosdpelm apply monitor chemical process steady state elm double parallel elm dpelm elm kernel elmk develop comparison experimental result demonstrate pniaosdpelm achieve good regression precision good stable ability elm dpelm elmk generalization phasen',\n",
       " 'Min Han Xiaoliang Tang problem prevent development extreme learn machine elm illconditioning hide layer output matrix reduce stability elm second complexity singular value decomposition svd compute moorepenrose generalize inverse limit learn speed elm problem paper propose partial lanczos elm plelm employ hybrid partial lanczos bidiagonalization svd compute output weight experimental result indicate compare elm plelm effectively improve stability generalization performance raise learn speedn',\n",
       " 'Kapil S. Balasundaram abstract paper new learn algorithm propose problem simultaneous learn function derivative extension study error minimize extreme learn machine single hide layer feedforward neural network formulation lead solve linear equation solution obtain moorexe2x80x93penrose generalize pseudoinverse approach number hide node automatically determine repeatedly add new hide node network group group update output weight incrementally efficient manner network output error give expect learn accuracy verification efficiency propose method number interest example consider result obtain propose method compare popular method observe propose method fast produce similar good generalization performance test datan',\n",
       " 'Huimin Feng Aixia Chen Xizhao Wang upper integral type nonlinear integral respect nonadditive measure represent maximum potential efficiency group feature interaction value upper integral evaluate solve linear program problem consider upper integral classifier paper investigate implementation performance fuse multiple upper integral classifier single layer neural network paper consider upper integral network classification learn mechanism elm train single layer neural network comparison performance single upper integral classifier upper integral network give number benchmark databasesn',\n",
       " 'Khamron Sunat Sirapat Chiewchanwattana Punyaphol Horata output weight compute extreme learn machine elm encounter problem computational outlier robustness problem computational problem occur hide layer output matrix column rank matrix illconditioned matrix randomly generate input weight bias exist solution problem singular value decomposition svd method train speed affect large complexity svd compute moorepenrose mp pseudo inverse outlier robustness problem occur train data set contaminate outlier accuracy rate elm extremely affect paper propose extend complete orthogonal decomposition ecod method solve computational problem elm weight compute ecodls algorithm paper propose algorithm iteratively reweighted square irwlselm elm base multivariate leasttrimmed square mltselm elm base onestep reweighted mlts rmltselm solve outlier robustness problem encounter computational problem ecod ecodls algorithm successfully propose algorithm experiment regression problem conduct toy realworld data set outlier type onesided twosided outlier experiment randomly contaminate outlier type 10 20 30 40 50 total train data size metametrics evaluation measure outlier robustness propose algorithm compare exist algorithm minimax probability machine regression mpmr ordinary elm experimental result show ecod effectively replace svd ecod robust column rank illconditional problem speed elm train ecod fast ordinary train algorithm metametrics measure show propose algorithm affect increase number outlier exist algorithmn',\n",
       " 'Ping Li Chuanhou Gao Xueyi Liu theory extreme learn machine elm recently increasingly popular new learn algorithm singlehidelayer feedforward neural network elm offer advantage low computational cost good generalization ability ease implementation comparison model selection elm kind stateoftheart machine learn approach significant attract research effort paper perform comparative analysis basic elm support vector machine svms viewpoint different previous work vapnikchervonenkis vc dimension performance different train sample size show vc dimension elm equal number hide node elm probability additionally generalization ability computational complexity exhibit change train sample size elm weak generalization ability svms small sample generalize svms large sample remarkably great superiority computational speed especially largescale sample problem elm result obtain provide insight essential relationship serve complementary knowledge past experimental theoretical comparisonsn',\n",
       " 'Bin Zou Xiaoyan Deng Yicong Zhou Hong Chen Peipei Yuan extreme learn machine elm gain increase attention computation feasibility application previous generalization analysis elm rely independent identically distribute iid sample paper far restriction investigate generalization bind elm classification associate uniform ergodic markov chain uemc sample upper bind misclassification error estimate elm classification show satisfactory learn rate achieve dependent sample empirical evaluation realword datasets provide compare predictive performance elm independent markov samplingn',\n",
       " 'BaoLiang Lu LiChen Shi human machine interaction system technique continuously estimate vigilance operator highly desirable ensure work safety signal study vigilance analysis electroencephalogram eeg commonly signal paper extreme learn machine elm modification l1 norm l2 norm penalty adopt eegbased vigilance estimation comparative study performance conduct ordinary elm modification support vector machine svms experimental result compare svms ordinary elm modification dramatically speed train process achieve similar good vigilance estimation accuracy addition follow observation experiment result ordinary elm elm l1 norm penalty larselm sensitive number hide node b elm l2 norm penalty regularizedelm elm l1 norm l2 norm penalty larsenelm tropelm stable insensitive number hide node c regularizedelm fast train speed larsenelm achieve good vigilance estimation accuracyn',\n",
       " 'Siwei Luo Zhe Wang Xiaoyue Luo Yaping Huang Liang Wang abstract image deblurring basic important task image process traditional filter base image deblurring method enhancement filter partial differential equation pde limit hypothesis natural image noise low high frequency term respectively noise removal edge protection dilemma traditional model paper study image deblurring problem brand new perspectivexe2x80x94classification generalize traditional pde model general case theory calculus variation furthermore inspire theory approximation function transform operatorlearn problem coefficientlearn problem mean select group basis build filterlearn model base extreme learn machine elm 1 2 3 4 algorithm design group filter learn effectively generalize image deblurring model learn filter pde lfpde build experiment verify effectiveness model correspond learn filter show model overcome drawback traditional model achieve good resultsn',\n",
       " 'Huibing Wang Yao Xiao Lin Feng Shenglan Liu semisupervised learn hot topic field pattern recognition paper analyze effective classification algorithm extreme learn machine elm elm widely application pattern recognition data mine extremely fast train speed highly recognition rate realworld application irregular distribution outlier problem low classification rate elm kernel elm mainly 1 overfitting cause outlier unreasonable selection activation function kernel function 2 label sample size small make use information unlabel data problem paper propose robust activation function raf base analyze different activation function indepth raf keep output activation function away zero possible minimize impact outlier algorithm improve performance elm kernel elm simultaneously raf apply kernel method neural network learn algorithm problem propose semisupervised kernel elm skelm experimental result synthetic realworld datasets demonstrate raf skelm outperform elm use activation function semisupervised kernel elm methodn',\n",
       " 'Zhongzhi Shi Fuzhen Zhuang Qun Wang Changying Du Qing He abstract classification algorithm extreme svm esvm propose recently prove provide good generalization performance relatively short time inappropriate deal largescale data set highly intensive computation propose implement efficient parallel esvm pesvm base current powerful parallel program framework mapreduce furthermore investigate new come train data brutal esvm retrain new model train data include old new come data line develop incremental learn algorithm esvm iesvm meet requirement online learn update exist model follow provide parallel version iesvm piesvm solve largescale problem online problem time experimental result propose parallel algorithm tackle largescale data set scale term evaluation metric speedup sizeup scaleup worth mention pesvm iesvm piesvm efficient esvm solution esvm exactly obtainedn',\n",
       " 'Francesco Piazza Stefano Squartini Yibin Ye abstract identification nonstationary environment represent challenge problem solve lot effort scientific community decade provide adequate solution purpose target work linearity assumption propose deal nonlinear case study particular author recently advance neural architecture timevarying neural network tvnn show remarkable identification property presence nonlinear nonstationary condition tvnn train issue high number free parameter extreme learn machine elm approach successfully purpose elm fast learn algorithm recently catch attention neural network nns research community variant elm appear recent literature specially stationary case study reference tvnn train name elmtv batchlearn type contribution online sequential version elmtv develop response need deal application sequential arrival large number train data occur algorithm generalize correspond counterpart work stationary condition performance evaluate nonstationary nonlinear identification task relate result advance technique produce comparable generalization performance elmtv ensure time benefit online sequential approachn',\n",
       " 'Zhaoyang Dong Junhua Zhao Jun Yi Hongming Yang paper novel optimization algorithm utilize key idea genetic algorithm ga extreme learn machine elm propose traditional genetic algorithm employ genetic operation selection mutation crossover generate optimal solution practice child solution generate crossover mutation largely random ensure fast convergence algorithm tackle weakness traditional ga elm introduce estimate nonlinear functional relationship parent population child population generate genetic operation train downwardclimbing upwardclimbing elm employ generate candidate solution form new population solution give genetic operation propose algorithm apply power economic dispatch problem demonstrate case study modify genetic algorithm able locate local minimum fast escape local minimum great probability propose algorithm ensure fast convergence provide economical dispatch plansn',\n",
       " 'Euntai Kim Baehoon Choi Jaehun Lee novel rangefree localization algorithm base multidimensional support vector regression msvr propose paper rangefree localization problem formulate multidimensional regression problem new msvr train method propose solve regression problem unlike standard support vector regression propose msvr allow multiple output localize sensor resort multilateration train msvr formulate directly primal space solve way formulate secondorder cone program train convex optimization second train method develop base newtonraphson method simulation conduct isotropic anisotropic network propose method exhibit excellent robust performance isotropic anisotropic networkn',\n",
       " 'LaiSheng Wang YuLin He XiZhao Wang AiMin Fu paper deliver study change rank input matrix extreme learn machine elm relationship rank input matrix residence error train elm viewpoint data analysis study reveal elm decrease residence error increase number node hide layer role sigmoid function play increase rank input matrix furthermore relationship stability solution rank output matrix discuss application residence error genetic algorithm minimize l1norm elm givenn',\n",
       " 'Mingyu Dong Min Liu Kefeng Ning actual industrial process data model usually contain outlier lead corrupt approximation function paper propose new robust extreme learn machine method base bayesian framework rbelm main idea rbelm replace gaussian distribution heavytailed distribution probability density function model output weight likelihood function base error model output make model robust outlier different heavytailed distribution paper laplace distribution correspond model denote rbelml rbelml posterior distribution replace approximate surrogate function parameter learn problem solve mean maximum likelihoodtype ii mlii studentu0027s tdistribution correspond model denote rbelmt order solve rbelmt studentu0027s tdistribution write scalemixture infinitely normal distribution gamma distribution variational inference method obtain approximate solution parameter propose method give weight likelihood function base concept robust statistic finally robust error measure propose evaluate performance model outlier test data result numerical comparison base synthesis data set real benchmark regression problem actual industrial model problem usefulness propose rbelml rbelmtn',\n",
       " 'Yoan Miche Mark van Heeswijk paper new hide layer construction method extreme learn machine elm investigate aim generate diverse set weight paper propose new elm variant binary elm weight initialization scheme base 0 1 weight ternary elm weight initialization scheme base 1 0 1 weight motivation approach feature different subspace neuron extract diverse information input neuron completely random feature traditionally elm ideally lead good elm experiment elm ternary weight generally achieve low test error furthermore experiment binary ternary elm robust irrelevant noisy variable fact perform implicit variable selection finally weight generation scheme adapt computational time elm unaffected improve accuracy add robustness implicit variable selection binary elm ternary elm come freen',\n",
       " 'Jun Wu Jun Wang FuLai Chung Shitong Wang order circumvent weakness slow convergence traditional learn algorithm single layer feedforward neural network extreme learn machine elm recently develop achieve extremely fast learn good performance train output weight apply multiplehide layer feedforward neural network mlfn challenge bottleneck elm work novel fast learn method flm feedforward neural network propose firstly base exist ridge regression theory hidefeaturespace ridge regression hfsr center ridge regression centerelm present connection elm theoretically reveal special kernel method inherently propagate prominent advantage elm mlfn novel fast learn method flm feedforward neural network propose unify framework hfsr centerelm flm apply slfn mlfn single multiple output flm parameter hide layer require adjust parameter hide layer randomly assign propose flm test state art method realworld datasets provide good reliable resultsn',\n",
       " 'Xue Wang Yuan Li Guoren Wang Yuhai Zhao Zhanghui Wang graph pattern widely define feature space build efficient graph classification model synergy graph pattern refer graph relationship node highly inseparable compare general graph pattern synergy graph pattern high discriminative power suitable classification feature extreme learn machine elm simple efficient singlehidden layer feedforward neural network slfns algorithm extremely fast learn capacity paper propose problem extend elm nonredundant synergy pattern base graph classificationthe graph classification framework widely consist step feature generation classification issue quickly obtain significant graph pattern feature graph database step effectively build graph classification model graph pattern feature efficient depthfirst algorithm call gin present nonredundant synergy graph pattern base propose support graph vector model sgvm elm algorithm graph classification model construct extensive experiment conduct series reallife datasets result gin efficient representative competitor generate graph pattern consider classification feature ginelm classification accuracy improve muchn',\n",
       " 'A. C. MartinezEstudillo P. A. Gutierrez C. HervasMartinez F. J. MartinezEstudillo paper propose classification method base special class feedforward neural network productunit neural network productunits base multiplicative node instead additive one nonlinear basis function express possible strong interaction variable apply evolutionary algorithm determine basic structure productunit model estimate coefficient model use softmax transformation decision rule crossentropy error function probabilistic interpretation approach see nonlinear multinomial logistic regression parameter estimate evolutionary computation empirical specific multiple comparison statistical test result carry benchmark data set complex real microbial listeria growthno growth problem propose model promise term classification accuracy number model coefficient yield stateoftheart performancen',\n",
       " 'Manuel Grana Borja Ayerdi paper propose hybrid extreme rotation forest herf innovative ensemble learn algorithm classification problem combine classical decision tree recently propose extreme learn machine elm train neural network herf algorithm train individual classifier involve step compute randomize data rotation transformation train data second train individual classifier rotate data test data subject transformation train data specific classifier ensemble experimental design paper involve comparison factorization approach compute randomize rotation matrix principal component analysis pca quartimax b assess effect data normalization bootstrapping train data selection c variant single combine elm decision tree include regularize elm experimental design effectively include stateoftheart ensemble approach comparison vote elm random forest report extensive result collection machine learn benchmark database rank crossvalidation result experimental dataset classifier test conclude herf significantly improve stateoftheart ensemble classifier result data rotation quartimax improve pca relative insensitivity approach regularization attributable facto regularization perform ensemble approachn',\n",
       " 'Jianhua Yang Zhaohui Wu Min Yao Xiaowei Xue extreme learn machine elm propose new learn algorithm train singlehidelayer feedforward neural network slfns elm prove perform high efficiency random determination parameter hide node unoptimal parameter generate influence generalization performance stability elm suffer overtrain problem entire train dataset minimize train error paper hybrid model propose alleviate weakness elm model adopt genetic algorithm gas produce group candidate network accord specific rank strategy network select ensemble new network verify performance method empirical comparison carry canonical elm eelm simple ensemble eeelm enelm bag adaboost solve regression classification problem result show method able generate robust network good generalization performancen',\n",
       " 'Shijie Cheng Youyi Wang Si Wu paper propose precise realtime wind speed estimation method sensorless control variablespeed variablepitch wind turbine power generation wtpgs wind speed estimation realize nonlinear inputoutput map extreme learn machine elm specific design characteristic wind turbine improve map accuracy consider variable pitch angle design independent environmental air density propose elm wind speed estimation method robust air density variation estimate wind speed determine optimal rotational speed command maximum power point track fast effective elm map base pitch controller propose wtpgs operate high wind speed region elm pitch controller act fast precise conventional pitch controller furthermore complicate design precess parameter conventional pitch controller avoid propose method effectiveness propose method verify simulation experiment wtpgs instal permanent magnet synchronous generator pmsgn',\n",
       " 'Zhaoyang Dong Dianhui Wang Jun Hua Zhao Wenjun Xu Hongming Yang abstract paper novel method base extreme learn machine elm copula function propose predict damage electricity transmission facility ice storm elm firstly train base historical data wind speed freeze precipitation temperature distribution parameter wind ice load elm employ predict distribution realtime wind ice load electricity transmission facility furthermore correlation wind load ice load model copula function basis elm copula function joint probability distribution wind ice load finally formulate apply predict potential damage electricity transmission facility transmission line tower propose method test real dataset demonstrate effectivenessn',\n",
       " 'Guangbin Huang Zhiping Lin Jiuwen Cao new structure wavelet neural network wnn extreme learn machine elm introduce paper propose wavelet neural network composite function apply hide node learn elm input information process wavelet function pass type bound nonconstant piecewise continuous activation function gru003er selection method take account domain input space wavelet zero initialize translation dilation parameter form wavelet neural network train computationally efficient elm algorithm experimental result regression nonlinear function realworld data prediction chaotic signal classification serval benchmark realworld data set propose neural network achieve good performance case relevant neural network learn fast neural network train traditional backpropagation bp algorithmn',\n",
       " 'Amaury Lendasse Michel Verleysen Yoan Miche Mark Van Heeswijk FreNay BenoiT context feature selection tradeoff number select feature generalisation error plot help summarise feature selection feature selection path sparsityerror tradeoff curve feature selection path show best feature subset subset size sparsityerror tradeoff curve show correspond generalisation error graphical tool help expert choose suitable feature subset extract useful domain knowledge order obtain tool extreme learn machine fast train estimate generalisation error easily obtain press statistic algorithm introduce add additional layer standard extreme learn machine order optimise subset select feature experimental result illustrate quality present methodn',\n",
       " 'CheeKheong Siew QinYu Zhu GuangBin Huang practical application neural network fast response external event extremely short time highly demand expect extensively gradientdescentbase learn algorithm obviously satisfy realtime learn need application especially largescale application andor high generalization performance require base huangu0027s constructive network model paper propose simple learn algorithm capable realtime learn automatically select appropriate value neural quantizers analytically determine parameter weight bias network time performance propose algorithm systematically investigate large batch benchmark realworld regression classification problem experimental result demonstrate algorithm produce good generalization performance realtime learn prediction capability provide alternative approach practical application neural network realtime learn prediction implementation requiren',\n",
       " 'GuangBin Huang reply refer wang wanu0027s comment research publication comment letter contain inaccurate statement comment letter contain contradiction welln',\n",
       " 'Zongben Xu Shaobo Lin Feilong Cao wellknown single hide layer feedforward neural network slfns n hide neuron learn n distinct sample zero error weight connect input neuron hide neuron hide node threshold choose randomly n distinct sample exist slfns n hide neuron interpolate network call exact interpolation network sample approximate target function continuous integrable function exact interpolation network good approximation effect paper functional approach rigorously prof give distinct sample exist slfn exactly interpolate sample near best approximate target functionn',\n",
       " 'Yingjie Jay Guo Defeng David Huang Lu Xu paper propose new blind learn algorithm benvenistexe2x80x93goursat inputxe2x80x93output decision bgiod enhance convergence performance neural networkbased equalizer nonlinear channel equalization contrast conventional blind learn algorithm output equalizer employ update parameter bgiod exploit new type extra information input decision information obtain input equalizer mitigate influence nonlinear equalizer structure parameter learn lead improve convergence performance prove input decision information desirable convergence capability output symbol error rate ser input ser input ser threshold achieve bg softswitching technique employ combine merit input output decision information guarantee ser convergence improve ser performance simulation result propose algorithm outperform conventional blind learn algorithm stochastic quadratic distance dual mode constant modulus algorithm term convergence performance ser performance nonlinear equalizationn',\n",
       " 'Hung Keng Pung GuangBin Huang Lei Chen paper systemically investigate convex incremental feedforward neural network firstly prove universal approximation convergence rate generalize convex incremental gci structure provide wide parameter selection second accord convergence rate proof gci prove convergence rate best convex incremental bci structure proof illustrate bci achieve good generalization performance gci note hide neuron bci gci construct maximum principle random introduce random neuron conception base cielm convex incremental extreme learn machine propose alternative algorithm improve cielm icielm cielm bci remove u0027u0027uselessu0027u0027 neuron cielm improve efficiency neural network icielm randomly generate group parameter determine best parameter lead small residual error icielm achieve fast convergence rate cielm retain convergence rate bci hand icielm provide alternative scheme replace conventional gradient method suitable differential function achieve local minimum experimental result base benchmark regression problem support claimsn',\n",
       " 'Rene Alquezar Enrique Romero recently error minimize extreme learn machine emelms propose simple efficient approach build singlehidelayer feedforward network slfns sequentially add random hide node group group update output weight incrementally minimize sumofsquares error train set similar method construct slfns sequentially report early main difference hidelayer weight subset data instead random approach refer support vector sequential feedforward neural network svfnns particular case sequential approximation optimal coefficient interact frequency saocif method paper firstly show emelms cast particular case saocif particular emelms easily extend test number random candidate step select best saocif demonstrate cost computation optimal outputlayer weight originally propose emelms improve replace include saocif secondly present result experimental study 10 benchmark classification 10 benchmark regression data set compare emelms svfnns carry condition model model efficient computational cost statistically significant improvement generalization performance svfnns v emelms 12 20 benchmark problemsn',\n",
       " 'Zhaohui Wu Huajun Chen Ningyu Zhang Cong Fang Guozhou Zheng Jiaoyan Chen nowadays landcover change detection play important role environment protection field current landcover change detection method encounter problem low accuracy low efficiency especially deal large scale remote sense r data paper present novel extreme learn machine elm base landcover change detection method high test accuracy fast process speed evaluation result elm outperform traditional method svm bp network term train speed generalization performance apply landcover classification experiment apply method analysis rapid land use change taihu lake region past decaden',\n",
       " 'Hongming Zhou Xiaojian Ding GuangBin Huang extreme learn machine elm emergent technology show good performance regression application large dataset andor multilabel classification application elm theory show hide node u0027u0027generalizedu0027u0027 singlehide layer feedforward network slfns need neuron alike randomly generate universal approximation capability slfns guarantee paper study elm classification aspect standard optimization method extend elm specific type u0027u0027generalizedu0027u0027 slfnssupport vector network paper show 1 elm learn framework svmu0027s maximal margin property minimal norm weight theory feedforward neural network actually consistent 2 standard optimization method point view elm classification svm equivalent elm optimization constraint special separability feature 3 analyze theory verify simulation result elm classification tend achieve good generalization performance traditional svm elm classification sensitive user specify parameter implement easilyn',\n",
       " 'Alexandros Iosifidis paper propose novel method supervise subspace learn base singlehide layer feedforward neural network propose method calculate appropriate network target vector formulate bayesian model exploit label information available train data geometric property train data represent feature space determine network hide layer output calculation network target vector extreme learn machinebase neural network train apply classification perform near neighbor classifier experimental result publicly available data set propose approach consistently outperform standard elm approach standard methodsn',\n",
       " 'ZongBen Xu GuangBin Huang Yuan Lan Rui Zhang extreme learn machine elm propose generalize singlehidelayer feedforward network need neuronlike perform regression classification application brief propose elm adaptive growth hide node agelm provide new approach automate design network different incremental elm ielm exist hide node freeze new hide node add agelm number hide node determine adaptive way sense exist network replace newly generate network few hide node good generalization performance prove agelm lebesgue pintegrable hide activation function approximate lebesgue pintegrable function compact input set simulation result demonstrate verify new approach achieve compact network architecture ielmn',\n",
       " 'Licheng Jiao Min Wang Shuyuan Yang paper new kind neural network propose combine ridgelet feedforward neural network fnn network adopt ridgelet activation function hide layer incremental constructive method employ determine structure network ridgelets efficient describe linear curvilinear hyperplane like structure high dimension accordingly network approximate wide range multivariate function stable efficient way especially certain kind spatial inhomogeneity incremental extreme learn machine make add hide node possible need adjust output weight link hide layer output layer hide neuron add define cost function difference previously approximate function currently approximate genetic algorithm determine optimal direction ridgelet neuron construction learn network present superiority propose model demonstrate simulation experiment function learn image compressionn',\n",
       " 'Zonghua Liu Liang Zhao Xiaoming Liang ability transmit amplify weak signal fundamental signal process artificial device engineer multilayer feedforward network couple doublewell oscillator fitzhughnagumo oscillator investigate condition weak signal receive layer transmit network amplitude attenuation couple strength nodesu0027 state layer act twostate switch determine transmission significantly enhance exponentially decrease hope find useful design artificial signal amplifiersn',\n",
       " 'Junhui Hou GuangBin Huang Chenwei Deng Jiexiong Tang extreme learn machine elm efficient train algorithm originally propose singlehide layer feedforward network slfns input weight randomly choose need finetuned paper present new stack architecture elm improve learn accuracy elm maintain advantage train speed exploit hide information elm random feature space recoverybased train model develop incorporate propose elm stack architecture experimental result mnist handwrite dataset demonstrate propose algorithm achieve good fast convergence stateoftheart elm deep learn methodsn',\n",
       " 'JunSeok Lim recently new learn algorithm singlehiddenlayer feedforward neural network slfn name complex extreme learn machine celm propose li et al fully complex extreme learn machine neurocomputing 68 2005 306314 show potential applicability area room improvement performance especially trainingbased equalization application noise receive data paper propose new solution apply data square dl method simulation dlbased celm outperform ordinaryleastsquarebased channel equalization problemsn',\n",
       " 'Wenjian Wang Changqian Men extreme learn machine elm emergent technology show good performance classification application elm algorithm need inversion matrix nature limit application occasion paper propose elm speedup algorithm base analysis elm algorithm apply randomize approximation method propose algorithm approximate key matrix example kernel matrix kernelbase elm lowrank matrix complexity inversion reduce o n 3 o kn 2 k 3 n size data set k numerical rank approximate matrix premise decrease accuracy train time cut substantially important significance practical application machine learn algorithm experimental result benchmark data set demonstrate effectiveness propose algorithmn',\n",
       " 'J.P.F. Sum ChiSing Leung classical train method node open fault need consider potential faulty network multinode fault situation consider space potential faulty network large objective function correspond learn algorithm computationally complicate paper u kullbackleibler divergence define objective function improve fault tolerance radial basis function rbf network assumption gaussian distribute noise term output data regularizer objective function identify finally correspond learn algorithm develop approach objective function learn algorithm computationally simple compare conventional approach include weightdecaybased regularizers approach good faulttolerant ability empirical study show approach improve generalization ability faultfree rbf networkn',\n",
       " 'A. Kouvelas E.B. Kosmatopoulos despite continuous advance field intelligent control compute design deployment efficient large scale nonlinear control system lncss require tedious finetuning lncs parameter actual operation majority lncss finetuning process perform experience personnel base field observation experimentation different combination controller parameter use systematic approach exist adaptiveneuralfuzzy control methodology development systematic automate finetuning procedure general lncs strict assumption impose control dynamic hand adaptive optimization methodology fail guarantee efficient safe performance finetuning process mainly fact methodology involve use random perturbation paper introduce analyze mean mathematical argument simulation experiment new learnadaptive algorithm provide convergent efficient safe finetuning general lncs propose algorithm consist combination different algorithm propose kosmatopoulos 2007 2008 incrementalextreme learn machine neural network ielmnns nice property propose algorithm significantly outperform algorithm propose kosmatopoulos exist adaptive optimization algorithm contrary algorithm propose kosmatopoulos propose algorithm operate efficiently case exogenous input disturbance command demand unbounded signalsn',\n",
       " 'Lingzhong Guo Yifan Zhao S.A. Billings HuaLiang Wei brief combine efficient wavelet representation couple map lattice model new family adaptive wavelet neural network call lattice dynamical wavelet neural network ldwnns introduce spatiotemporal identification new orthogonal projection pursuit opp method couple particle swarm optimization pso algorithm propose augment propose network novel twostage hybrid train scheme develop construct parsimonious network model stage apply opp algorithm significant wavelet neuron adaptively successively recruit network adjustable parameter associate wavelet neuron optimize particle swarm optimizer resultant network model obtain stage redundant second stage orthogonal square algorithm apply refine improve initially train network remove redundant wavelet neuron network example real spatiotemporal identification problem present demonstrate performance propose new model frameworkn',\n",
       " 'C.R. Wan L.P. Wang comment letter point essence ldquoextreme learn machine elmrdquo recently appear propose early broomhead lowe pao discuss author necessary introduce new elmn',\n",
       " 'V.E. Maiorov R. Meir problem approximate function neural network incremental algorithm study function belong general class characterize certain smoothness property respect lsub 2 norm compute upper bind approximation error error measure lsub q norm 1spl lesqspl lesspl infin result extend previous work applicable case q2 provide explicit algorithm achieve derive approximation error rate range qspl les2 nearoptimal rate convergence demonstrate gap remain respect recently establish low bind case qu003e2 rate achieve provably good obtain optimal linear approximation extension result lsub 2 norm lsub p discuss interest conclusion result loss generality suffer network positive hiddentooutput weight explicit bind size hiddentooutput weight establish sufficient guarantee establish convergence raten',\n",
       " 'Amaury Lendasse Anton Akusok Rui Nian KajMikael Bjork Yoan Miche Emil Eirola Dusan Sovilj paper examine general regression problem miss data scenario order provide reliable estimate regression function approximation novel methodology base gaussian mixture model extreme learn machine develop gaussian mixture model model data distribution adapt handle miss value extreme learn machine enable devise multiple imputation strategy final estimation multiple imputation ensemble approach extreme learn machine final estimation improve mean imputation perform complete data propose methodology long run time compare simple method overall increase accuracy justify tradeoffn',\n",
       " 'Zhiping Lin KarAnn Toh Badong Chen Lei Sun sequential extreme learn machine incorporate noise compensation scheme information measure develop design computationally simple extreme learn machine architecture maintain survival error information potential function provide mechanism noise compensation error compensation update online error codebook design error tolerant stable solution obtain develop method test chaotic time sequence benchmark data set experimental result potential application develop methodn',\n",
       " 'BaoLiang Lu Hai Zhao YangYang Chen XiaoLin Wang extreme learn machine elm emergent technology show promise performance application paper propose parallelize elm ensemble base minmax modular network m3network meet challenge socalled big data propose m3elm decompose classification problem small subproblem train elm subproblem end ensemble elm m3network data set include benchmark realworld application employ test propose method experimental result m3elm speed train phrase 1646 time reduce test error 0371951 compare normal elm result indicate m3elm posse scalability largescale task accuracy improvement imbalanced taskn',\n",
       " 'Jintao Li Yongdong Zhang Sheng Tang Zhenyu Chen Xingyu Gao abnormal activity recognition pay attention field healthcare relate application especially elderly peopleu0027s physical mental health high risk fall accident cause injure gradually attract concern present wearable device base fall detection technology effectively timely monitor occurrence fall accident help injure person receive aid build classifier traditional approach fall detect monitor suffer high falsealarm rate reach relatively high detection accuracy face imbalance problem sensor data abnormal activity usually rare realistic application address challenge propose twostage adaptive weight extreme learn machine awelm method eyeglass watch wearable base fall detect monitor experimental result validate illustrate significant efficiency effectiveness propose method approach firstly achieve good balance high detection accuracy low falsealarm rate base twostage recognition scheme secondly enable imbalance learn approach scarce abnormal activity data twostage adaptive weight method thirdly provide lightweight classifier solution resource constrain wearable device extreme learn machine fast train speed good generalization capability enable largescale mhealth application especially help elderly people greatly reduce risk fall accident finallyn',\n",
       " 'Fuchun Sun Wenbing Huang Lele Cao randomhiddennode extreme learn machine elm generalize cluster singlehiddenlayer feedforward neural network slfns part random projection nonlinear transformation ridge regression rr model network deep architecture demonstrate stateoftheart performance variety set especially vision task deep learn algorithm stack autoencoder sae deep belief network dbn build learn level representation input simply learn feature stack autoencoders ae need increase robustness noise reinforce sparsity weight easy discover interest prominent feature sparse ae denoising ae develop purpose paper propose approach ssdaerr stack sparse denoising autoencoder ridge regression effectively integrate advantage sae sparse ae denoising ae rr implementation elm algorithm conduct experimental study realworld classification binary multiclass regression problem different scale relevant approach ssdaerr elm dbn neural network nn sae performance analysis show ssdaerr tend achieve good generalization ability relatively large datasets large sample size high dimension preprocessed feature abstraction 16 18 test datasets performance ssdaerr stable test approach note sparsity regularization denoising mechanism mandatory construct interpretable feature representation fact ssdaerr approach comparable train time elm make useful real applicationsn',\n",
       " 'Wenjing Li Honggui Han Fanjun Li Junfei Qiao paper novel constructive algorithm name fast cascade neural network fcnn propose design fully connect cascade feedforward neural network fccfnn modify index base orthogonal square method derive select new hide unit candidate pool hide unit lead maximal reduction sum square error secondly input weight bias hide unit randomly generate remain unchanged learn process weight connect input hide unit output unit calculate necessary unit add thirdly convergence fcnn guarantee theory finally performance fcnn evaluate artificial realworld benchmark problem simulation result propose fcnn algorithm good generalization performance fast learn speed exist algorithmsn',\n",
       " 'SuJing Wang WenBin Liu ZhenNao Cai Chao Ma Gang Wang HuiLing Chen paper explore potential extreme learn machine elm kernel elm kelm early diagnosis parkinsonu0027s disease pd propose method key parameter include number hide neuron type activation function elm constant parameter c kernel parameter xcexb3 kelm investigate obtain optimal parameter elm kelm manage train optimal predictive model pd diagnosis order improve performance elm kelm model feature selection technique implement prior construction classification model effectiveness propose method rigorously evaluate pd data set term classification accuracy sensitivity specificity area roc receiver operate characteristic curve auc compare exist method previous study propose method achieve promise classification accuracy 10fold crossvalidation cv analysis high accuracy 9647 average accuracy 9597 10 run 10fold cvn',\n",
       " 'QingHua Zheng YewSoon Ong WanYu Deng paper present fast accurate kernelbase supervise algorithm refer reduce kernel extreme learn machine rkelm contrast work support vector machine svm square svm lssvm identify support vector weight vector iteratively propose rkelm randomly select subset available data sample support vector map sample avoid iterative step svm significant cost save train process readily attain especially big datasets rkelm establish base rigorous proof universal learn involve reduce kernelbase slfn particular prove rkelm approximate nonlinear function accurately condition support vector sufficiency experimental result wide variety real world small instance size large instance size application context binary classification multiclass problem regression report rkelm perform competitive level generalize performance svmlssvm fraction computational effort incurredn',\n",
       " 'Xinping Guan Yinggan Tang Changchun Hua Junpeng Li extreme learn machine elm competitive machine learn technique singlehidelayer feedforward neural network slfnns prove efficient effective algorithm regression classification problem traditional elm involve large number hide node complex real world regression classification problem increase computation burden paper decomposition base fast elm dfelm algorithm propose effectively reduce computational burden large number hide node condition compare elm algorithm dfelm algorithm fast train time large number hide node maintain accuracy performance experiment regression problem classification problem complex blast furnace model problem carry verify performance dfelm algorithm decomposition method extend modify elm algorithm reduce train timen',\n",
       " 'YiWen Zhang Jie Chen YanPing Zhang YuanTing Yan extreme learn machine elm propose new efficient learn algorithm singlehidden layer feedforward neural network slfn recent year feature fast train speed good generalization performance traditional slfn learn technique elm deal directly incomplete data widely exist realworld application paper propose new algorithm handle incomplete data vote base extreme learn machine velmi velmi rely assumption miss value obtain group data subset accord miss value train set apply mutual information measure importance degree data subset train group subclassifiers data subset apply elm base learn algorithm finally give test sample miss value velmi select subclassifiers input require miss value predict final prediction determine weight majority vote accord mean value norm output weight importance degree available subclassifier experimental result 15 uci incomplete datasets 5 uci complete datasets show velmi generally good performance algorithm compare compare classification algorithm base neural network ensemble nne velmi greatly improve algorithm computational efficiencyn',\n",
       " 'Ramon Huerta YongPing Zhao abstract recently novel constructive destructive parsimonious extreme learn machine cpelm dpelm arise cope regression problem foundation improvement cpelm dpelm suggest cpelm improve replace give rotation householder transformation yield improve cpelm icpelm result acceleration train speed hamper generalization performance subsequently hybrid constructivexe2x80x93destructive elm cdpelm generate integrate element cpelm dpelm goal combine advantage train speed parsimony cpelm dpelm finally experiment regression data set realworld identification robot arm example test feasibility efficacy variant include icpelm cdpelmn',\n",
       " 'Ioannis Pitas Anastasios Tefas Alexandros Iosifidis paper propose scheme largescale nonlinear facial image classification problem approximate solution kernel extreme learn machine classifier formulate evaluate experiment publicly available facial image datasets popular facial image representation illustrate effectiveness efficiency propose approach propose approximate kernel extreme learn machine classifier able scale time memory achieve good generalization performance specifically show outperform standard elm approach time memory requirement compare original kernel elm approach achieve similar good performance scale time memory respect train set cardinalityn',\n",
       " 'Danilo Maccio Cristiano Cervellera traditional extreme learn machine elm approach base random assignment hide weight value linear coefficient output layer determine analytically brief present analysis base geometric property sample point assign weight value investigate replacement random generation value lowdiscrepancy sequence ldss sequence family sample method commonly employ numerical integration yield efficient cover multidimensional set respect random sequence need computationally intensive procedure particular prove universal approximation property elm guarantee ldss employ efficient cover affect convergence positively furthermore ldss generate deterministically result probabilistic nature simulation result confirm practice good theoretical property give combination elm ldssn',\n",
       " 'Enrico Zio Olga Fink Ronay Ak increase liberalization european electricity market grow proportion intermittent renewable energy feed energy grid new challenge pattern energy consumption electric mobility require flexible intelligent power grid capable provide efficient reliable economical sustainable energy production distribution supplier particularly integration renewable energy source wind solar grid impose engineer economic challenge limit ability control dispatch energy source intermittent characteristic timeseries prediction wind speed wind power production particularly important challenge task prediction interval pi preferable result prediction point estimate provide information confidence prediction paper different machine learn approach as pi timeseries prediction consider compare 1 multilayer perceptron neural network train multiobjective genetic algorithm 2 extreme learn machine combine near neighbor approach propose approach apply shortterm wind speed prediction real data set hourly wind speed measurement region regina saskatchewan canada approach demonstrate good prediction precision provide complementary advantage respect different evaluation criterian',\n",
       " 'YongPing Zhao recently extreme learn machine elm popular topic machine learn community replace socalled elm feature map nonlinear map induce kernel function kernel elm pkelm dkelm obtain primal dual perspective respectively unfortunately pkelm dkelm posse dense solution direct proportion number train data end constructive algorithm pkelm ccpkelm propose virtue cholesky factorization train data incur large reduction objective function recruit significant vector reduce train cost pccpkelm obtain application probabilistic speedup scheme ccpkelm correspond ccpkelm destructive pkelm cdpkelm present partial cholesky factorization strategy train data incur small reduction objective function removal prune current set significant vector finally verify efficacy feasibility propose algorithm paper experiment small large benchmark data set investigatedn',\n",
       " 'Ali Akbar Abdoos wind power plant clean energy resource increasingly utilize power system wind power forecast complicate volatile nature wind speed accurate wind power forecast crucial wind power plant new hybrid pattern recognition method propose study short term wind power forecast time series produce power estimate combination main step include preprocessing feature selection regression step step time series wind power decompose different mode variational mode decomposition vmd technique mode construct train pattern forecast output second step eliminate redundant property feature selection method base gramschmidt orthogonalization gso apply potential candidate step extreme learn machine elm efficient fast regression tool train subset select feature eventually power generate wind farm estimate sum predict mode value performance propose wind power forecaster evaluate real data collect wind farm locate sotavento galicia spain texas obtain result justify superiority propose method accurate forecast save computational time compare previously report methodsn',\n",
       " 'Ling Chen Peng Zhang Zhihua Cai Jia Wu Yongshan Zhang extreme learn machine elm promise model train singlehide layer feedforward network slfns widely classification elm face challenge arbitrarily select parameter network weight hide bias effort enhance performance elm evolutionary algorithm explore promise area solution space evolutionary algorithm explore promise area solution space able locate global optimum efficiently paper present new memetic algorithm mabased extreme learn machine melm short melm embed local search strategy global optimization framework obtain optimal network parameter experiment comparison 46 uci data set validate performance melm correspond result demonstrate melm significantly outperform stateoftheart elm algorithm highlightswe propose selfadaptive network parameter optimize method elmmemetic algorithm ma network parameter optimizationwe analyze time complexity convergence propose methodexperiment result 46 uci dataset demonstrate algorithm performancen',\n",
       " 'Amaury Lendasse Paula Lauren Rui Nian Kaj Mikael Bjork Yoan Miche Stephen Baek Anton Akusok abstract paper present fast algorithm accelerate toolbox 1 data visualization visualization state assignment problem data sample number give visualization point map function approximate extreme learn machine provide error current assignment work present new mathematical formulation error function base cosine similarity provide close form equation change error exchange assignment random sample call swap extreme speedup original method large corpus like mnist handwritten digit dataset method start random assignment continue greedy optimization algorithm randomly swap pair sample keep swap reduce error toolbox speed reach million swap second thousand model update second successful swap gpu implementation large dataset like mnist handwritten digitn',\n",
       " 'Qinghua Zheng Jun Guo Jun Liu Lingling Zhang Minnan Luo extreme learn machine generalize singlehiddenlayer feedforward network achieve attention extremely fast learn speed good generalization performance big data make challenge large scale learn elm limitation memory single machine distribute manner large scale data storage collection application purpose relieve limitation memory big data paper exploit novel distribute extreme learn machine implement extreme learn machine algorithm parallel largescale data set correspond distribute algorithm develop basis alternate direction method multiplier show effectiveness distribute convex optimization finally numerical experiment wellknown benchmark data set carry illustrate effectiveness propose delm method provide analysis performance speedup scaleup sizeupn',\n",
       " 'Marco Sciandrone Andrea Manno Luigi Grippo paper consider learn problem multilayer perceptrons mlps formulate problem minimize smooth error function know learn problem mlps difficult nonlinear nonconvex optimization problem typical difficulty presence extensive flat region steep side valley error surface possible large number train data free network parameter define wide class batch learn algorithm mlp base use block decomposition technique minimization error function learn problem decompose sequence small structure minimization problem order advantageously exploit structure objective function theoretical convergence result establish specific algorithm construct evaluate extensive numerical experimentation comparison stateoftheart learn algorithm effectiveness propose techniquen',\n",
       " 'Lei Chen Guoren Wang Ye Yuan Yuhai Zhao Zhanghui Wang discriminative subgraph mine large collection graph object crucial problem graph classification extreme learn machine elm simple efficient singlehidden layer feedforward neural network slfns algorithm extremely fast learn capacity paper propose discriminative subgraph mine approach base elmfilter strategy scalable mapreduce compute model randomly partition collection graph worker node worker apply fast pattern evolutionary method set discriminative subgraph help elmfilter strategy partition set discriminative subgraph produce high elm train accuracy union discriminative subgraph mine result input largescale graph extensive experimental result real synthetic datasets method obviously outperform approach term classification accuracy runtime efficiencyn',\n",
       " 'Guyu Hu Zhisong Pan Yajun Yu identification classification graph data hot research issue pattern recognition conventional method graph classification usually convert graph data vector representation ignore sparsity graph data paper propose new graph classification algorithm call graph classification base sparse graph feature selection extreme learn machine key method lasso select sparse feature sparsity correspond feature space graph data extreme learn machine elm introduce follow classification task good performance extensive experimental result series benchmark graph datasets validate effectiveness propose methodn',\n",
       " 'Xizhao Wang Joshua Zhexue Huang Rana Aamir Ashfaq Yulin He imbalanced extreme learn machine base kernel density estimation imelmkde late classification algorithm handle imbalanced binaryclass classification adjust real output train data intersection point probability density function pdfs correspond predictive output majority minority class imelmkde update elm train base original train data improve performance elmbase imbalanced classifier paper analyze shortcoming imelmkde propose improve version imelmkde parzen window method imelmkde lead multiple intersection point pdfs majority minority class addition unreasonable update real output intersection point pdfs estimate base predictive output order improve shortcoming imelmkde imbalanced elm base normal density estimation imelmnde propose paper imelmnde pdfs predictive output correspond majority minority class compute normal density estimation intersection point update predictive output instead real output make train probability density estimationbase imbalanced elm simple feasible comparative result propose imelmnde perform good unweighted elm imelmkde imbalanced binaryclass classification problemn',\n",
       " 'Ioannis Pitas Anastasios Tefas Alexandros Iosifidis Vasileios Mygdalis paper apply oneclass classification method facial image analysis problem consider case available train data information originate class available class high importance propose novel extension oneclass extreme learn machine algorithm aim minimize train error data dispersion consider solution generate decision function elm space elm space arbitrary dimensionality evaluate performance publicly available datasets propose method compare favourably stateoftheart choicesn',\n",
       " 'Adel AlJumaily Khairul Anam success myoelectric pattern recognition mpr rely feature extract classifier employ paper propose evaluate fast classifier extreme learn machine elm classify individual combine finger movement amputee nonamputee elm single hide layer feedforward network slfn avoid iterative learn determine input weight randomly output weight analytically accelerate train time slfns addition classifier evaluation paper evaluate feature combination improve performance mpr investigate feature projection improve class separability feature different study implementation elm myoelectric controller paper present complete thorough investigation type elm include nodebased kernelbased elm furthermore paper provide comparison elm wellknown classifier linear discriminant analysis lda knearest neighbour knn support vector machine svm leastsquare svm lssvm experimental result accurate elm classifier radial basis function elm rbfelm comparison rbfelm wellknown classifier show rbfelm accurate svm lssvm fast svm family superior lda knn experimental result indicate accuracy gap mpr amputee nonamputee accuracy 9855 amputee 995 nonamputee electromyography emg channelsn',\n",
       " 'QingHua Ling JianMing Zhang MinRu Zhao Fei Han abstract determine network structure open problem extreme learn machine elm error minimize extreme learn machine emelm simple efficient approach determine number hide node similar constructive elm emelm lay emphasis convergence accuracy obtain singlehidelayer feedforward neural network slfn good convergence performance bad condition paper effective approach base error minimize elm particle swarm optimization pso propose adaptively determine structure slfn regression problem new method establish compact wellconditioning slfn hide node optimize pso add slfn regression accuracy condition value hide output matrix network consider optimization process experiment result regression problem verify propose algorithm achieve good generalization performance few hide node constructive elmn',\n",
       " 'Yulin He Jame N.K. Liu Jane Jia You Yuan Wang Yanxing Hu research community recently attention extreme learn machine elm algorithm neural network nn area elm fast traditional gradientdescentbased learn algorithm analytical determination output weight random choice input weight hide layer bias input weight bias randomly assign adjust elm model show instability repeat experiment time instability make elm reliable computational intelligence model investigation try solve problem random production layer elm reduce chance random weight assignment elm remove bias hide layer experiment son different data set demonstrate propose model high stability reliability classical elmn',\n",
       " 'Rana Aamir Raza Ashfaq XiZhao Wang Eric C.C. Tsang Hong Zhu monotonic classification problem mean feature value class label order monotonicity relationship exist feature decision label extreme learn machine elm singlehidden layer feedforward neural network fast train rate good generalization capability existence train error elm directly handle monotonic classification problem work propose generalization elm process monotonic classification name monotonic classification extreme learn machine mcelm monotonicity constraint impose original elm model mathematically mcelm quadratic program problem monotonicity relationship consider constraint train error objective minimize mathematical model mcelm generate classifier monotonic minimize classification error mcelm need tune parameter iteratively keep advantage extremely fast train essential characteristic elm mcelm require monotonic relationship exist feature output consistent essentially relax assumption consistent monotonicity exist approach handle monotonic classification problem comparison exit approach handle monotonic classification mcelm generate monotonicityreserving classifier experimentally show good generalization capability artificial real world datasetsn',\n",
       " 'Dalibor Petkovic Ljubisa Kocic Vojislav V. Mitic Vlastimir Nikolic fluctuation wind speed affect wind energy system potential wind power proportional cube wind speed precise prediction wind speed important improve performance system unstable behavior wind speed different terrain study fractal characteristic wind speed series analyze accord selfsimilarity characteristic scale invariance fractal extrapolate interpolation prediction perform extend fractal characteristic internal interval external interval afterward neurofuzzy technique apply fractal data high nonlinearity data neurofuzzy approach detect important variable affect wind speed accord fractal dimension main goal investigate influence terrain roughness length different height wind speed wind speed predictionn',\n",
       " 'Yong Dou Yao Lu Qi Lv Kai Chen extreme learn machine elm regression field easyimplementation fast train speed good generalization performance basic elm xc5xba2norm loss function sensitive outlier recently xc5xba1norm loss function huber loss function elm enhance robustness xc5xba1norm loss function huber loss function effect outlier linear correlation error exist robust elm method use xc5xba2norm regularization regularization term study propose unify model robust regularize elm regression iteratively reweighted square irls relmirls perform comprehensive study robust loss function regularization term robust elm regression loss function xc5xba1norm huber bisquare welsch enhance robustness type regularization xc5xba2norm xc5xba1norm avoid overfitting experiment propose relmirls xc5xba2norm xc5xba1norm regularization stable accurate data 0 40 outlier level relmirls xc5xba1norm regularization obtain compact network highly sparse output weight networkn',\n",
       " 'TingHao Chen Liguo Sun Dong Liang PengPeng Xi ZhiQiang Li YongPing Zhao compact architecture extreme learn machine elm incremental learn algorithm propose paper previous incremental learn algorithm elm recruit hide node randomly equivalent implement random selection candidate set infinite size impossible recruit good hide node usually require hide node traditional neural network achieve match performance improve quality hide node recruit incremental learn algorithm elm present base gramschmidt process gsielm recruit best hide node random subset fix size define evaluate criterion learn step xe2x80x9cnesting effectxe2x80x9d exist gsielm hide node recruit gsielm late discard treat xe2x80x9cnesting problemxe2x80x9d improve gsielm igsielm generate elimination mechanism learn step igsielm eliminate bad hide node alreadyrecruit group newlyrecruit finally verify efficacy feasibility propose algorithm gsielm igsielm paper experiment regression classification benchmark data set investigatedn',\n",
       " 'Xueyi Liu Wenhui Wang abstract random assignment strategy input weight bring extreme learn machine elm advantage fast learn speed minimal manual intervention monte carlo mc base random sample method typically generate input weight elm poor capability sample structure preserve ssp degenerate learn generalization performance reason quasimonte carlo qmc method revisit show distortion error qmc projection obtain fast convergence rate mc relatively lowdimensional problem unify random orthogonal ro projection method propose show ro method provide optimal transformation term minimize loss distance sample experimental result realworld benchmark data set verify rationality theoretical analysis indicate enhance ssp capability input weight qmc ro projection method tend bring elm algorithm good generalization performancen',\n",
       " 'Zhiping Lin Nan Liu Yan Yang Yong Kiang Yeo Chung Soo Ahn Lei Sun BeomSeok Oh abstract paper propose efficient parameter tuningfree squaredloss mutual information smi estimator form radial basis function rbf network input layer propose network propagate sample pair random variable hide layer propagate sample transform set gaussian rbf kernel randomly determine kernel center width similar extreme learn machine output layer adopt linear weight scheme analytically estimate empirical result propose estimator outperform compete stateoftheart smi estimator term computational efficiency show comparable estimation accuracy performance propose model achieve promise result application study timeseries changepoints detection drive stressn',\n",
       " 'Baojun Zhao GuangBin Huang Weisi Lin Baoxian Wang Chenwei Deng abstract robust visual track appearance model able separate object background accurately adapt appearance variation exist track method mainly focus aspect design different module combine price double computational cost paper pairwise metric learn present novel appearance model robust visual track specifically visual track view pairwise regression problem extreme learn machine elm utilize construct pairwise regression framework elmbased pairwise train constraint enforce target observation different regression output background one target observation track approximate regression output discriminative generative capability fully consider single object track model online sequential elm oselm update result appearance model lead robust track process extensive experimental evaluation challenge video sequence demonstrate effectiveness efficiency propose trackern',\n",
       " 'Song Guo Chenguang Li Bin Zhang Yuhai Zhao Ying Yin abstract elm efficient classification technology popular application domain elm weak generalization performance data set small respect feature space paper enhance elm algorithm base representative feature propose address problem method automatically generate discrete interval continuous feature remove irrelevant feature method consider feature interaction reduce weakly relevant feature mutual information base method reduction redundancy feature conduct instead construct large bayesian network feature select feature high relevance object node improve markov boundary identify algorithm finally obtain enhance elm classifier train elm extract representative feature genetic algorithm base weight assignment mechanism experiment conduct real synthetic small sample data set demonstrate enhance elm classifier base representative feature outperform method comparison study term efficiency effectivenessn',\n",
       " 'Fuad E. Alsaadi Jinling Liang Weibo Liu Hong Zhang Nianyin Zeng paper hybrid learn approach combine extreme learn machine elm new switch delay pso sdpso algorithm propose problem shortterm load forecast stlf particular input weight bias elm optimize new develop sdpso algorithm delay information locally best particle globally best particle exploit update velocity particle test propose sdpsoelm comprehensive manner tanh function approach obtain good generalization performance avoid add unnecessary hide node overtraining problem show outstanding performance stateoftheart elm finally propose sdpsoelm algorithm successfully apply stlf power experiment result demonstrate propose learn algorithm good forecast result comparison radial basis function neural network rbfnn algorithmn',\n",
       " 'Zhihua Cai Chuan Zhou Jia Wu Yongshan Zhang instance clone technique improve extreme learn machinethe propose method effective local learn method classification problemwe analyze sensitivity key parameter propose methodexperimental result large number dataet demonstrate algorithm performance extreme learn machine elm popular machine learn method flexibly simulate relationship realworld classification application face problem data set small number sample instance elm result overfitting trouble paper propose new instance clone extreme learn machine icelm short handle numerous different classification problem icelm u instance clone method balance input data distribution extend train data set alleviate overfitting issue enhance test classification accuracy experiment comparison 20 uci data set validation image text classification application demonstrate icelm able achieve superior result compare original elm algorithm variant classical machine learn algorithmsn',\n",
       " 'Jochen Jakob Steil Rene Felix Reinhart Witali Aswolinskiy abstract consider model parametrized process goal model process new parameter value combination compare classical regression approach modular approach base regression model space process parametrization model learn second map process parameter model parameter learn evaluate approach synthetic realworld data set advantage regression model spacen',\n",
       " 'Badong Chen HaiJun Rong Feng Ye Jing Yang real industrial process measurement sample noise different statistical characteristic obtain sample usually online sequential learn algorithm achieve good learn performance system noise statistic necessary paper propose new online extreme learn machine elm huang et al algorithm recursive mean ppower elm rlmpelm rlmpelm novel error criterion cost function mean ppower lmp error criterion provide mechanism update output weight sequentially lmp error criterion aim minimize mean ppower error generalization mean square error criterion elm propose online learn algorithm able provide online prediction variable noise different statistic obtain good performance elm online sequential elm oselm nongaussian noise impact process simulation report demonstrate performance effectiveness propose methodsn',\n",
       " 'Xizhao Wang Hongyu Zhu aim reduce total cost costensitive learn paper introduce semisupervised learn model base uncertainty sample output central idea 1 categorize sample train set group base uncertaintymagnitude output 2 add group sample uncertainty predict label original train set 3 retain new classifier total cost reduction ratio cost class impact learn improvement discuss theoretical analysis experimental demonstration model effectively improve performance costensitive learn algorithm certain type classifiersn',\n",
       " 'Lingjie Sun Haiwen Yuan Yingyi Liu Zhao Ma Boyang Zhang ensemble trick widely extreme learn machine elm paradigm concern train phase expectation improve generalization ability unlike traditional strategy paper pay attention prediction phase propose discriminatory approach call ensemble base reactivate regularization elm er2elm short novel literature consist interrelate step probability density estimation conduct degree difficulty identify instance random factor adopt determine elm base learner sequentially reactivate instance easily identify cost compute burden vague one take carefully consideration compare ensemble method prediction computation overhead decrease end number example include uci benchmark datasets handwritten digit object detection employ illustrate stateoftheart performancen',\n",
       " 'Julio J. Valdes abstract current advance communication sensor compute technology generate information see amount constantly increase rate information explosion internet thing point view data analytics information compose diversity data type contain uncertainty incompleteness different degree add extra component original heterogeneity data mine machine learn method handle heterogeneity extreme learn machine elm interest computational algorithm simplicity good performance speed extend process information compose heterogeneous data type htelm capable address classification regression problem complex data approach discuss work directly heterogeneous data transform information simple homogeneous space preserve structural property standard learn method apply include classical elm approach illustrate real world example involve heterogeneous predictor variable compose mixture nominal ordinal interval ratio fuzzy variable entire empirical probability distribution case htelm elm model produce result compare favorably wellestablished methodn',\n",
       " 'Shi Liu Xueyao Wang Qibin Liu Huaiping Mu Jing Lei abstract appeal superiority include highspeed data acquisition nonintrusive measurement low cost high safety visual presentation lead success electrical capacitance tomography ect technique monitor industrial process highaccuracy tomographic image play crucial role reliability ect measurement result provide powerful scientific evidence investigate complicate mechanism behavior image object io exist numerical algorithm develop solution inverse problem ect area datadriven twostage reconstruction method propose improve reconstruction quality rq paper stage learn stage regularize extreme learn machine relm model solve split bregman technique develop extract map tomographic image reconstruct algorithm true image accord set train sample second stage prediction stage new io reconstruct algorithm compute train sample image result consider input train relm model predict final result performance propose reconstruction method compare evaluate mean numerical simulation approach clean noisy capacitance data different noise level nls quantitative qualitative comparison result validate practicability effectiveness propose datadriven reconstruction method research find provide new insight improvement reconstruction accuracy robustness ect arean',\n",
       " 'Abdullah Al Mamun R. Savitha N. Krishna Kumar intense increase offshore operational activity warrant periodical accurate prediction wave characteristic usually complex numerical model require high computational power prediction overcome challenge numerical model paper propose use ensemble extreme learn machine enselm predict daily wave height exploit randomness initialization elm obtain good generalization performance construct ensemble elm parameter elm initialize distinct region input space sample data set output elm mean square sample data set report output study performance enselm predict daily wave height 10 station vary terrain gulf mexico brazil korean region enselm network train past wave data measure atmospheric condition obtain station jan 1 2011 dec 31 2014 test data station jan 1 2015 aug 30 2015 study performance enselm evaluate comparison elm online sequential elm oselm support vector regression svr study infer enselm perform elm oselm svr daily wave height predictionn',\n",
       " 'Deqin Yan Sen Luo Lin Feng Jun Wu Shenglan Liu abstract classical nonlinear dimensionality reduction algorithm locally linear embed lle show powerful performance research field limitation lle 1 traditional lle sensitive highcurvature noise 2 computation expensive solve problem present quasicurvature lle qlle take curvature local neighborhood consideration map local configuration lowdimensional coordinate novel learn framework call quasicurvature local linear projection qllp propose efficient dimensionality reduction framework select small landmark original data obtain lowdimensional coordinate qlle adopt extreme learn machine elm learn explicit map function original data lowdimensional coordinate nonlinear dimensionality reduction extensive experiment synthetic frey facial expression datasets demonstrate framework greatly improve efficiency nonlinear dimensionality reductionn',\n",
       " 'Chi Man Vong Ka In Wong Xiang Hui Gao Pak Kin Wong abstract online extreme learn machine elm base model optimization approach pointbypoint engine calibration propose improve efficiency conventional modelbase calibration approach instead build hundred local engine model engine operate point elm model necessary process elm model firstly construct start operate point calibration start point conduct determine optimal parameter model elm model reuse base model nearby target operate point optimization perform model search best parameter design experiment strategy best parameter obtain new measurement target operate point collect update model repeat optimization model update procedure optimal parameter target point iteration model target point base model nearby operate point repeat process calibration operate point online efficiently contribution propose method save number experiment calibration process verify effectiveness propose approach experiment commercial engine simulation software conduct variant online elm utilize model update process comparison result engine calibration carry few measurement time propose approach initial train free online elm efficient online model method applicationn',\n",
       " 'Koh Hosoda Fabio DallaLibera Shuhei Ikemoto stochastic resonance sr phenomenon input signal nonlinear magnitude small affect output observable add nonzero level noise sr know assist biological be cop noisy environment provide sophisticate information process adaptive behavior sr effect interpret decrease inputoutput information loss nonlinear make stochastically close linear work show sr improve performance desire inputoutput relationship nonlinear specifically case neural network hide layer consist threshold function universal approximation capability neural network exploit sr discuss network consist threshold activation function prove universal approximator context extreme learn machine elm sr take account deem classic threelayer neural network universality previously prove simple proof prove universal approximation capability infinite number hide unit performance achieve finite number hide unit evaluate train algorithm backpropagation elm result highlight sr effect occur propose relationship number hide unit noise intensity approximation performancen',\n",
       " 'David V. Anderson Muhammad Rizwan speech recognition system exhibit performance degradation variability speech cause accent dialect speaker overcome correctly identify accent dialect speaker accent dialect information adapt speech recognition system paper apply extreme learn machine elm support vector machine svms problem accentdialect classification timit dataset mel frequency cepstrum coefficient mfccs normalize energy parameter second derivative raw feature train elm svms weight accent classification algorithm propose u novel architecture classify north american accent seven group algorithm obtain classification accuracy 7788 elm knowledge best result report accent classification timit dataset compare performance elm svms classifier weight accent classification algorithm multiclass classification elm svmsn',\n",
       " 'Jinzhu Gao Zhong Ming Xizhao Wang Weipeng Cao big data field increase compute capability artificial neural network show great strength solve data classification regression problem traditional train neural network depend generally error propagation method iteratively tune parameter number hide layer increase kind train problem slow convergence time consume local minimum avoid problem neural network random weight nnrw propose weight hide layer input layer randomly select weight output layer hide layer obtain analytically researcher show nnrw low train complexity comparison traditional train feedforward neural network paper objectively review advantage disadvantage nnrw model try reveal essence nnrw give comment remark nnrw provide useful guideline user choose mechanism train feedforward neural networkn',\n",
       " 'Zhaoyi Zhou Liguang Zang Junhai Zhai abstract extreme learn machine elm simple efficient algorithm train single hide layer feedforward neural network slfns fast speed good generalization ability elm successfully apply field pattern recognition vision biological information process problem elm architecture selection second prediction instability order deal problem base dropout technique ensemble learn method propose paper propose method solve problem improve prediction stability experimental result statistical analysis 14 data set confirm conclusion furthermore experimental result propose approach outperform original elm prediction stability classification accuracyn',\n",
       " 'Qiang Zhang ChangJun Zhou XiaoJun Wang QuanYi Zou abstract online learn contribution old sample model decrease time pas old sample gradually invalid online sequential extreme learn machine oselm avoid repetitive train old sample invalid sample go improve accuracy oselm model online sequence extreme learn machine forget mechanism foselm timely discard invalid sample consider difference valid sample hasxc2xa0the limitation boost accuracy generalization solve issue memory degradation base oselm mdoselm propose paper mdoselm adjust weight old new sample real time selfadaptive memory factor simultaneously discard invalid sample selfadaptive memory factor determine element similarity new old sample prediction error current train sample previous model performance propose mdoselm validate regression classification datasets include artificial dataset twentytwo realworld dataset result demonstrate mdoselm model outperform oselm foselm model accuracy generalizationn',\n",
       " 'TingHao Chen Liguo Sun FangQuan Song YingTing Pan YongPing Zhao abstract recently generalize singlehidden layer feedforward network propose extension original extreme learn machine elm different traditional elm generalize elm gelm utilize p order reduce polynomial function complete input feature output weight accord empirical result insignificant redundant input feature construct p order reduce polynomial function output weight gelm date work select appropriate input feature construct output weight gelm paper greedy learn algorithm forward feature selection algorithm ffsgelm backward feature selection algorithm bfsgelm propose tackle issue reduce computational complexity iterative strategy ffsgelm convergence prove bfsgelm decrease iteration apply decay model process accelerate scheme propose speed computation remove insignificant redundant feature effectiveness propose ffsgelm bfsgelm benchmark data set employ experiment report demonstrate ffsgelm bfsgelm select appropriate input feature construct p order reduce polynomial function output weight gelm ffsgelm bfsgelm enhance generalization performance simultaneously reduce test time compare original gelm bfsgelm work good ffsgelm term sparsity ratio test time train time slightly lose advantage generalization performance ffsgelmn',\n",
       " 'Omer Faruk Ertugrul abstract determine optimal activation function artificial neural network important issue directly link obtain success rate unfortunately way determine analytically optimal activation function generally determine trial tune paper address simple effective approach determine optimal activation function approach call train activation function activation function train particular neuron linear regression train process base train dataset consist sum input neuron hide layer desire output way different activation function generate neuron hide layer approach employ random weight artificial neural network rwn validate 50 benchmark datasets achieve success rate rwn train activation function high obtain success rate rwn traditional activation function obtain result show propose approach successful simple effective way determine optimal activation function instead trial tune randomize single multilayer annsn',\n",
       " 'Jianhua Zhang Zhong Yin abstract electroencephalography eeg base machinelearn model mental fatigue recognition evaluate reliability human operator performance taskgeneric model particularly important time cost prepare taskpecific train eeg dataset avoid study develop novel mental fatigue classifier dynamical deep extreme learn machine ddelm adapt variation eeg feature distribution mental task different static deep learn approach ddelm iteratively update shallow weight multiple time step test stage propose method incorporate merit deep network eeg feature abstraction elm autoencoder fast weight recompuation feasibility ddelm validate investigate eeg datasets record paradigm autocams humanxe2x80x93machine task accuracy comparison indicate new classifier significantly outperform stateoftheart mental fatigue estimator examine cpu time computational burden ddelm acceptable highdimensional eeg featuresn',\n",
       " 'Konstantinos G. Margaritis Yiannis Kokkinos abstract typical model selection strategy apply extreme learn machine elm paper depend kfold crossvalidation grid search select best pair l c adjustable hyperparameters number l hide elm node regularisation parameter c minimize validation error test 30 value l 30 value c 10fold crossvalidation learn phase build 9000 different elm model different pair l c model independent essence manage drastically reduce computational cost elm model selection rely matrix decomposition avoid direct matrix inversion allow produce reusable matrix crossvalidations matrix decomposition crossvalidation version result combination paper identify combination analyse theoretically experimentally discover fast compare singular value decomposition svd eigenvalue decomposition evd cholesky decomposition qr decomposition produce reusable matrix orthogonal eigen singular upper triangular decomposition combine different crossvalidation approach present direct thorough comparison kfold crossvalidation version leaveoneout crossvalidation analyse computational cost demonstrate theoretically experimentally type matrix decomposition play important role equally important role play version crossvalidation finally scalable computationallyeffective algorithm present significantly reduce computational timen',\n",
       " 'P.K. Dash Mrutyunjaya Sahani abstract paper variational mode decomposition vmd newly develop weight online sequential extreme learn machine woselm integrate detect classify power quality event pqes realtime feasibility vmd validate apply pqes harmonic flicker estimation magnitude phaseand frequency estimate result prove usefulness vmd efficacious power quality index bandlimited intrinsic mode function blimfs extract index classification single multiple pqes different advance classifier recognition architecture variational mode decomposition weight online sequential extreme learn machine vmdwoselm test compare withother method robust antinoise performance fast learn speed le computational complexity superior classification accuracy short event detection time prove propose vmdwoselm method implement electrical power system finally pc interface base hardware prototype develop verify cogency propose method real time feasibility propose method test validate simulation laboratory experimentsn',\n",
       " 'Lihong Xu Jing Wang Dongpo Xu Yuanyuan Wang Huisheng Zhang abstract emerge learn model extreme learn machine elm attract attention fast learn powerful data process ability extend elm complex domain complex elm celm propose process complexvalued data exist celms fully capture secondorder statistic complex signal provide suboptimal solution real world application deal noncircular improper signal end paper propose augment celm model incorporate conjugate information complex input hide layer respectively approximation capability ability capture secondorder statistic signal theoretically analyse base wirtinger calculus correspond regularize model derive sake overcome possible overfitting problem superiority propose model verify simulation resultsn',\n",
       " 'Sen Zhang Yanjiao Li Wendong Xiao Jie Zhang abstract extreme learn machine elm propose train single hide layer feedforward neural network slfns provide efficient learn solution regression problem prediction error elm unavoidable limit model capability nonlinear stochastic nature regression problem paper novel elm residual compensation elm rcelm propose regression problem employ multilayer structure baseline layer build feature map input output layer residual compensation layer layer iteratively real world application devicefree localization dfl gas utilization ratio gur prediction blast furnace experimental test propose rcelm experimental result rcelm good generalization performance robustness machine learn approach include classic elm weight knearest neighbor wknn support vector machine svm propagation neural network bpnnn',\n",
       " 'Lin Xu Xia Liu abstract extreme learn machine elm consider singlehide layer feedforward neural network fnntype learn input weight hide layer bias randomly assign output weight need tune framework regression fundamental problem elm learn elm estimator universally consistent approximate arbitrary regression function accuracy provide number train sample sufficiently large aim paper twofold verify strongly universal consistency elm estimator present sufficient necessary condition activation function correspond elm estimator strongly universally consistent obtain result underlie feasibility elm provide theoretical guidance selection activation function elm learnn',\n",
       " 'Lian Li Caihong Li Yi Yang Marius Kloft Yanhua Chen abstract short term electric load forecast important tool electricity market play critical role management electric system propose accuracy optimization method challenge task indispensable energy accurate forecast method need different people different area paper propose novel shortterm electric load forecast method emdmixelm base empirical mode decomposition emd extreme learn machine elm emdmixelm u empirical mode decomposition decompose load series capture complicate feature electric load denoising data consider performance extreme learn machine elm greatly influence choice kernel mix kernel method propose elm mix kernel combine rbf kernel ukf kernel forecast result emdmixelm prove good method rbfelm ukfelm mixelm exist method mfes esplssvm combine method verify forecast ability emdmixelm halfhourly electric load data state new south wale victoria queensland australia paper case study experimental result clearly indicate datasets forecast accuracy propose method superior methodn',\n",
       " 'Gavin Brown Alexandros T. Tzallas Nikolaos Giannakeas Markos G. Tsipouras Vasileios Christou abstract paper hybrid learn approach combine extreme learn machine elm genetic algorithm ga propose utilization hybrid algorithm enable creation heterogeneous single layer neural network slnns good generalization ability traditional elm term low mean square error mse regression problem high accuracy classification problem architecture method limit traditional linear neuron input participate equally neuronxe2x80x99s activation extend support high order neuron affect networkxe2x80x99s generalization ability initially propose heterogeneous hybrid extreme learn machine hehyelm algorithm create number custom create neuron different structure creation homogeneous slnns network train elm application specific ga evolve heterogeneous network accord fitness criterion utilize uniform crossover operator recombination process completion evolution process network best fitness select optimal experimental result demonstrate propose learn algorithm good result traditional elm homogeneous hybrid extreme learn machine hohyelm optimally prune extreme learn machine opelm homogeneous heterogeneous slnnsn',\n",
       " 'Jianping Yin Xinwang Liu Yongkai Ye Mao Wang En Zhu Yuewei Ming abstract machine learn application embrace large data size model complexity practitioner turn distribute cluster satisfy increase computational memory demand recently parallel variant extreme learn machine elm propose base cluster limitation computation memory variant address data model large goal build scalable elm large number sample hide neuron parallel run cluster computational memory bottleneck have output result sequential elm paper propose parallel variant elm refer local data model parallel elm ldmpelm global data model parallel elm gdmpelm variant implement cluster message pass interface mpi environment tradeoff efficiency scalability complementary advantage collectively variant call data model parallel elm dmpelm advantage dmpelm exist variant highlight follow 1 simultaneously utilize data model parallel technique improve parallelism elm 2 good scalability support large data model address memory computational bottleneck appear exist variant extensive experiment conduct largescale datasets propose algorithm good scalability achieve ideal speedup best knowledge time successfully train large elm model 50000 hide neuron mnist8m dataset 81 million sample 784 featuresn',\n",
       " 'Peiyu Guo Lei Wang Xiaobin Zhu Zhuangzi Li know convolutional neural network good learn invariant feature optimal classification contrarily kernel extreme learn machine kelms good approximate target continuous function extremely fast speed learn complicate invariance paper propose novel image classification framework kelm instead softmax function adopt classifier convolutional neural network cnn architecture promote performance image classification experiment conduct publicly available datasets demonstrate superior performance propose methodn',\n",
       " ...]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_auth.sentences_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghassenabdedayem/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1330: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf matrix generated in 12 sec\n"
     ]
    }
   ],
   "source": [
    "#Now we will compute a logarithmic tf-idf matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "t = time()\n",
    "\n",
    "# Create a TfidfVectorizer object with logarithmic tf\n",
    "# And as we are intrested in links between words, we will take only words that occured at least in two abstracts\n",
    "words_multiple = {key:value for (key, value) in voc_auth.word_occurrence.items() if value >= 1}\n",
    "vectorizer = TfidfVectorizer(vocabulary=list(words_multiple.keys()), sublinear_tf=True)\n",
    "\n",
    "\n",
    "# Fit the vectorizer to the sentences and transform them into a TF-IDF matrix\n",
    "\n",
    "tfidf_matrix_complete = vectorizer.fit_transform(voc_auth.sentences_list)\n",
    "\n",
    "# Print the TF-IDF matrix\n",
    "print('tf-idf matrix generated in {:.0f} sec'.format(time()-t))\n",
    "\n",
    "#del (abstracts_list_sentences, voc, abstracts, abstracts_dict_list_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<138499x275240 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8449259 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from scipy import sparse\n",
    "\n",
    "# Specify the file path on your desktop\n",
    "save_path = os.path.expanduser(\"tfidf_complete_author.npz\")\n",
    "\n",
    "# Save the tfidf_matrix\n",
    "sparse.save_npz(save_path, tfidf_matrix_complete)\n",
    "\n",
    "print(\"TF-IDF matrix saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o5mW4OXyLIGQ",
    "outputId": "8edaed15-29fa-4e50-d83e-017e47c7bfef"
   },
   "outputs": [],
   "source": [
    "sentence_embeddings = model.encode(voc.sentences_list)\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentence_embeddings_BERT.plk', 'wb') as f:\n",
    "    pickle.dump(sentence_embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2czqslmiVfcQ"
   },
   "source": [
    "# Read processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf-_fYkF5xiO",
    "outputId": "3197fbbb-73a7-4b2e-c97f-4e9408afb7cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wv walks loaded from GCP in 0 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(138499, 64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time()\n",
    "from io import BytesIO\n",
    "\n",
    "walks_url = 'https://storage.googleapis.com/link_prediction_processed_data/walks_wv.npy'\n",
    "with urlopen(walks_url) as url:\n",
    "    data = url.read()\n",
    "\n",
    "# Create a seekable file-like object from the data\n",
    "fileobj = BytesIO(data)\n",
    "\n",
    "# Load the data from the file object\n",
    "walks_wv = np.load(fileobj)\n",
    "print('wv walks loaded from GCP in {:.0f} sec'.format(time()-t))\n",
    "walks_wv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGlREMRlrZKa"
   },
   "outputs": [],
   "source": [
    "# # Load max embeddings wv_192\n",
    "\n",
    "# url='https://storage.googleapis.com/link_prediction_processed_data/max_abstract_embedding.pkl'\n",
    "\n",
    "# response = requests.get(url)\n",
    "# data = response.content\n",
    "# max_abstract_embedding = pickle.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IrvRUVduhoAI"
   },
   "outputs": [],
   "source": [
    "# # Load the BART embedding torch tensor\n",
    "\n",
    "# url = \"https://storage.googleapis.com/link_prediction_processed_data/bart_embeddings.pt\"\n",
    "# response = requests.get(url)\n",
    "\n",
    "# with open(\"bart_embeddings.pt\", \"wb\") as f:\n",
    "#     f.write(response.content)\n",
    "\n",
    "# abstracts_bart_embeddings = torch.load('bart_embeddings.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LlQCihvfmBQP",
    "outputId": "133327d1-e540-4cd7-df63-e2ae446a0e75"
   },
   "outputs": [],
   "source": [
    "# Load the wv300 mean embedding\n",
    "url = 'https://storage.googleapis.com/link_prediction_processed_data/embedded_abstracts_local_wv300.npy'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "with open('embedded_abstracts_local_wv300.npy', 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Load the numpy array from the saved file\n",
    "local_wv300_abstracts = np.load('embedded_abstracts_local_wv300.npy')\n",
    "\n",
    "local_wv300_abstracts.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4jEFSE6in_w"
   },
   "outputs": [],
   "source": [
    "# # Load the goog300 mean embedding\n",
    "# url = 'https://storage.googleapis.com/link_prediction_processed_data/embedded_abstracts_goog300.npy'\n",
    "\n",
    "# response = requests.get(url)\n",
    "\n",
    "# with open('embedded_abstracts_goog300.npy', 'wb') as f:\n",
    "#     f.write(response.content)\n",
    "\n",
    "# # Load the numpy array from the saved file\n",
    "# goog300_abstracts = np.load('embedded_abstracts_goog300.npy')\n",
    "\n",
    "# goog300_abstracts.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_xIGkD_wuUn"
   },
   "outputs": [],
   "source": [
    "# Load TF-IDF matrix\n",
    "\n",
    "url = \"https://storage.googleapis.com/link_prediction_processed_data/tfidf_matrix.npz\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(\"tfidf_matrix.npz\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "tfidf_matrix = sparse.load_npz(\"tfidf_matrix.npz\")\n",
    "\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138499, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Initialize TruncatedSVD with desired number of components\n",
    "n_components = 1000  # Adjust the number of components as needed\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "# Apply TruncatedSVD to the TF-IDF matrix\n",
    "tfidf_reduced = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "# The tfidf_reduced matrix will have the reduced dimensionality based on TruncatedSVD\n",
    "print(tfidf_reduced.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJjyDTiVPblx"
   },
   "outputs": [],
   "source": [
    "# # Load words 192 embedding\n",
    "# import gzip\n",
    "# import pickle\n",
    "\n",
    "# with gzip.open('embedded_abstracts_dict_192array.pkl.gz', 'rb') as f:\n",
    "#     words_embedding_192 = pickle.load(f)\n",
    "\n",
    "# len(words_embedding_192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RuZSYpPv0zNr"
   },
   "outputs": [],
   "source": [
    "# len(words_embedding_192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20sWKHiSsgKV"
   },
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "# import numpy as np\n",
    "\n",
    "# words_embedding_192_trunc128 = dict ()\n",
    "# for i in tqdm(range(len(words_embedding_192))):\n",
    "#     if len(words_embedding_192[i])>0:\n",
    "#         arr = np.zeros((128, 192))\n",
    "#         vec = words_embedding_192[i][:128, :]\n",
    "#         arr[:vec.shape[0],:] = vec\n",
    "#         words_embedding_192_trunc128[i] = torch.tensor(arr).to(device)\n",
    "#     else:\n",
    "#         words_embedding_192_trunc128[i] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkzNXQF1taxU"
   },
   "outputs": [],
   "source": [
    "# tensor_list = []\n",
    "# i = 0\n",
    "# for i in tqdm(range(len(words_embedding_192_trunc128))):\n",
    "#     if len(words_embedding_192_trunc128[i])>0:\n",
    "#         tensor_list.append(words_embedding_192_trunc128[i])\n",
    "#     else:\n",
    "#         tensor_list.append(torch.zeros((128, 192)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pM27YApj0zNs"
   },
   "outputs": [],
   "source": [
    "# del(words_embedding_192_trunc128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAo6RBG-xy47"
   },
   "outputs": [],
   "source": [
    "# import gzip\n",
    "\n",
    "# filename = 'words_embedding_192_trunc128.pkl.gz'\n",
    "\n",
    "# # open the file in binary mode and write the dictionary to it, compressing the data with gzip\n",
    "# with gzip.open(filename, 'wb') as f:\n",
    "#     pickle.dump(words_embedding_192_trunc128, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNgqJRoY0zNt"
   },
   "outputs": [],
   "source": [
    "# tensor_list_float = []\n",
    "# for i, tensor in tqdm(enumerate(tensor_list)):\n",
    "#     tensor_list_float.append(tensor_list[i].float())\n",
    "# del(tensor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebgY7dPF0zNt"
   },
   "outputs": [],
   "source": [
    "# t = time ()\n",
    "# url = 'https://storage.googleapis.com/link_prediction_processed_data/words_embedding_192_trunc128.pkl.gz'\n",
    "\n",
    "\n",
    "# # download the file from the URL\n",
    "# response = requests.get(url)\n",
    "\n",
    "# # save the file to disk\n",
    "# with open(filename, 'wb') as f:\n",
    "#     f.write(response.content)\n",
    "\n",
    "# # load the data from the file\n",
    "# with gzip.open(filename, 'rb') as f:\n",
    "#     words_embedding_192_trunc128 = pickle.load(f)\n",
    "\n",
    "# print('padded truncated embeddings loaded in {:.0f} sec'.format(time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNfoJp-U0zNt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxvIuCP_bFrN"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "ROBXxQaZnfQ3"
   },
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_text, n_auth, n_feat, n_hidden, n_class, sub_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        #self.auth_emb = nn.Linear(n_auth, n_hidden)\n",
    "        self.abstract_emb = nn.Linear(n_text, 3*n_hidden)\n",
    "        self.fc1 = nn.Linear(n_feat+3*n_hidden, n_hidden)        \n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(2*n_hidden, n_hidden)\n",
    "        self.fc4 = nn.Linear(n_hidden, sub_class)\n",
    "        self.fc5 = nn.Linear(sub_class, n_class)        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x_in, abstract, auth, adj, pairs):\n",
    "\n",
    "        y = self.abstract_emb(abstract)\n",
    "        y = self.relu(y)\n",
    "        y = self.dropout(y)\n",
    "        del(abstract)\n",
    "\n",
    "#         y = self.auth_emb(auth)\n",
    "#         y = self.relu(y)\n",
    "#         y = self.dropout(y)\n",
    "#         del(auth)\n",
    "\n",
    "        x_in = torch.cat((x_in, y), dim=1)\n",
    "        \n",
    "        h1 = self.fc1(x_in)\n",
    "        z1 = self.relu(torch.spmm(adj, h1))\n",
    "        z1 = self.dropout(z1)\n",
    "        del(x_in, h1)\n",
    "\n",
    "        h2 = self.fc2(z1)\n",
    "        z2 = self.relu(torch.spmm(adj, h2))\n",
    "        z2 = self.dropout(z2)\n",
    "        del(h2, z1, adj)\n",
    "        \n",
    "        #x = torch.cat((z2[pairs[0]] , y[pairs[0]], z2[pairs[1]] , y[pairs[1]]), dim=1)\n",
    "        x = torch.cat((z2[pairs[0]] , z2[pairs[1]]), dim=1)\n",
    "        del(z2)\n",
    "\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        #x = torch.cat((x, pairs[2][:, None], pairs[3][:, None]), dim=1)        \n",
    "        del(pairs)\n",
    "        \n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ognzQbsFDb13"
   },
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "0azz_rVlDyn9"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, model, patience, delta, path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.model = model\n",
    "        self.val_loss_min = np.Inf\n",
    "        \n",
    "    def __call__(self, val_loss, path='checkpoint.pt'):\n",
    "        score = val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(path)\n",
    "        elif score > self.best_score + 0:\n",
    "            self.counter += 1\n",
    "            #print(self.counter)\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def load_checkpoint(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = (pairs_torch[0:2].permute(1, 0)[:int(len(pairs_torch[0])/2)])\n",
    "tensor2 = pairs_torch[0:2].permute(1, 0)[int(len(pairs_torch[0])/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_indices = torch.nonzero(torch.all(torch.eq(tensor1[:, None], tensor2[None, :]), dim=-1))\n",
    "\n",
    "# Print the common indices\n",
    "print(common_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.isin((pairs_torch[0:2].permute(1, 0)[:int(len(pairs_torch[0])/2)]), (pairs_torch[0:2].permute(1, 0)[int(len(pairs_torch[0])/2):]))\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensors\n",
    "tensor1 = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "tensor2 = torch.tensor([[3, 4], [7, 8], [9, 10]])\n",
    "\n",
    "# Reshape tensors to (1, -1) to get pairs of elements\n",
    "pairs1 = tensor1.reshape(1, -1)\n",
    "pairs2 = tensor2.reshape(1, -1)\n",
    "\n",
    "# Check if there are any common pairs of elements\n",
    "common_pairs = torch.eq(pairs1, pairs2).any()\n",
    "\n",
    "# Print the result\n",
    "print(common_pairs.item())  # True if there are common pairs, False otherwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.isin(tensor1, tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "bnRNz5z4nh6x"
   },
   "outputs": [],
   "source": [
    "def train_model(model, learning_rate, abstract, auth, features, adj, indices, y, val_indices, y_val, epochs, run_number, window = 10):\n",
    "    # Train model\n",
    "    start_time = time()\n",
    "    \n",
    "    print('Initializing the optimizer with learning rate:', learning_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) #optimizer with halving learning rate in training\n",
    "    try: os.mkdir('./outputs')\n",
    "    except: pass\n",
    "\n",
    "    today = datetime.today().strftime('%Y-%m-%d-%H:%M')\n",
    "    list_loss_val = []\n",
    "    list_loss_train = []\n",
    "    list_epochs = []\n",
    "    \n",
    "    halving_lr = 0 # counter of the number of halving lr\n",
    "    patience = 16\n",
    "    early_stopping = EarlyStopping(model, patience=patience, delta=0.1, path='checkpoint.pt')\n",
    "    print('Start training...')\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        rand_indices = torch.randint(0, features.shape[0], size=(indices.shape[0],indices.shape[1])).to(device)\n",
    "        \n",
    "        pairs = torch.cat((indices, rand_indices), dim=1)\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        output = model(features, abstract, auth, adj, pairs).to(device) # we run the model that gives the output.\n",
    "        loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "        acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "        loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "        optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, abstract, auth, adj, val_indices).to(device)\n",
    "        #y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        list_loss_val.append(loss_val.item())\n",
    "        list_loss_train.append(loss_train.item())\n",
    "        list_epochs.append(epoch)\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "        \n",
    "\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {} s'.format(int(round(time()) - round(t))),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "            \n",
    "            if epoch % 50 == 0:\n",
    "                model_path = \"outputs/{}-model-{}epochs-{}.pt\".format(today, epoch, run_number)\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "\n",
    "        \n",
    "        early_stopping(loss_val)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            halving_lr += 1\n",
    "            if halving_lr > 4:\n",
    "                break\n",
    "            learning_rate = learning_rate/10\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            early_stopping = EarlyStopping(model, patience=patience, delta=0.01, path='checkpoint.pt')\n",
    "            print('Deviding the learning rate by 10. New learning rate: {}'.format(learning_rate))\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model, list_loss_val, list_loss_train, list_epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJqiI7fxbVhl"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZqvqcdPg4X6",
    "outputId": "8e844bbe-4744-4a1d-d25c-8788bd906535"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the data for training...\n",
      "Data converted into torch tensors and authors added to indices in 0 min\n"
     ]
    }
   ],
   "source": [
    "features_torch, adj_torch, auth_torch, indices_torch, y_torch, val_indices_torch, y_val_torch = prepare_data_to_train(walks_wv, authors, adj, auth_matrix, indices, val_indices, y_val)\n",
    "#max_abstract_embedding_torch = torch.FloatTensor(max_abstract_embedding_array).to(device)\n",
    "#abstracts_bart_embeddings = abstracts_bart_embeddings.to(device)\n",
    "#goog300_abstracts_torch = torch.FloatTensor(goog300_abstracts).to(device)\n",
    "#local_wv300_abstracts_torch = torch.FloatTensor(local_wv300_abstracts).to(device)\n",
    "tfidf_matrix_torch = sparse_mx_to_torch_sparse_tensor(tfidf_matrix).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47r-jwjb4jl4"
   },
   "outputs": [],
   "source": [
    "auth_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "7i8UpGHuNhQp"
   },
   "outputs": [],
   "source": [
    "n_hidden = 64\n",
    "dropout_rate = 0.2\n",
    "sub_class = 32\n",
    "n_class = 2\n",
    "text_embedding = tfidf_matrix_torch\n",
    "n_text = text_embedding.shape[1]\n",
    "n_auth = auth_torch.shape[1] \n",
    "n_features = features_torch.shape[1]\n",
    "\n",
    "\n",
    "# Creates the model\n",
    "model = GNN(n_text, n_auth, n_features, n_hidden, n_class, sub_class, dropout_rate).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "p5llTPM64kQj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([138499, 147481]),\n",
       " torch.Size([138499, 64]),\n",
       " torch.Size([138499, 138499]),\n",
       " torch.Size([2, 1965666]),\n",
       " torch.Size([3931332]),\n",
       " torch.Size([2, 218244]),\n",
       " torch.Size([218244]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auth_torch.shape, features_torch.shape, adj_torch.shape, indices_torch.shape, y_torch.shape, val_indices_torch.shape, y_val_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tiyrR1an0zNx",
    "outputId": "f0043171-1805-4b3e-f281-1014860a1187"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the optimizer with learning rate: 0.01\n",
      "Start training...\n",
      "Epoch: 001 loss_train: 0.6963 loss_val: 0.6712 acc_train: 0.5000 acc_val: 0.5000 time: 24 s total_time: 0 min\n",
      "Epoch: 006 loss_train: 0.6655 loss_val: 0.6443 acc_train: 0.6429 acc_val: 0.7426 time: 23 s total_time: 2 min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m run_number \u001b[38;5;241m=\u001b[39m randint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m      6\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m----> 9\u001b[0m trained_model, list_loss_val, list_loss_train, list_epochs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mauth_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                            \u001b[49m\u001b[43my_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_indices_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_number\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[87], line 29\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, learning_rate, abstract, auth, features, adj, indices, y, val_indices, y_val, epochs, run_number, window)\u001b[0m\n\u001b[1;32m     25\u001b[0m pairs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((indices, rand_indices), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 29\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mabstract\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# we run the model that gives the output.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output, y) \u001b[38;5;66;03m# we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m acc_train \u001b[38;5;241m=\u001b[39m accuracy_score(torch\u001b[38;5;241m.\u001b[39margmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), y\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\u001b[38;5;66;03m# just to show it in the out put message of the training\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[61], line 51\u001b[0m, in \u001b[0;36mGNN.forward\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#x = torch.cat((x, pairs[2][:, None], pairs[3][:, None]), dim=1)        \u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m(pairs)\n\u001b[0;32m---> 51\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     54\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc5(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/activation.py:102\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "epochs = 400\n",
    "run_number = randint(0, 1000)\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "trained_model, list_loss_val, list_loss_train, list_epochs = train_model(model, learning_rate, text_embedding, \n",
    "                            auth_torch, features_torch, adj_torch, indices_torch, \n",
    "                            y_torch, val_indices_torch, y_val_torch, epochs, run_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idFEWIJO4k5L"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "y1 = pd.Series(list_loss_val[:400])\n",
    "y2 = pd.Series(list_loss_train[:400])\n",
    "x = pd.Series(list_epochs[:400])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(x, y1, label='Validation Loss')\n",
    "plt.plot(x, y2, label='Training Loss')\n",
    "\n",
    "plt.title('Loss over Epochs, G not shuffled')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oErtNBi1Ra66"
   },
   "outputs": [],
   "source": [
    "pairs_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0KKqG0rk1HM4"
   },
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDss4SBl0zNx"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "epochs = 602\n",
    "run_number = randint(0, 1000)\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "trained_model = train_model(model, learning_rate, text_embedding, \n",
    "                            features_torch, adj_torch, pairs_torch, \n",
    "                            y_torch, val_indices_torch, y_val_torch, epochs, run_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSvrKVnoLGfn"
   },
   "outputs": [],
   "source": [
    "# before shuffling\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "epochs = 602\n",
    "run_number = randint(0, 1000)\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "trained_model = train_model(model, learning_rate, text_embedding, \n",
    "                            features_torch, adj_torch, pairs_torch, \n",
    "                            y_torch, val_indices_torch, y_val_torch, epochs, run_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoZHWc0G0zNy"
   },
   "outputs": [],
   "source": [
    "np.shape(pairs_torch), y_torch.shape, val_indices_torch.shape, y_val_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uihFEZKFsjwH"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "epochs = 602\n",
    "run_number = randint(0, 1000)\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "trained_model = train_model(model, learning_rate, text_embedding, \n",
    "                            features_torch, adj_torch, pairs_torch, \n",
    "                            y_torch, val_indices_torch, y_val_torch, epochs, run_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lJ2Kxomh20h"
   },
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PAjwmZMg4X6"
   },
   "outputs": [],
   "source": [
    "# with dense2 1024 of BART\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "run_number = randint(0, 1000)\n",
    "learning_rate = 0.005\n",
    "\n",
    "\n",
    "trained_model = train_model(model, learning_rate, abstracts_bart_embeddings, features_torch, adj_torch, pairs_torch, y_torch, val_indices_torch, y_val_torch, epochs, run_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZPrUfJnFnzB"
   },
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDQ_DCCJavQZ"
   },
   "outputs": [],
   "source": [
    "# Without dense 1000\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "run_number = randint(0, 1000)\n",
    "\n",
    "\n",
    "trained_model = train_model(model, 0.01, tfidf_matrix_torch, features_torch, adj_torch, pairs_torch, y_torch, val_indices_torch, y_val_torch, epochs, run_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_XPsPMFbZji"
   },
   "source": [
    "# Generate test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmFO4qI7_245"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "test_path = 'https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/test.txt'\n",
    "node_pairs = list()\n",
    "f = urlopen(test_path)\n",
    "\n",
    "for line in f:\n",
    "    t = str(line).split(',')\n",
    "    t[0] = int(re.sub(\"[^0-9]\", \"\", t[0]))\n",
    "    t[1] = int(re.sub(\"[^0-9]\", \"\", t[1]))\n",
    "    node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "node_pairs = np.transpose(node_pairs)\n",
    "node_pairs = add_authors_to_pairs(node_pairs, authors)\n",
    "#node_pairs = torch.LongTensor(node_pairs).to(device)\n",
    "\n",
    "adj_torch = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "features_torch = torch.FloatTensor(walks_wv).to(device)\n",
    "\n",
    "test_output = model(features_torch, text_embedding, adj_torch, node_pairs)\n",
    "y_pred = torch.exp(test_output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "model_nb = 4\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns=['predicted']).to_csv(\n",
    "\"{}-submission-{}-{}.csv\".format(today, model_nb, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pD505btvfDG0"
   },
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bfa1O6lLg4X2"
   },
   "source": [
    "# BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Md1ARLd-g4X3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BartTokenizer, BartModel\n",
    "\n",
    "# Load the BART model and tokenizer\n",
    "model = BartModel.from_pretrained('facebook/bart-large')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ov9PcB9wg4X3"
   },
   "outputs": [],
   "source": [
    "abstracts_bart_embeddings = []\n",
    "\n",
    "# Define a function to generate embeddings for text\n",
    "def get_bart_embeddings(text, model):\n",
    "    # Tokenize the input text\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Generate embeddings using the BART model\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "        embeddings = model_output.last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_FBpRYsg4X3"
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(abstracts_bart_embeddings), len(voc.sentences_list))):\n",
    "    abstract = voc.sentences_list[i]\n",
    "    abstracts_bart_embeddings.append(get_bart_embeddings(abstract, model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8RxfG2Eg4X3"
   },
   "outputs": [],
   "source": [
    "saved_abstracts_bart_embeddings = abstracts_bart_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mi0TkxyFg4X3"
   },
   "outputs": [],
   "source": [
    "len(saved_abstracts_bart_embeddings), len(abstracts_bart_embeddings), len(voc.sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "411JkYvrg4X4"
   },
   "outputs": [],
   "source": [
    "abstracts_bart_embeddings = torch.stack(abstracts_bart_embeddings)\n",
    "abstracts_bart_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iLzw5akgg4X4"
   },
   "outputs": [],
   "source": [
    "abstracts_bart_embeddings.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTZC7PDEg4X4"
   },
   "outputs": [],
   "source": [
    "torch.save(abstracts_bart_embeddings, 'bart_embeddings.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6oNCPyubfSd"
   },
   "source": [
    "#Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2UWL4y-K1jd"
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "node_pairs = torch.LongTensor(node_pairs).to(device)\n",
    "\n",
    "test_output = model(features, adj, node_pairs)\n",
    "y_pred = torch.exp(test_output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"{}-submission-{}-{}.csv\".format(today, model_nb, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmUL7tGAR9Yt"
   },
   "outputs": [],
   "source": [
    "#### New script with batches\n",
    "\n",
    "def early_stopping(loss_train, list_loss_train, loss_val, list_loss_val, \n",
    "                   tolerance=0.01, patience=15):\n",
    "    list_loss_val = list(list_loss_val)[-patience:]\n",
    "    list_loss_train = list(list_loss_train)[-patience:]\n",
    "    if (len(list_loss_val) == patience and loss_val > (sum(list_loss_val)/len(list_loss_val)) and loss_train + tolerance < loss_val) or (len(list_loss_train) == patience and loss_train > (sum(list_loss_train)/len(list_loss_train))):\n",
    "        #print('train: {:.5f} val: {:.5f} mean val: {:.5f}'.format(loss_train, loss_val, (sum(list_loss_val)/len(list_loss_val))))\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "    \n",
    "def train_model(model, learning_rate, features, adj, indices_mc, y, val_indices, \n",
    "                y_val, epochs, batch_size, wv_walk_size, \n",
    "                tolerence = 0.01, patience = 15, run_number=randint(0, 1000)):\n",
    "    # Train model\n",
    "    start_time = time()\n",
    "    print('Initializing the optimizer with learning rate:', learning_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) #optimizer with halving learning rate in training\n",
    "    try: os.mkdir('./outputs')\n",
    "    except: pass\n",
    "    print('Preparing the data for training...')        \n",
    "\n",
    "    today = datetime.today().strftime('%Y-%m-%d-%H:%M')\n",
    "    list_loss_val = []\n",
    "    list_loss_train = []\n",
    "\n",
    "    \n",
    "    halving_lr = 0 # counter of the number of halving lr\n",
    "    print('Start training...')\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "\n",
    "        # we create the rand indices corresponding to non edges (their y = 0)\n",
    "        # we could apply a condition on epoch to run rand_indices (for speed purposes)\n",
    "        rand_indices = np.random.randint(0, len(indices_mc), size=(indices_mc.shape[0], indices_mc.shape[1]))\n",
    "        rand_indices = add_authors_to_pairs(rand_indices, authors)\n",
    "        pairs = np.concatenate((indices_mc, rand_indices), axis=1)\n",
    "        pairs = torch.LongTensor(pairs).to(device)\n",
    "\n",
    "        permutation = torch.randperm(pairs.size()[1])\n",
    "        \n",
    "        # batches\n",
    "        for i in range(0, pairs.size()[1], batch_size):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            elts_indices = permutation[i:i+batch_size]\n",
    "            batch_pairs = pairs[:, elts_indices]\n",
    "            batch_y = y[elts_indices]\n",
    "\n",
    "        \n",
    "            model.train()\n",
    "\n",
    "            output = model(features, adj, batch_pairs, wv_walk_size).to(device) # we run the model that gives the output.\n",
    "            loss_train = F.nll_loss(output, batch_y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "            acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), batch_y.cpu().numpy())# just to show it in the out put message of the training\n",
    "            loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "            optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, adj, val_indices, wv_walk_size).to(device)\n",
    "        #y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        list_loss_val.append(loss_val.item())\n",
    "        list_loss_train.append(loss_train.item())\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {} s'.format(int(round(time()) - round(t))),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "        if epoch % 50 == 0:\n",
    "            model_path = \"outputs/{}-model-{}epochs-{}.pt\".format(today, epoch, run_number)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "        if int(loss_val.item()) > 5:\n",
    "            break\n",
    "            \n",
    "        early = early_stopping(loss_train.item(), list_loss_train, loss_val.item(), list_loss_val, patience=15)        \n",
    "        if early:\n",
    "            halving_lr += 1\n",
    "            if halving_lr > 5:\n",
    "                break\n",
    "            list_loss_val=[]\n",
    "            learning_rate = learning_rate/10\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            print('Deviding the learning rate by 2. New learning rate: {:.6f}'.format(learning_rate))\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwNIJmkGn6-G"
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "trained_model = train_model(model, 0.01, features, authors, adj, indices, y, torch.tensor(val_indices).to(device), torch.tensor(y_val).to(device), epochs)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "dF0MJHXZVRpt",
    "Ubeigqung4X0",
    "zxvIuCP_bFrN",
    "ognzQbsFDb13"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "029723804c3b420bacce0e1ddd6bc87a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "059d6431e6f14c66b9fc272836660c56": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b776c922cd9412a8dfd8d49ae48bf43": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "121f5c36582c4275807e15cb4ba84f30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dea8d64c83754027b88f54ebc4ffd50c",
      "placeholder": "",
      "style": "IPY_MODEL_f428b21f4ed24ac49092742038b20606",
      "value": ""
     }
    },
    "15f1079aff04427d9eab2d7c97fb014e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "18700b2d971f42d5973f0573dbe397c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1888371c988242c48ba436784b533166": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f0f112951ca4057960fe2bc33b10acd",
      "max": 138499,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_029723804c3b420bacce0e1ddd6bc87a",
      "value": 138499
     }
    },
    "2b681147457b4f859a7d9274012a004a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c2b5fbf8e41412c949964f2b6cef0d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6da88af0c4414d758f684ff1258953bd",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_74cbbff36ac0492b937ad026df2803b1",
      "value": 1
     }
    },
    "2cb1d8e27a5346388abcebec1a1ece58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d524bbfc74a346f6954f15dad0eeedf1",
      "placeholder": "",
      "style": "IPY_MODEL_980cc9d1a24d459f901915403956e030",
      "value": ""
     }
    },
    "35ed948eaffd448eb483d863af5b492c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f0f112951ca4057960fe2bc33b10acd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5faac6ba5e604a3198ad9a5c49f1e2a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b26bf5844884b34b5cc3c2b8765f6d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6da88af0c4414d758f684ff1258953bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "74cbbff36ac0492b937ad026df2803b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7bd39cd6acfb4f21ace5dd23f1a82360": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "7c4a409918d24f71a2b24ea4bf9b1049": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_35ed948eaffd448eb483d863af5b492c",
      "placeholder": "",
      "style": "IPY_MODEL_2b681147457b4f859a7d9274012a004a",
      "value": " 147950/? [00:00&lt;00:00, 1337911.69it/s]"
     }
    },
    "88102ef32915417da9df4562ec493adb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8b37044a7d4a4da1adfd9f40b9e27303": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "980cc9d1a24d459f901915403956e030": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9b86291845ec4f5b83c8764ed836289e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2cb1d8e27a5346388abcebec1a1ece58",
       "IPY_MODEL_acaa2745e8fc468fadcd0fcc0ff9d324",
       "IPY_MODEL_e7460e2510704b458993133be5fc07bf"
      ],
      "layout": "IPY_MODEL_c5812332ac6849d1b6e0b28f40d98f5f"
     }
    },
    "aa38ef49eae14de1800c0d7fb216754b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_121f5c36582c4275807e15cb4ba84f30",
       "IPY_MODEL_2c2b5fbf8e41412c949964f2b6cef0d7",
       "IPY_MODEL_7c4a409918d24f71a2b24ea4bf9b1049"
      ],
      "layout": "IPY_MODEL_8b37044a7d4a4da1adfd9f40b9e27303"
     }
    },
    "acaa2745e8fc468fadcd0fcc0ff9d324": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7bd39cd6acfb4f21ace5dd23f1a82360",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6b26bf5844884b34b5cc3c2b8765f6d5",
      "value": 1
     }
    },
    "b6221c60dfdb4309b6d69db47f5621f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b768368f52cb4f3d9539caf48fc459c1",
       "IPY_MODEL_1888371c988242c48ba436784b533166",
       "IPY_MODEL_f9282ecb07174c38be1a3dc45ea4a5c2"
      ],
      "layout": "IPY_MODEL_5faac6ba5e604a3198ad9a5c49f1e2a7"
     }
    },
    "b768368f52cb4f3d9539caf48fc459c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b776c922cd9412a8dfd8d49ae48bf43",
      "placeholder": "",
      "style": "IPY_MODEL_15f1079aff04427d9eab2d7c97fb014e",
      "value": "100%"
     }
    },
    "c5812332ac6849d1b6e0b28f40d98f5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d524bbfc74a346f6954f15dad0eeedf1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dea8d64c83754027b88f54ebc4ffd50c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7460e2510704b458993133be5fc07bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_18700b2d971f42d5973f0573dbe397c3",
      "placeholder": "",
      "style": "IPY_MODEL_88102ef32915417da9df4562ec493adb",
      "value": " 138499/? [00:00&lt;00:00, 289440.89it/s]"
     }
    },
    "ef74a17c91af4def9f5082d345ba5b33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f428b21f4ed24ac49092742038b20606": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9282ecb07174c38be1a3dc45ea4a5c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_059d6431e6f14c66b9fc272836660c56",
      "placeholder": "",
      "style": "IPY_MODEL_ef74a17c91af4def9f5082d345ba5b33",
      "value": " 138499/138499 [00:00&lt;00:00, 1644505.90it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
