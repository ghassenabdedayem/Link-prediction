{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "46a7427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from random import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from random import choice\n",
    "from gensim.models import Word2Vec\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77e11dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import identity, diags\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d22582c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes of training set: 138499\n",
      "Number of edges of training set: 983233\n"
     ]
    }
   ],
   "source": [
    "\n",
    "G = nx.read_edgelist('edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "nodes = list(G.nodes())\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "edges = list(G.edges())\n",
    "\n",
    "val_edges = list()\n",
    "G_train = G\n",
    "\n",
    "for edge in edges:\n",
    "    if random() < 0.1:\n",
    "        val_edges.append(edge)\n",
    "\n",
    "# We remove the val edges from the graph G\n",
    "for edge in val_edges:\n",
    "    G_train.remove_edge(edge[0], edge[1])\n",
    "\n",
    "n = G_train.number_of_nodes()\n",
    "m = G_train.number_of_edges()\n",
    "train_edges = list(G_train.edges())\n",
    "    \n",
    "print('Number of nodes of training set:', n)\n",
    "print('Number of edges of training set:', m)\n",
    "\n",
    "y_val = [1]*len(val_edges)\n",
    "\n",
    "n_val_edges = len(val_edges)\n",
    "\n",
    "# Create random pairs of nodes\n",
    "for i in range(n_val_edges):\n",
    "    n1 = nodes[randint(0, n-1)]\n",
    "    n2 = nodes[randint(0, n-1)]\n",
    "    (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "    val_edges.append((n1, n2))\n",
    "    \n",
    "# Remove from val_edges edges that exist in both train and val\n",
    "\n",
    "for edge in list(set(val_edges) & set(train_edges)):\n",
    "    val_edges.remove(edge)\n",
    "    \n",
    "n_val_edges = len(val_edges) - len(y_val) #because we removed from val_edges edges that exist in both\n",
    "y_val.extend([0]*n_val_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "24fbbba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pd/glcnl2497w5b6xn3p94tnwlr0000gn/T/ipykernel_93135/3274917368.py:1: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138499 138499\n"
     ]
    }
   ],
   "source": [
    "adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix\n",
    "print(adj.shape[0], adj.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "669cdbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adjacency(A):\n",
    "    n = A.shape[0]\n",
    "    A = A + identity(n)\n",
    "    degs = A.dot(np.ones(n))\n",
    "    inv_degs = np.power(degs, -1)\n",
    "    D_inv = diags(inv_degs)\n",
    "    A_hat = D_inv.dot(A)\n",
    "    return A_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "189d36bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "# Is it equivalent to Word2vec for dimensionality reduction ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d4447804",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = normalize_adjacency(adj) # Normalizes the adjacency matrix\n",
    "indices = np.array(adj.nonzero())\n",
    "\n",
    "# Do we create the adjencency matrix based on the Training G ? And then we need to create one for the val G ?\n",
    "\n",
    "# Combine input and output to get indices matrix undirected for the undirected Graph.\n",
    "# You need to experiment with different hyperparameters and number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0e097cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_np = np.random.randn(G.number_of_edges(), 32) # Generates node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d6458fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class labels\n",
    "y = np.zeros(2*G.number_of_edges())\n",
    "y[:G.number_of_edges()] = 1\n",
    "\n",
    "# Transforms the numpy matrices/vectors to torch tensors\n",
    "features = torch.FloatTensor(features_np).to(device)\n",
    "y = torch.LongTensor(y).to(device)\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "indices = torch.LongTensor(indices).to(device)\n",
    "\n",
    "# What does it mean a Torch Vector ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f1089778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([138499, 138499])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56e0db3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_feat, n_hidden, n_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc4 = nn.Linear(n_hidden, n_class)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x_in, adj, pairs):\n",
    "        \n",
    "        h1 = self.fc1(x_in)\n",
    "        z1 = self.relu(torch.mm(adj, h1))\n",
    "        z1 = self.dropout(z1)\n",
    "\n",
    "        h2 = self.fc2(z1)\n",
    "        z2 = self.relu(torch.mm(adj, h2))\n",
    "        \n",
    "        x = z2[pairs[0,:],:] - z2[pairs[1,:],:]\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "14a08291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 25\n",
    "n_hidden = 128\n",
    "dropout_rate = 0.2\n",
    "n_class = 2\n",
    "\n",
    "# Creates the model and specifies the optimizer\n",
    "model = GNN(features.shape[1], n_hidden, n_class, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "daac32ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "addmm: Argument #3 (dense): Expected dim 0 size 138499, got 983233",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m rand_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, features\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), (indices\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m),indices\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)), device\u001b[38;5;241m=\u001b[39madj\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      9\u001b[0m pairs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((indices, rand_indices), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output, y)\n\u001b[1;32m     12\u001b[0m acc_train \u001b[38;5;241m=\u001b[39m accuracy_score(torch\u001b[38;5;241m.\u001b[39margmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), y\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m, in \u001b[0;36mGNN.forward\u001b[0;34m(self, x_in, adj, pairs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_in, adj, pairs):\n\u001b[1;32m     13\u001b[0m     h1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x_in)\n\u001b[0;32m---> 14\u001b[0m     z1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh1\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     15\u001b[0m     z1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(z1)\n\u001b[1;32m     17\u001b[0m     h2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(z1)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: addmm: Argument #3 (dense): Expected dim 0 size 138499, got 983233"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Train model\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    t = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)\n",
    "    pairs = torch.cat((indices, rand_indices), dim=1)\n",
    "    output = model(features, adj, pairs)\n",
    "    loss_train = F.nll_loss(output, y)\n",
    "    acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "14a12bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1091955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2183910])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(m)\n",
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d4a063d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499\n",
      "Number of edges: 1091955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pd/glcnl2497w5b6xn3p94tnwlr0000gn/T/ipykernel_93135/1139600007.py:21: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 0.6939 acc_train: 0.4978 time: 107.0968s\n",
      "Epoch: 006 loss_train: 0.5953 acc_train: 0.7321 time: 88.3711s\n",
      "Epoch: 011 loss_train: 0.5209 acc_train: 0.7463 time: 90.1575s\n",
      "Epoch: 016 loss_train: 0.4776 acc_train: 0.7748 time: 90.9185s\n",
      "Epoch: 021 loss_train: 0.4495 acc_train: 0.7881 time: 93.3329s\n",
      "Epoch: 026 loss_train: 0.4256 acc_train: 0.8027 time: 93.6235s\n",
      "Epoch: 031 loss_train: 0.4012 acc_train: 0.8164 time: 128.5414s\n",
      "Epoch: 036 loss_train: 0.3807 acc_train: 0.8289 time: 93.6701s\n",
      "Epoch: 041 loss_train: 0.3654 acc_train: 0.8370 time: 102.4795s\n",
      "Epoch: 046 loss_train: 0.3506 acc_train: 0.8451 time: 90.6169s\n",
      "Epoch: 051 loss_train: 0.3412 acc_train: 0.8488 time: 91.5261s\n",
      "Epoch: 056 loss_train: 0.3257 acc_train: 0.8582 time: 2066.7488s\n",
      "Epoch: 061 loss_train: 0.3268 acc_train: 0.8575 time: 91.7834s\n",
      "Epoch: 066 loss_train: 0.3109 acc_train: 0.8656 time: 87.3504s\n",
      "Epoch: 071 loss_train: 0.3055 acc_train: 0.8681 time: 90.2472s\n",
      "Epoch: 076 loss_train: 0.2988 acc_train: 0.8716 time: 103.3113s\n",
      "Epoch: 081 loss_train: 0.2944 acc_train: 0.8741 time: 89.3202s\n",
      "Epoch: 086 loss_train: 0.2972 acc_train: 0.8724 time: 84.7849s\n",
      "Epoch: 091 loss_train: 0.2902 acc_train: 0.8758 time: 85.4929s\n",
      "Epoch: 096 loss_train: 0.2834 acc_train: 0.8792 time: 84.4685s\n",
      "Epoch: 101 loss_train: 0.2762 acc_train: 0.8825 time: 79.8481s\n",
      "Epoch: 106 loss_train: 0.2805 acc_train: 0.8801 time: 113.0736s\n",
      "Epoch: 111 loss_train: 0.2706 acc_train: 0.8854 time: 94.3189s\n",
      "Epoch: 116 loss_train: 0.2705 acc_train: 0.8852 time: 88.4341s\n",
      "Epoch: 121 loss_train: 0.2736 acc_train: 0.8834 time: 83.2953s\n",
      "Epoch: 126 loss_train: 0.2691 acc_train: 0.8856 time: 93.6717s\n",
      "Epoch: 131 loss_train: 0.2626 acc_train: 0.8890 time: 93.3268s\n",
      "Epoch: 136 loss_train: 0.2601 acc_train: 0.8901 time: 84.8428s\n",
      "Epoch: 141 loss_train: 0.2663 acc_train: 0.8871 time: 88.3930s\n",
      "Epoch: 146 loss_train: 0.2603 acc_train: 0.8900 time: 106.7185s\n",
      "Epoch: 151 loss_train: 0.2553 acc_train: 0.8926 time: 85.9146s\n",
      "Epoch: 156 loss_train: 0.2545 acc_train: 0.8928 time: 97.6240s\n",
      "Epoch: 161 loss_train: 0.2498 acc_train: 0.8954 time: 81.8116s\n",
      "Epoch: 166 loss_train: 0.2457 acc_train: 0.8969 time: 86.7300s\n",
      "Epoch: 171 loss_train: 0.2474 acc_train: 0.8961 time: 84.9865s\n",
      "Epoch: 176 loss_train: 0.2452 acc_train: 0.8970 time: 82.2722s\n",
      "Epoch: 181 loss_train: 0.2414 acc_train: 0.8991 time: 83.2605s\n",
      "Epoch: 186 loss_train: 0.2484 acc_train: 0.8955 time: 81.5128s\n",
      "Epoch: 191 loss_train: 0.2392 acc_train: 0.9003 time: 84.2015s\n",
      "Epoch: 196 loss_train: 0.2400 acc_train: 0.8994 time: 87.0397s\n",
      "Epoch: 201 loss_train: 0.2384 acc_train: 0.9004 time: 83.4138s\n",
      "Epoch: 206 loss_train: 0.2387 acc_train: 0.9003 time: 79.4298s\n",
      "Epoch: 211 loss_train: 0.2351 acc_train: 0.9018 time: 79.9246s\n",
      "Epoch: 216 loss_train: 0.2373 acc_train: 0.9008 time: 73.6328s\n",
      "Epoch: 221 loss_train: 0.2329 acc_train: 0.9028 time: 81.0066s\n",
      "Epoch: 226 loss_train: 0.2322 acc_train: 0.9033 time: 76.4391s\n",
      "Epoch: 231 loss_train: 0.2555 acc_train: 0.8915 time: 84.0905s\n",
      "Epoch: 236 loss_train: 0.2476 acc_train: 0.8955 time: 76.1565s\n",
      "Epoch: 241 loss_train: 0.2326 acc_train: 0.9028 time: 75.7044s\n",
      "Epoch: 246 loss_train: 0.2295 acc_train: 0.9046 time: 76.6481s\n",
      "Epoch: 251 loss_train: 0.2300 acc_train: 0.9046 time: 83.1686s\n",
      "Epoch: 256 loss_train: 0.2282 acc_train: 0.9050 time: 73.9894s\n",
      "Epoch: 261 loss_train: 0.2313 acc_train: 0.9033 time: 74.4509s\n",
      "Epoch: 266 loss_train: 0.2271 acc_train: 0.9057 time: 75.3939s\n",
      "Epoch: 271 loss_train: 0.2255 acc_train: 0.9060 time: 76.7566s\n",
      "Epoch: 276 loss_train: 0.2351 acc_train: 0.9017 time: 75.7649s\n",
      "Epoch: 281 loss_train: 0.2350 acc_train: 0.9018 time: 80.7686s\n",
      "Epoch: 286 loss_train: 0.2296 acc_train: 0.9044 time: 78.6517s\n",
      "Epoch: 291 loss_train: 0.2272 acc_train: 0.9056 time: 90.1169s\n",
      "Epoch: 296 loss_train: 0.2254 acc_train: 0.9063 time: 76.9158s\n",
      "Epoch: 301 loss_train: 0.2228 acc_train: 0.9076 time: 75.4521s\n",
      "Epoch: 306 loss_train: 0.2206 acc_train: 0.9086 time: 75.2450s\n",
      "Epoch: 311 loss_train: 0.2245 acc_train: 0.9063 time: 74.9826s\n",
      "Epoch: 316 loss_train: 0.2184 acc_train: 0.9097 time: 76.3797s\n",
      "Epoch: 321 loss_train: 0.2222 acc_train: 0.9079 time: 75.5965s\n",
      "Epoch: 326 loss_train: 0.2230 acc_train: 0.9075 time: 87.3084s\n",
      "Epoch: 331 loss_train: 0.2182 acc_train: 0.9099 time: 77.6969s\n",
      "Epoch: 336 loss_train: 0.2170 acc_train: 0.9103 time: 79.9931s\n",
      "Epoch: 341 loss_train: 0.2220 acc_train: 0.9077 time: 75.3802s\n",
      "Epoch: 346 loss_train: 0.2169 acc_train: 0.9102 time: 77.1783s\n",
      "Epoch: 351 loss_train: 0.2162 acc_train: 0.9106 time: 77.7938s\n",
      "Epoch: 356 loss_train: 0.2162 acc_train: 0.9106 time: 80.8638s\n",
      "Epoch: 361 loss_train: 0.2147 acc_train: 0.9112 time: 77.8144s\n",
      "Epoch: 366 loss_train: 0.2135 acc_train: 0.9117 time: 73.6415s\n",
      "Epoch: 371 loss_train: 0.2133 acc_train: 0.9120 time: 80.6998s\n",
      "Epoch: 376 loss_train: 0.2128 acc_train: 0.9120 time: 75.6576s\n",
      "Epoch: 381 loss_train: 0.2133 acc_train: 0.9117 time: 74.6977s\n",
      "Epoch: 386 loss_train: 0.2142 acc_train: 0.9113 time: 79.7485s\n",
      "Epoch: 391 loss_train: 0.2221 acc_train: 0.9082 time: 75.5073s\n",
      "Epoch: 396 loss_train: 0.2232 acc_train: 0.9077 time: 73.0003s\n",
      "Epoch: 401 loss_train: 0.2174 acc_train: 0.9102 time: 74.5960s\n",
      "Epoch: 406 loss_train: 0.2117 acc_train: 0.9126 time: 76.2704s\n",
      "Epoch: 411 loss_train: 0.2097 acc_train: 0.9137 time: 73.1976s\n",
      "Epoch: 416 loss_train: 0.2098 acc_train: 0.9135 time: 104.3555s\n",
      "Epoch: 421 loss_train: 0.2073 acc_train: 0.9147 time: 73.5018s\n",
      "Epoch: 426 loss_train: 0.2077 acc_train: 0.9144 time: 78.2995s\n",
      "Epoch: 431 loss_train: 0.2211 acc_train: 0.9087 time: 73.3712s\n",
      "Epoch: 436 loss_train: 0.2134 acc_train: 0.9118 time: 75.8921s\n",
      "Epoch: 441 loss_train: 0.2129 acc_train: 0.9122 time: 78.4575s\n",
      "Epoch: 446 loss_train: 0.2080 acc_train: 0.9144 time: 74.0775s\n",
      "Epoch: 451 loss_train: 0.2082 acc_train: 0.9142 time: 73.2999s\n",
      "Epoch: 456 loss_train: 0.2059 acc_train: 0.9153 time: 77.3346s\n",
      "Epoch: 461 loss_train: 0.2040 acc_train: 0.9162 time: 85.5765s\n",
      "Epoch: 466 loss_train: 0.2055 acc_train: 0.9154 time: 77.9502s\n",
      "Epoch: 471 loss_train: 0.2038 acc_train: 0.9163 time: 77.1301s\n",
      "Epoch: 476 loss_train: 0.2065 acc_train: 0.9150 time: 75.4018s\n",
      "Epoch: 481 loss_train: 0.2040 acc_train: 0.9160 time: 90.4849s\n",
      "Epoch: 486 loss_train: 0.2034 acc_train: 0.9162 time: 75.9637s\n",
      "Epoch: 491 loss_train: 0.2103 acc_train: 0.9128 time: 98.5458s\n",
      "Epoch: 496 loss_train: 0.2047 acc_train: 0.9157 time: 75.0521s\n",
      "Epoch: 501 loss_train: 0.2030 acc_train: 0.9162 time: 77.9390s\n",
      "Epoch: 506 loss_train: 0.2014 acc_train: 0.9172 time: 80.5295s\n",
      "Epoch: 511 loss_train: 0.2019 acc_train: 0.9170 time: 78.1281s\n",
      "Epoch: 516 loss_train: 0.2022 acc_train: 0.9170 time: 76.3543s\n",
      "Epoch: 521 loss_train: 0.2010 acc_train: 0.9176 time: 74.9880s\n",
      "Epoch: 526 loss_train: 0.1997 acc_train: 0.9180 time: 73.3921s\n",
      "Epoch: 531 loss_train: 0.2013 acc_train: 0.9173 time: 77.5063s\n",
      "Epoch: 536 loss_train: 0.2080 acc_train: 0.9139 time: 70.4448s\n",
      "Epoch: 541 loss_train: 0.1988 acc_train: 0.9185 time: 73.6648s\n",
      "Epoch: 546 loss_train: 0.1990 acc_train: 0.9184 time: 71.7971s\n",
      "Epoch: 551 loss_train: 0.2015 acc_train: 0.9172 time: 72.0393s\n",
      "Epoch: 556 loss_train: 0.1972 acc_train: 0.9191 time: 77.0060s\n",
      "Epoch: 561 loss_train: 0.1995 acc_train: 0.9181 time: 73.5232s\n",
      "Epoch: 566 loss_train: 0.2008 acc_train: 0.9176 time: 76.4559s\n",
      "Epoch: 571 loss_train: 0.1976 acc_train: 0.9190 time: 74.4965s\n",
      "Epoch: 576 loss_train: 0.2052 acc_train: 0.9151 time: 82.5980s\n",
      "Epoch: 581 loss_train: 0.1995 acc_train: 0.9180 time: 76.4253s\n",
      "Epoch: 586 loss_train: 0.1975 acc_train: 0.9190 time: 74.3559s\n",
      "Epoch: 591 loss_train: 0.2015 acc_train: 0.9171 time: 74.6071s\n",
      "Epoch: 596 loss_train: 0.2001 acc_train: 0.9179 time: 72.0844s\n",
      "Epoch: 601 loss_train: 0.2024 acc_train: 0.9167 time: 73.7905s\n",
      "Epoch: 606 loss_train: 0.1990 acc_train: 0.9185 time: 74.5899s\n",
      "Epoch: 611 loss_train: 0.1976 acc_train: 0.9193 time: 73.9443s\n",
      "Epoch: 616 loss_train: 0.1955 acc_train: 0.9200 time: 73.8317s\n",
      "Epoch: 621 loss_train: 0.1946 acc_train: 0.9203 time: 81.3159s\n",
      "Epoch: 626 loss_train: 0.1946 acc_train: 0.9203 time: 71.6888s\n",
      "Epoch: 631 loss_train: 0.1936 acc_train: 0.9209 time: 76.0126s\n",
      "Epoch: 636 loss_train: 0.1946 acc_train: 0.9203 time: 80.0277s\n",
      "Epoch: 641 loss_train: 0.1979 acc_train: 0.9191 time: 84.3726s\n",
      "Epoch: 646 loss_train: 0.1933 acc_train: 0.9208 time: 75.0495s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 651 loss_train: 0.2015 acc_train: 0.9168 time: 76.8172s\n",
      "Epoch: 656 loss_train: 0.2009 acc_train: 0.9171 time: 76.9294s\n",
      "Epoch: 661 loss_train: 0.2009 acc_train: 0.9170 time: 90.9247s\n",
      "Epoch: 666 loss_train: 0.1974 acc_train: 0.9189 time: 80.3145s\n",
      "Epoch: 671 loss_train: 0.1954 acc_train: 0.9199 time: 76.5271s\n",
      "Epoch: 676 loss_train: 0.1936 acc_train: 0.9206 time: 77.5281s\n",
      "Epoch: 681 loss_train: 0.1920 acc_train: 0.9215 time: 71.5695s\n",
      "Epoch: 686 loss_train: 0.1905 acc_train: 0.9221 time: 81.2636s\n",
      "Epoch: 691 loss_train: 0.1920 acc_train: 0.9215 time: 73.2045s\n",
      "Epoch: 696 loss_train: 0.1911 acc_train: 0.9220 time: 81.6198s\n",
      "Epoch: 701 loss_train: 0.1913 acc_train: 0.9217 time: 70.1465s\n",
      "Epoch: 706 loss_train: 0.1904 acc_train: 0.9221 time: 77.3040s\n",
      "Epoch: 711 loss_train: 0.1945 acc_train: 0.9205 time: 71.8196s\n",
      "Epoch: 716 loss_train: 0.1950 acc_train: 0.9203 time: 72.5725s\n",
      "Epoch: 721 loss_train: 0.1890 acc_train: 0.9230 time: 76.0245s\n",
      "Epoch: 726 loss_train: 0.1928 acc_train: 0.9210 time: 71.2883s\n",
      "Epoch: 731 loss_train: 0.1931 acc_train: 0.9209 time: 71.4307s\n",
      "Epoch: 736 loss_train: 0.1887 acc_train: 0.9230 time: 87.2588s\n",
      "Epoch: 741 loss_train: 0.1923 acc_train: 0.9214 time: 72.8833s\n",
      "Epoch: 746 loss_train: 0.1889 acc_train: 0.9228 time: 74.0289s\n",
      "Epoch: 751 loss_train: 0.1885 acc_train: 0.9230 time: 75.7774s\n",
      "Epoch: 756 loss_train: 0.1929 acc_train: 0.9212 time: 74.1283s\n",
      "Epoch: 761 loss_train: 0.1920 acc_train: 0.9214 time: 74.3232s\n",
      "Epoch: 766 loss_train: 0.1937 acc_train: 0.9204 time: 75.4868s\n",
      "Epoch: 771 loss_train: 0.1923 acc_train: 0.9212 time: 71.8730s\n",
      "Epoch: 776 loss_train: 0.1885 acc_train: 0.9232 time: 76.3094s\n",
      "Epoch: 781 loss_train: 0.1880 acc_train: 0.9233 time: 91.1107s\n",
      "Epoch: 786 loss_train: 0.1976 acc_train: 0.9193 time: 75.9115s\n",
      "Epoch: 791 loss_train: 0.1925 acc_train: 0.9216 time: 81.6471s\n",
      "Epoch: 796 loss_train: 0.1907 acc_train: 0.9222 time: 76.6261s\n",
      "Epoch: 801 loss_train: 0.1887 acc_train: 0.9230 time: 83.4262s\n",
      "Epoch: 806 loss_train: 0.1872 acc_train: 0.9237 time: 73.6804s\n",
      "Epoch: 811 loss_train: 0.1914 acc_train: 0.9217 time: 77.4059s\n",
      "Epoch: 816 loss_train: 0.1897 acc_train: 0.9225 time: 73.8305s\n",
      "Epoch: 821 loss_train: 0.1909 acc_train: 0.9223 time: 76.2124s\n",
      "Epoch: 826 loss_train: 0.1890 acc_train: 0.9231 time: 75.2922s\n",
      "Epoch: 831 loss_train: 0.1869 acc_train: 0.9241 time: 77.2442s\n",
      "Epoch: 836 loss_train: 0.1863 acc_train: 0.9240 time: 74.0373s\n",
      "Epoch: 841 loss_train: 0.1855 acc_train: 0.9245 time: 73.6613s\n",
      "Epoch: 846 loss_train: 0.1851 acc_train: 0.9246 time: 67.4072s\n",
      "Epoch: 851 loss_train: 0.1850 acc_train: 0.9246 time: 70.4389s\n",
      "Epoch: 856 loss_train: 0.1853 acc_train: 0.9244 time: 76.1298s\n",
      "Epoch: 861 loss_train: 0.1842 acc_train: 0.9249 time: 82.3449s\n",
      "Epoch: 866 loss_train: 0.1859 acc_train: 0.9243 time: 69.6627s\n",
      "Epoch: 871 loss_train: 0.1917 acc_train: 0.9212 time: 70.9849s\n",
      "Epoch: 876 loss_train: 0.1854 acc_train: 0.9244 time: 70.8111s\n",
      "Epoch: 881 loss_train: 0.1844 acc_train: 0.9250 time: 73.2673s\n",
      "Epoch: 886 loss_train: 0.1886 acc_train: 0.9231 time: 72.3573s\n",
      "Epoch: 891 loss_train: 0.1924 acc_train: 0.9215 time: 72.2209s\n",
      "Epoch: 896 loss_train: 0.1852 acc_train: 0.9248 time: 69.5139s\n",
      "Epoch: 901 loss_train: 0.1873 acc_train: 0.9237 time: 74.9533s\n",
      "Epoch: 906 loss_train: 0.1860 acc_train: 0.9241 time: 73.3690s\n",
      "Epoch: 911 loss_train: 0.1832 acc_train: 0.9254 time: 72.0541s\n",
      "Epoch: 916 loss_train: 0.1831 acc_train: 0.9257 time: 71.0572s\n",
      "Epoch: 921 loss_train: 0.1829 acc_train: 0.9256 time: 70.1421s\n",
      "Epoch: 926 loss_train: 0.1822 acc_train: 0.9258 time: 70.0124s\n",
      "Epoch: 931 loss_train: 0.1829 acc_train: 0.9255 time: 74.3014s\n",
      "Epoch: 936 loss_train: 0.1825 acc_train: 0.9256 time: 79.4730s\n",
      "Epoch: 941 loss_train: 0.1819 acc_train: 0.9259 time: 75.9976s\n",
      "Epoch: 946 loss_train: 0.1828 acc_train: 0.9257 time: 72.4525s\n",
      "Epoch: 951 loss_train: 0.1924 acc_train: 0.9209 time: 72.9937s\n",
      "Epoch: 956 loss_train: 0.1884 acc_train: 0.9229 time: 71.8254s\n",
      "Epoch: 961 loss_train: 0.1893 acc_train: 0.9223 time: 70.0442s\n",
      "Epoch: 966 loss_train: 0.1917 acc_train: 0.9214 time: 73.5569s\n",
      "Epoch: 971 loss_train: 0.1935 acc_train: 0.9203 time: 76.2399s\n",
      "Epoch: 976 loss_train: 0.1861 acc_train: 0.9240 time: 73.4500s\n",
      "Epoch: 981 loss_train: 0.1824 acc_train: 0.9257 time: 77.6091s\n",
      "Epoch: 986 loss_train: 0.1812 acc_train: 0.9262 time: 76.0651s\n",
      "Epoch: 991 loss_train: 0.1811 acc_train: 0.9265 time: 72.6298s\n",
      "Epoch: 996 loss_train: 0.1802 acc_train: 0.9267 time: 73.2409s\n",
      "Optimization Finished!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a non(?) directed graph\n",
    "G = nx.read_edgelist('edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "nodes = list(G.nodes())\n",
    "node_to_idx = dict()\n",
    "for i, node in enumerate(nodes):\n",
    "    node_to_idx[node] = i\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "print('Number of nodes:', n)\n",
    "print('Number of edges:', m)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 1000\n",
    "n_hidden = 256\n",
    "dropout_rate = 0.2\n",
    "\n",
    "n_class = 2\n",
    "n_nodes = G.number_of_nodes()\n",
    "adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix\n",
    "indices = np.array(adj.nonzero())\n",
    "adj = normalize_adjacency(adj) # Normalizes the adjacency matrix\n",
    "features_np = np.random.randn(n_nodes, 32) # Generates node features\n",
    "\n",
    "# Create class labels\n",
    "y = np.zeros(4*m)\n",
    "y[:2*m] = 1\n",
    "\n",
    "# Transforms the numpy matrices/vectors to torch tensors\n",
    "features = torch.FloatTensor(features_np).to(device)\n",
    "y = torch.LongTensor(y).to(device)\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "indices = torch.LongTensor(indices).to(device)\n",
    "\n",
    "# Creates the model and specifies the optimizer\n",
    "model = GNN(features.shape[1], n_hidden, n_class, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train model\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    t = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)\n",
    "    pairs = torch.cat((indices, rand_indices), dim=1)\n",
    "    output = model(features, adj, pairs)\n",
    "    loss_train = F.nll_loss(output, y)\n",
    "    acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print()\n",
    "\n",
    "# # Read test data. Each sample is a pair of nodes\n",
    "# node_pairs = list()\n",
    "# with open('test.txt', 'r') as f:\n",
    "#     for line in f:\n",
    "#         t = line.split(',')\n",
    "#         node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "# # Testing\n",
    "# model.eval()\n",
    "# node_pairs = np.array(np.transpose(node_pairs))\n",
    "# pairs = torch.LongTensor(node_pairs).to(device)\n",
    "# output = model(features, adj, pairs)\n",
    "# y_pred = torch.exp(output)\n",
    "# y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "# # Compute log loss\n",
    "# y_test = np.loadtxt('y_test.txt', delimiter=',')[:,1]\n",
    "# y_pred = y_pred[:,1]\n",
    "# y_pred[y_pred>0.9999] = 0.9999\n",
    "# y_pred[y_pred<0.0001] = 0.0001\n",
    "# print('Log loss:', log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "11a6cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "output = model(features, adj, pairs)\n",
    "y_pred = torch.exp(output)\n",
    "y_pred = y_pred.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0a67d76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "0bf905d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[172], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m y_pred_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcolumn_stack((\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_pred)), y_pred_true))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# y_pred_df = pd.DataFrame(y_pred)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df_pred \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_pred)), y_pred_true}, columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;66;03m#, columns={'id', 'predicted'}).astype({'id':'int'})\u001b[39;00m\n\u001b[1;32m      6\u001b[0m df_pred\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# y_pred_df.head()\n",
    "\n",
    "y_pred_array = np.column_stack((range(len(y_pred)), y_pred_true))\n",
    "# y_pred_df = pd.DataFrame(y_pred)\n",
    "df_pred = pd.DataFrame({range(len(y_pred)), y_pred_true}, columns={'id','predicted'})#, columns={'id', 'predicted'}).astype({'id':'int'})\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "bf68fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(y_pred_array, columns={'id', 'predicted'}).astype({'id':'int'}).head()\n",
    "\n",
    "# pd.DataFrame(y_pred_array, columns={'id', 'predicted'}).astype({'id':'int'}).to_csv(\n",
    "# \"submission.csv\", header=True, index=False\n",
    "# )\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"submission.csv\", header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8cbc4a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to a file\n",
    "predictions = zip(range(len(y_pred)), y_pred)\n",
    "with open(\"submission.csv\",\"w\") as pred:\n",
    "    csv_out = csv.writer(pred)\n",
    "    csv_out.writerow(['id','predicted'])\n",
    "    for row in y_pred_array:\n",
    "        csv_out.writerow(row) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf7aa39",
   "metadata": {},
   "source": [
    "CNN with Sigmoid to predict if they connected based on the abstract."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cad23c",
   "metadata": {},
   "source": [
    "A good way to aggregate the text is to calculate the average (mean)\n",
    "\n",
    "Another approach is to use directly a GNN. Take the abstract, compute the embeddng of the words. Take the mean of the node.\n",
    "Then you can take the features.\n",
    "\n",
    "For thr GN, we have only the \n",
    "\n",
    "pairs is a tensor, contains a pair of nodes that contains all the positive samples and some of the negative samples. y: half of them are equal to one, and half of them are connected. rand_indices are random pairs that are considered as not connected.\n",
    "\n",
    "As we have a non directed. We can take twice every edge (2*m instead of 2*m for y. Or we can take the edges only once.\n",
    "\n",
    "One vector for the abstract using the word2vec embedding or any other similar approach.\n",
    "\n",
    "Or we can directly use a CNN.We can take a CNN and feed pais of abstracts in the CNN, the CNN will produce one vector for the first abstract and one vector for the second. We can combine these two vectors.\n",
    "\n",
    "Then we can use an MLP to produce a vector, and then we can concatenate the two vectors from CNN and MLP.\n",
    "\n",
    "There is a pretrained word embedding (Google provided a pretrained embedding).\n",
    "\n",
    "Embedding of each word. Then the CNN will provide one vector for the abstract.\n",
    "\n",
    "Each abstract has a different number of words. the representation of the CNN will have a fixed size of the embedding vector.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
