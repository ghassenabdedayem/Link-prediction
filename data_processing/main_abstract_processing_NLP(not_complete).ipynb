{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2b81c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_data import read_and_clean_abstracts, read_train_val_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20bbfd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        self.word_occurrence = {}\n",
    "        self.word2node = {}\n",
    "        self.words_list = []\n",
    "        self.sentences_list = []\n",
    "        self.sentences_list_words = []\n",
    "        self.num_words = 0\n",
    "        self.num_sentences = 0\n",
    "        self.longest_sentence = 0\n",
    "\n",
    "    def add_word(self, word, node):\n",
    "        if word not in self.word2index:\n",
    "            # First entry of word into vocabulary\n",
    "            self.words_list.append(word)\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "            self.word_occurrence[word] = 1\n",
    "            self.word2node[word] = [node]\n",
    "        else:\n",
    "            # Word exists; increase word count\n",
    "            self.word2count[word] += 1\n",
    "            self.word_occurrence[word] += 1\n",
    "            if node not in self.word2node[word]:\n",
    "                self.word2node[word].append(node)\n",
    "            # self.num_words += 1\n",
    "            \n",
    "    def add_sentence(self, sentence, node):\n",
    "        sentence_len = 0\n",
    "        for word in sentence.split()[:-1]:\n",
    "            sentence_len += 1\n",
    "            self.add_word(word, node)\n",
    "        if sentence_len > self.longest_sentence:\n",
    "            # This is the longest sentence\n",
    "            self.longest_sentence = sentence_len\n",
    "        # Count the number of sentences\n",
    "        self.num_sentences += 1\n",
    "        self.sentences_list.append(sentence)\n",
    "        self.sentences_list_words.append(sentence.split()[:-1])\n",
    "\n",
    "    def to_word(self, index):\n",
    "        return self.index2word[index]\n",
    "\n",
    "    def to_index(self, word):\n",
    "        return self.word2index[word]\n",
    "\n",
    "    def words(self):\n",
    "        return self.words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1ef002c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499 number of edges: 1091955 in the Complete set\n",
      "Number of nodes: 138499 number of edges: 982293 in the Training set\n",
      "len(nodes) 138499\n",
      "Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects\n",
      "Loaded from edgelist.txt and with a training validation split ratio = 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be5463cc23846a797effcb5228b9b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'lemmatizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m _, _, _, _, nodes, _ \u001b[38;5;241m=\u001b[39m read_train_val_graph()\n\u001b[0;32m----> 2\u001b[0m abstracts \u001b[38;5;241m=\u001b[39m \u001b[43mread_and_clean_abstracts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Data/Polytechnique/5- Data Challenge/data_challenge_2022/read_data.py:88\u001b[0m, in \u001b[0;36mread_and_clean_abstracts\u001b[0;34m(nodes, sample_length, abstracts_path)\u001b[0m\n\u001b[1;32m     84\u001b[0m abstract \u001b[38;5;241m=\u001b[39m remove_stopwords(abstract)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m abstract\u001b[38;5;241m.\u001b[39msplit()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m#abstract = abstract.replace(word, stemmer.stem(word))\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     abstract \u001b[38;5;241m=\u001b[39m abstract\u001b[38;5;241m.\u001b[39mreplace(word, \u001b[43mlemmatizer\u001b[49m\u001b[38;5;241m.\u001b[39mlemmatize(lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word), pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m), pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m), pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m), pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m'\u001b[39m), pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     90\u001b[0m node \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[^0-9]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, node)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mint\u001b[39m(node):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lemmatizer' is not defined"
     ]
    }
   ],
   "source": [
    "_, _, _, _, nodes, _ = read_train_val_graph()\n",
    "abstracts = read_and_clean_abstracts(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f2d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "n = -1 #length of the sample to develop and test the pipeline (-1 or negative values to take all the dataset)\n",
    "\n",
    "#takes 4 minutes to process all the abstracts\n",
    "abstracts = read_and_clean_abstracts(nodes, sample_length=n)  #149s #194s\n",
    "abstracts_dict_list_words = {i: abstracts[i].split()[:-1] for i in nodes}\n",
    "abstracts_list_sentences = [list(item)[1][:-3] for item in abstracts.items()]\n",
    "\n",
    "#we create a vacabulary of words and sentences (abstracts)\n",
    "#we take only a sample of 3 abstracts (i=2) to explore the approach\n",
    "voc = Vocabulary('abstracts') \n",
    "for i in nodes:\n",
    "    voc.add_sentence(abstracts[i])\n",
    "\n",
    "print('Text cleaned and vocab built in {:.0f} min'.format((time()-t)/60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
