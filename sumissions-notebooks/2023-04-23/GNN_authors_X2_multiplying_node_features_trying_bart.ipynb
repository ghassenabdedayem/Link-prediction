{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dF0MJHXZVRpt"
   },
   "source": [
    "# Packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5GfuP1hlTmg",
    "outputId": "e90f5945-58c1-4cfe-84db-9ca6cc294c22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\r\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode\n",
    "\n",
    "import numpy as np\n",
    "from random import random\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from random import choice\n",
    "from scipy.sparse import identity, diags\n",
    "from unidecode import unidecode\n",
    "from urllib.request import urlopen\n",
    "import gzip\n",
    "import pickle\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BJSlSwejljfp"
   },
   "outputs": [],
   "source": [
    "def save_subgraph_in_file(nbr_nodes, source_path='../input_data/edgelist.txt', destination_path='../input_data/small_edgelist.txt'):\n",
    "    G = nx.read_edgelist(source_path, delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "    G = G.subgraph(range(nbr_nodes))\n",
    "    nx.write_edgelist(G, path=destination_path, delimiter=',')\n",
    "    print(G.number_of_nodes(), 'nodes,', G.number_of_edges(), 'edges Graph extracted from', source_path[source_path.rfind('/')+1:])\n",
    "    G = nx.read_edgelist(destination_path, delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "    print(G.number_of_nodes(), 'nodes,', G.number_of_edges(), 'edges Graph saved in', destination_path[destination_path.rfind('/')+1:])\n",
    "    print(max(G.nodes))\n",
    "    return\n",
    "\n",
    "\n",
    "def read_train_val_graph(path='../input_data/edgelist.txt', val_ratio=0.1):\n",
    "    #gets the data from the file on the distant server\n",
    "    G = nx.read_edgelist(urlopen('https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/edgelist.txt'), delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "    nodes = list(G.nodes())\n",
    "    n = G.number_of_nodes()\n",
    "    m = G.number_of_edges()\n",
    "    edges = list(G.edges())\n",
    "\n",
    "    print('Number of nodes:', n, 'number of edges:', m,'in the Complete set')\n",
    "\n",
    "    node_to_idx = dict()\n",
    "    for i, node in enumerate(nodes):\n",
    "        node_to_idx[node] = i\n",
    "\n",
    "    val_edges = list()\n",
    "    G_train = G.copy()\n",
    "\n",
    "    for edge in edges:\n",
    "        if random() < val_ratio and edge[0] < n and edge[1] < n:\n",
    "            val_edges.append(edge)\n",
    "            G_train.remove_edge(edge[0], edge[1]) # We remove the val edges from the graph G\n",
    "\n",
    "   \n",
    "    #for edge in val_edges:\n",
    "        \n",
    "\n",
    "    n = G_train.number_of_nodes()\n",
    "    m = G_train.number_of_edges()\n",
    "    train_edges = list(G_train.edges())\n",
    "\n",
    "    print('Number of nodes:', n, 'number of edges:', m, 'in the Training set')\n",
    "    print('len(nodes)', len(nodes))\n",
    "\n",
    "    y_val = [1]*len(val_edges)\n",
    "\n",
    "    n_val_edges = len(val_edges)\n",
    "    \n",
    "    print('Creating random val_edges...')\n",
    "    for i in range(n_val_edges):\n",
    "        n1 = nodes[randint(0, n-1)]\n",
    "        n2 = nodes[randint(0, n-1)]\n",
    "        (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "        while n2 >= n: #or (n1, n2) in train_edges:\n",
    "            if (n1, n2) in train_edges:\n",
    "                print((n1, n2), 'in train_edges:')\n",
    "            n1 = nodes[randint(0, n-1)]\n",
    "            n2 = nodes[randint(0, n-1)]\n",
    "            (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "        val_edges.append((n1, n2))\n",
    "\n",
    "    y_val.extend([0]*(n_val_edges))\n",
    "    \n",
    "    ### From Giannis /!\\\n",
    "    val_indices = np.zeros((2,len(val_edges)))\n",
    "    for i,edge in enumerate(val_edges):\n",
    "        val_indices[0,i] = node_to_idx[edge[0]]\n",
    "        val_indices[1,i] = node_to_idx[edge[1]]\n",
    "    \n",
    "    print('Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects')\n",
    "    print('Loaded from', path[path.rfind('/')+1:], 'and with a training validation split ratio =', val_ratio)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx\n",
    "\n",
    "def random_walk(G, node, walk_length):\n",
    "    walk = [node]\n",
    "  \n",
    "    for i in range(walk_length-1):\n",
    "        neibor_nodes = list(G.neighbors(walk[-1]))\n",
    "        if len(neibor_nodes) > 0:\n",
    "            next_node = choice(neibor_nodes)\n",
    "            walk.append(next_node)\n",
    "    walk = [node for node in walk] # in case the nodes are in string format, we don't need to cast into string, but if the nodes are in numeric or integer, we need this line to cast into string\n",
    "    return walk\n",
    "\n",
    "\n",
    "def generate_walks(G, num_walks, walk_length):\n",
    "  # Runs \"num_walks\" random walks from each node, and returns a list of all random walk\n",
    "    t = time()\n",
    "    print('Start generating walks....')\n",
    "    walks = list()  \n",
    "    for i in range(num_walks):\n",
    "        for node in G.nodes():\n",
    "            walk = random_walk(G, node, walk_length)\n",
    "            walks.append(walk)\n",
    "        #print('walks : ', walks)\n",
    "    print('Random walks generated in in {}s!'.format(round(time()-t)))\n",
    "    return walks\n",
    "\n",
    "def apply_word2vec_on_features(features, nodes, vector_size=128, window=5, min_count=0, sg=1, workers=8):\n",
    "    t = time()\n",
    "    print('Start applying Word2Vec...')\n",
    "    wv_model = Word2Vec(vector_size=vector_size, window=window, min_count=min_count, sg=sg, workers=workers)\n",
    "    wv_model.build_vocab(features)\n",
    "    wv_model.train(features, total_examples=wv_model.corpus_count, epochs=5) \n",
    "    print('Word2vec model trained on features in {} min!'.format(round((time()-t)/60)))\n",
    "    features_np = []\n",
    "    for node in nodes:\n",
    "        features_np.append(wv_model.wv[node])\n",
    "\n",
    "    features_np = np.array(features_np)\n",
    "    print(features_np.shape, 'features numpy array created in {} min!'.format(round((time()-t)/60)))\n",
    "    return features_np\n",
    "\n",
    "\n",
    "\n",
    "def normalize_adjacency(A):\n",
    "    n = A.shape[0]\n",
    "    A = A + identity(n)\n",
    "    degs = A.dot(np.ones(n))\n",
    "    inv_degs = np.power(degs, -1)\n",
    "    D_inv = diags(inv_degs)\n",
    "    A_hat = D_inv.dot(A)\n",
    "    return A_hat\n",
    "\n",
    "def create_and_normalize_adjacency(G):\n",
    "    adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix of the training graph\n",
    "    adj = normalize_adjacency(adj)\n",
    "    print('Created a normalized adjancency matrix of shape', adj.shape)\n",
    "    indices = np.array(adj.nonzero()) # Gets the positions of non zeros of adj into indices\n",
    "    print('Created indices', indices.shape, 'with the positions of non zeros in adj matrix')\n",
    "    return adj, indices\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "def text_to_list(text):\n",
    "    return unidecode(text).split(',')\n",
    "\n",
    "def intersection(lst1, lst2): # a function that returns the number of common items of two lists and 1 or 0 if there are common. This function will be used in add_authors_to_pairs to add this features to the pairs.\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    is_common = 1 if len(lst3)>0 else 0\n",
    "    return len(lst3)+1, is_common+1\n",
    "\n",
    "\n",
    "def add_authors_to_pairs (pairs, authors):\n",
    "    authors = pd.DataFrame(authors)\n",
    "    try: \n",
    "        pairs = pairs.detach().cpu().numpy()\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "\n",
    "    pairs_df = pd.DataFrame(np.transpose(pairs)).rename(columns={0: \"paper_1\", 1: \"paper_2\"})\n",
    "    pairs_df = pairs_df.merge(authors, left_on='paper_1', right_on='paper_id', how='left').rename(columns={'authors': \"authors_1\"})\n",
    "    pairs_df = pairs_df.merge(authors, left_on='paper_2', right_on='paper_id', how='left').rename(columns={'authors': \"authors_2\"})\n",
    "    pairs_df.drop(['paper_id_x', 'paper_id_y'], axis=1, inplace=True)\n",
    "\n",
    "    pairs_df['nb_common_author'] = pairs_df.apply(lambda row: intersection(row['authors_1'], row['authors_2'])[0], axis=1)\n",
    "    pairs_df['is_common_author'] = pairs_df.apply(lambda row: intersection(row['authors_1'], row['authors_2'])[1], axis=1)\n",
    "\n",
    "    pairs_tensor = torch.LongTensor(np.transpose(pairs_df[[\"paper_1\", \"paper_2\", 'is_common_author', 'nb_common_author']].values.tolist())).to(device)\n",
    "    \n",
    "    return pairs_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_clean_abstracts (nodes, sample_length=-1, abstracts_path = 'https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/abstracts.txt'):\n",
    "    t = time()\n",
    "    abstracts = dict()\n",
    "    abstracts_list = list()\n",
    "    f = urlopen(abstracts_path)\n",
    "    \n",
    "    for i, line in tqdm(enumerate(f)):\n",
    "        if i == sample_length:\n",
    "            break\n",
    "        if i in nodes:\n",
    "            node, abstract = str(line).lower().split('|--|')\n",
    "            abstract = remove_stopwords(abstract)\n",
    "            #abstract = re.sub(r\"[,.;@#?!&$()-]\", \" \", abstract)\n",
    "            abstract = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", abstract)\n",
    "            #abstract = re.sub(r\"\\\\\", \" \", abstract)\n",
    "            abstract = remove_stopwords(abstract)\n",
    "\n",
    "            for word in abstract.split()[:-1]:\n",
    "                #abstract = abstract.replace(word, stemmer.stem(word))\n",
    "                abstract = abstract.replace(word, lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word), pos='s'), pos='a'), pos='n'), pos='v'), pos='r'))\n",
    "            \n",
    "            node = re.sub(\"[^0-9]\", \"\", node)\n",
    "            if i != int(node):\n",
    "                print('i and node not the same', i, node)\n",
    "            abstracts[int(node)] = abstract\n",
    "            abstracts_list.append(abstract)\n",
    "        \n",
    "    print('Text loaded and cleaned in {:.0f} min'.format((time()-t)/60))\n",
    "    return abstracts\n",
    "\n",
    "def doc_counter (documents, word): #a function that return the number of documents containing a word\n",
    "    counter = 0\n",
    "    for i in documents:\n",
    "        if word in documents[i]:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        self.word_occurrence = {}\n",
    "        self.word2node = {}\n",
    "        self.words_list = []\n",
    "        self.sentences_list = []\n",
    "        self.sentences_list_words = []\n",
    "        self.num_words = 0\n",
    "        self.num_sentences = 0\n",
    "        self.longest_sentence = 0\n",
    "\n",
    "    def add_word(self, word, node):\n",
    "        if word not in self.word2index:\n",
    "            # First entry of word into vocabulary\n",
    "            self.words_list.append(word)\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "            self.word_occurrence[word] = 1\n",
    "            self.word2node[word] = [node]\n",
    "        else:\n",
    "            # Word exists; increase word count\n",
    "            self.word2count[word] += 1\n",
    "            self.word_occurrence[word] += 1\n",
    "            if node not in self.word2node[word]:\n",
    "                self.word2node[word].append(node)\n",
    "            # self.num_words += 1\n",
    "            \n",
    "    def add_sentence(self, sentence, node):\n",
    "        sentence_len = 0\n",
    "        for word in sentence.split()[:-1]:\n",
    "            sentence_len += 1\n",
    "            self.add_word(word, node)\n",
    "        if sentence_len > self.longest_sentence:\n",
    "            # This is the longest sentence\n",
    "            self.longest_sentence = sentence_len\n",
    "        # Count the number of sentences\n",
    "        self.num_sentences += 1\n",
    "        self.sentences_list.append(sentence)\n",
    "        self.sentences_list_words.append(sentence.split()[:-1])\n",
    "\n",
    "    def to_word(self, index):\n",
    "        return self.index2word[index]\n",
    "\n",
    "    def to_index(self, word):\n",
    "        return self.word2index[word]\n",
    "\n",
    "    def words(self):\n",
    "        return self.words_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded vocab object from vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "# with open('vocab.pkl', 'wb') as f:\n",
    "#     pickle.load(voc, f)\n",
    "    \n",
    "if os.path.isfile('vocab.pkl'):\n",
    "    with open('vocab.pkl', 'rb') as f:\n",
    "        voc = pickle.load(f)\n",
    "    print('Successfully loaded vocab object from vocab.pkl')\n",
    "else:\n",
    "    print('File does not exist.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d82ab88cc134c2f8c31ec07e106285b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7249 11217 95 12\n"
     ]
    }
   ],
   "source": [
    "empty_abstracts = 0\n",
    "long_abstracts = []\n",
    "very_long_abstracts = []\n",
    "huge_abstracts = []\n",
    "for i, sentence in tqdm(enumerate(voc.sentences_list_words)):\n",
    "    if len(sentence) == 0:\n",
    "        empty_abstracts += 1\n",
    "    if len(sentence) > 128:\n",
    "        long_abstracts.append(i)\n",
    "    if len(sentence) > 256:\n",
    "        very_long_abstracts.append(i)\n",
    "    if len(sentence) > 512:\n",
    "        huge_abstracts.append(i)\n",
    "print(empty_abstracts, len(long_abstracts), len(very_long_abstracts), len(huge_abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11280217"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load graph and authors data from sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oe6DhotaSuqg",
    "outputId": "fcbc9c84-555d-4e99-f910-d6292ca149d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499 number of edges: 1091955 in the Complete set\n",
      "Number of nodes: 138499 number of edges: 982779 in the Training set\n",
      "len(nodes) 138499\n",
      "Creating random val_edges...\n",
      "Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects\n",
      "Loaded from edgelist.txt and with a training validation split ratio = 0.1\n"
     ]
    }
   ],
   "source": [
    "G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx = read_train_val_graph(val_ratio=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EyBibxd6zQhV",
    "outputId": "1223b134-85a1-45da-8006-eb530505c279"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pd/glcnl2497w5b6xn3p94tnwlr0000gn/T/ipykernel_71728/3479570820.py:129: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix of the training graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a normalized adjancency matrix of shape (138499, 138499)\n",
      "Created indices (2, 2104057) with the positions of non zeros in adj matrix\n"
     ]
    }
   ],
   "source": [
    "adj, indices = create_and_normalize_adjacency(G_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "dWAzHqmxmYff",
    "outputId": "67fca796-3365-4d46-90ed-cc98a401fa2a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[James H. Niblock, Jian-Xun Peng, Karen R. McM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Jian-Xun Peng, Kang Li, De-Shuang Huang]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[J. Heikkila]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[Long Zhang, Kang Li, Er-Wei Bai, George W. Ir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id                                            authors\n",
       "0         0  [James H. Niblock, Jian-Xun Peng, Karen R. McM...\n",
       "1         1          [Jian-Xun Peng, Kang Li, De-Shuang Huang]\n",
       "2         2                                      [J. Heikkila]\n",
       "3         3    [L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]\n",
       "4         4  [Long Zhang, Kang Li, Er-Wei Bai, George W. Ir..."
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors = pd.read_csv(urlopen('https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/authors.txt'), sep = '|', header=None)\n",
    "authors = authors.rename(columns={0: \"paper_id\", 2: \"authors\"})\n",
    "authors['authors'] = authors['authors'].apply(text_to_list)\n",
    "authors = authors[[\"paper_id\", \"authors\"]]\n",
    "authors = authors[authors['paper_id'] <= max(G.nodes())]\n",
    "authors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "138499it [06:41, 345.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text loaded and cleaned in 7 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 138499/138499 [28:52<00:00, 79.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab built in 29 min\n",
      "Vocab size is: 11280217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# n = -1 #length of the sample to develop and test the pipeline (-1 or negative values to take all the dataset)\n",
    "\n",
    "#takes 4 minutes to process all the abstracts\n",
    "abstracts = read_and_clean_abstracts(nodes, sample_length=n)  #149s #194s\n",
    "abstracts_dict_list_words = {i: abstracts[i].split()[:-1] for i in nodes}\n",
    "abstracts_list_sentences = [list(item)[1][:-3] for item in abstracts.items()]\n",
    "\n",
    "#we create a vacabulary of words and sentences (abstracts)\n",
    "t = time()\n",
    "voc = Vocabulary('abstracts') \n",
    "for node in tqdm(nodes):\n",
    "    voc.add_sentence(abstracts[node], node)\n",
    "\n",
    "print('Vocab built in {:.0f} min'.format((time()-t)/60))\n",
    "print('Vocab size is:', voc.num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62907"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_multiple = {key:value for (key, value) in voc.word2node.items() if len(value) >= 2}\n",
    "\n",
    "len(words_multiple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf matrix generated in 7 sec\n",
      "tf-idf shape: (138499, 62907)\n"
     ]
    }
   ],
   "source": [
    "#Now we will compute a logarithmic tf-idf matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "t = time()\n",
    "\n",
    "# Create a TfidfVectorizer object with logarithmic tf\n",
    "# And as we are intrested in links between words, we will take only words that occured at least in two abstracts\n",
    "words_multiple = {key:value for (key, value) in voc.word2node.items() if len(value) >= 2}\n",
    "vectorizer = TfidfVectorizer(vocabulary=list(words_multiple.keys()), sublinear_tf=True)\n",
    "\n",
    "\n",
    "# Fit the vectorizer to the sentences and transform them into a TF-IDF matrix\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(abstracts_list_sentences)\n",
    "\n",
    "# Print the TF-IDF matrix\n",
    "print('tf-idf matrix generated in {:.0f} sec'.format(time()-t))\n",
    "print('tf-idf shape:', tfidf_matrix.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "sparse.save_npz(\"tfidf_matrix.npz\", tfidf_matrix)\n",
    "your_matrix_back = sparse.load_npz(\"tfidf_matrix.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138499, 62907)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = sparse.load_npz(\"tfidf_matrix.npz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-28 19:44:39.933220: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b00de290324175bd82cfb972e5447c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d965d2322e46d5912e712178c70060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971512120afb4d368e3ad3240f841bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7d5d9fd1fb47eda9708391510b836a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd44c4d79b6e4cb18d7cf5df3488b353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BartTokenizer, BartModel\n",
    "\n",
    "# Load the BART model and tokenizer\n",
    "model = BartModel.from_pretrained('facebook/bart-large')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_bart_embeddings = []\n",
    "\n",
    "# Define a function to generate embeddings for text\n",
    "def get_bart_embeddings(text, model):\n",
    "    # Tokenize the input text\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Generate embeddings using the BART model\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "        embeddings = model_output.last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4021fe08e2b49ddaf4bda8b7ee2714c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(range(len(abstracts_bart_embeddings), len(voc.sentences_list))):\n",
    "    abstract = voc.sentences_list[i]\n",
    "    abstracts_bart_embeddings.append(get_bart_embeddings(abstract, model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_abstracts_bart_embeddings = abstracts_bart_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138499, 138499, 138499)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(saved_abstracts_bart_embeddings), len(abstracts_bart_embeddings), len(voc.sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([138499, 1024])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts_bart_embeddings = torch.stack(abstracts_bart_embeddings)\n",
    "abstracts_bart_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9488e-01,  1.0046e+00, -5.2291e-01,  ..., -2.5941e-01,\n",
       "          5.3308e-02,  6.6539e-02],\n",
       "        [ 1.5396e-01,  1.2314e+00, -7.1842e-01,  ...,  6.7827e-02,\n",
       "         -6.3357e-02, -8.9790e-02],\n",
       "        [ 2.2133e-01,  9.1838e-01, -3.4919e-01,  ..., -7.9309e-02,\n",
       "          1.4963e-01,  6.1544e-01],\n",
       "        ...,\n",
       "        [ 1.6774e-01,  1.1953e+00, -1.5907e-01,  ...,  1.8041e-01,\n",
       "          5.2979e-01, -1.7008e-01],\n",
       "        [ 6.9840e-02,  7.6416e-01, -6.6232e-05,  ..., -1.8154e-01,\n",
       "          4.7777e-01, -3.7687e-01],\n",
       "        [ 2.7550e-04,  1.8085e+00, -7.2895e-01,  ...,  3.4084e-01,\n",
       "         -7.1908e-02,  3.1836e-01]])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts_bart_embeddings.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(abstracts_bart_embeddings, 'bart_embeddings.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n"
     ]
    }
   ],
   "source": [
    "bart_vocab = tokenizer.get_vocab()\n",
    "\n",
    "print(len(bart_vocab.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2czqslmiVfcQ"
   },
   "source": [
    "# Read processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf-_fYkF5xiO",
    "outputId": "fcb2a001-9964-48f4-f14c-5952227e0338"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wv walks loaded from GCP in 8 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(138499, 64)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time()\n",
    "from io import BytesIO\n",
    "\n",
    "walks_url = 'https://storage.googleapis.com/link_prediction_processed_data/walks_wv.npy'\n",
    "with urlopen(walks_url) as url:\n",
    "    data = url.read()\n",
    "\n",
    "# Create a seekable file-like object from the data\n",
    "fileobj = BytesIO(data)\n",
    "\n",
    "# Load the data from the file object\n",
    "walks_wv = np.load(fileobj)\n",
    "print('wv walks loaded from GCP in {:.0f} sec'.format(time()-t))\n",
    "walks_wv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sbhiufK3kLG7",
    "outputId": "e5fa6ef1-176c-468d-b2d1-8158f8058c47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded and decompressed in 5 min\n",
      "len(my_dict): 138499\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "# abstract_url = 'https://storage.googleapis.com/link_prediction_processed_data/embedded_abstracts_dict_192array.pkl.gz'\n",
    "\n",
    "# with urlopen(abstract_url) as response:\n",
    "#     compressed_data = response.read()\n",
    "\n",
    "with open('embedded_abstracts_dict_192array.pkl.gz', 'rb') as f:\n",
    "    compressed_data = f.read()\n",
    "\n",
    "\n",
    "# Decompress the data\n",
    "words_embedding_192 = pickle.loads(gzip.decompress(compressed_data))\n",
    "print('File loaded and decompressed in {:.0f} min'.format((time()-t)/60))\n",
    "print('len(my_dict):', len(words_embedding_192))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdcd836a97db4e64a49d9b3d12cec5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assume words_embedding_192 is the dictionary of arrays\n",
    "max_length = 1024\n",
    "embedding_size = 192\n",
    "\n",
    "# Create a matrix of zeros to hold the padded embeddings\n",
    "padded_embeddings = np.zeros((len(words_embedding_192), max_length, embedding_size))\n",
    "\n",
    "# Loop over each abstract and pad/truncate its embedding\n",
    "for i, embedding in tqdm(enumerate(words_embedding_192.values())):\n",
    "    length = min(len(embedding), max_length)\n",
    "    try: padded_embeddings[i, :length, :] = embedding[:length, :]\n",
    "    except: pass\n",
    "\n",
    "# Convert the matrix of padded embeddings to a PyTorch tensor\n",
    "tensor_embeddings = torch.from_numpy(padded_embeddings).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming dict of embedding abstracts words into a list of embedding abstracts\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "t = time()\n",
    "my_list = list(my_dict.values())\n",
    "tensor_list = [torch.tensor(arr) for arr in my_list]\n",
    "print('transformed the dict of lists into a list of tensors in {:.0f} min'.format((time()-t)/60))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9A1A7DsGR9Yp",
    "outputId": "abf99938-aba3-403a-e6a8-808ffce1c0ab"
   },
   "outputs": [],
   "source": [
    "# max and mean pooling of words of the abstracts\n",
    "\n",
    "t = time()\n",
    "\n",
    "# mean pooling of words\n",
    "mean_abstract_embedding = []\n",
    "for key in my_dict.keys():\n",
    "    if len(my_dict[key]) > 0:\n",
    "        mean_abstract_embedding.append(np.mean(my_dict[key], axis=0))\n",
    "    else:\n",
    "        mean_abstract_embedding.append(np.zeros(my_dict[0].shape[1]))\n",
    "mean_abstract_embedding = np.array(mean_abstract_embedding)\n",
    "\n",
    "# max pooling\n",
    "max_abstract_embedding = []\n",
    "for key in my_dict.keys():\n",
    "    if len(my_dict[key]) > 0:\n",
    "        max_abstract_embedding.append(np.max(my_dict[key], axis=0))\n",
    "    else:\n",
    "        max_abstract_embedding.append(np.zeros(my_dict[0].shape[1]))\n",
    "max_abstract_embedding = np.array(max_abstract_embedding)\n",
    "\n",
    "print('max and mean pooling performed in {:.0f} sec'.format((time()-t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxvIuCP_bFrN"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "id": "ROBXxQaZnfQ3"
   },
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_feat, n_hidden, n_class, sub_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        #self.dense = nn.Linear(vocab_size, 1000)\n",
    "        self.dense2 = nn.Linear(1024, n_feat)\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.double_fc3 = nn.Linear((3*n_hidden), n_hidden)\n",
    "        self.fc4 = nn.Linear(n_hidden, sub_class)\n",
    "        self.fc5 = nn.Linear(sub_class, n_class)        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x_in, bart_abstract, adj, pairs):\n",
    "        #print('tfidf_matrix', tfidf_matrix.shape)\n",
    "        \n",
    "        #print('h_abstr', h_abstr.shape)\n",
    "        #x_in = torch.cat((x_in, h_abstr), dim=1)\n",
    "        #print('x_in', x_in.shape)\n",
    "        \n",
    "              \n",
    "        h1 = self.fc1(x_in)\n",
    "        z1 = self.relu(torch.spmm(adj, h1)) # sparce matrix multiplication\n",
    "        z1 = self.dropout(z1)\n",
    "        del(x_in, h1)\n",
    "\n",
    "        h2 = self.fc2(z1)\n",
    "        z2 = self.relu(torch.spmm(adj, h2))\n",
    "        z2 = self.dropout(z2)\n",
    "        del(h2, z1, adj)\n",
    "\n",
    "        #third round of GNN is not used\n",
    "        #h3 = self.fc2(z2)\n",
    "        #z3 = self.relu(torch.spmm(adj, h3))\n",
    "        #z3 = self.dropout(z3)\n",
    "        #print('z2', z2.shape)\n",
    "        #del(h3, z3, adj)\n",
    "\n",
    "        x = z2[pairs[0]] * z2[pairs[1]] # embedded features (z2) of node 0 - embedded features of node 1\n",
    "        #print('x', x.shape)\n",
    "        x = pairs[3][:, None] * x\n",
    "        #print('x', x.shape)\n",
    "        #x1 = z2[pairs[0]]\n",
    "        #x2 = z2[pairs[1]]\n",
    "        \n",
    "        #x = torch.cat((x, x1, x2), dim=1)\n",
    "        #del(x1, x2)\n",
    "        \n",
    "        #h_abstr = self.dense(tfidf_matrix)\n",
    "        h_abstr = self.dense2(bart_abstract)\n",
    "        y = torch.cat((h_abstr[pairs[0]],h_abstr[pairs[1]]), dim=1)\n",
    "        \n",
    "        #print('h_abstr', h_abstr.shape)\n",
    "        x = torch.cat((x, y), dim=1)\n",
    "        del(h_abstr)\n",
    "        del(pairs)\n",
    "\n",
    "        x = self.relu(self.double_fc3(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_feat, n_hidden, n_class, sub_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc4 = nn.Linear(n_hidden, sub_class)\n",
    "        self.fc5 = nn.Linear(sub_class, n_class)        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x_in, adj, pairs):\n",
    "        \n",
    "              \n",
    "        h1 = self.fc1(x_in)\n",
    "        z1 = self.relu(torch.spmm(adj, h1)) # sparce matrix multiplication\n",
    "        z1 = self.dropout(z1)\n",
    "        del(x_in, h1)\n",
    "\n",
    "        h2 = self.fc2(z1)\n",
    "        z2 = self.relu(torch.spmm(adj, h2))\n",
    "        z2 = self.dropout(z2)\n",
    "        del(h2, z1, adj)\n",
    "\n",
    "        x = z2[pairs[0]] * z2[pairs[1]] # embedded features (z2) of node 0 - embedded features of node 1\n",
    "\n",
    "\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "id": "bnRNz5z4nh6x"
   },
   "outputs": [],
   "source": [
    "def prepare_data_to_train (features, authors, adj, indices, val_indices, y_val):\n",
    "    \n",
    "    print('Preparing the data for training...')\n",
    "    \n",
    "    t = time()\n",
    "    \n",
    "    y_val = torch.LongTensor(y_val).to(device)\n",
    "\n",
    "    # Create class labels\n",
    "    y = np.zeros(2*indices.shape[1])\n",
    "    y[:indices.shape[1]] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
    "    y = torch.LongTensor(y).to(device)\n",
    "    \n",
    "    features = torch.FloatTensor(features).to(device)\n",
    "    \n",
    "    indices = torch.LongTensor(indices).to(device)\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "    #tfidf_matrix = sparse_mx_to_torch_sparse_tensor(tfidf_matrix).to(device)\n",
    "    \n",
    "    # the function add_authors_to_pairs converts into torch tensors and sends to Device    \n",
    "    val_indices = add_authors_to_pairs(val_indices, authors) #we add the authors to val_pairs\n",
    "    indices = add_authors_to_pairs(indices, authors) #we add the authors to indices    \n",
    "    rand_indices = np.random.randint(0, features.shape[0], (indices.shape[0],indices.shape[1]))# We take random indices each time we run an epoch\n",
    "    rand_indices = add_authors_to_pairs(rand_indices, authors)\n",
    "\n",
    "    pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices. \n",
    "    del(authors, indices, rand_indices)\n",
    "    \n",
    "    print('Data converted into torch tensors and authors added to indices in {:.0f} min'.format((time()-t)/60))\n",
    "\n",
    "    return features, adj, pairs, y, val_indices, y_val\n",
    "\n",
    "\n",
    "    \n",
    "def early_stopping(loss_train, list_loss_train, loss_val, list_loss_val, window=10, tolerance=0.01):\n",
    "    if (len(list_loss_val) == window and loss_val > (sum(list_loss_val)/len(list_loss_val)) and loss_train + tolerance < loss_val) or (len(list_loss_train) == window and loss_train > (sum(list_loss_train)/len(list_loss_train))):\n",
    "        print('train: {:.5f} val: {:.5f} mean val: {:.5f}'.format(loss_train, loss_val, (sum(list_loss_val)/len(list_loss_val))))\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def early_stopping(list_loss_val, list_loss_train, tolerance=0.1):\n",
    "    if list_loss_val[-1] > list_loss_train[-1] + tolerance and len(list_loss_val) > 15:\n",
    "        return True\n",
    "    for element in list_loss_val[:-1]:\n",
    "        if list_loss_val[-1] < element:\n",
    "            return False\n",
    "    for element in list_loss_val[:-2]:\n",
    "        if list_loss_val[-2] < element:\n",
    "            return False\n",
    "    for element in list_loss_val[:-3]:\n",
    "        if list_loss_val[-3] < element:\n",
    "            return False\n",
    "    for element in list_loss_val[:-4]:\n",
    "        if list_loss_val[-4] < element:\n",
    "            return False\n",
    "    for element in list_loss_val[:-5]:\n",
    "        if list_loss_val[-5] < element:\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "def train_model(model, learning_rate, bart_abstract, features, adj, pairs, y, val_indices, y_val, epochs, run_number):\n",
    "    # Train model\n",
    "    start_time = time()\n",
    "    \n",
    "    print('Initializing the optimizer with learning rate:', learning_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) #optimizer with halving learning rate in training\n",
    "    try: os.mkdir('./outputs')\n",
    "    except: pass\n",
    "\n",
    "    today = datetime.today().strftime('%Y-%m-%d-%H:%M')\n",
    "    list_loss_val = []\n",
    "    list_loss_train = []\n",
    "    window = 20\n",
    "    \n",
    "    halving_lr = 0 # counter of the number of halving lr\n",
    "    print('Start training...')\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        output = model(features, bart_abstract, adj, pairs).to(device) # we run the model that gives the output.\n",
    "        loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "        acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "        loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "        optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, bart_abstract, adj, val_indices).to(device)\n",
    "        #y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        list_loss_val.append(loss_val.item())\n",
    "        list_loss_train.append(loss_train.item())\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {} s'.format(int(round(time()) - round(t))),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "            \n",
    "            if epoch % 50 == 0:\n",
    "                model_path = \"outputs/{}-model-{}epochs-{}.pt\".format(today, epoch, run_number)\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "            early = False\n",
    "            if epoch > 20:\n",
    "                early = early_stopping(list_loss_train[-window:], list_loss_val[-window:])        \n",
    "            if early:\n",
    "                halving_lr += 1\n",
    "                if halving_lr > 4:\n",
    "                    break\n",
    "                list_loss_val=[]\n",
    "                list_loss_train=[]\n",
    "                learning_rate = learning_rate/10\n",
    "                optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                print('Deviding the learning rate by 10. New learning rate: {}'.format(learning_rate))\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJqiI7fxbVhl"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "id": "7i8UpGHuNhQp"
   },
   "outputs": [],
   "source": [
    "n_hidden = 64\n",
    "dropout_rate = 0.2\n",
    "sub_class = 8\n",
    "n_class = 2\n",
    "features = walks_wv#max_abstract_embedding #walks_wv\n",
    "n_features = features.shape[1]\n",
    "vocab_size = tfidf_matrix.shape[1]\n",
    "\n",
    "# Creates the model\n",
    "model = GNN(n_features, n_hidden, n_class, sub_class, dropout_rate).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the data for training...\n",
      "Data converted into torch tensors and authors added to indices in 2 min\n"
     ]
    }
   ],
   "source": [
    "features_torch, adj_torch, pairs_torch, y_torch, val_indices_torch, y_val_torch = prepare_data_to_train(features, authors, adj, indices, val_indices, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the optimizer with learning rate: 0.01\n",
      "Start training...\n",
      "Epoch: 001 loss_train: 0.8050 loss_val: 0.7155 acc_train: 0.5016 acc_val: 0.5000 time: 21 s total_time: 0 min\n",
      "Epoch: 006 loss_train: 0.6976 loss_val: 0.6874 acc_train: 0.5187 acc_val: 0.5864 time: 17 s total_time: 2 min\n",
      "Epoch: 011 loss_train: 0.6887 loss_val: 0.6802 acc_train: 0.5332 acc_val: 0.6108 time: 28 s total_time: 4 min\n",
      "Epoch: 016 loss_train: 0.7082 loss_val: 0.7137 acc_train: 0.5156 acc_val: 0.5000 time: 24 s total_time: 6 min\n",
      "Epoch: 021 loss_train: 0.6749 loss_val: 0.6747 acc_train: 0.5676 acc_val: 0.5871 time: 32 s total_time: 8 min\n",
      "Epoch: 026 loss_train: 0.6438 loss_val: 0.6458 acc_train: 0.6162 acc_val: 0.6327 time: 25 s total_time: 10 min\n",
      "Epoch: 031 loss_train: 0.6211 loss_val: 0.6102 acc_train: 0.6453 acc_val: 0.6798 time: 23 s total_time: 11 min\n",
      "Epoch: 036 loss_train: 0.5965 loss_val: 0.5760 acc_train: 0.6703 acc_val: 0.7183 time: 21 s total_time: 13 min\n",
      "Epoch: 041 loss_train: 0.5608 loss_val: 0.5261 acc_train: 0.7054 acc_val: 0.7508 time: 22 s total_time: 15 min\n",
      "Epoch: 046 loss_train: 0.5273 loss_val: 0.5005 acc_train: 0.7399 acc_val: 0.7573 time: 21 s total_time: 17 min\n",
      "Epoch: 051 loss_train: 0.4939 loss_val: 0.4798 acc_train: 0.7569 acc_val: 0.7553 time: 21 s total_time: 18 min\n",
      "Epoch: 056 loss_train: 0.4780 loss_val: 0.4582 acc_train: 0.7674 acc_val: 0.7773 time: 23 s total_time: 20 min\n",
      "Epoch: 061 loss_train: 0.4627 loss_val: 0.4369 acc_train: 0.7741 acc_val: 0.7980 time: 20 s total_time: 22 min\n",
      "Epoch: 066 loss_train: 0.4412 loss_val: 0.4361 acc_train: 0.7947 acc_val: 0.8077 time: 17 s total_time: 23 min\n",
      "Epoch: 071 loss_train: 0.4268 loss_val: 0.4107 acc_train: 0.8052 acc_val: 0.8135 time: 18 s total_time: 25 min\n",
      "Epoch: 076 loss_train: 0.4215 loss_val: 0.3933 acc_train: 0.8074 acc_val: 0.8248 time: 19 s total_time: 27 min\n",
      "Epoch: 081 loss_train: 0.4026 loss_val: 0.3832 acc_train: 0.8219 acc_val: 0.8349 time: 19 s total_time: 28 min\n",
      "Epoch: 086 loss_train: 0.4010 loss_val: 0.3749 acc_train: 0.8264 acc_val: 0.8410 time: 18 s total_time: 30 min\n",
      "Epoch: 091 loss_train: 0.3856 loss_val: 0.3576 acc_train: 0.8335 acc_val: 0.8426 time: 18 s total_time: 32 min\n",
      "Epoch: 096 loss_train: 0.3800 loss_val: 0.3521 acc_train: 0.8377 acc_val: 0.8433 time: 19 s total_time: 33 min\n",
      "Epoch: 101 loss_train: 0.3714 loss_val: 0.3425 acc_train: 0.8424 acc_val: 0.8529 time: 18 s total_time: 35 min\n",
      "Epoch: 106 loss_train: 0.3640 loss_val: 0.3361 acc_train: 0.8464 acc_val: 0.8579 time: 22 s total_time: 36 min\n",
      "Epoch: 111 loss_train: 0.3571 loss_val: 0.3314 acc_train: 0.8501 acc_val: 0.8616 time: 18 s total_time: 38 min\n",
      "Epoch: 116 loss_train: 0.3616 loss_val: 0.3297 acc_train: 0.8491 acc_val: 0.8613 time: 16 s total_time: 39 min\n",
      "Epoch: 121 loss_train: 0.3557 loss_val: 0.3247 acc_train: 0.8515 acc_val: 0.8668 time: 19 s total_time: 41 min\n",
      "Epoch: 126 loss_train: 0.3438 loss_val: 0.3209 acc_train: 0.8576 acc_val: 0.8605 time: 20 s total_time: 43 min\n",
      "Epoch: 131 loss_train: 0.3423 loss_val: 0.3144 acc_train: 0.8586 acc_val: 0.8649 time: 18 s total_time: 44 min\n",
      "Epoch: 136 loss_train: 0.3362 loss_val: 0.3137 acc_train: 0.8620 acc_val: 0.8640 time: 18 s total_time: 46 min\n",
      "Epoch: 141 loss_train: 0.3336 loss_val: 0.3128 acc_train: 0.8636 acc_val: 0.8634 time: 19 s total_time: 48 min\n",
      "Epoch: 146 loss_train: 0.3326 loss_val: 0.3058 acc_train: 0.8642 acc_val: 0.8713 time: 17 s total_time: 49 min\n",
      "Epoch: 151 loss_train: 0.3271 loss_val: 0.3000 acc_train: 0.8678 acc_val: 0.8764 time: 17 s total_time: 51 min\n",
      "Epoch: 156 loss_train: 0.3182 loss_val: 0.2991 acc_train: 0.8717 acc_val: 0.8792 time: 25 s total_time: 52 min\n",
      "Epoch: 161 loss_train: 0.3198 loss_val: 0.2969 acc_train: 0.8691 acc_val: 0.8809 time: 17 s total_time: 54 min\n",
      "Epoch: 166 loss_train: 0.3167 loss_val: 0.2882 acc_train: 0.8723 acc_val: 0.8834 time: 19 s total_time: 56 min\n",
      "Epoch: 171 loss_train: 0.3105 loss_val: 0.2880 acc_train: 0.8748 acc_val: 0.8868 time: 17 s total_time: 57 min\n",
      "Epoch: 176 loss_train: 0.3084 loss_val: 0.2958 acc_train: 0.8763 acc_val: 0.8853 time: 18 s total_time: 59 min\n",
      "Epoch: 181 loss_train: 0.3038 loss_val: 0.2911 acc_train: 0.8786 acc_val: 0.8860 time: 21 s total_time: 60 min\n",
      "Epoch: 186 loss_train: 0.2995 loss_val: 0.2946 acc_train: 0.8808 acc_val: 0.8835 time: 20 s total_time: 62 min\n",
      "Epoch: 191 loss_train: 0.3038 loss_val: 0.2918 acc_train: 0.8789 acc_val: 0.8867 time: 22 s total_time: 64 min\n",
      "Epoch: 196 loss_train: 0.2972 loss_val: 0.2714 acc_train: 0.8823 acc_val: 0.8926 time: 22 s total_time: 65 min\n",
      "Optimization Finished in 67 min!\n"
     ]
    }
   ],
   "source": [
    "# with dense2 1024 of BART\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "run_number = randint(0, 1000)\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "trained_model = train_model(model, learning_rate, abstracts_bart_embeddings, features_torch, adj_torch, pairs_torch, y_torch, val_indices_torch, y_val_torch, epochs, run_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KDQ_DCCJavQZ",
    "outputId": "0c81af49-7f33-4b68-8b9c-d5e941aed2a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the optimizer with learning rate: 0.01\n",
      "Start training...\n",
      "Epoch: 001 loss_train: 0.7205 loss_val: 0.7085 acc_train: 0.5000 acc_val: 0.5000 time: 25 s total_time: 0 min\n",
      "Epoch: 006 loss_train: 0.6909 loss_val: 0.6988 acc_train: 0.5182 acc_val: 0.5000 time: 20 s total_time: 2 min\n",
      "Epoch: 011 loss_train: 0.6843 loss_val: 0.6764 acc_train: 0.5874 acc_val: 0.6041 time: 18 s total_time: 4 min\n",
      "Epoch: 016 loss_train: 0.6125 loss_val: 0.6026 acc_train: 0.6735 acc_val: 0.6763 time: 19 s total_time: 5 min\n",
      "Epoch: 021 loss_train: 0.5949 loss_val: 0.5804 acc_train: 0.6832 acc_val: 0.6994 time: 17 s total_time: 7 min\n",
      "Epoch: 026 loss_train: 0.5624 loss_val: 0.5546 acc_train: 0.7285 acc_val: 0.7277 time: 19 s total_time: 8 min\n",
      "Epoch: 031 loss_train: 0.5239 loss_val: 0.5169 acc_train: 0.7717 acc_val: 0.7671 time: 18 s total_time: 10 min\n",
      "Epoch: 036 loss_train: 0.4932 loss_val: 0.4884 acc_train: 0.7939 acc_val: 0.7946 time: 18 s total_time: 12 min\n",
      "Epoch: 041 loss_train: 0.4685 loss_val: 0.4661 acc_train: 0.8123 acc_val: 0.8098 time: 18 s total_time: 13 min\n",
      "Epoch: 046 loss_train: 0.4411 loss_val: 0.4269 acc_train: 0.8305 acc_val: 0.8417 time: 18 s total_time: 15 min\n",
      "Epoch: 051 loss_train: 0.4185 loss_val: 0.3995 acc_train: 0.8422 acc_val: 0.8544 time: 22 s total_time: 16 min\n",
      "Epoch: 056 loss_train: 0.3927 loss_val: 0.3730 acc_train: 0.8565 acc_val: 0.8672 time: 18 s total_time: 18 min\n",
      "Epoch: 061 loss_train: 0.3894 loss_val: 0.3344 acc_train: 0.8331 acc_val: 0.8738 time: 17 s total_time: 19 min\n",
      "Epoch: 066 loss_train: 0.3296 loss_val: 0.2941 acc_train: 0.8727 acc_val: 0.8806 time: 20 s total_time: 21 min\n",
      "Epoch: 071 loss_train: 0.3032 loss_val: 0.2753 acc_train: 0.8780 acc_val: 0.8870 time: 16 s total_time: 23 min\n",
      "Epoch: 076 loss_train: 0.2825 loss_val: 0.2613 acc_train: 0.8895 acc_val: 0.8934 time: 18 s total_time: 24 min\n",
      "Epoch: 081 loss_train: 0.2696 loss_val: 0.2530 acc_train: 0.8915 acc_val: 0.8973 time: 19 s total_time: 26 min\n",
      "Epoch: 086 loss_train: 0.2577 loss_val: 0.2446 acc_train: 0.9010 acc_val: 0.9010 time: 18 s total_time: 27 min\n",
      "Epoch: 091 loss_train: 0.2478 loss_val: 0.2377 acc_train: 0.9054 acc_val: 0.9042 time: 20 s total_time: 29 min\n",
      "Epoch: 096 loss_train: 0.2359 loss_val: 0.2338 acc_train: 0.9098 acc_val: 0.9060 time: 18 s total_time: 30 min\n",
      "Epoch: 101 loss_train: 0.2238 loss_val: 0.2287 acc_train: 0.9128 acc_val: 0.9091 time: 18 s total_time: 32 min\n",
      "Epoch: 106 loss_train: 0.2328 loss_val: 0.2493 acc_train: 0.9108 acc_val: 0.8991 time: 18 s total_time: 33 min\n",
      "train: 0.23276 val: 0.24931 mean val: 0.23705\n",
      "Deviding the learning rate by 10. New learning rate: 0.001\n",
      "Epoch: 111 loss_train: 0.2136 loss_val: 0.2263 acc_train: 0.9167 acc_val: 0.9104 time: 18 s total_time: 35 min\n",
      "Epoch: 116 loss_train: 0.2117 loss_val: 0.2253 acc_train: 0.9159 acc_val: 0.9109 time: 18 s total_time: 36 min\n",
      "Epoch: 121 loss_train: 0.2102 loss_val: 0.2252 acc_train: 0.9180 acc_val: 0.9111 time: 17 s total_time: 38 min\n",
      "Epoch: 126 loss_train: 0.2096 loss_val: 0.2250 acc_train: 0.9167 acc_val: 0.9109 time: 22 s total_time: 39 min\n",
      "Epoch: 131 loss_train: 0.2081 loss_val: 0.2241 acc_train: 0.9188 acc_val: 0.9113 time: 18 s total_time: 41 min\n",
      "Epoch: 136 loss_train: 0.2071 loss_val: 0.2239 acc_train: 0.9183 acc_val: 0.9114 time: 20 s total_time: 42 min\n",
      "Epoch: 141 loss_train: 0.2058 loss_val: 0.2230 acc_train: 0.9198 acc_val: 0.9117 time: 18 s total_time: 44 min\n",
      "Epoch: 146 loss_train: 0.2052 loss_val: 0.2227 acc_train: 0.9195 acc_val: 0.9119 time: 17 s total_time: 45 min\n",
      "Epoch: 151 loss_train: 0.2030 loss_val: 0.2225 acc_train: 0.9209 acc_val: 0.9120 time: 19 s total_time: 47 min\n",
      "Epoch: 156 loss_train: 0.2022 loss_val: 0.2220 acc_train: 0.9208 acc_val: 0.9122 time: 19 s total_time: 48 min\n",
      "Epoch: 161 loss_train: 0.2014 loss_val: 0.2219 acc_train: 0.9213 acc_val: 0.9126 time: 23 s total_time: 50 min\n",
      "Epoch: 166 loss_train: 0.2006 loss_val: 0.2210 acc_train: 0.9218 acc_val: 0.9128 time: 20 s total_time: 51 min\n",
      "Epoch: 171 loss_train: 0.1993 loss_val: 0.2207 acc_train: 0.9221 acc_val: 0.9132 time: 17 s total_time: 53 min\n",
      "Epoch: 176 loss_train: 0.1984 loss_val: 0.2201 acc_train: 0.9227 acc_val: 0.9134 time: 17 s total_time: 54 min\n",
      "Epoch: 181 loss_train: 0.1969 loss_val: 0.2197 acc_train: 0.9233 acc_val: 0.9136 time: 17 s total_time: 56 min\n",
      "Epoch: 186 loss_train: 0.1949 loss_val: 0.2196 acc_train: 0.9241 acc_val: 0.9138 time: 18 s total_time: 58 min\n",
      "Epoch: 191 loss_train: 0.1934 loss_val: 0.2189 acc_train: 0.9248 acc_val: 0.9141 time: 17 s total_time: 59 min\n",
      "Epoch: 196 loss_train: 0.1926 loss_val: 0.2183 acc_train: 0.9254 acc_val: 0.9143 time: 19 s total_time: 61 min\n",
      "Optimization Finished in 62 min!\n"
     ]
    }
   ],
   "source": [
    "# Without dense 1000\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "run_number = randint(0, 1000)\n",
    "\n",
    "\n",
    "trained_model = train_model(model, 0.01, tfidf_matrix_torch, features_torch, adj_torch, pairs_torch, y_torch, val_indices_torch, y_val_torch, epochs, run_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_XPsPMFbZji"
   },
   "source": [
    "# Generate test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmFO4qI7_245"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "test_path = 'https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/test.txt'\n",
    "node_pairs = list()\n",
    "f = urlopen(test_path)\n",
    "\n",
    "for line in f:\n",
    "    t = str(line).split(',')\n",
    "    t[0] = int(re.sub(\"[^0-9]\", \"\", t[0]))\n",
    "    t[1] = int(re.sub(\"[^0-9]\", \"\", t[1]))\n",
    "    node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "node_pairs = np.transpose(node_pairs)\n",
    "node_pairs = add_authors_to_pairs(node_pairs, authors)\n",
    "#node_pairs = torch.LongTensor(node_pairs).to(device)\n",
    "\n",
    "adj_torch = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "features_torch = torch.FloatTensor(features).to(device)\n",
    "\n",
    "test_output = model(features_torch, adj_torch, node_pairs)\n",
    "y_pred = torch.exp(test_output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "model_nb = 1\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns=['predicted']).to_csv(\n",
    "\"{}-submission-{}-{}.csv\".format(today, model_nb, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pD505btvfDG0",
    "outputId": "98276ce8-aeba-408c-c9e5-87e392f3d9cb"
   },
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6oNCPyubfSd"
   },
   "source": [
    "#Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2UWL4y-K1jd"
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "node_pairs = torch.LongTensor(node_pairs).to(device)\n",
    "\n",
    "test_output = model(features, adj, node_pairs)\n",
    "y_pred = torch.exp(test_output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"{}-submission-{}-{}.csv\".format(today, model_nb, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmUL7tGAR9Yt"
   },
   "outputs": [],
   "source": [
    "#### New script with batches\n",
    "\n",
    "def early_stopping(loss_train, list_loss_train, loss_val, list_loss_val, \n",
    "                   tolerance=0.01, patience=15):\n",
    "    list_loss_val = list(list_loss_val)[-patience:]\n",
    "    list_loss_train = list(list_loss_train)[-patience:]\n",
    "    if (len(list_loss_val) == patience and loss_val > (sum(list_loss_val)/len(list_loss_val)) and loss_train + tolerance < loss_val) or (len(list_loss_train) == patience and loss_train > (sum(list_loss_train)/len(list_loss_train))):\n",
    "        #print('train: {:.5f} val: {:.5f} mean val: {:.5f}'.format(loss_train, loss_val, (sum(list_loss_val)/len(list_loss_val))))\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "    \n",
    "def train_model(model, learning_rate, features, adj, indices_mc, y, val_indices, \n",
    "                y_val, epochs, batch_size, wv_walk_size, \n",
    "                tolerence = 0.01, patience = 15, run_number=randint(0, 1000)):\n",
    "    # Train model\n",
    "    start_time = time()\n",
    "    print('Initializing the optimizer with learning rate:', learning_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) #optimizer with halving learning rate in training\n",
    "    try: os.mkdir('./outputs')\n",
    "    except: pass\n",
    "    print('Preparing the data for training...')        \n",
    "\n",
    "    today = datetime.today().strftime('%Y-%m-%d-%H:%M')\n",
    "    list_loss_val = []\n",
    "    list_loss_train = []\n",
    "\n",
    "    \n",
    "    halving_lr = 0 # counter of the number of halving lr\n",
    "    print('Start training...')\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "\n",
    "        # we create the rand indices corresponding to non edges (their y = 0)\n",
    "        # we could apply a condition on epoch to run rand_indices (for speed purposes)\n",
    "        rand_indices = np.random.randint(0, len(indices_mc), size=(indices_mc.shape[0], indices_mc.shape[1]))\n",
    "        rand_indices = add_authors_to_pairs(rand_indices, authors)\n",
    "        pairs = np.concatenate((indices_mc, rand_indices), axis=1)\n",
    "        pairs = torch.LongTensor(pairs).to(device)\n",
    "\n",
    "        permutation = torch.randperm(pairs.size()[1])\n",
    "        \n",
    "        # batches\n",
    "        for i in range(0, pairs.size()[1], batch_size):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            elts_indices = permutation[i:i+batch_size]\n",
    "            batch_pairs = pairs[:, elts_indices]\n",
    "            batch_y = y[elts_indices]\n",
    "\n",
    "        \n",
    "            model.train()\n",
    "\n",
    "            output = model(features, adj, batch_pairs, wv_walk_size).to(device) # we run the model that gives the output.\n",
    "            loss_train = F.nll_loss(output, batch_y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "            acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), batch_y.cpu().numpy())# just to show it in the out put message of the training\n",
    "            loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "            optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, adj, val_indices, wv_walk_size).to(device)\n",
    "        #y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        list_loss_val.append(loss_val.item())\n",
    "        list_loss_train.append(loss_train.item())\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {} s'.format(int(round(time()) - round(t))),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "        if epoch % 50 == 0:\n",
    "            model_path = \"outputs/{}-model-{}epochs-{}.pt\".format(today, epoch, run_number)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "        if int(loss_val.item()) > 5:\n",
    "            break\n",
    "            \n",
    "        early = early_stopping(loss_train.item(), list_loss_train, loss_val.item(), list_loss_val, patience=15)        \n",
    "        if early:\n",
    "            halving_lr += 1\n",
    "            if halving_lr > 5:\n",
    "                break\n",
    "            list_loss_val=[]\n",
    "            learning_rate = learning_rate/10\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            print('Deviding the learning rate by 2. New learning rate: {:.6f}'.format(learning_rate))\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwNIJmkGn6-G"
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "trained_model = train_model(model, 0.01, features, authors, adj, indices, y, torch.tensor(val_indices).to(device), torch.tensor(y_val).to(device), epochs)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
