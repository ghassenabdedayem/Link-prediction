{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dF0MJHXZVRpt"
   },
   "source": [
    "# Packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5GfuP1hlTmg",
    "outputId": "e90f5945-58c1-4cfe-84db-9ca6cc294c22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\r\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode\n",
    "\n",
    "import numpy as np\n",
    "from random import random\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from random import choice\n",
    "from scipy.sparse import identity, diags\n",
    "from unidecode import unidecode\n",
    "from urllib.request import urlopen\n",
    "import gzip\n",
    "import pickle\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BJSlSwejljfp"
   },
   "outputs": [],
   "source": [
    "def save_subgraph_in_file(nbr_nodes, source_path='../input_data/edgelist.txt', destination_path='../input_data/small_edgelist.txt'):\n",
    "    G = nx.read_edgelist(source_path, delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "    G = G.subgraph(range(nbr_nodes))\n",
    "    nx.write_edgelist(G, path=destination_path, delimiter=',')\n",
    "    print(G.number_of_nodes(), 'nodes,', G.number_of_edges(), 'edges Graph extracted from', source_path[source_path.rfind('/')+1:])\n",
    "    G = nx.read_edgelist(destination_path, delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "    print(G.number_of_nodes(), 'nodes,', G.number_of_edges(), 'edges Graph saved in', destination_path[destination_path.rfind('/')+1:])\n",
    "    print(max(G.nodes))\n",
    "    return\n",
    "\n",
    "\n",
    "def read_train_val_graph(path='../input_data/edgelist.txt', val_ratio=0.1):\n",
    "    #gets the data from the file on the distant server\n",
    "    G = nx.read_edgelist(urlopen('https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/edgelist.txt'), delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "    nodes = list(G.nodes())\n",
    "    n = G.number_of_nodes()\n",
    "    m = G.number_of_edges()\n",
    "    edges = list(G.edges())\n",
    "\n",
    "    print('Number of nodes:', n, 'number of edges:', m,'in the Complete set')\n",
    "\n",
    "    node_to_idx = dict()\n",
    "    for i, node in enumerate(nodes):\n",
    "        node_to_idx[node] = i\n",
    "\n",
    "    val_edges = list()\n",
    "    G_train = G.copy()\n",
    "\n",
    "    for edge in edges:\n",
    "        if random() < val_ratio and edge[0] < n and edge[1] < n:\n",
    "            val_edges.append(edge)\n",
    "            G_train.remove_edge(edge[0], edge[1]) # We remove the val edges from the graph G\n",
    "\n",
    "   \n",
    "    #for edge in val_edges:\n",
    "        \n",
    "\n",
    "    n = G_train.number_of_nodes()\n",
    "    m = G_train.number_of_edges()\n",
    "    train_edges = list(G_train.edges())\n",
    "\n",
    "    print('Number of nodes:', n, 'number of edges:', m, 'in the Training set')\n",
    "    print('len(nodes)', len(nodes))\n",
    "\n",
    "    y_val = [1]*len(val_edges)\n",
    "\n",
    "    n_val_edges = len(val_edges)\n",
    "    \n",
    "    print('Creating random val_edges...')\n",
    "    for i in range(n_val_edges):\n",
    "        n1 = nodes[randint(0, n-1)]\n",
    "        n2 = nodes[randint(0, n-1)]\n",
    "        (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "        while n2 >= n: #or (n1, n2) in train_edges:\n",
    "            if (n1, n2) in train_edges:\n",
    "                print((n1, n2), 'in train_edges:')\n",
    "            n1 = nodes[randint(0, n-1)]\n",
    "            n2 = nodes[randint(0, n-1)]\n",
    "            (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "        val_edges.append((n1, n2))\n",
    "\n",
    "    y_val.extend([0]*(n_val_edges))\n",
    "    \n",
    "    ### From Giannis /!\\\n",
    "    val_indices = np.zeros((2,len(val_edges)))\n",
    "    for i,edge in enumerate(val_edges):\n",
    "        val_indices[0,i] = node_to_idx[edge[0]]\n",
    "        val_indices[1,i] = node_to_idx[edge[1]]\n",
    "    \n",
    "    print('Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects')\n",
    "    print('Loaded from', path[path.rfind('/')+1:], 'and with a training validation split ratio =', val_ratio)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx\n",
    "\n",
    "def random_walk(G, node, walk_length):\n",
    "    walk = [node]\n",
    "  \n",
    "    for i in range(walk_length-1):\n",
    "        neibor_nodes = list(G.neighbors(walk[-1]))\n",
    "        if len(neibor_nodes) > 0:\n",
    "            next_node = choice(neibor_nodes)\n",
    "            walk.append(next_node)\n",
    "    walk = [node for node in walk] # in case the nodes are in string format, we don't need to cast into string, but if the nodes are in numeric or integer, we need this line to cast into string\n",
    "    return walk\n",
    "\n",
    "\n",
    "def generate_walks(G, num_walks, walk_length):\n",
    "  # Runs \"num_walks\" random walks from each node, and returns a list of all random walk\n",
    "    t = time()\n",
    "    print('Start generating walks....')\n",
    "    walks = list()  \n",
    "    for i in range(num_walks):\n",
    "        for node in G.nodes():\n",
    "            walk = random_walk(G, node, walk_length)\n",
    "            walks.append(walk)\n",
    "        #print('walks : ', walks)\n",
    "    print('Random walks generated in in {}s!'.format(round(time()-t)))\n",
    "    return walks\n",
    "\n",
    "def apply_word2vec_on_features(features, nodes, vector_size=128, window=5, min_count=0, sg=1, workers=8):\n",
    "    t = time()\n",
    "    print('Start applying Word2Vec...')\n",
    "    wv_model = Word2Vec(vector_size=vector_size, window=window, min_count=min_count, sg=sg, workers=workers)\n",
    "    wv_model.build_vocab(features)\n",
    "    wv_model.train(features, total_examples=wv_model.corpus_count, epochs=5) \n",
    "    print('Word2vec model trained on features in {} min!'.format(round((time()-t)/60)))\n",
    "    features_np = []\n",
    "    for node in nodes:\n",
    "        features_np.append(wv_model.wv[node])\n",
    "\n",
    "    features_np = np.array(features_np)\n",
    "    print(features_np.shape, 'features numpy array created in {} min!'.format(round((time()-t)/60)))\n",
    "    return features_np\n",
    "\n",
    "\n",
    "\n",
    "def normalize_adjacency(A):\n",
    "    n = A.shape[0]\n",
    "    A = A + identity(n)\n",
    "    degs = A.dot(np.ones(n))\n",
    "    inv_degs = np.power(degs, -1)\n",
    "    D_inv = diags(inv_degs)\n",
    "    A_hat = D_inv.dot(A)\n",
    "    return A_hat\n",
    "\n",
    "def create_and_normalize_adjacency(G):\n",
    "    adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix of the training graph\n",
    "    adj = normalize_adjacency(adj)\n",
    "    print('Created a normalized adjancency matrix of shape', adj.shape)\n",
    "    indices = np.array(adj.nonzero()) # Gets the positions of non zeros of adj into indices\n",
    "    print('Created indices', indices.shape, 'with the positions of non zeros in adj matrix')\n",
    "    return adj, indices\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "def text_to_list(text):\n",
    "    return unidecode(text).split(',')\n",
    "\n",
    "def intersection(lst1, lst2): # a function that returns the number of common items of two lists and 1 or 0 if there are common. This function will be used in add_authors_to_pairs to add this features to the pairs.\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    is_common = 1 if len(lst3)>0 else 0\n",
    "    return len(lst3)+1, is_common+1\n",
    "\n",
    "\n",
    "def add_authors_to_pairs (pairs, authors):\n",
    "    authors = pd.DataFrame(authors)\n",
    "    try: \n",
    "        pairs = pairs.detach().cpu().numpy()\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "\n",
    "    pairs_df = pd.DataFrame(np.transpose(pairs)).rename(columns={0: \"paper_1\", 1: \"paper_2\"})\n",
    "    pairs_df = pairs_df.merge(authors, left_on='paper_1', right_on='paper_id', how='left').rename(columns={'authors': \"authors_1\"})\n",
    "    pairs_df = pairs_df.merge(authors, left_on='paper_2', right_on='paper_id', how='left').rename(columns={'authors': \"authors_2\"})\n",
    "    pairs_df.drop(['paper_id_x', 'paper_id_y'], axis=1, inplace=True)\n",
    "\n",
    "    pairs_df['nb_common_author'] = pairs_df.apply(lambda row: intersection(row['authors_1'], row['authors_2'])[0], axis=1)\n",
    "    pairs_df['is_common_author'] = pairs_df.apply(lambda row: intersection(row['authors_1'], row['authors_2'])[1], axis=1)\n",
    "\n",
    "    pairs_tensor = torch.LongTensor(np.transpose(pairs_df[[\"paper_1\", \"paper_2\", 'is_common_author', 'nb_common_author']].values.tolist())).to(device)\n",
    "    \n",
    "    return pairs_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_clean_abstracts (nodes, sample_length=-1, abstracts_path = 'https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/abstracts.txt'):\n",
    "    t = time()\n",
    "    abstracts = dict()\n",
    "    abstracts_list = list()\n",
    "    f = urlopen(abstracts_path)\n",
    "    \n",
    "    for i, line in tqdm(enumerate(f)):\n",
    "        if i == sample_length:\n",
    "            break\n",
    "        if i in nodes:\n",
    "            node, abstract = str(line).lower().split('|--|')\n",
    "            abstract = remove_stopwords(abstract)\n",
    "            #abstract = re.sub(r\"[,.;@#?!&$()-]\", \" \", abstract)\n",
    "            abstract = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", abstract)\n",
    "            #abstract = re.sub(r\"\\\\\", \" \", abstract)\n",
    "            abstract = remove_stopwords(abstract)\n",
    "\n",
    "            for word in abstract.split()[:-1]:\n",
    "                #abstract = abstract.replace(word, stemmer.stem(word))\n",
    "                abstract = abstract.replace(word, lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word), pos='s'), pos='a'), pos='n'), pos='v'), pos='r'))\n",
    "            \n",
    "            node = re.sub(\"[^0-9]\", \"\", node)\n",
    "            if i != int(node):\n",
    "                print('i and node not the same', i, node)\n",
    "            abstracts[int(node)] = abstract\n",
    "            abstracts_list.append(abstract)\n",
    "        \n",
    "    print('Text loaded and cleaned in {:.0f} min'.format((time()-t)/60))\n",
    "    return abstracts\n",
    "\n",
    "def doc_counter (documents, word): #a function that return the number of documents containing a word\n",
    "    counter = 0\n",
    "    for i in documents:\n",
    "        if word in documents[i]:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        self.word_occurrence = {}\n",
    "        self.word2node = {}\n",
    "        self.words_list = []\n",
    "        self.sentences_list = []\n",
    "        self.sentences_list_words = []\n",
    "        self.num_words = 0\n",
    "        self.num_sentences = 0\n",
    "        self.longest_sentence = 0\n",
    "\n",
    "    def add_word(self, word, node):\n",
    "        if word not in self.word2index:\n",
    "            # First entry of word into vocabulary\n",
    "            self.words_list.append(word)\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "            self.word_occurrence[word] = 1\n",
    "            self.word2node[word] = [node]\n",
    "        else:\n",
    "            # Word exists; increase word count\n",
    "            self.word2count[word] += 1\n",
    "            self.word_occurrence[word] += 1\n",
    "            if node not in self.word2node[word]:\n",
    "                self.word2node[word].append(node)\n",
    "            # self.num_words += 1\n",
    "            \n",
    "    def add_sentence(self, sentence, node):\n",
    "        sentence_len = 0\n",
    "        for word in sentence.split()[:-1]:\n",
    "            sentence_len += 1\n",
    "            self.add_word(word, node)\n",
    "        if sentence_len > self.longest_sentence:\n",
    "            # This is the longest sentence\n",
    "            self.longest_sentence = sentence_len\n",
    "        # Count the number of sentences\n",
    "        self.num_sentences += 1\n",
    "        self.sentences_list.append(sentence)\n",
    "        self.sentences_list_words.append(sentence.split()[:-1])\n",
    "\n",
    "    def to_word(self, index):\n",
    "        return self.index2word[index]\n",
    "\n",
    "    def to_index(self, word):\n",
    "        return self.word2index[word]\n",
    "\n",
    "    def words(self):\n",
    "        return self.words_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load graph and authors data from sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oe6DhotaSuqg",
    "outputId": "fcbc9c84-555d-4e99-f910-d6292ca149d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499 number of edges: 1091955 in the Complete set\n",
      "Number of nodes: 138499 number of edges: 982779 in the Training set\n",
      "len(nodes) 138499\n",
      "Creating random val_edges...\n",
      "Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects\n",
      "Loaded from edgelist.txt and with a training validation split ratio = 0.1\n"
     ]
    }
   ],
   "source": [
    "G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx = read_train_val_graph(val_ratio=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EyBibxd6zQhV",
    "outputId": "1223b134-85a1-45da-8006-eb530505c279"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pd/glcnl2497w5b6xn3p94tnwlr0000gn/T/ipykernel_71728/3479570820.py:129: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix of the training graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a normalized adjancency matrix of shape (138499, 138499)\n",
      "Created indices (2, 2104057) with the positions of non zeros in adj matrix\n"
     ]
    }
   ],
   "source": [
    "adj, indices = create_and_normalize_adjacency(G_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "dWAzHqmxmYff",
    "outputId": "67fca796-3365-4d46-90ed-cc98a401fa2a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[James H. Niblock, Jian-Xun Peng, Karen R. McM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Jian-Xun Peng, Kang Li, De-Shuang Huang]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[J. Heikkila]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[Long Zhang, Kang Li, Er-Wei Bai, George W. Ir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id                                            authors\n",
       "0         0  [James H. Niblock, Jian-Xun Peng, Karen R. McM...\n",
       "1         1          [Jian-Xun Peng, Kang Li, De-Shuang Huang]\n",
       "2         2                                      [J. Heikkila]\n",
       "3         3    [L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]\n",
       "4         4  [Long Zhang, Kang Li, Er-Wei Bai, George W. Ir..."
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors = pd.read_csv(urlopen('https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/authors.txt'), sep = '|', header=None)\n",
    "authors = authors.rename(columns={0: \"paper_id\", 2: \"authors\"})\n",
    "authors['authors'] = authors['authors'].apply(text_to_list)\n",
    "authors = authors[[\"paper_id\", \"authors\"]]\n",
    "authors = authors[authors['paper_id'] <= max(G.nodes())]\n",
    "authors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "138499it [06:41, 345.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text loaded and cleaned in 7 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 138499/138499 [28:52<00:00, 79.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab built in 29 min\n",
      "Vocab size is: 11280217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# n = -1 #length of the sample to develop and test the pipeline (-1 or negative values to take all the dataset)\n",
    "\n",
    "#takes 4 minutes to process all the abstracts\n",
    "abstracts = read_and_clean_abstracts(nodes, sample_length=n)  #149s #194s\n",
    "abstracts_dict_list_words = {i: abstracts[i].split()[:-1] for i in nodes}\n",
    "abstracts_list_sentences = [list(item)[1][:-3] for item in abstracts.items()]\n",
    "\n",
    "#we create a vacabulary of words and sentences (abstracts)\n",
    "t = time()\n",
    "voc = Vocabulary('abstracts') \n",
    "for node in tqdm(nodes):\n",
    "    voc.add_sentence(abstracts[node], node)\n",
    "\n",
    "print('Vocab built in {:.0f} min'.format((time()-t)/60))\n",
    "print('Vocab size is:', voc.num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62907"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_multiple = {key:value for (key, value) in voc.word2node.items() if len(value) >= 2}\n",
    "\n",
    "len(words_multiple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf matrix generated in 7 sec\n",
      "tf-idf shape: (138499, 62907)\n"
     ]
    }
   ],
   "source": [
    "#Now we will compute a logarithmic tf-idf matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "t = time()\n",
    "\n",
    "# Create a TfidfVectorizer object with logarithmic tf\n",
    "# And as we are intrested in links between words, we will take only words that occured at least in two abstracts\n",
    "words_multiple = {key:value for (key, value) in voc.word2node.items() if len(value) >= 2}\n",
    "vectorizer = TfidfVectorizer(vocabulary=list(words_multiple.keys()), sublinear_tf=True)\n",
    "\n",
    "\n",
    "# Fit the vectorizer to the sentences and transform them into a TF-IDF matrix\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(abstracts_list_sentences)\n",
    "\n",
    "# Print the TF-IDF matrix\n",
    "print('tf-idf matrix generated in {:.0f} sec'.format(time()-t))\n",
    "print('tf-idf shape:', tfidf_matrix.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "sparse.save_npz(\"tfidf_matrix.npz\", tfidf_matrix)\n",
    "your_matrix_back = sparse.load_npz(\"tfidf_matrix.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = sparse.load_npz(\"tfidf_matrix.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-28 19:44:39.933220: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b00de290324175bd82cfb972e5447c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d965d2322e46d5912e712178c70060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971512120afb4d368e3ad3240f841bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7d5d9fd1fb47eda9708391510b836a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd44c4d79b6e4cb18d7cf5df3488b353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BartTokenizer, BartModel\n",
    "\n",
    "# Load the BART model and tokenizer\n",
    "model = BartModel.from_pretrained('facebook/bart-large')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate embeddings for text\n",
    "def get_bart_embeddings(text):\n",
    "    # Tokenize the input text\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Generate embeddings using the BART model\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "        embeddings = model_output.last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bart_embeddings(voc.sentences_list[1]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2czqslmiVfcQ"
   },
   "source": [
    "# Read processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf-_fYkF5xiO",
    "outputId": "fcb2a001-9964-48f4-f14c-5952227e0338"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wv walks loaded from GCP in 23 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(138499, 64)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time()\n",
    "from io import BytesIO\n",
    "\n",
    "walks_url = 'https://storage.googleapis.com/link_prediction_processed_data/walks_wv.npy'\n",
    "with urlopen(walks_url) as url:\n",
    "    data = url.read()\n",
    "\n",
    "# Create a seekable file-like object from the data\n",
    "fileobj = BytesIO(data)\n",
    "\n",
    "# Load the data from the file object\n",
    "walks_wv = np.load(fileobj)\n",
    "print('wv walks loaded from GCP in {:.0f} sec'.format(time()-t))\n",
    "walks_wv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sbhiufK3kLG7",
    "outputId": "e5fa6ef1-176c-468d-b2d1-8158f8058c47"
   },
   "outputs": [],
   "source": [
    "t = time()\n",
    "# abstract_url = 'https://storage.googleapis.com/link_prediction_processed_data/embedded_abstracts_dict_192array.pkl.gz'\n",
    "\n",
    "# with urlopen(abstract_url) as response:\n",
    "#     compressed_data = response.read()\n",
    "\n",
    "with open('embedded_abstracts_dict_192array.pkl.gz', 'rb') as f:\n",
    "    compressed_data = f.read()\n",
    "\n",
    "\n",
    "# Decompress the data\n",
    "my_dict = pickle.loads(gzip.decompress(compressed_data))\n",
    "print('File loaded and decompressed in {:.0f} min'.format((time()-t)/60))\n",
    "print('len(my_dict):', len(my_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming dict of embedding abstracts words into a list of embedding abstracts\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "t = time()\n",
    "my_list = list(my_dict.values())\n",
    "tensor_list = [torch.tensor(arr) for arr in my_list]\n",
    "print('transformed the dict of lists into a list of tensors in {:.0f} min'.format((time()-t)/60))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9A1A7DsGR9Yp",
    "outputId": "abf99938-aba3-403a-e6a8-808ffce1c0ab"
   },
   "outputs": [],
   "source": [
    "# max and mean pooling of words of the abstracts\n",
    "\n",
    "t = time()\n",
    "\n",
    "# mean pooling of words\n",
    "mean_abstract_embedding = []\n",
    "for key in my_dict.keys():\n",
    "    if len(my_dict[key]) > 0:\n",
    "        mean_abstract_embedding.append(np.mean(my_dict[key], axis=0))\n",
    "    else:\n",
    "        mean_abstract_embedding.append(np.zeros(my_dict[0].shape[1]))\n",
    "mean_abstract_embedding = np.array(mean_abstract_embedding)\n",
    "\n",
    "# max pooling\n",
    "max_abstract_embedding = []\n",
    "for key in my_dict.keys():\n",
    "    if len(my_dict[key]) > 0:\n",
    "        max_abstract_embedding.append(np.max(my_dict[key], axis=0))\n",
    "    else:\n",
    "        max_abstract_embedding.append(np.zeros(my_dict[0].shape[1]))\n",
    "max_abstract_embedding = np.array(max_abstract_embedding)\n",
    "\n",
    "print('max and mean pooling performed in {:.0f} sec'.format((time()-t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxvIuCP_bFrN"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "id": "ROBXxQaZnfQ3"
   },
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, vocab_size, n_feat, n_hidden, n_class, sub_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        self.dense = nn.Linear(vocab_size, 1000)\n",
    "        self.dense2 = nn.Linear(1000, n_feat)\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.double_fc3 = nn.Linear((3*n_hidden), n_hidden)\n",
    "        self.fc4 = nn.Linear(n_hidden, sub_class)\n",
    "        self.fc5 = nn.Linear(sub_class, n_class)        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x_in, tfidf_matrix, adj, pairs):\n",
    "        #print('tfidf_matrix', tfidf_matrix.shape)\n",
    "        \n",
    "        #print('h_abstr', h_abstr.shape)\n",
    "        #x_in = torch.cat((x_in, h_abstr), dim=1)\n",
    "        #print('x_in', x_in.shape)\n",
    "        \n",
    "              \n",
    "        h1 = self.fc1(x_in)\n",
    "        z1 = self.relu(torch.spmm(adj, h1)) # sparce matrix multiplication\n",
    "        z1 = self.dropout(z1)\n",
    "        del(x_in, h1)\n",
    "\n",
    "        h2 = self.fc2(z1)\n",
    "        z2 = self.relu(torch.spmm(adj, h2))\n",
    "        z2 = self.dropout(z2)\n",
    "        del(h2, z1)\n",
    "\n",
    "        h3 = self.fc2(z2)\n",
    "        z3 = self.relu(torch.spmm(adj, h3))\n",
    "        z3 = self.dropout(z3)\n",
    "        #print('z2', z2.shape)\n",
    "        del(h3, z3, adj)\n",
    "\n",
    "        x = z2[pairs[0]] * z2[pairs[1]] # embedded features (z2) of node 0 - embedded features of node 1\n",
    "        #print('x', x.shape)\n",
    "        x = pairs[3][:, None] * x\n",
    "        #print('x', x.shape)\n",
    "        #x1 = z2[pairs[0]]\n",
    "        #x2 = z2[pairs[1]]\n",
    "        \n",
    "        #x = torch.cat((x, x1, x2), dim=1)\n",
    "        #del(x1, x2)\n",
    "        \n",
    "        h_abstr = self.dense(tfidf_matrix)\n",
    "        h_abstr = self.dense2(h_abstr)\n",
    "        y = torch.cat((h_abstr[pairs[0]],h_abstr[pairs[1]]), dim=1)\n",
    "        \n",
    "        #print('h_abstr', h_abstr.shape)\n",
    "        x = torch.cat((x, y), dim=1)\n",
    "        del(h_abstr, tfidf_matrix)\n",
    "        del(pairs)\n",
    "\n",
    "        x = self.relu(self.double_fc3(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "id": "bnRNz5z4nh6x"
   },
   "outputs": [],
   "source": [
    "def prepare_data_to_train (tfidf_matrix, features, authors, adj, indices, val_indices, y_val):\n",
    "    \n",
    "    print('Preparing the data for training...')\n",
    "    \n",
    "    t = time()\n",
    "    \n",
    "    y_val = torch.LongTensor(y_val).to(device)\n",
    "\n",
    "    # Create class labels\n",
    "    y = np.zeros(2*indices.shape[1])\n",
    "    y[:indices.shape[1]] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
    "    y = torch.LongTensor(y).to(device)\n",
    "    \n",
    "    features = torch.FloatTensor(features).to(device)\n",
    "    \n",
    "    indices = torch.LongTensor(indices).to(device)\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "    tfidf_matrix = sparse_mx_to_torch_sparse_tensor(tfidf_matrix).to(device)\n",
    "    \n",
    "    # the function add_authors_to_pairs converts into torch tensors and sends to Device    \n",
    "    val_indices = add_authors_to_pairs(val_indices, authors) #we add the authors to val_pairs\n",
    "    indices = add_authors_to_pairs(indices, authors) #we add the authors to indices    \n",
    "    rand_indices = np.random.randint(0, features.shape[0], (indices.shape[0],indices.shape[1]))# We take random indices each time we run an epoch\n",
    "    rand_indices = add_authors_to_pairs(rand_indices, authors)\n",
    "\n",
    "    pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices. \n",
    "    del(authors, indices, rand_indices)\n",
    "    \n",
    "    print('Data converted into torch tensors and authors added to indices in {:.0f} min'.format((time()-t)/60))\n",
    "\n",
    "    return tfidf_matrix, features, adj, pairs, y, val_indices, y_val\n",
    "\n",
    "\n",
    "    \n",
    "def early_stopping(loss_train, list_loss_train, loss_val, list_loss_val, window=10, tolerance=0.01):\n",
    "    if (len(list_loss_val) == window and loss_val > (sum(list_loss_val)/len(list_loss_val)) and loss_train + tolerance < loss_val) or (len(list_loss_train) == window and loss_train > (sum(list_loss_train)/len(list_loss_train))):\n",
    "        print('train: {:.5f} val: {:.5f} mean val: {:.5f}'.format(loss_train, loss_val, (sum(list_loss_val)/len(list_loss_val))))\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "    \n",
    "def train_model(model, learning_rate, tfidf_matrix, features, adj, pairs, y, val_indices, y_val, epochs, run_number):\n",
    "    # Train model\n",
    "    start_time = time()\n",
    "    \n",
    "    print('Initializing the optimizer with learning rate:', learning_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) #optimizer with halving learning rate in training\n",
    "    try: os.mkdir('./outputs')\n",
    "    except: pass\n",
    "\n",
    "    today = datetime.today().strftime('%Y-%m-%d-%H:%M')\n",
    "    list_loss_val = []\n",
    "    list_loss_train = []\n",
    "    window = 20\n",
    "    \n",
    "    halving_lr = 0 # counter of the number of halving lr\n",
    "    print('Start training...')\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        output = model(features, tfidf_matrix, adj, pairs).to(device) # we run the model that gives the output.\n",
    "        loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "        acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "        loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "        optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, tfidf_matrix, adj, val_indices).to(device)\n",
    "        #y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        list_loss_val.append(loss_val.item())\n",
    "        list_loss_train.append(loss_train.item())\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {} s'.format(int(round(time()) - round(t))),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "            \n",
    "            if epoch % 50 == 0:\n",
    "                model_path = \"outputs/{}-model-{}epochs-{}.pt\".format(today, epoch, run_number)\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "            early = early_stopping(loss_train.item(), list_loss_train[-window:], loss_val.item(), list_loss_val[-window:], window)        \n",
    "            if early:\n",
    "                halving_lr += 1\n",
    "                if halving_lr > 4:\n",
    "                    break\n",
    "                list_loss_val=[]\n",
    "                list_loss_train=[]\n",
    "                learning_rate = learning_rate/10\n",
    "                optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                print('Deviding the learning rate by 10. New learning rate: {}'.format(learning_rate))\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJqiI7fxbVhl"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "id": "7i8UpGHuNhQp"
   },
   "outputs": [],
   "source": [
    "n_hidden = 64\n",
    "dropout_rate = 0.2\n",
    "sub_class = 8\n",
    "n_class = 2\n",
    "features = walks_wv#max_abstract_embedding #walks_wv\n",
    "n_features = features.shape[1]\n",
    "vocab_size = tfidf_matrix.shape[1]\n",
    "\n",
    "# Creates the model\n",
    "model = GNN(vocab_size, n_features, n_hidden, n_class, sub_class, dropout_rate).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the data for training...\n",
      "Data converted into torch tensors and authors added to indices in 2 min\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix_torch, features_torch, adj_torch, pairs_torch, y_torch, val_indices_torch, y_val_torch = prepare_data_to_train(tfidf_matrix, features, authors, adj, indices, val_indices, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the optimizer with learning rate: 0.01\n",
      "Start training...\n",
      "Epoch: 001 loss_train: 0.7033 loss_val: 0.6957 acc_train: 0.5000 acc_val: 0.5000 time: 51 s total_time: 1 min\n",
      "Epoch: 006 loss_train: 0.6894 loss_val: 0.6806 acc_train: 0.5425 acc_val: 0.5391 time: 50 s total_time: 5 min\n",
      "Epoch: 011 loss_train: 0.6961 loss_val: 0.6940 acc_train: 0.4889 acc_val: 0.4676 time: 50 s total_time: 9 min\n",
      "Epoch: 016 loss_train: 0.6903 loss_val: 0.6860 acc_train: 0.5347 acc_val: 0.5405 time: 48 s total_time: 13 min\n",
      "Epoch: 021 loss_train: 0.6665 loss_val: 0.6747 acc_train: 0.6035 acc_val: 0.5883 time: 49 s total_time: 17 min\n",
      "Epoch: 026 loss_train: 0.6351 loss_val: 0.6536 acc_train: 0.6484 acc_val: 0.5982 time: 49 s total_time: 22 min\n",
      "Epoch: 031 loss_train: 0.6135 loss_val: 0.6493 acc_train: 0.6609 acc_val: 0.6615 time: 48 s total_time: 26 min\n",
      "Epoch: 036 loss_train: 0.6723 loss_val: 0.6527 acc_train: 0.5630 acc_val: 0.6269 time: 48 s total_time: 30 min\n",
      "train: 0.67228 val: 0.65273 mean val: 0.64969\n",
      "Deviding the learning rate by 10. New learning rate: 0.001\n",
      "Epoch: 041 loss_train: 0.6181 loss_val: 0.6077 acc_train: 0.6512 acc_val: 0.6534 time: 48 s total_time: 34 min\n",
      "Epoch: 046 loss_train: 0.6018 loss_val: 0.5930 acc_train: 0.6784 acc_val: 0.6793 time: 49 s total_time: 38 min\n",
      "Epoch: 051 loss_train: 0.5984 loss_val: 0.5856 acc_train: 0.6779 acc_val: 0.6858 time: 48 s total_time: 42 min\n",
      "Epoch: 056 loss_train: 0.5970 loss_val: 0.5839 acc_train: 0.6837 acc_val: 0.6865 time: 48 s total_time: 46 min\n",
      "Epoch: 061 loss_train: 0.5943 loss_val: 0.5818 acc_train: 0.6819 acc_val: 0.6887 time: 49 s total_time: 50 min\n",
      "Epoch: 066 loss_train: 0.5911 loss_val: 0.5790 acc_train: 0.6873 acc_val: 0.6926 time: 48 s total_time: 54 min\n",
      "Epoch: 071 loss_train: 0.5874 loss_val: 0.5778 acc_train: 0.6898 acc_val: 0.6942 time: 47 s total_time: 58 min\n",
      "Epoch: 076 loss_train: 0.5834 loss_val: 0.5730 acc_train: 0.6936 acc_val: 0.6976 time: 49 s total_time: 62 min\n",
      "Epoch: 081 loss_train: 0.5783 loss_val: 0.5710 acc_train: 0.6975 acc_val: 0.7000 time: 49 s total_time: 66 min\n",
      "Epoch: 086 loss_train: 0.5710 loss_val: 0.5648 acc_train: 0.7042 acc_val: 0.7049 time: 49 s total_time: 70 min\n",
      "Epoch: 091 loss_train: 0.5572 loss_val: 0.5560 acc_train: 0.7202 acc_val: 0.7131 time: 49 s total_time: 74 min\n",
      "Epoch: 096 loss_train: 0.5310 loss_val: 0.5328 acc_train: 0.7408 acc_val: 0.7311 time: 49 s total_time: 78 min\n",
      "Epoch: 101 loss_train: 0.5107 loss_val: 0.5083 acc_train: 0.7548 acc_val: 0.7497 time: 48 s total_time: 82 min\n",
      "Epoch: 106 loss_train: 0.4934 loss_val: 0.4911 acc_train: 0.7641 acc_val: 0.7605 time: 48 s total_time: 86 min\n",
      "Epoch: 111 loss_train: 0.4745 loss_val: 0.4721 acc_train: 0.7734 acc_val: 0.7671 time: 48 s total_time: 90 min\n",
      "Epoch: 116 loss_train: 0.4591 loss_val: 0.4577 acc_train: 0.7786 acc_val: 0.7744 time: 49 s total_time: 94 min\n",
      "Epoch: 121 loss_train: 0.4457 loss_val: 0.4448 acc_train: 0.7843 acc_val: 0.7839 time: 49 s total_time: 98 min\n",
      "Epoch: 126 loss_train: 0.4350 loss_val: 0.4344 acc_train: 0.7893 acc_val: 0.7868 time: 48 s total_time: 103 min\n",
      "Epoch: 131 loss_train: 0.4267 loss_val: 0.4274 acc_train: 0.7937 acc_val: 0.7905 time: 50 s total_time: 107 min\n",
      "Epoch: 136 loss_train: 0.4187 loss_val: 0.4215 acc_train: 0.7977 acc_val: 0.7939 time: 48 s total_time: 111 min\n",
      "Epoch: 141 loss_train: 0.4124 loss_val: 0.4157 acc_train: 0.8011 acc_val: 0.7969 time: 48 s total_time: 115 min\n",
      "Epoch: 146 loss_train: 0.4052 loss_val: 0.4088 acc_train: 0.8059 acc_val: 0.8013 time: 49 s total_time: 119 min\n",
      "Epoch: 151 loss_train: 0.3980 loss_val: 0.4013 acc_train: 0.8100 acc_val: 0.8067 time: 49 s total_time: 123 min\n",
      "Epoch: 156 loss_train: 0.3902 loss_val: 0.3929 acc_train: 0.8159 acc_val: 0.8121 time: 49 s total_time: 127 min\n",
      "Epoch: 161 loss_train: 0.3823 loss_val: 0.3836 acc_train: 0.8215 acc_val: 0.8180 time: 49 s total_time: 131 min\n",
      "Epoch: 166 loss_train: 0.3742 loss_val: 0.3726 acc_train: 0.8269 acc_val: 0.8262 time: 49 s total_time: 135 min\n",
      "Epoch: 171 loss_train: 0.3664 loss_val: 0.3667 acc_train: 0.8323 acc_val: 0.8310 time: 49 s total_time: 139 min\n",
      "Epoch: 176 loss_train: 0.3604 loss_val: 0.3611 acc_train: 0.8369 acc_val: 0.8346 time: 48 s total_time: 143 min\n",
      "Epoch: 181 loss_train: 0.3542 loss_val: 0.3517 acc_train: 0.8412 acc_val: 0.8415 time: 48 s total_time: 147 min\n",
      "Epoch: 186 loss_train: 0.3459 loss_val: 0.3442 acc_train: 0.8457 acc_val: 0.8465 time: 48 s total_time: 151 min\n",
      "Epoch: 191 loss_train: 0.3416 loss_val: 0.3386 acc_train: 0.8483 acc_val: 0.8496 time: 48 s total_time: 155 min\n",
      "Epoch: 196 loss_train: 0.3358 loss_val: 0.3321 acc_train: 0.8516 acc_val: 0.8533 time: 48 s total_time: 159 min\n",
      "Optimization Finished in 163 min!\n"
     ]
    }
   ],
   "source": [
    "# with dense2 1000\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "run_number = randint(0, 1000)\n",
    "\n",
    "\n",
    "trained_model = train_model(model, 0.01, tfidf_matrix_torch, features_torch, adj_torch, pairs_torch, y_torch, val_indices_torch, y_val_torch, epochs, run_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KDQ_DCCJavQZ",
    "outputId": "0c81af49-7f33-4b68-8b9c-d5e941aed2a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the optimizer with learning rate: 0.01\n",
      "Start training...\n",
      "Epoch: 001 loss_train: 0.7205 loss_val: 0.7085 acc_train: 0.5000 acc_val: 0.5000 time: 25 s total_time: 0 min\n",
      "Epoch: 006 loss_train: 0.6909 loss_val: 0.6988 acc_train: 0.5182 acc_val: 0.5000 time: 20 s total_time: 2 min\n",
      "Epoch: 011 loss_train: 0.6843 loss_val: 0.6764 acc_train: 0.5874 acc_val: 0.6041 time: 18 s total_time: 4 min\n",
      "Epoch: 016 loss_train: 0.6125 loss_val: 0.6026 acc_train: 0.6735 acc_val: 0.6763 time: 19 s total_time: 5 min\n",
      "Epoch: 021 loss_train: 0.5949 loss_val: 0.5804 acc_train: 0.6832 acc_val: 0.6994 time: 17 s total_time: 7 min\n",
      "Epoch: 026 loss_train: 0.5624 loss_val: 0.5546 acc_train: 0.7285 acc_val: 0.7277 time: 19 s total_time: 8 min\n",
      "Epoch: 031 loss_train: 0.5239 loss_val: 0.5169 acc_train: 0.7717 acc_val: 0.7671 time: 18 s total_time: 10 min\n",
      "Epoch: 036 loss_train: 0.4932 loss_val: 0.4884 acc_train: 0.7939 acc_val: 0.7946 time: 18 s total_time: 12 min\n",
      "Epoch: 041 loss_train: 0.4685 loss_val: 0.4661 acc_train: 0.8123 acc_val: 0.8098 time: 18 s total_time: 13 min\n",
      "Epoch: 046 loss_train: 0.4411 loss_val: 0.4269 acc_train: 0.8305 acc_val: 0.8417 time: 18 s total_time: 15 min\n",
      "Epoch: 051 loss_train: 0.4185 loss_val: 0.3995 acc_train: 0.8422 acc_val: 0.8544 time: 22 s total_time: 16 min\n",
      "Epoch: 056 loss_train: 0.3927 loss_val: 0.3730 acc_train: 0.8565 acc_val: 0.8672 time: 18 s total_time: 18 min\n",
      "Epoch: 061 loss_train: 0.3894 loss_val: 0.3344 acc_train: 0.8331 acc_val: 0.8738 time: 17 s total_time: 19 min\n",
      "Epoch: 066 loss_train: 0.3296 loss_val: 0.2941 acc_train: 0.8727 acc_val: 0.8806 time: 20 s total_time: 21 min\n",
      "Epoch: 071 loss_train: 0.3032 loss_val: 0.2753 acc_train: 0.8780 acc_val: 0.8870 time: 16 s total_time: 23 min\n",
      "Epoch: 076 loss_train: 0.2825 loss_val: 0.2613 acc_train: 0.8895 acc_val: 0.8934 time: 18 s total_time: 24 min\n",
      "Epoch: 081 loss_train: 0.2696 loss_val: 0.2530 acc_train: 0.8915 acc_val: 0.8973 time: 19 s total_time: 26 min\n",
      "Epoch: 086 loss_train: 0.2577 loss_val: 0.2446 acc_train: 0.9010 acc_val: 0.9010 time: 18 s total_time: 27 min\n",
      "Epoch: 091 loss_train: 0.2478 loss_val: 0.2377 acc_train: 0.9054 acc_val: 0.9042 time: 20 s total_time: 29 min\n",
      "Epoch: 096 loss_train: 0.2359 loss_val: 0.2338 acc_train: 0.9098 acc_val: 0.9060 time: 18 s total_time: 30 min\n",
      "Epoch: 101 loss_train: 0.2238 loss_val: 0.2287 acc_train: 0.9128 acc_val: 0.9091 time: 18 s total_time: 32 min\n",
      "Epoch: 106 loss_train: 0.2328 loss_val: 0.2493 acc_train: 0.9108 acc_val: 0.8991 time: 18 s total_time: 33 min\n",
      "train: 0.23276 val: 0.24931 mean val: 0.23705\n",
      "Deviding the learning rate by 10. New learning rate: 0.001\n",
      "Epoch: 111 loss_train: 0.2136 loss_val: 0.2263 acc_train: 0.9167 acc_val: 0.9104 time: 18 s total_time: 35 min\n",
      "Epoch: 116 loss_train: 0.2117 loss_val: 0.2253 acc_train: 0.9159 acc_val: 0.9109 time: 18 s total_time: 36 min\n",
      "Epoch: 121 loss_train: 0.2102 loss_val: 0.2252 acc_train: 0.9180 acc_val: 0.9111 time: 17 s total_time: 38 min\n",
      "Epoch: 126 loss_train: 0.2096 loss_val: 0.2250 acc_train: 0.9167 acc_val: 0.9109 time: 22 s total_time: 39 min\n",
      "Epoch: 131 loss_train: 0.2081 loss_val: 0.2241 acc_train: 0.9188 acc_val: 0.9113 time: 18 s total_time: 41 min\n",
      "Epoch: 136 loss_train: 0.2071 loss_val: 0.2239 acc_train: 0.9183 acc_val: 0.9114 time: 20 s total_time: 42 min\n",
      "Epoch: 141 loss_train: 0.2058 loss_val: 0.2230 acc_train: 0.9198 acc_val: 0.9117 time: 18 s total_time: 44 min\n",
      "Epoch: 146 loss_train: 0.2052 loss_val: 0.2227 acc_train: 0.9195 acc_val: 0.9119 time: 17 s total_time: 45 min\n",
      "Epoch: 151 loss_train: 0.2030 loss_val: 0.2225 acc_train: 0.9209 acc_val: 0.9120 time: 19 s total_time: 47 min\n",
      "Epoch: 156 loss_train: 0.2022 loss_val: 0.2220 acc_train: 0.9208 acc_val: 0.9122 time: 19 s total_time: 48 min\n",
      "Epoch: 161 loss_train: 0.2014 loss_val: 0.2219 acc_train: 0.9213 acc_val: 0.9126 time: 23 s total_time: 50 min\n",
      "Epoch: 166 loss_train: 0.2006 loss_val: 0.2210 acc_train: 0.9218 acc_val: 0.9128 time: 20 s total_time: 51 min\n",
      "Epoch: 171 loss_train: 0.1993 loss_val: 0.2207 acc_train: 0.9221 acc_val: 0.9132 time: 17 s total_time: 53 min\n",
      "Epoch: 176 loss_train: 0.1984 loss_val: 0.2201 acc_train: 0.9227 acc_val: 0.9134 time: 17 s total_time: 54 min\n",
      "Epoch: 181 loss_train: 0.1969 loss_val: 0.2197 acc_train: 0.9233 acc_val: 0.9136 time: 17 s total_time: 56 min\n",
      "Epoch: 186 loss_train: 0.1949 loss_val: 0.2196 acc_train: 0.9241 acc_val: 0.9138 time: 18 s total_time: 58 min\n",
      "Epoch: 191 loss_train: 0.1934 loss_val: 0.2189 acc_train: 0.9248 acc_val: 0.9141 time: 17 s total_time: 59 min\n",
      "Epoch: 196 loss_train: 0.1926 loss_val: 0.2183 acc_train: 0.9254 acc_val: 0.9143 time: 19 s total_time: 61 min\n",
      "Optimization Finished in 62 min!\n"
     ]
    }
   ],
   "source": [
    "# Without dense 1000\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "run_number = randint(0, 1000)\n",
    "\n",
    "\n",
    "trained_model = train_model(model, 0.01, tfidf_matrix_torch, features_torch, adj_torch, pairs_torch, y_torch, val_indices_torch, y_val_torch, epochs, run_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_XPsPMFbZji"
   },
   "source": [
    "# Generate test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmFO4qI7_245"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "test_path = 'https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/test.txt'\n",
    "node_pairs = list()\n",
    "f = urlopen(test_path)\n",
    "\n",
    "for line in f:\n",
    "    t = str(line).split(',')\n",
    "    t[0] = int(re.sub(\"[^0-9]\", \"\", t[0]))\n",
    "    t[1] = int(re.sub(\"[^0-9]\", \"\", t[1]))\n",
    "    node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "node_pairs = np.transpose(node_pairs)\n",
    "node_pairs = add_authors_to_pairs(node_pairs, authors)\n",
    "#node_pairs = torch.LongTensor(node_pairs).to(device)\n",
    "\n",
    "adj_torch = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "features_torch = torch.FloatTensor(features).to(device)\n",
    "\n",
    "test_output = model(features_torch, adj_torch, node_pairs)\n",
    "y_pred = torch.exp(test_output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "model_nb = 1\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns=['predicted']).to_csv(\n",
    "\"{}-submission-{}-{}.csv\".format(today, model_nb, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pD505btvfDG0",
    "outputId": "98276ce8-aeba-408c-c9e5-87e392f3d9cb"
   },
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6oNCPyubfSd"
   },
   "source": [
    "#Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2UWL4y-K1jd"
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "node_pairs = torch.LongTensor(node_pairs).to(device)\n",
    "\n",
    "test_output = model(features, adj, node_pairs)\n",
    "y_pred = torch.exp(test_output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"{}-submission-{}-{}.csv\".format(today, model_nb, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmUL7tGAR9Yt"
   },
   "outputs": [],
   "source": [
    "#### New script with batches\n",
    "\n",
    "def early_stopping(loss_train, list_loss_train, loss_val, list_loss_val, \n",
    "                   tolerance=0.01, patience=15):\n",
    "    list_loss_val = list(list_loss_val)[-patience:]\n",
    "    list_loss_train = list(list_loss_train)[-patience:]\n",
    "    if (len(list_loss_val) == patience and loss_val > (sum(list_loss_val)/len(list_loss_val)) and loss_train + tolerance < loss_val) or (len(list_loss_train) == patience and loss_train > (sum(list_loss_train)/len(list_loss_train))):\n",
    "        #print('train: {:.5f} val: {:.5f} mean val: {:.5f}'.format(loss_train, loss_val, (sum(list_loss_val)/len(list_loss_val))))\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "    \n",
    "def train_model(model, learning_rate, features, adj, indices_mc, y, val_indices, \n",
    "                y_val, epochs, batch_size, wv_walk_size, \n",
    "                tolerence = 0.01, patience = 15, run_number=randint(0, 1000)):\n",
    "    # Train model\n",
    "    start_time = time()\n",
    "    print('Initializing the optimizer with learning rate:', learning_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) #optimizer with halving learning rate in training\n",
    "    try: os.mkdir('./outputs')\n",
    "    except: pass\n",
    "    print('Preparing the data for training...')        \n",
    "\n",
    "    today = datetime.today().strftime('%Y-%m-%d-%H:%M')\n",
    "    list_loss_val = []\n",
    "    list_loss_train = []\n",
    "\n",
    "    \n",
    "    halving_lr = 0 # counter of the number of halving lr\n",
    "    print('Start training...')\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "\n",
    "        # we create the rand indices corresponding to non edges (their y = 0)\n",
    "        # we could apply a condition on epoch to run rand_indices (for speed purposes)\n",
    "        rand_indices = np.random.randint(0, len(indices_mc), size=(indices_mc.shape[0], indices_mc.shape[1]))\n",
    "        rand_indices = add_authors_to_pairs(rand_indices, authors)\n",
    "        pairs = np.concatenate((indices_mc, rand_indices), axis=1)\n",
    "        pairs = torch.LongTensor(pairs).to(device)\n",
    "\n",
    "        permutation = torch.randperm(pairs.size()[1])\n",
    "        \n",
    "        # batches\n",
    "        for i in range(0, pairs.size()[1], batch_size):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            elts_indices = permutation[i:i+batch_size]\n",
    "            batch_pairs = pairs[:, elts_indices]\n",
    "            batch_y = y[elts_indices]\n",
    "\n",
    "        \n",
    "            model.train()\n",
    "\n",
    "            output = model(features, adj, batch_pairs, wv_walk_size).to(device) # we run the model that gives the output.\n",
    "            loss_train = F.nll_loss(output, batch_y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "            acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), batch_y.cpu().numpy())# just to show it in the out put message of the training\n",
    "            loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "            optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, adj, val_indices, wv_walk_size).to(device)\n",
    "        #y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        list_loss_val.append(loss_val.item())\n",
    "        list_loss_train.append(loss_train.item())\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {} s'.format(int(round(time()) - round(t))),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "        if epoch % 50 == 0:\n",
    "            model_path = \"outputs/{}-model-{}epochs-{}.pt\".format(today, epoch, run_number)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "        if int(loss_val.item()) > 5:\n",
    "            break\n",
    "            \n",
    "        early = early_stopping(loss_train.item(), list_loss_train, loss_val.item(), list_loss_val, patience=15)        \n",
    "        if early:\n",
    "            halving_lr += 1\n",
    "            if halving_lr > 5:\n",
    "                break\n",
    "            list_loss_val=[]\n",
    "            learning_rate = learning_rate/10\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            print('Deviding the learning rate by 2. New learning rate: {:.6f}'.format(learning_rate))\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwNIJmkGn6-G"
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "trained_model = train_model(model, 0.01, features, authors, adj, indices, y, torch.tensor(val_indices).to(device), torch.tensor(y_val).to(device), epochs)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
