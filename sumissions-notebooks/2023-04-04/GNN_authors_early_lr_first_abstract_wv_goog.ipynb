{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l5GfuP1hlTmg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from random import random\n",
        "from random import randint\n",
        "from datetime import datetime\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from time import time\n",
        "from gensim.models import Word2Vec\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "#device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode\n",
        "#!pip install torch\n",
        "#==1.13.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myUuo4hey73G",
        "outputId": "2a13a84b-a354-4c44-823b-dfca6e0c3a3b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n"
      ],
      "metadata": {
        "id": "Cb1U3_4-Hdp4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "from random import randint\n",
        "from random import random\n",
        "from time import time\n",
        "from random import choice\n",
        "from gensim.models import Word2Vec\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from scipy.sparse import identity, diags\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "from unidecode import unidecode\n",
        "import pandas as pd\n",
        "from urllib.request import urlopen\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "def text_to_list(text):\n",
        "    return unidecode(text).split(',')\n",
        "\n",
        "def intersection(lst1, lst2): # a function that returns the number of common items of two lists and 1 or 0 if there are common. This function will be used in add_authors_to_pairs to add this features to the pairs.\n",
        "    lst3 = [value for value in lst1 if value in lst2]\n",
        "    is_common = 1 if len(lst3)>0 else 0\n",
        "    return len(lst3), is_common\n",
        "\n",
        "\n",
        "def save_subgraph_in_file(nbr_nodes, source_path='../input_data/edgelist.txt', destination_path='../input_data/small_edgelist.txt'):\n",
        "    G = nx.read_edgelist(source_path, delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
        "    G = G.subgraph(range(nbr_nodes))\n",
        "    nx.write_edgelist(G, path=destination_path, delimiter=',')\n",
        "    print(G.number_of_nodes(), 'nodes,', G.number_of_edges(), 'edges Graph extracted from', source_path[source_path.rfind('/')+1:])\n",
        "    G = nx.read_edgelist(destination_path, delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
        "    print(G.number_of_nodes(), 'nodes,', G.number_of_edges(), 'edges Graph saved in', destination_path[destination_path.rfind('/')+1:])\n",
        "    print(max(G.nodes))\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def read_train_val_graph(path='../input_data/edgelist.txt', val_ratio=0.1):\n",
        "    #gets the data from the file on the distant server\n",
        "    G = nx.read_edgelist(urlopen('https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/edgelist.txt'), delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
        "    nodes = list(G.nodes())\n",
        "    n = G.number_of_nodes()\n",
        "    m = G.number_of_edges()\n",
        "    edges = list(G.edges())\n",
        "\n",
        "    print('Number of nodes:', n, 'number of edges:', m,'in the Complete the set')\n",
        "\n",
        "    node_to_idx = dict()\n",
        "    for i, node in enumerate(nodes):\n",
        "        node_to_idx[node] = i\n",
        "\n",
        "    val_edges = list()\n",
        "    G_train = G.copy()\n",
        "\n",
        "    for edge in edges:\n",
        "        if random() < val_ratio and edge[0] < n and edge[1] < n:\n",
        "            val_edges.append(edge)\n",
        "            G_train.remove_edge(edge[0], edge[1]) # We remove the val edges from the graph G\n",
        "\n",
        "   \n",
        "    #for edge in val_edges:\n",
        "        \n",
        "\n",
        "    n = G_train.number_of_nodes()\n",
        "    m = G_train.number_of_edges()\n",
        "    train_edges = list(G_train.edges())\n",
        "\n",
        "    print('Number of nodes:', n, 'number of edges:', m, 'in the Training set')\n",
        "    print('len(nodes)', len(nodes))\n",
        "\n",
        "    y_val = [1]*len(val_edges)\n",
        "\n",
        "    n_val_edges = len(val_edges)\n",
        "    \n",
        "    print('Creating random val_edges...')\n",
        "    for i in range(n_val_edges):\n",
        "        n1 = nodes[randint(0, n-1)]\n",
        "        n2 = nodes[randint(0, n-1)]\n",
        "        (n1, n2) = (min(n1, n2), max(n1, n2))\n",
        "        while n2 >= n: #or (n1, n2) in train_edges:\n",
        "            if (n1, n2) in train_edges:\n",
        "                print((n1, n2), 'in train_edges:')\n",
        "            n1 = nodes[randint(0, n-1)]\n",
        "            n2 = nodes[randint(0, n-1)]\n",
        "            (n1, n2) = (min(n1, n2), max(n1, n2))\n",
        "        val_edges.append((n1, n2))\n",
        "\n",
        "    y_val.extend([0]*(n_val_edges))\n",
        "    \n",
        "    ### From Giannis /!\\\n",
        "    val_indices = np.zeros((2,len(val_edges)))\n",
        "    for i,edge in enumerate(val_edges):\n",
        "        val_indices[0,i] = node_to_idx[edge[0]]\n",
        "        val_indices[1,i] = node_to_idx[edge[1]]\n",
        "    \n",
        "    print('Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects')\n",
        "    print('Loaded from', path[path.rfind('/')+1:], 'and with a training validation split ratio =', val_ratio)\n",
        "    \n",
        "    \n",
        "    \n",
        "    return G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx\n",
        "\n",
        "def random_walk(G, node, walk_length):\n",
        "    walk = [node]\n",
        "  \n",
        "    for i in range(walk_length-1):\n",
        "        neibor_nodes = list(G.neighbors(walk[-1]))\n",
        "        if len(neibor_nodes) > 0:\n",
        "            next_node = choice(neibor_nodes)\n",
        "            walk.append(next_node)\n",
        "    walk = [node for node in walk] # in case the nodes are in string format, we don't need to cast into string, but if the nodes are in numeric or integer, we need this line to cast into string\n",
        "    return walk\n",
        "\n",
        "\n",
        "def generate_walks(G, num_walks, walk_length):\n",
        "  # Runs \"num_walks\" random walks from each node, and returns a list of all random walk\n",
        "    t = time()\n",
        "    print('Start generating walks....')\n",
        "    walks = list()  \n",
        "    for i in range(num_walks):\n",
        "        for node in G.nodes():\n",
        "            walk = random_walk(G, node, walk_length)\n",
        "            walks.append(walk)\n",
        "        #print('walks : ', walks)\n",
        "    print('Random walks generated in in {}s!'.format(round(time()-t)))\n",
        "    return walks\n",
        "\n",
        "def apply_word2vec_on_features(features, nodes, vector_size=128, window=5, min_count=0, sg=1, workers=8):\n",
        "    t = time()\n",
        "    print('Start applying Word2Vec...')\n",
        "    wv_model = Word2Vec(vector_size=vector_size, window=window, min_count=min_count, sg=sg, workers=workers)\n",
        "    wv_model.build_vocab(features)\n",
        "    wv_model.train(features, total_examples=wv_model.corpus_count, epochs=5) \n",
        "    print('Word2vec model trained on features in {} min!'.format(round((time()-t)/60)))\n",
        "    features_np = []\n",
        "    for node in nodes:\n",
        "        features_np.append(wv_model.wv[node])\n",
        "\n",
        "    features_np = np.array(features_np)\n",
        "    print(features_np.shape, 'features numpy array created in {} min!'.format(round((time()-t)/60)))\n",
        "    return features_np\n",
        "\n",
        "\n",
        "\n",
        "def normalize_adjacency(A):\n",
        "    n = A.shape[0]\n",
        "    A = A + identity(n)\n",
        "    degs = A.dot(np.ones(n))\n",
        "    inv_degs = np.power(degs, -1)\n",
        "    D_inv = diags(inv_degs)\n",
        "    A_hat = D_inv.dot(A)\n",
        "    return A_hat\n",
        "\n",
        "def create_and_normalize_adjacency(G):\n",
        "    adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix of the training graph\n",
        "    adj = normalize_adjacency(adj)\n",
        "    print('Created a normalized adjancency matrix of shape', adj.shape)\n",
        "    indices = np.array(adj.nonzero()) # Gets the positions of non zeros of adj into indices\n",
        "    print('Created indices', indices.shape, 'with the positions of non zeros in adj matrix')\n",
        "    return adj, indices\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)\n",
        " \n",
        "\n",
        "def add_authors_to_pairs (pairs, authors):\n",
        "    authors = pd.DataFrame(authors)\n",
        "    try: \n",
        "        pairs = pairs.detach().cpu().numpy()\n",
        "    except:\n",
        "        pass\n",
        "        \n",
        "\n",
        "    pairs_df = pd.DataFrame(np.transpose(pairs)).rename(columns={0: \"paper_1\", 1: \"paper_2\"})\n",
        "    #pairs = torch.tensor(pairs).to(device)\n",
        "    pairs_df = pairs_df.merge(authors, left_on='paper_1', right_on='paper_id', how='left').rename(columns={'authors': \"authors_1\"})\n",
        "    pairs_df = pairs_df.merge(authors, left_on='paper_2', right_on='paper_id', how='left').rename(columns={'authors': \"authors_2\"})\n",
        "    pairs_df.drop(['paper_id_x', 'paper_id_y'], axis=1, inplace=True)\n",
        "\n",
        "    pairs_df['nb_common_author'] = pairs_df.apply(lambda row: intersection(row['authors_1'], row['authors_2'])[0], axis=1)\n",
        "    pairs_df['is_common_author'] = pairs_df.apply(lambda row: intersection(row['authors_1'], row['authors_2'])[1], axis=1)\n",
        "\n",
        "    pairs_tensor = torch.LongTensor(np.transpose(pairs_df[[\"paper_1\", \"paper_2\", 'is_common_author', 'nb_common_author']].values.tolist()))\n",
        "    \n",
        "    return pairs_tensor\n"
      ],
      "metadata": {
        "id": "BJSlSwejljfp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "print('downloading word2vec google news 300...')\n",
        "t = time.time()\n",
        "wv = api.load('word2vec-google-news-300')\n",
        "print('Model downloaded in {} s'.format(round(time.time()-t)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbhiufK3kLG7",
        "outputId": "e569dacd-9cd3-4d2f-bd49-0a8a47cb22c3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading word2vec google news 300...\n",
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_w2v (abstract):\n",
        "    sum = np.zeros(300)\n",
        "    l = [len (abstract) if len (abstract)>0 else 1]\n",
        "    word_vect = np.zeros(300)\n",
        "\n",
        "    for i, word in enumerate(abstract):\n",
        "        try: \n",
        "            word_vect = wv[word]\n",
        "        except:\n",
        "            pass\n",
        "        sum = sum + word_vect                \n",
        "    return sum / l"
      ],
      "metadata": {
        "id": "fshtnsNBfkHy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "import re\n",
        "import numpy as np\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "\n",
        "abstracts_path = 'https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/abstracts.txt'\n",
        "abstracts = dict()\n",
        "f = urlopen(abstracts_path)\n",
        "\n",
        "for line in f:\n",
        "    node, abstract = str(line).lower().split('|--|')\n",
        "    abstract = remove_stopwords(abstract)\n",
        "    abstract = re.sub(r\"[,.;@#?!&$()-]\", \" \", abstract, flags=re.VERBOSE)\n",
        "    node = re.sub(\"[^0-9]\", \"\", node)\n",
        "    abstracts[int(node)] = abstract.split()[:-1]\n"
      ],
      "metadata": {
        "id": "QANntEbJYlTz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_mean_abstracts = dict()\n",
        "\n",
        "for node, abstract in enumerate(abstracts):\n",
        "    embedded_mean_abstracts[node] = mean_w2v(abstracts[node])\n"
      ],
      "metadata": {
        "id": "ziyf1qTCWJYK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx = read_train_val_graph(val_ratio=0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oe6DhotaSuqg",
        "outputId": "9abc6f50-76aa-42b5-ae9a-a0e72ab63f6b"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes: 138499 number of edges: 1091955 in the Complete the set\n",
            "Number of nodes: 138499 number of edges: 982371 in the Training set\n",
            "len(nodes) 138499\n",
            "Creating random val_edges...\n",
            "Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects\n",
            "Loaded from edgelist.txt and with a training validation split ratio = 0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_abstracts = list()  \n",
        "for node in G_train.nodes():\n",
        "    list_abstracts.append(embedded_mean_abstracts[node])\n",
        "\n",
        "print(np.shape(list_abstracts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z9k2xOw3DrQ",
        "outputId": "3e83dd29-9eed-4664-a371-e897be57c5b1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(138499, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adj, indices = create_and_normalize_adjacency(G_train)\n",
        "features_np = list_abstracts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyBibxd6zQhV",
        "outputId": "7302d77b-220b-4ad9-918b-a1b999f6d252"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created a normalized adjancency matrix of shape (138499, 138499)\n",
            "Created indices (2, 2104357) with the positions of non zeros in adj matrix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bf-_fYkF5xiO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pLOpK5p05Ywl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vWBRHuBNmrLJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "2xztNp1wZQUA",
        "outputId": "a6669383-3d78-4906-dc36-288369f6b99f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-6aaf1f276005>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '../input_data/edgelist.txt' #not used\n",
        "G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx = read_train_val_graph(val_ratio=0.1, path=path)\n",
        "walks = generate_walks(G=G_train, num_walks=10, walk_length=15)\n",
        "walks_wv = apply_word2vec_on_features(features=walks, nodes=nodes, vector_size=16)\n",
        "adj, indices = create_and_normalize_adjacency(G_train)\n",
        "\n",
        "features_np = walks_wv"
      ],
      "metadata": {
        "id": "oHyYv78NlkXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "authors = pd.read_csv(urlopen('https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/authors.txt'), sep = '|', header=None)\n",
        "authors = authors.rename(columns={0: \"paper_id\", 2: \"authors\"})\n",
        "authors['authors'] = authors['authors'].apply(text_to_list)\n",
        "authors = authors[[\"paper_id\", \"authors\"]]\n",
        "authors = authors[authors['paper_id'] <= max(G.nodes())]\n",
        "authors.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dWAzHqmxmYff",
        "outputId": "ed1a9255-3698-4cd0-8b6e-ed79f3df4e97"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   paper_id                                            authors\n",
              "0         0  [James H. Niblock, Jian-Xun Peng, Karen R. McM...\n",
              "1         1          [Jian-Xun Peng, Kang Li, De-Shuang Huang]\n",
              "2         2                                      [J. Heikkila]\n",
              "3         3    [L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]\n",
              "4         4  [Long Zhang, Kang Li, Er-Wei Bai, George W. Ir..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fae7593a-bd71-40a6-9b5b-7bc310141b60\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_id</th>\n",
              "      <th>authors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[James H. Niblock, Jian-Xun Peng, Karen R. McM...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[Jian-Xun Peng, Kang Li, De-Shuang Huang]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[J. Heikkila]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[Long Zhang, Kang Li, Er-Wei Bai, George W. Ir...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fae7593a-bd71-40a6-9b5b-7bc310141b60')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fae7593a-bd71-40a6-9b5b-7bc310141b60 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fae7593a-bd71-40a6-9b5b-7bc310141b60');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create class labels\n",
        "y = np.zeros(2*indices.shape[1])\n",
        "y[:indices.shape[1]] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
        "\n",
        "# Transforms the numpy matrices/vectors to torch tensors.\n",
        "features = torch.FloatTensor(features_np).to(device)\n",
        "y = torch.LongTensor(y).to(device)\n",
        "if type(adj) != torch.Tensor:\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
        "indices = torch.LongTensor(indices).to(device)\n",
        "val_indices = torch.LongTensor(val_indices).to(device)\n",
        "y_val = torch.LongTensor(y_val).to(device)\n",
        "\n",
        "del (G, G_train, train_edges, val_edges, nodes, abstracts, embedded_mean_abstracts)\n",
        "\n"
      ],
      "metadata": {
        "id": "ekTGAyTrneWx"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dmob3xD4pd5R"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GNN(nn.Module):\n",
        "    def __init__(self, n_feat, n_hidden, n_class, sub_class, dropout):\n",
        "        super(GNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(n_feat, n_hidden)\n",
        "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
        "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
        "        self.double_fc3 = nn.Linear((3*n_hidden), n_hidden)\n",
        "        self.fc4 = nn.Linear(n_hidden, sub_class)\n",
        "        self.fc5 = nn.Linear(sub_class, n_class)        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        \n",
        "\n",
        "    def forward(self, x_in, adj, pairs):\n",
        "        \n",
        "        h1 = self.fc1(x_in)\n",
        "        z1 = self.relu(torch.mm(adj, h1))\n",
        "        z1 = self.dropout(z1)\n",
        "        del(x_in, h1)\n",
        "\n",
        "        h2 = self.fc2(z1)\n",
        "        z2 = self.relu(torch.mm(adj, h2))\n",
        "        z2 = self.dropout(z2)\n",
        "        del(h2, z1, adj)\n",
        "\n",
        "        x = z2[pairs[0]] - z2[pairs[1]] # embedded features (z2) of node 0 - embedded features of node 1\n",
        "        \n",
        "        x = pairs[3][:, None] * x\n",
        "        x = x.to(device)\n",
        "        x1 = z2[pairs[0]]#.to(device)\n",
        "        x2 = z2[pairs[1]]#.to(device)\n",
        "        del(pairs)\n",
        "        x = torch.cat((x, x1, x2), dim=1)\n",
        "        del(x1, x2)\n",
        "        \n",
        "\n",
        "        x = self.relu(self.double_fc3(x))        \n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x = self.relu(self.fc4(x))\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x = self.fc5(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ROBXxQaZnfQ3"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def early_stopping(loss_train, list_loss_train, loss_val, list_loss_val, window=10, tolerance=0.01):\n",
        "    list_loss_val = list(list_loss_val)[-window:]\n",
        "    list_loss_train = list(list_loss_train)[-window:]\n",
        "    if (len(list_loss_val) == window and loss_val > (sum(list_loss_val)/len(list_loss_val)) and loss_train + tolerance < loss_val) or (len(list_loss_train) == window and loss_train > (sum(list_loss_train)/len(list_loss_train))):\n",
        "        print('train: {:.5f} val: {:.5f} mean val: {:.5f}'.format(loss_train, loss_val, (sum(list_loss_val)/len(list_loss_val))))\n",
        "        return True\n",
        "    return False\n",
        "    \n",
        "\n",
        "    \n",
        "def train_model(model, learning_rate, features, authors, adj, indices, y, val_indices, y_val, epochs, run_number):\n",
        "    # Train model\n",
        "    start_time = time()\n",
        "    print('Initializing the optimizer with learning rate:', learning_rate)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) #optimizer with halving learning rate in training\n",
        "    try: os.mkdir('./outputs')\n",
        "    except: pass\n",
        "    print('Preparing the data for training...')        \n",
        "    val_indices = add_authors_to_pairs(val_indices, authors) #we add the authors to val_pairs\n",
        "    indices = add_authors_to_pairs(indices, authors) #we add the authors to indices\n",
        "    rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)# We take random indices each time we run an epoch\n",
        "    rand_indices = add_authors_to_pairs(rand_indices, authors)\n",
        "    pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices. \n",
        "    pairs = torch.tensor(pairs).to(device)\n",
        "    del(authors, indices, rand_indices)\n",
        "\n",
        "    today = datetime.today().strftime('%Y-%m-%d-%H:%M')\n",
        "    list_loss_val = []\n",
        "    list_loss_train = []\n",
        "    \n",
        "    features = features.to(device)\n",
        "    adj = adj.to(device)\n",
        "    pairs = pairs.to(device)\n",
        "    \n",
        "    halving_lr = 0 # counter of the number of halving lr\n",
        "    print('Start training...')\n",
        "    for epoch in range(epochs):\n",
        "        t = time()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        model.train()\n",
        "\n",
        "        output = model(features, adj, pairs).to(device) # we run the model that gives the output.\n",
        "        loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
        "        acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
        "        loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
        "        optimizer.step() # Performs a single optimization step (parameter update).\n",
        "        \n",
        "        model.eval()\n",
        "        output = model(features, adj, val_indices).to(device)\n",
        "        #y_val = torch.LongTensor(y_val).to(device)\n",
        "        loss_val = F.nll_loss(output, y_val)\n",
        "        list_loss_val.append(loss_val.item())\n",
        "        list_loss_train.append(loss_train.item())\n",
        "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
        "        \n",
        "        if epoch % 5 == 0:\n",
        "            print('Epoch: {:03d}'.format(epoch+1),\n",
        "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "                  'time: {} s'.format(int(round(time()) - round(t))),\n",
        "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
        "        if epoch % 20 == 0:\n",
        "            model_path = \"outputs/{}-model-{}epochs-{}.pt\".format(today, epoch, run_number)\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            \n",
        "        early = early_stopping(loss_train.item(), list_loss_train, loss_val.item(), list_loss_val, window=10)        \n",
        "        if early:\n",
        "            halving_lr += 1\n",
        "            if halving_lr > 10:\n",
        "                break\n",
        "            list_loss_val=[]\n",
        "            learning_rate = learning_rate/2\n",
        "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "            print('Deviding the learning rate by 2. New learning rate: {:.6f}'.format(learning_rate))\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "bnRNz5z4nh6x"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_authors_to_pairs (pairs, authors):\n",
        "    #authors = pd.DataFrame(authors)\n",
        "    try: \n",
        "        pairs = pairs.detach().cpu().numpy()\n",
        "    except:\n",
        "        pass\n",
        "        \n",
        "\n",
        "    pairs_df = pd.DataFrame(np.transpose(pairs)).rename(columns={0: \"paper_1\", 1: \"paper_2\"})\n",
        "    #pairs = torch.tensor(pairs).to(device)\n",
        "    pairs_df = pairs_df.merge(authors, left_on='paper_1', right_on='paper_id', how='left').rename(columns={'authors': \"authors_1\"})\n",
        "    pairs_df = pairs_df.merge(authors, left_on='paper_2', right_on='paper_id', how='left').rename(columns={'authors': \"authors_2\"})\n",
        "    pairs_df.drop(['paper_id_x', 'paper_id_y'], axis=1, inplace=True)\n",
        "\n",
        "    pairs_df['nb_common_author'] = pairs_df.apply(lambda row: intersection(row['authors_1'], row['authors_2'])[0], axis=1)\n",
        "    pairs_df['is_common_author'] = pairs_df.apply(lambda row: intersection(row['authors_1'], row['authors_2'])[1], axis=1)\n",
        "\n",
        "    #pairs_tensor = torch.LongTensor(np.transpose(pairs_df[[\"paper_1\", \"paper_2\", 'is_common_author', 'nb_common_author']].values.tolist())).to(device)\n",
        "    \n",
        "    return np.transpose(pairs_df[[\"paper_1\", \"paper_2\", 'is_common_author', 'nb_common_author']].values.tolist())"
      ],
      "metadata": {
        "id": "V42H3MbEh6Ba"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_hidden = 64\n",
        "dropout_rate = 0.2\n",
        "sub_class = 8\n",
        "n_class = 2\n",
        "n_features = features.shape[1]\n",
        "\n",
        "# Creates the model\n",
        "model = GNN(n_features, n_hidden, n_class, sub_class, dropout_rate).to(device)"
      ],
      "metadata": {
        "id": "7i8UpGHuNhQp"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features.get_device()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Von8ehIosDPa",
        "outputId": "5f009e33-7459-487e-fd0f-06f57024a97a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "#import gc\n",
        "#del variables\n",
        "#gc.collect()\n",
        "\n",
        "epochs = 1000\n",
        "run_number = randint(0, 1000)\n",
        "\n",
        "\n",
        "trained_model = train_model(model, 0.02, features, authors, adj, indices, y, val_indices, y_val, epochs, run_number)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDQ_DCCJavQZ",
        "outputId": "3559e2b5-f3e1-474d-9053-0f32cf8d0608"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing the optimizer with learning rate: 0.02\n",
            "Preparing the data for training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-69-985e3742de27>:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  pairs = torch.tensor(pairs).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "Epoch: 001 loss_train: 0.2710 loss_val: 0.6674 acc_train: 0.8971 acc_val: 0.7534 time: 1 s total_time: 2 min\n",
            "Epoch: 006 loss_train: 0.3558 loss_val: 0.2758 acc_train: 0.8514 acc_val: 0.8895 time: 1 s total_time: 2 min\n",
            "Epoch: 011 loss_train: 0.3252 loss_val: 0.2746 acc_train: 0.8704 acc_val: 0.8919 time: 1 s total_time: 2 min\n",
            "Epoch: 016 loss_train: 0.3184 loss_val: 0.2651 acc_train: 0.8718 acc_val: 0.8972 time: 1 s total_time: 2 min\n",
            "Epoch: 021 loss_train: 0.3064 loss_val: 0.2578 acc_train: 0.8796 acc_val: 0.8962 time: 1 s total_time: 2 min\n",
            "Epoch: 026 loss_train: 0.2980 loss_val: 0.2525 acc_train: 0.8856 acc_val: 0.8992 time: 1 s total_time: 2 min\n",
            "Epoch: 031 loss_train: 0.2927 loss_val: 0.2460 acc_train: 0.8883 acc_val: 0.9021 time: 1 s total_time: 2 min\n",
            "Epoch: 036 loss_train: 0.2868 loss_val: 0.2434 acc_train: 0.8907 acc_val: 0.9036 time: 1 s total_time: 3 min\n",
            "Epoch: 041 loss_train: 0.2808 loss_val: 0.2340 acc_train: 0.8935 acc_val: 0.9087 time: 1 s total_time: 3 min\n",
            "Epoch: 046 loss_train: 0.2780 loss_val: 0.2307 acc_train: 0.8948 acc_val: 0.9103 time: 1 s total_time: 3 min\n",
            "Epoch: 051 loss_train: 0.2756 loss_val: 0.2294 acc_train: 0.8955 acc_val: 0.9109 time: 1 s total_time: 3 min\n",
            "Epoch: 056 loss_train: 0.2753 loss_val: 0.2277 acc_train: 0.8958 acc_val: 0.9113 time: 1 s total_time: 3 min\n",
            "Epoch: 061 loss_train: 0.2722 loss_val: 0.2267 acc_train: 0.8967 acc_val: 0.9115 time: 1 s total_time: 3 min\n",
            "train: 0.27337 val: 0.22621 mean val: 0.22677\n",
            "Deviding the learning rate by 2. New learning rate: 0.010000\n",
            "Epoch: 066 loss_train: 0.2721 loss_val: 0.3409 acc_train: 0.8966 acc_val: 0.8697 time: 1 s total_time: 3 min\n",
            "train: 0.35771 val: 0.21379 mean val: 0.27736\n",
            "Deviding the learning rate by 2. New learning rate: 0.005000\n",
            "train: 0.28676 val: 0.26430 mean val: 0.26430\n",
            "Deviding the learning rate by 2. New learning rate: 0.002500\n",
            "train: 0.29368 val: 0.23266 mean val: 0.23266\n",
            "Deviding the learning rate by 2. New learning rate: 0.001250\n",
            "Epoch: 071 loss_train: 0.2768 loss_val: 0.2228 acc_train: 0.8947 acc_val: 0.9130 time: 1 s total_time: 3 min\n",
            "Epoch: 076 loss_train: 0.2724 loss_val: 0.2284 acc_train: 0.8964 acc_val: 0.9108 time: 1 s total_time: 3 min\n",
            "Epoch: 081 loss_train: 0.2714 loss_val: 0.2280 acc_train: 0.8967 acc_val: 0.9106 time: 1 s total_time: 3 min\n",
            "Epoch: 086 loss_train: 0.2718 loss_val: 0.2273 acc_train: 0.8967 acc_val: 0.9111 time: 1 s total_time: 3 min\n",
            "train: 0.27184 val: 0.22727 mean val: 0.22793\n",
            "Deviding the learning rate by 2. New learning rate: 0.000625\n",
            "train: 0.27259 val: 0.22663 mean val: 0.22687\n",
            "Deviding the learning rate by 2. New learning rate: 0.000313\n",
            "Epoch: 091 loss_train: 0.2705 loss_val: 0.2252 acc_train: 0.8970 acc_val: 0.9121 time: 1 s total_time: 4 min\n",
            "train: 0.27138 val: 0.22511 mean val: 0.22529\n",
            "Deviding the learning rate by 2. New learning rate: 0.000156\n",
            "Epoch: 096 loss_train: 0.2695 loss_val: 0.2263 acc_train: 0.8977 acc_val: 0.9116 time: 1 s total_time: 4 min\n",
            "Epoch: 101 loss_train: 0.2703 loss_val: 0.2260 acc_train: 0.8974 acc_val: 0.9117 time: 1 s total_time: 4 min\n",
            "train: 0.27047 val: 0.22594 mean val: 0.22606\n",
            "Deviding the learning rate by 2. New learning rate: 0.000078\n",
            "Epoch: 106 loss_train: 0.2696 loss_val: 0.2256 acc_train: 0.8978 acc_val: 0.9119 time: 1 s total_time: 4 min\n",
            "train: 0.26977 val: 0.22546 mean val: 0.22567\n",
            "Deviding the learning rate by 2. New learning rate: 0.000039\n",
            "train: 0.27160 val: 0.22566 mean val: 0.22566\n",
            "Deviding the learning rate by 2. New learning rate: 0.000020\n",
            "Epoch: 111 loss_train: 0.2706 loss_val: 0.2256 acc_train: 0.8971 acc_val: 0.9118 time: 1 s total_time: 4 min\n",
            "train: 0.27059 val: 0.22561 mean val: 0.22562\n",
            "Optimization Finished in 4 min!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.transpose(node_pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfvPqy79pGEW",
        "outputId": "12ca7991-4d4b-4b3f-d47d-d103826d6788"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 32519,  45898,  26751, ...,  93184,  18331,  26561],\n",
              "       [ 24241,  73020,   2239, ..., 118019,  18266,  58313]])"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "test_path = 'https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/test.txt'\n",
        "node_pairs = list()\n",
        "f = urlopen(test_path)\n",
        "\n",
        "for line in f:\n",
        "    t = str(line).split(',')\n",
        "    t[0] = int(re.sub(\"[^0-9]\", \"\", t[0]))\n",
        "    t[1] = int(re.sub(\"[^0-9]\", \"\", t[1]))\n",
        "    node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
        "\n",
        "node_pairs = np.transpose(node_pairs)\n",
        "node_pairs = add_authors_to_pairs(node_pairs, authors)\n",
        "node_pairs = torch.LongTensor(node_pairs).to(device)\n",
        "\n",
        "test_output = model(features, adj, node_pairs)\n",
        "y_pred = torch.exp(test_output)\n",
        "y_pred = y_pred.detach().cpu().numpy()\n",
        "\n",
        "y_pred_true = list()\n",
        "for element in y_pred:\n",
        "    y_pred_true.append(element[1])\n",
        "\n",
        "today = datetime.today().strftime('%Y-%m-%d')\n",
        "random_nb = randint(0, 1000)\n",
        "model_nb = 1\n",
        "\n",
        "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
        "\"{}-submission-{}-{}.csv\".format(today, model_nb, random_nb), header=True, index=True, index_label='id'\n",
        ")"
      ],
      "metadata": {
        "id": "cmFO4qI7_245"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "model.eval()\n",
        "node_pairs = np.array(np.transpose(node_pairs))\n",
        "node_pairs = torch.LongTensor(node_pairs).to(device)\n",
        "\n",
        "test_output = model(features, adj, node_pairs)\n",
        "y_pred = torch.exp(test_output)\n",
        "y_pred = y_pred.detach().cpu().numpy()\n",
        "\n",
        "y_pred_true = list()\n",
        "for element in y_pred:\n",
        "    y_pred_true.append(element[1])\n",
        "    \n",
        "\n",
        "    \n",
        "today = datetime.today().strftime('%Y-%m-%d')\n",
        "random_nb = randint(0, 1000)\n",
        "\n",
        "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
        "\"{}-submission-{}-{}.csv\".format(today, model_nb, random_nb), header=True, index=True, index_label='id'\n",
        ")"
      ],
      "metadata": {
        "id": "h2UWL4y-K1jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1000\n",
        "\n",
        "trained_model = train_model(model, 0.01, features, authors, adj, indices, y, torch.tensor(val_indices).to(device), torch.tensor(y_val).to(device), epochs)\n"
      ],
      "metadata": {
        "id": "dwNIJmkGn6-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(features), type(adj), type(indices), type(y))\n",
        "print(features.get_device(), adj.get_device(), indices.get_device(), y.get_device())\n",
        "\n",
        "#torch.tensor(np.array(authors)).to(device).get_device()\n",
        "authors\n",
        "print(type(torch.tensor(val_indices)))\n",
        "print(torch.tensor(val_indices).to(device).get_device())\n",
        "\n",
        "type(y_val)\n",
        "\n",
        "####### randindicies to check on which device"
      ],
      "metadata": {
        "id": "1OSAu20O0zri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print (torch.__version__)\n",
        "!pip install torchvision==0.14.0\n",
        "!pip install torchtext==0.14.0\n",
        "!pip install torchaudio==0.13.0\n",
        "!pip install torch==1.13.0\n",
        "import torch\n",
        "print (torch.__version__)"
      ],
      "metadata": {
        "id": "t_5ih-4EQQqW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}