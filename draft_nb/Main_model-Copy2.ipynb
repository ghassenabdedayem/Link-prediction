{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787a2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import random_walk, generate_walks, read_train_val_graph, sparse_mx_to_torch_sparse_tensor\n",
    "from utils import apply_word2vec_on_features, create_and_normalize_adjacency, save_subgraph_in_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d30a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d25112fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e873d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_val_graph(path='../input_data/edgelist.txt', val_ratio=0.1):\n",
    "    G = nx.read_edgelist(path, delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "    nodes = list(G.nodes())\n",
    "    n = G.number_of_nodes()\n",
    "    m = G.number_of_edges()\n",
    "    edges = list(G.edges())\n",
    "\n",
    "    print('Number of nodes:', n, 'number of edges:', m,'in All the set')\n",
    "\n",
    "    node_to_idx = dict()\n",
    "    for i, node in enumerate(nodes):\n",
    "        node_to_idx[node] = i\n",
    "\n",
    "    val_edges = list()\n",
    "    G_train = G\n",
    "\n",
    "    for edge in edges:\n",
    "        if random() < val_ratio and edge[0] < n and edge[1] < n:\n",
    "            val_edges.append(edge)\n",
    "\n",
    "    # We remove the val edges from the graph G\n",
    "    for edge in val_edges:\n",
    "        G_train.remove_edge(edge[0], edge[1])\n",
    "\n",
    "    n = G_train.number_of_nodes()\n",
    "    m = G_train.number_of_edges()\n",
    "    train_edges = list(G_train.edges())\n",
    "\n",
    "    print('Number of nodes:', n, 'number of edges:', m, 'in the Training set')\n",
    "    print('len(nodes)', len(nodes))\n",
    "\n",
    "    y_val = [1]*len(val_edges)\n",
    "\n",
    "    n_val_edges = len(val_edges)\n",
    "    \n",
    "    print('Creating random val_edges...')\n",
    "    for i in range(n_val_edges):\n",
    "        n1 = nodes[randint(0, n-1)]\n",
    "        n2 = nodes[randint(0, n-1)]\n",
    "        (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "        #I added the next secion to handle nodes that are out of range when I work with a subgraph to test quickly\n",
    "        while n2 > n: #or (n1, n2) in train_edges:\n",
    "            n1 = nodes[randint(0, n-1)]\n",
    "            n2 = nodes[randint(0, n-1)]\n",
    "            (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "        val_edges.append((n1, n2))\n",
    "\n",
    "\n",
    "    y_val.extend([0]*(n_val_edges))\n",
    "    \n",
    "    print('Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects')\n",
    "    print('Loaded from', path[path.rfind('/')+1:], 'and with a training validation split ratio =', val_ratio)\n",
    "    \n",
    "    return G_train, train_edges, val_edges, y_val, nodes, node_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94769d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(np.transpose(val_edges)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e894f370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499 number of edges: 1091955 in All the set\n",
      "Number of nodes: 138499 number of edges: 983215 in the Training set\n",
      "len(nodes) 138499\n",
      "Creating random val_edges...\n",
      "Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects\n",
      "Loaded from edgelist.txt and with a training validation split ratio = 0.1\n",
      "Start generating walks....\n",
      "Random walks generated in in 70s!\n",
      "Start applying Word2Vec...\n",
      "Word2vec model trained on features in 3 min!\n",
      "(138499, 128) features numpy array created in 3 min!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghassenabdedayem/Documents/Data/Polytechnique/5- Data Challenge/data_challenge_2022/draft_nb/utils.py:147: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix of the training graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a normalized adjancency matrix of shape (138499, 138499)\n",
      "Created indices (2, 2104929) with the positions of non zeros in adj matrix\n"
     ]
    }
   ],
   "source": [
    "path = '../input_data/edgelist.txt'\n",
    "\n",
    "G_train, train_edges, val_edges, y_val, nodes, node_to_idx = read_train_val_graph(val_ratio=0.1, path=path)\n",
    "walks = generate_walks(G=G_train, num_walks=10, walk_length=15)\n",
    "features_np = apply_word2vec_on_features(features=walks, nodes=nodes)\n",
    "adj, indices = create_and_normalize_adjacency(G_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3bf540ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class labels\n",
    "y = np.zeros(2*indices.shape[1])\n",
    "y[:indices.shape[1]] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c20a3980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms the numpy matrices/vectors to torch tensors.\n",
    "features = torch.FloatTensor(features_np).to(device)\n",
    "y = torch.LongTensor(y).to(device)\n",
    "if type(adj) != torch.Tensor:\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "indices = torch.LongTensor(indices).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c48af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_feat, n_hidden, n_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.double_fc3 = nn.Linear((2*n_hidden), n_hidden)\n",
    "        self.fc4 = nn.Linear(n_hidden, n_class)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x_in, adj, pairs):\n",
    "        \n",
    "        h1 = self.fc1(x_in)\n",
    "        z1 = self.relu(torch.mm(adj, h1))\n",
    "        z1 = self.dropout(z1)\n",
    "\n",
    "        h2 = self.fc2(z1)\n",
    "        z2 = self.relu(torch.mm(adj, h2))\n",
    "\n",
    "        #x = z2[pairs[0,:],:] - z2[pairs[1,:],:] # embedded features (z2) of node 0 - embedded features of node 1\n",
    "        x1 = z2[pairs[0]]\n",
    "        x2 = z2[pairs[1]]\n",
    "        # could we add a new dimension to pairs to specify if same author(s)? and then what could we do?\n",
    "              \n",
    "        x = torch.cat((x1, x2), dim=1)        \n",
    "        x = self.relu(self.double_fc3(x))        \n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7eb8d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "n_hidden = 128\n",
    "dropout_rate = 0.2\n",
    "n_class = 2\n",
    "n_features = features.shape[1]\n",
    "\n",
    "# Creates the model and specifies the optimizer\n",
    "model = GNN(n_features, n_hidden, n_class, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "033efbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 0.6937 acc_train: 0.5000 time: 36.8019 s total_time: 1 min\n",
      "Epoch: 006 loss_train: 0.6661 acc_train: 0.6584 time: 26.8333 s total_time: 3 min\n",
      "Epoch: 011 loss_train: 0.5805 acc_train: 0.6776 time: 26.8028 s total_time: 5 min\n",
      "Epoch: 016 loss_train: 0.4668 acc_train: 0.7837 time: 25.6495 s total_time: 8 min\n",
      "Epoch: 021 loss_train: 0.4163 acc_train: 0.8175 time: 26.5373 s total_time: 10 min\n",
      "Epoch: 026 loss_train: 0.3964 acc_train: 0.8271 time: 27.8933 s total_time: 12 min\n",
      "Epoch: 031 loss_train: 0.3858 acc_train: 0.8342 time: 28.1627 s total_time: 14 min\n",
      "Epoch: 036 loss_train: 0.3716 acc_train: 0.8395 time: 28.2425 s total_time: 17 min\n",
      "Epoch: 041 loss_train: 0.3572 acc_train: 0.8439 time: 24.3255 s total_time: 19 min\n",
      "Epoch: 046 loss_train: 0.3482 acc_train: 0.8488 time: 24.2366 s total_time: 21 min\n",
      "Epoch: 051 loss_train: 0.3369 acc_train: 0.8538 time: 24.5765 s total_time: 23 min\n",
      "Epoch: 056 loss_train: 0.3244 acc_train: 0.8617 time: 24.9516 s total_time: 25 min\n",
      "Epoch: 061 loss_train: 0.3053 acc_train: 0.8725 time: 22.9794 s total_time: 27 min\n",
      "Epoch: 066 loss_train: 0.2927 acc_train: 0.8815 time: 23.2770 s total_time: 29 min\n",
      "Epoch: 071 loss_train: 0.2781 acc_train: 0.8876 time: 23.2426 s total_time: 31 min\n",
      "Epoch: 076 loss_train: 0.2526 acc_train: 0.9004 time: 24.0516 s total_time: 32 min\n",
      "Epoch: 081 loss_train: 0.2373 acc_train: 0.9060 time: 25.3570 s total_time: 34 min\n",
      "Epoch: 086 loss_train: 0.2263 acc_train: 0.9104 time: 23.8008 s total_time: 36 min\n",
      "Epoch: 091 loss_train: 0.2298 acc_train: 0.9095 time: 23.4036 s total_time: 38 min\n",
      "Epoch: 096 loss_train: 0.2305 acc_train: 0.9087 time: 23.8143 s total_time: 40 min\n",
      "Epoch: 101 loss_train: 0.2145 acc_train: 0.9158 time: 23.9306 s total_time: 42 min\n",
      "Epoch: 106 loss_train: 0.2111 acc_train: 0.9173 time: 24.1211 s total_time: 44 min\n",
      "Epoch: 111 loss_train: 0.2047 acc_train: 0.9200 time: 26.5329 s total_time: 47 min\n",
      "Epoch: 116 loss_train: 0.2026 acc_train: 0.9210 time: 24.2161 s total_time: 49 min\n",
      "Epoch: 121 loss_train: 0.2009 acc_train: 0.9218 time: 24.7745 s total_time: 51 min\n",
      "Epoch: 126 loss_train: 0.1954 acc_train: 0.9243 time: 23.6150 s total_time: 53 min\n",
      "Epoch: 131 loss_train: 0.1900 acc_train: 0.9267 time: 23.7773 s total_time: 55 min\n",
      "Epoch: 136 loss_train: 0.1952 acc_train: 0.9243 time: 24.5463 s total_time: 57 min\n",
      "Epoch: 141 loss_train: 0.1945 acc_train: 0.9251 time: 24.9196 s total_time: 59 min\n",
      "Epoch: 146 loss_train: 0.1794 acc_train: 0.9315 time: 24.1079 s total_time: 61 min\n",
      "Epoch: 151 loss_train: 0.1783 acc_train: 0.9317 time: 26.1543 s total_time: 63 min\n",
      "Epoch: 156 loss_train: 0.1783 acc_train: 0.9320 time: 24.9553 s total_time: 65 min\n",
      "Epoch: 161 loss_train: 0.1729 acc_train: 0.9340 time: 25.0025 s total_time: 67 min\n",
      "Epoch: 166 loss_train: 0.1623 acc_train: 0.9384 time: 23.7410 s total_time: 69 min\n",
      "Epoch: 171 loss_train: 0.1572 acc_train: 0.9402 time: 25.2520 s total_time: 71 min\n",
      "Epoch: 176 loss_train: 0.1505 acc_train: 0.9428 time: 24.8942 s total_time: 73 min\n",
      "Epoch: 181 loss_train: 0.1828 acc_train: 0.9302 time: 23.8820 s total_time: 75 min\n",
      "Epoch: 186 loss_train: 0.1509 acc_train: 0.9432 time: 28.1014 s total_time: 77 min\n",
      "Epoch: 191 loss_train: 0.1567 acc_train: 0.9406 time: 23.8362 s total_time: 79 min\n",
      "Epoch: 196 loss_train: 0.1431 acc_train: 0.9456 time: 24.8721 s total_time: 82 min\n",
      "Optimization Finished in 83 min!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Train model\n",
    "model.train()\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    t = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)# We take random indices each time we run an epoch\n",
    "    pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices.   \n",
    "    output = model(features, adj, pairs) # we run the model that gives the output.\n",
    "    loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "    #print(type(loss_train), '\\n', loss_train.shape)\n",
    "    acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "    loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "    optimizer.step() # Performs a single optimization step (parameter update).\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "              'time: {:.4f} s'.format(time.time() - t),\n",
    "             'total_time: {} min'.format(round((time.time() - start_time)/60)))\n",
    "\n",
    "print(\"Optimization Finished in {} min!\".format(round((time.time() - start_time)/60)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b650a15",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluating the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      3\u001b[0m eval_pairs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(np\u001b[38;5;241m.\u001b[39mtranspose(val_edges))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(eval_pairs\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "model.eval()\n",
    "eval_pairs = np.array(np.transpose(val_edges))\n",
    "print(eval_pairs.shape)\n",
    "eval_pairs = torch.LongTensor(eval_pairs).to(device)\n",
    "print(eval_pairs.shape)\n",
    "eval_output = model(features, adj, eval_pairs)\n",
    "print(eval_output.shape)\n",
    "y_pred = torch.exp(eval_output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "    \n",
    "print('Log loss:', log_loss(y_val, y_pred_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7bcd7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('../test_data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "output = model(features, adj, pairs)\n",
    "y_pred = torch.exp(output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"../submissions_files/{}-submission-{}.csv\".format(today, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
