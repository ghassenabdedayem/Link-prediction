{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "787a2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import random_walk, generate_walks, sparse_mx_to_torch_sparse_tensor \n",
    "from utils import text_to_list, intersection, read_train_val_graph, save_subgraph_in_file\n",
    "from utils import apply_word2vec_on_features, create_and_normalize_adjacency, train_model, add_authors_to_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d25112fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02ef54ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499 number of edges: 1091955 in the Complete the set\n",
      "Number of nodes: 138499 number of edges: 983042 in the Training set\n",
      "len(nodes) 138499\n",
      "Creating random val_edges...\n",
      "Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects\n",
      "Loaded from edgelist.txt and with a training validation split ratio = 0.1\n"
     ]
    }
   ],
   "source": [
    "path = '../input_data/edgelist.txt'\n",
    "G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx = read_train_val_graph(val_ratio=0.1, path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bbcfb057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello 3\n"
     ]
    }
   ],
   "source": [
    "print('hello {}'.format(int(float(3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a94ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../input_data/edgelist.txt'\n",
    "G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx = read_train_val_graph(val_ratio=0.1, path=path)\n",
    "walks = generate_walks(G=G_train, num_walks=10, walk_length=15)\n",
    "walks_wv = apply_word2vec_on_features(features=walks, nodes=nodes, vector_size=64)\n",
    "adj, indices = create_and_normalize_adjacency(G_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f55cbc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CountVectorizer in module pyspark.ml.feature:\n",
      "\n",
      "class CountVectorizer(pyspark.ml.wrapper.JavaEstimator, _CountVectorizerParams, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.JavaMLWritable)\n",
      " |  CountVectorizer(*, minTF=1.0, minDF=1.0, maxDF=9223372036854775807, vocabSize=262144, binary=False, inputCol=None, outputCol=None)\n",
      " |  \n",
      " |  Extracts a vocabulary from document collections and generates a :py:attr:`CountVectorizerModel`.\n",
      " |  \n",
      " |  .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> df = spark.createDataFrame(\n",
      " |  ...    [(0, [\"a\", \"b\", \"c\"]), (1, [\"a\", \"b\", \"b\", \"c\", \"a\"])],\n",
      " |  ...    [\"label\", \"raw\"])\n",
      " |  >>> cv = CountVectorizer()\n",
      " |  >>> cv.setInputCol(\"raw\")\n",
      " |  CountVectorizer...\n",
      " |  >>> cv.setOutputCol(\"vectors\")\n",
      " |  CountVectorizer...\n",
      " |  >>> model = cv.fit(df)\n",
      " |  >>> model.setInputCol(\"raw\")\n",
      " |  CountVectorizerModel...\n",
      " |  >>> model.transform(df).show(truncate=False)\n",
      " |  +-----+---------------+-------------------------+\n",
      " |  |label|raw            |vectors                  |\n",
      " |  +-----+---------------+-------------------------+\n",
      " |  |0    |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      " |  |1    |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      " |  +-----+---------------+-------------------------+\n",
      " |  ...\n",
      " |  >>> sorted(model.vocabulary) == ['a', 'b', 'c']\n",
      " |  True\n",
      " |  >>> countVectorizerPath = temp_path + \"/count-vectorizer\"\n",
      " |  >>> cv.save(countVectorizerPath)\n",
      " |  >>> loadedCv = CountVectorizer.load(countVectorizerPath)\n",
      " |  >>> loadedCv.getMinDF() == cv.getMinDF()\n",
      " |  True\n",
      " |  >>> loadedCv.getMinTF() == cv.getMinTF()\n",
      " |  True\n",
      " |  >>> loadedCv.getVocabSize() == cv.getVocabSize()\n",
      " |  True\n",
      " |  >>> modelPath = temp_path + \"/count-vectorizer-model\"\n",
      " |  >>> model.save(modelPath)\n",
      " |  >>> loadedModel = CountVectorizerModel.load(modelPath)\n",
      " |  >>> loadedModel.vocabulary == model.vocabulary\n",
      " |  True\n",
      " |  >>> loadedModel.transform(df).take(1) == model.transform(df).take(1)\n",
      " |  True\n",
      " |  >>> fromVocabModel = CountVectorizerModel.from_vocabulary([\"a\", \"b\", \"c\"],\n",
      " |  ...     inputCol=\"raw\", outputCol=\"vectors\")\n",
      " |  >>> fromVocabModel.transform(df).show(truncate=False)\n",
      " |  +-----+---------------+-------------------------+\n",
      " |  |label|raw            |vectors                  |\n",
      " |  +-----+---------------+-------------------------+\n",
      " |  |0    |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      " |  |1    |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      " |  +-----+---------------+-------------------------+\n",
      " |  ...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CountVectorizer\n",
      " |      pyspark.ml.wrapper.JavaEstimator\n",
      " |      _CountVectorizerParams\n",
      " |      pyspark.ml.wrapper.JavaParams\n",
      " |      pyspark.ml.wrapper.JavaWrapper\n",
      " |      pyspark.ml.base.Estimator\n",
      " |      pyspark.ml.param.shared.HasInputCol\n",
      " |      pyspark.ml.param.shared.HasOutputCol\n",
      " |      pyspark.ml.param.Params\n",
      " |      pyspark.ml.util.Identifiable\n",
      " |      pyspark.ml.util.JavaMLReadable\n",
      " |      pyspark.ml.util.MLReadable\n",
      " |      pyspark.ml.util.JavaMLWritable\n",
      " |      pyspark.ml.util.MLWritable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, minTF=1.0, minDF=1.0, maxDF=9223372036854775807, vocabSize=262144, binary=False, inputCol=None, outputCol=None)\n",
      " |      __init__(self, \\*, minTF=1.0, minDF=1.0, maxDF=2 ** 63 - 1, vocabSize=1 << 18,                 binary=False, inputCol=None,outputCol=None)\n",
      " |  \n",
      " |  setBinary(self, value)\n",
      " |      Sets the value of :py:attr:`binary`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  setInputCol(self, value)\n",
      " |      Sets the value of :py:attr:`inputCol`.\n",
      " |  \n",
      " |  setMaxDF(self, value)\n",
      " |      Sets the value of :py:attr:`maxDF`.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |  \n",
      " |  setMinDF(self, value)\n",
      " |      Sets the value of :py:attr:`minDF`.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  setMinTF(self, value)\n",
      " |      Sets the value of :py:attr:`minTF`.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  setOutputCol(self, value)\n",
      " |      Sets the value of :py:attr:`outputCol`.\n",
      " |  \n",
      " |  setParams(self, *, minTF=1.0, minDF=1.0, maxDF=9223372036854775807, vocabSize=262144, binary=False, inputCol=None, outputCol=None)\n",
      " |      setParams(self, \\*, minTF=1.0, minDF=1.0, maxDF=2 ** 63 - 1, vocabSize=1 << 18,                  binary=False, inputCol=None, outputCol=None)\n",
      " |      Set the params for the CountVectorizer\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  setVocabSize(self, value)\n",
      " |      Sets the value of :py:attr:`vocabSize`.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _CountVectorizerParams:\n",
      " |  \n",
      " |  getBinary(self)\n",
      " |      Gets the value of binary or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  getMaxDF(self)\n",
      " |      Gets the value of maxDF or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |  \n",
      " |  getMinDF(self)\n",
      " |      Gets the value of minDF or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  getMinTF(self)\n",
      " |      Gets the value of minTF or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  getVocabSize(self)\n",
      " |      Gets the value of vocabSize or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from _CountVectorizerParams:\n",
      " |  \n",
      " |  binary = Param(parent='undefined', name='binary', doc='Bi...vents rath...\n",
      " |  \n",
      " |  maxDF = Param(parent='undefined', name='maxDF', doc='Spe...ts the term...\n",
      " |  \n",
      " |  minDF = Param(parent='undefined', name='minDF', doc='Spe...pecifies th...\n",
      " |  \n",
      " |  minTF = Param(parent='undefined', name='minTF', doc=\"Fil...rModel and ...\n",
      " |  \n",
      " |  vocabSize = Param(parent='undefined', name='vocabSize', doc='max size ...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n",
      " |  \n",
      " |  clear(self, param)\n",
      " |      Clears a param from the param map if it has been explicitly set.\n",
      " |  \n",
      " |  copy(self, extra=None)\n",
      " |      Creates a copy of this instance with the same uid and some\n",
      " |      extra params. This implementation first calls Params.copy and\n",
      " |      then make a copy of the companion Java pipeline component with\n",
      " |      extra params. So both the Python wrapper and the Java pipeline\n",
      " |      component get copied.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      extra : dict, optional\n",
      " |          Extra parameters to copy to the new instance\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`JavaParams`\n",
      " |          Copy of this instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.base.Estimator:\n",
      " |  \n",
      " |  fit(self, dataset, params=None)\n",
      " |      Fits a model to the input dataset with optional parameters.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset : :py:class:`pyspark.sql.DataFrame`\n",
      " |          input dataset.\n",
      " |      params : dict or list or tuple, optional\n",
      " |          an optional param map that overrides embedded params. If a list/tuple of\n",
      " |          param maps is given, this calls fit on each param map and returns a list of\n",
      " |          models.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`Transformer` or a list of :py:class:`Transformer`\n",
      " |          fitted model(s)\n",
      " |  \n",
      " |  fitMultiple(self, dataset, paramMaps)\n",
      " |      Fits a model to the input dataset for each param map in `paramMaps`.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset : :py:class:`pyspark.sql.DataFrame`\n",
      " |          input dataset.\n",
      " |      paramMaps : :py:class:`collections.abc.Sequence`\n",
      " |          A Sequence of param maps.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`_FitMultipleIterator`\n",
      " |          A thread safe iterable which contains one model for each param map. Each\n",
      " |          call to `next(modelIterator)` will return `(index, model)` where model was fit\n",
      " |          using `paramMaps[index]`. `index` values may not be sequential.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasInputCol:\n",
      " |  \n",
      " |  getInputCol(self)\n",
      " |      Gets the value of inputCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasInputCol:\n",
      " |  \n",
      " |  inputCol = Param(parent='undefined', name='inputCol', doc='input colum...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasOutputCol:\n",
      " |  \n",
      " |  getOutputCol(self)\n",
      " |      Gets the value of outputCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasOutputCol:\n",
      " |  \n",
      " |  outputCol = Param(parent='undefined', name='outputCol', doc='output co...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  explainParam(self, param)\n",
      " |      Explains a single param and returns its name, doc, and optional\n",
      " |      default value and user-supplied value in a string.\n",
      " |  \n",
      " |  explainParams(self)\n",
      " |      Returns the documentation of all params with their optionally\n",
      " |      default values and user-supplied values.\n",
      " |  \n",
      " |  extractParamMap(self, extra=None)\n",
      " |      Extracts the embedded default param values and user-supplied\n",
      " |      values, and then merges them with extra values from input into\n",
      " |      a flat param map, where the latter value is used if there exist\n",
      " |      conflicts, i.e., with ordering: default param values <\n",
      " |      user-supplied values < extra.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      extra : dict, optional\n",
      " |          extra param values\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |          merged param map\n",
      " |  \n",
      " |  getOrDefault(self, param)\n",
      " |      Gets the value of a param in the user-supplied param map or its\n",
      " |      default value. Raises an error if neither is set.\n",
      " |  \n",
      " |  getParam(self, paramName)\n",
      " |      Gets a param by its name.\n",
      " |  \n",
      " |  hasDefault(self, param)\n",
      " |      Checks whether a param has a default value.\n",
      " |  \n",
      " |  hasParam(self, paramName)\n",
      " |      Tests whether this instance contains a param with a given\n",
      " |      (string) name.\n",
      " |  \n",
      " |  isDefined(self, param)\n",
      " |      Checks whether a param is explicitly set by user or has\n",
      " |      a default value.\n",
      " |  \n",
      " |  isSet(self, param)\n",
      " |      Checks whether a param is explicitly set by user.\n",
      " |  \n",
      " |  set(self, param, value)\n",
      " |      Sets a parameter in the embedded param map.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  params\n",
      " |      Returns all params ordered by name. The default implementation\n",
      " |      uses :py:func:`dir` to get all attributes of type\n",
      " |      :py:class:`Param`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.Identifiable:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n",
      " |  \n",
      " |  read() from abc.ABCMeta\n",
      " |      Returns an MLReader instance for this class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.MLReadable:\n",
      " |  \n",
      " |  load(path) from abc.ABCMeta\n",
      " |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n",
      " |  \n",
      " |  write(self)\n",
      " |      Returns an MLWriter instance for this ML instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.MLWritable:\n",
      " |  \n",
      " |  save(self, path)\n",
      " |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "def multi_label_binarizer(df, labels_col='labels', output_col='new_labels'):\n",
    "    \"\"\"\n",
    "    Function that takes as input:\n",
    "    - `df`, pyspark.sql.dataframe \n",
    "    - `labels_col`, string that indicates an array column containing labels\n",
    "    - `output_col`, string that indicates the name of the new labels column\n",
    "    \n",
    "    and returns a multi-label binarized column.\n",
    "    \"\"\"\n",
    "    \n",
    "    # get set of unique labels and sort them\n",
    "    labels_set = df\\\n",
    "        .withColumn('exploded', F.explode('labels'))\\\n",
    "        .agg(F.collect_set('exploded'))\\\n",
    "        .collect()[0][0]\n",
    "    labels_set = sorted(labels_set)\n",
    "    \n",
    "    # dynamically create columns for each value in `labels_set`\n",
    "    for i in labels_set:\n",
    "        df = df.withColumn(i, F.when(F.array_contains(labels_col, i), 1).otherwise(0))\n",
    "        \n",
    "    # create new, multi-label binarized array column\n",
    "    df = df.withColumn(output_col, F.array(*labels_set))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "312fbfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------------------------------------------------------+----------------------------------------------+\n",
      "|paper_id|authors                                                              |vectors                                       |\n",
      "+--------+---------------------------------------------------------------------+----------------------------------------------+\n",
      "|0       |[James H. Niblock, Jian-Xun Peng, Karen R. McMenemy, George W. Irwin]|(2577,[99,147,1770,2454],[1.0,1.0,1.0,1.0])   |\n",
      "|1       |[Jian-Xun Peng, Kang Li, De-Shuang Huang]                            |(2577,[24,99,1200],[1.0,1.0,1.0])             |\n",
      "|2       |[J. Heikkila]                                                        |(2577,[2315],[1.0])                           |\n",
      "|3       |[L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]                      |(2577,[454,941,2062,2231],[1.0,1.0,1.0,1.0])  |\n",
      "|4       |[Long Zhang, Kang Li, Er-Wei Bai, George W. Irwin]                   |(2577,[24,147,1063,1570],[1.0,1.0,1.0,1.0])   |\n",
      "|5       |[S. Chen, X. Hong, B. L. Luk, C. J. Harris]                          |(2577,[565,944,1328,1637],[1.0,1.0,1.0,1.0])  |\n",
      "|6       |[Dajun Du, Kang Li, Minrui Fei]                                      |(2577,[24,162,1477],[1.0,1.0,1.0])            |\n",
      "|7       |[Yuan Lan, Yeng Chai Soh, Guang-Bin Huang]                           |(2577,[385,492,1094],[1.0,1.0,1.0])           |\n",
      "|8       |[Jing Deng, Kang Li, George W. Irwin]                                |(2577,[24,147,1771],[1.0,1.0,1.0])            |\n",
      "|9       |[Wen Yao, Xiaoqian Chen, Yong Zhao, M. van Tooren]                   |(2577,[779,884,1308,2560],[1.0,1.0,1.0,1.0])  |\n",
      "|10      |[Jian-Xun Peng, Kang Li, G.W. Irwin]                                 |(2577,[24,99,1161],[1.0,1.0,1.0])             |\n",
      "|11      |[Jer-Guang Hsieh, Yih-Lon Lin, Jyh-Horng Jeng]                       |(2577,[664,1748,2024],[1.0,1.0,1.0])          |\n",
      "|12      |[K. M. Adeney, M. J. Korenberg]                                      |(2577,[1425,1655],[1.0,1.0])                  |\n",
      "|13      |[Nan Xie, Henry Leung]                                               |(2577,[1583,2338],[1.0,1.0])                  |\n",
      "|14      |[Hui Peng, T. Ozaki, V. Haggan-Ozaki, Y. Toyoda]                     |(2577,[767,1390,1495,2389],[1.0,1.0,1.0,1.0]) |\n",
      "|15      |[F.H.F. Leung, H.K. Lam, S.H. Ling, P.K.S. Tam]                      |(2577,[771,983,1006,1957],[1.0,1.0,1.0,1.0])  |\n",
      "|16      |[Guang-Bin Huang, P. Saratchandran, N. Sundararajan]                 |(2577,[385,1618,2261],[1.0,1.0,1.0])          |\n",
      "|17      |[Yahui Li, Sheng Qiang, Xianyi Zhuang, O. Kaynak]                    |(2577,[821,1195,1913,2164],[1.0,1.0,1.0,1.0]) |\n",
      "|18      |[N. Ampazis, S.J. Perantonis]                                        |(2577,[1048,1841],[1.0,1.0])                  |\n",
      "|19      |[C. Panchapakesan, M. Palaniswami, D. Ralph, C. Manzie]              |(2577,[1303,1783,1824,2133],[1.0,1.0,1.0,1.0])|\n",
      "+--------+---------------------------------------------------------------------+----------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "# from pyspark.sql import SparkSession\n",
    "# #Create PySpark SparkSession\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local[1]\") \\\n",
    "#     .appName(\"SparkByExamples.com\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "source_path='../input_data/authors.txt'\n",
    "authors = pd.read_csv(source_path, sep = '|', header=None)\n",
    "authors = authors.rename(columns={0: \"paper_id\", 2: \"authors\"})\n",
    "authors = authors[[\"paper_id\", \"authors\"]]\n",
    "authors = authors[authors['paper_id'] <= 1000]\n",
    "authors['authors'] = authors['authors'].apply(text_to_list)\n",
    "\n",
    "spark_df=spark.createDataFrame(authors) \n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv.setInputCol(\"authors\")\n",
    "cv.setOutputCol(\"vectors\")\n",
    "model = cv.fit(spark_df)\n",
    "model.setInputCol(\"authors\")\n",
    "model.transform(spark_df).show(truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "# spark_df.printSchema()\n",
    "# spark_df.show()\n",
    "\n",
    "# a_col = spark_df.authors\n",
    "# a_vec = CountVectorizer(inputCol = a_col, outputCol = a_col + \"_index\", binary=True)\n",
    "# tmp_df = a_vec.fit(tmp_df).transform(tmp_df)\n",
    "\n",
    "# tmp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "115d0ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"how to read csv file\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "def load_authors(papers_by_author, max_node, source_path='../input_data/authors.txt', dest_folder='../outputs/features'):\n",
    "\n",
    "    papers_by_author=4\n",
    "    max_node=1000\n",
    "\n",
    "    authors = pd.read_csv(source_path, sep = '|', header=None)\n",
    "    authors = authors.rename(columns={0: \"paper_id\", 2: \"authors\"})\n",
    "    authors = authors[[\"paper_id\", \"authors\"]]\n",
    "    authors = authors[authors['paper_id'] <= max_node]\n",
    "    authors['authors'] = authors['authors'].apply(text_to_list)\n",
    "\n",
    "    ## Applying hot vector sparece representation\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    \n",
    "    l = 4\n",
    "    for i in range(l):\n",
    "        sub_auth = authors[i*ceil(len(authors)/l): (i+1)*ceil(len(authors)/l)]\n",
    "        df = pd.DataFrame(mlb.fit_transform(sub_auth['authors']),columns=mlb.classes_, index=sub_auth.index)\n",
    "        df['paper_id'] = df.index\n",
    "        df = pd.merge(left=authors.paper_id, right=df, left_on='paper_id', right_on='paper_id')\n",
    "        dest_path = dest_folder+'/sparce_authors-{}-{}.csv'.format(df.shape, i)\n",
    "        df.to_csv(dest_path)\n",
    "        \n",
    "    df_spark = spark.read.csv(dest_path)\n",
    "\n",
    "        \n",
    "# break    \n",
    "#     df = pd.DataFrame(mlb.fit_transform(authors['authors']),columns=mlb.classes_, index=authors.index)\n",
    "    \n",
    "    \n",
    "#     sum_df = pd.DataFrame(df.sum())\n",
    "#     list_authors = list(sum_df[sum_df[0]<papers_by_author].index)\n",
    "#     df.drop(columns=list_authors, inplace=True)\n",
    "#     dest_path = dest_folder+'/sparce_authors-{}.csv'.format(df.shape)\n",
    "#     df.to_csv(dest_path)\n",
    "#     sparce_authors_list = df.values.tolist()\n",
    "#     return \n",
    "\n",
    "load_authors(4, 1000)\n",
    "\n",
    "#authors_wv = apply_word2vec_on_features(sparce_authors_list, nodes, vector_size=64)\n",
    "\n",
    "#authors_wv = apply_word2vec_on_features(authors_np, nodes, vector_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4db5d087",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/Users/ghassenabdedayem/Documents/Data/Polytechnique/5- Data Challenge/data_challenge_2022/draft_nb/data/sample_data.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      2\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhow to read csv file\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/sample_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/readwriter.py:737\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    735\u001b[0m     path \u001b[38;5;241m=\u001b[39m [path]\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m--> 737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/Users/ghassenabdedayem/Documents/Data/Polytechnique/5- Data Challenge/data_challenge_2022/draft_nb/data/sample_data.csv"
     ]
    }
   ],
   "source": [
    "\n",
    "df = spark.read.csv('data/sample_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a9ff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "\n",
    "def load_authors(papers_by_author, max_node, source_path='../input_data/authors.txt', dest_folder='../outputs/features'):\n",
    "\n",
    "    papers_by_author=4\n",
    "    max_node=1000\n",
    "    source_path='../input_data/authors.txt'\n",
    "    dest_folder='../outputs/features'\n",
    "\n",
    "    authors = pd.read_csv(source_path, sep = '|', header=None)\n",
    "    authors = authors.rename(columns={0: \"paper_id\", 2: \"authors\"})\n",
    "    authors = authors[[\"paper_id\", \"authors\"]]\n",
    "    authors = authors[authors['paper_id'] <= len(nodes)]\n",
    "    authors['authors'] = authors['authors'].apply(text_to_list)\n",
    "\n",
    "    ## Applying hot vector sparece representation\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    \n",
    " \n",
    "    df = pd.DataFrame(mlb.fit_transform(authors['authors']),columns=mlb.classes_, index=authors.index)\n",
    "    df['paper_id'] = df.index\n",
    "    df = pd.merge(left=authors.paper_id, right=df, left_on='paper_id', right_on='paper_id')\n",
    "    sum_df = pd.DataFrame(df.sum())\n",
    "    list_authors = list(sum_df[sum_df[0]<papers_by_author].index)\n",
    "    df.drop(columns=list_authors, inplace=True)\n",
    "    dest_path = dest_folder+'/sparce_authors-{}.csv'.format(df.shape)\n",
    "    df.to_csv(dest_path)\n",
    "    sparce_authors_list = df.values.tolist()\n",
    "    return authors, sparce_authors_list\n",
    "\n",
    "authors, sparce_authors_list = load_authors(4, len(nodes))\n",
    "\n",
    "authors_wv = apply_word2vec_on_features(sparce_authors_list, nodes, vector_size=64)\n",
    "\n",
    "#authors_wv = apply_word2vec_on_features(authors_np, nodes, vector_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fc5751",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69776784",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_np = np.concatenate([walks_wv, authors_wv], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf540ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class labels\n",
    "y = np.zeros(2*indices.shape[1])\n",
    "y[:indices.shape[1]] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
    "\n",
    "# Transforms the numpy matrices/vectors to torch tensors.\n",
    "features = torch.FloatTensor(features_np).to(device)\n",
    "y = torch.LongTensor(y).to(device)\n",
    "if type(adj) != torch.Tensor:\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "indices = torch.LongTensor(indices).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c48af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_feat, n_hidden, n_class, sub_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.double_fc3 = nn.Linear((2*n_hidden), n_hidden)\n",
    "        self.fc4 = nn.Linear(n_hidden, sub_class)\n",
    "        self.fc5 = nn.Linear(sub_class+2, n_class)        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x_in, adj, pairs):\n",
    "        \n",
    "        h1 = self.fc1(x_in)\n",
    "        z1 = self.relu(torch.mm(adj, h1))\n",
    "        z1 = self.dropout(z1)\n",
    "\n",
    "        h2 = self.fc2(z1)\n",
    "        z2 = self.relu(torch.mm(adj, h2))\n",
    "        z2 = self.dropout(z2)\n",
    "\n",
    "        x = z2[pairs[0]] - z2[pairs[1]] # embedded features (z2) of node 0 - embedded features of node 1\n",
    "        \n",
    "        x_auth = pairs[2].reshape([len(pairs[2]), 1])\n",
    "        x_nb_auth = pairs[3].reshape([len(pairs[3]), 1]) \n",
    "        \n",
    "        \n",
    "        \n",
    "        #x1 = z2[pairs[0]]\n",
    "        #x2 = z2[pairs[1]]\n",
    "        # could we add a new dimension to pairs to specify if same author(s)? and then what could we do?\n",
    "              \n",
    "        #x = torch.cat((x1, x2), dim=1)        \n",
    "        #x = self.relu(self.double_fc3(x))        \n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = torch.cat((x, x_auth, x_nb_auth), dim=1) #pairs[3] : number of same authors or 1 if same author\n",
    "        x = self.fc5(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f02edeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb8d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 101\n",
    "n_hidden = 128\n",
    "dropout_rate = 0.2\n",
    "sub_class = 6\n",
    "n_class = 2\n",
    "n_features = features.shape[1]\n",
    "\n",
    "# Creates the model and specifies the optimizer\n",
    "model = GNN(n_features, n_hidden, n_class, sub_class, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c97d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, features, authors, adj, indices, y, val_indices, y_val, epochs):\n",
    "    # Train model\n",
    "    model.train()\n",
    "    start_time = time()\n",
    "    val_indices = add_authors_to_pairs(val_indices, authors) #we add the authors to val_pairs\n",
    "    indices = add_authors_to_pairs(indices, authors) #we add the authors to indices\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "        optimizer.zero_grad()\n",
    "        rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)# We take random indices each time we run an epoch\n",
    "        rand_indices = add_authors_to_pairs(rand_indices, authors)\n",
    "        pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices.   \n",
    "        #pairs = add_authors_to_pairs(pairs) #we add the authors to the pairs\n",
    "        output = model(features, adj, pairs) # we run the model that gives the output.\n",
    "        loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "        acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "        loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "        optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, adj, val_indices)\n",
    "        y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {:.4f} s'.format(round(time()) - round(t)),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "        if epoch % 20 == 0:\n",
    "            model_path = \"../outputs/models/{}-model-{}epochs.pt\".format(today, epoch)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7fbd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = train_model(model, optimizer, features, authors, adj, indices, y, val_indices, y_val, epochs)\n",
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091b817a",
   "metadata": {},
   "source": [
    "Essayer de remplacer les tags des auteurs par des 1 ones pour voir si l'autheur améliore vraiment le résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf86a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, features, adj, indices, y, val_indices, y_val, epochs):\n",
    "    # Train model\n",
    "    model.train()\n",
    "    start_time = time()\n",
    "    val_indices = add_authors_to_pairs(val_indices) #we add the authors to val_pairs\n",
    "    indices = add_authors_to_pairs(indices) #we add the authors to indices\n",
    "    rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)# We take random indices each time we run an epoch\n",
    "    rand_indices = add_authors_to_pairs(rand_indices)\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "        optimizer.zero_grad()\n",
    "        pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices.   \n",
    "        output = model(features, adj, pairs) # we run the model that gives the output.\n",
    "        loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "        acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "        loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "        optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, adj, val_indices)\n",
    "        y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {:.4f} s'.format(round(time() - t)),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "        if epochs % 50 == 0:\n",
    "            model_path = \"../outputs/models/{}-model-{}epochs.pt\".format(today, epoch)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model\n",
    "\n",
    "epochs = 100\n",
    "trained_model = train_model(model, optimizer, features, adj, indices, y, val_indices, y_val, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "break\n",
    "#y_val = torch.FloatTensor(y_val).to(device)\n",
    "trained_model = train_model(model, optimizer, features, adj, indices, y, val_indices, y_val, epochs)\n",
    "model_nb = randint(0, 1000)\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "model_path = \"../submissions_files/{}-model-{}epochs-{}.pt\".format(today, epochs, model_nb)\n",
    "torch.save(trained_model.state_dict(), model_path)\n",
    "print('Model saved in', model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c0b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "output = model(features, adj, val_indices)\n",
    "y_val = torch.LongTensor(y_val).to(device)\n",
    "loss_val = F.nll_loss(output, y_val)\n",
    "acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "print(loss_val.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b650a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "trained_model.eval()\n",
    "eval_pairs = np.array(np.transpose(val_edges))\n",
    "#print(eval_pairs.shape)\n",
    "eval_pairs = torch.LongTensor(eval_pairs).to(device)\n",
    "#print(eval_pairs.shape)\n",
    "eval_output = trained_model(features, adj, eval_pairs)\n",
    "#print(eval_output.shape)\n",
    "y_pred = torch.exp(eval_output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "#y_val_pred_true = list()\n",
    "\n",
    "y_val_pred_true = y_pred[:, 1]\n",
    "\n",
    "    \n",
    "print('Log loss:', log_loss(y_val, y_val_pred_true))\n",
    "\n",
    "#y_val = torch.tensor(y_val).to(device)\n",
    "#y_val_pred_true = torch.tensor(y_val_pred_true).to(device)\n",
    "#print(y_val, y_val_pred_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('../test_data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "output = model(features, adj, pairs)\n",
    "y_pred = torch.exp(output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"../submissions_files/{}-submission-{}-{}.csv\".format(today, model_nb, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f130ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bc8b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = y_val.detach().cpu().numpy()\n",
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80600b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred)\n",
    "df['y_val'] = list(y_val)\n",
    "df.to_csv('../submissions_files/comparison_file.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae66fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(y_val, y_pred[:, 1]), mean_absolute_error(y_val, y_pred[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1822541",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_val, y_pred[:, 1]), log_loss(y_val, y_pred[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7e912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5717e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff03bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a828859c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e3c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c29d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b974905",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = np.array([[5, 3, 0], [2, 1, 0], [4, 0, 1], [0, 3, 2]])\n",
    "test_features = torch.LongTensor(test_features).to(device)\n",
    "test_pairs = [[0, 0, 1, 2, 3, 2, 0, 1, 2, 3], [1, 2, 0, 0, 2, 3, 0, 1, 2, 3]]\n",
    "test_pairs = torch.LongTensor(test_pairs).to(device)\n",
    "print(test_features.shape, test_pairs.shape)\n",
    "test_features[test_pairs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0639bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699d53ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape, np.shape(y_val), np.shape(y_pred_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf150c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ccd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(output.shape)\n",
    "print(features.shape, adj.shape, eval_pairs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4446f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "z2[pairs[1,:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bdb8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e2dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff0f64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425d6600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8607dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0728cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ad5476",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71737f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class labels\n",
    "y = np.zeros(4*G_train.number_of_edges())\n",
    "y[:2*G_train.number_of_edges()] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
    "train_edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede8a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "X_train = np.zeros((4*m, 2*features_np.shape[1]))\n",
    "\n",
    "for i, edge in enumerate(train_edges):\n",
    "    X_train[i] = np.concatenate((features_np[train_edges[i][0]], features_np[train_edges[i][1]]), axis=0)\n",
    "    X_train[m+i] = np.concatenate((features_np[train_edges[i][0]], features_np[train_edges[i][1]]), axis=0)\n",
    "    X_train[2*m+i] = np.concatenate((features_np[randint(0,n-1)], features_np[randint(0,n-1)]), axis=0)\n",
    "    X_train[3*m+i] = np.concatenate((features_np[randint(0,n-1)], features_np[randint(0,n-1)]), axis=0)\n",
    "    \n",
    "X_val = np.zeros((len(val_edges), 2*features_np.shape[1]))\n",
    "for i, edge in enumerate(val_edges):\n",
    "    X_val[i] = np.concatenate((features_np[val_edges[i][0]], features_np[val_edges[i][1]]), axis=0)\n",
    "    \n",
    "print('X_train and X_val created in {} s!'.format(round(time()-t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44a3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6fa791",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y)\n",
    "y_pred = clf.predict_proba(X_val)\n",
    "y_pred = y_pred[:,1]\n",
    "print('Logistic regression performed in {} s!'.format(round(time()-t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c60b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Log loss:', log_loss(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b1ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac02a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Read test data. Each sample is a pair of nodes\n",
    "test_edges = list()\n",
    "with open('../test_data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        test_edges.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "X_test = np.zeros((len(test_edges), 2*features_np.shape[1]))\n",
    "for i, edge in enumerate(test_edges):\n",
    "    X_test[i] = np.concatenate((features_np[test_edges[i][0]], features_np[test_edges[i][1]]), axis=0)\n",
    "\n",
    "t = time()\n",
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "y_pred = y_pred[:,1]\n",
    "print('Logistic regression performed in {} s!'.format(round(time()-t)))\n",
    "\n",
    "\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 100000)\n",
    "\n",
    "pd.DataFrame(y_pred, columns={'predicted'}).to_csv(\n",
    "\"../submissions_files/{}-submission-{}.csv\".format(today, random_nb), header=True, index=True, index_label='id'\n",
    ")\n",
    "    \n",
    "    \n",
    "# # Testing\n",
    "# node_pairs = np.array(np.transpose(node_pairs))\n",
    "# pairs = torch.LongTensor(node_pairs).to(device)\n",
    "# output = model(features, adj, pairs)\n",
    "# y_pred = torch.exp(output)\n",
    "# y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "# y_pred_true = list()\n",
    "# for element in y_pred:\n",
    "#     y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "# today = datetime.today().strftime('%Y-%m-%d')\n",
    "# random_nb = randint(0, 100000)\n",
    "\n",
    "# pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "# \"../submissions_files/{}-submission-{}.csv\".format(today, random_nb), header=True, index=True, index_label='id'\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
