{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787a2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import random_walk, generate_walks, sparse_mx_to_torch_sparse_tensor \n",
    "from utils import text_to_list, intersection, read_train_val_graph, save_subgraph_in_file\n",
    "from utils import apply_word2vec_on_features, create_and_normalize_adjacency, train_model, add_authors_to_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d25112fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "667086c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499 number of edges: 1091955 in the Complete the set\n",
      "Number of nodes: 138499 number of edges: 982107 in the Training set\n",
      "len(nodes) 138499\n",
      "Creating random val_edges...\n",
      "Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects\n",
      "Loaded from edgelist.txt and with a training validation split ratio = 0.1\n",
      "Start generating walks....\n",
      "Random walks generated in in 57s!\n",
      "Start applying Word2Vec...\n",
      "Word2vec model trained on features in 2 min!\n",
      "(138499, 64) features numpy array created in 2 min!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghassenabdedayem/Documents/Data/Polytechnique/5- Data Challenge/data_challenge_2022/draft_nb/utils.py:157: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix of the training graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a normalized adjancency matrix of shape (138499, 138499)\n",
      "Created indices (2, 2102713) with the positions of non zeros in adj matrix\n"
     ]
    }
   ],
   "source": [
    "path = '../input_data/edgelist.txt'\n",
    "G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx = read_train_val_graph(val_ratio=0.1, path=path)\n",
    "walks = generate_walks(G=G_train, num_walks=10, walk_length=15)\n",
    "walks_wv = apply_word2vec_on_features(features=walks, nodes=nodes, vector_size=64)\n",
    "adj, indices = create_and_normalize_adjacency(G_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69776784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_np = np.concatenate([walks_wv, authors_wv], axis=1)\n",
    "features_np = walks_wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02015432",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_csv('../input_data/authors.txt', sep = '|', header=None)\n",
    "authors = authors.rename(columns={0: \"paper_id\", 2: \"authors\"})\n",
    "authors['authors'] = authors['authors'].apply(text_to_list)\n",
    "authors = authors[[\"paper_id\", \"authors\"]]\n",
    "authors = authors[authors['paper_id'] <= max(G.nodes())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bf540ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class labels\n",
    "y = np.zeros(2*indices.shape[1])\n",
    "y[:indices.shape[1]] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
    "\n",
    "# Transforms the numpy matrices/vectors to torch tensors.\n",
    "features = torch.FloatTensor(features_np).to(device)\n",
    "y = torch.LongTensor(y).to(device)\n",
    "if type(adj) != torch.Tensor:\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "indices = torch.LongTensor(indices).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc36c754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c48af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_feat, n_hidden, n_class, sub_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.double_fc3 = nn.Linear((3*n_hidden), n_hidden)\n",
    "        self.fc4 = nn.Linear(n_hidden, sub_class)\n",
    "        self.fc5 = nn.Linear(sub_class, n_class)        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x_in, adj, pairs):\n",
    "        \n",
    "        h1 = self.fc1(x_in)\n",
    "        z1 = self.relu(torch.mm(adj, h1))\n",
    "        z1 = self.dropout(z1)\n",
    "\n",
    "        h2 = self.fc2(z1)\n",
    "        z2 = self.relu(torch.mm(adj, h2))\n",
    "        z2 = self.dropout(z2)\n",
    "\n",
    "        x = z2[pairs[0]] - z2[pairs[1]] # embedded features (z2) of node 0 - embedded features of node 1\n",
    "        x = pairs[3][:, None] * x\n",
    "        x1 = z2[pairs[0]]\n",
    "        x2 = z2[pairs[1]]\n",
    "        x = torch.cat((x, x1, x2), dim=1)\n",
    "        \n",
    "#         x_auth = pairs[2].reshape([len(pairs[2]), 1])\n",
    "#         x_nb_auth = pairs[3].reshape([len(pairs[3]), 1]) \n",
    "        \n",
    "        \n",
    "        \n",
    "        #x1 = z2[pairs[0]]\n",
    "        #x2 = z2[pairs[1]]\n",
    "        # could we add a new dimension to pairs to specify if same author(s)? and then what could we do?\n",
    "              \n",
    "        #x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.relu(self.double_fc3(x))        \n",
    "        #x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        #x = torch.cat((x, x_auth, x_nb_auth), dim=1) #pairs[3] : number of same authors or 1 if same author\n",
    "        x = self.fc5(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eb8d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "n_hidden = 128\n",
    "dropout_rate = 0.2\n",
    "sub_class = 8\n",
    "n_class = 2\n",
    "n_features = features.shape[1]\n",
    "\n",
    "# Creates the model and specifies the optimizer\n",
    "model = GNN(n_features, n_hidden, n_class, sub_class, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2e28f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0c97d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def value_greater_than_list(list1, val):\n",
    "#     if val >\n",
    "# #     for x in list1: \n",
    "# #         # compare val with all the values of the list\n",
    "# #         if val < x:\n",
    "# #             return False\n",
    "#     return True\n",
    "\n",
    "def early_stopping(list_loss_val, loss_val, window=10):\n",
    "    lst = list(list_loss_val)[-window:]\n",
    "    if len(lst) == window and loss_val > (sum(lst)/len(lst)):\n",
    "        print('mean: {:.5f} val: {:.5f}'.format((sum(lst)/len(lst)),loss_val))\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "    \n",
    "def train_model(model, learning_rate, features, authors, adj, indices, y, val_indices, y_val, epochs):\n",
    "    # Train model\n",
    "    start_time = time()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    val_indices = add_authors_to_pairs(val_indices, authors) #we add the authors to val_pairs\n",
    "    indices = add_authors_to_pairs(indices, authors) #we add the authors to indices\n",
    "    rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)# We take random indices each time we run an epoch\n",
    "    rand_indices = add_authors_to_pairs(rand_indices, authors)\n",
    "    pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices. \n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    list_loss_val = []\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model.train()\n",
    "        output = model(features, adj, pairs) # we run the model that gives the output.\n",
    "        loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "        acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "        loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "        optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, adj, val_indices)\n",
    "        y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        list_loss_val.append(loss_val.item())\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {} s'.format(int(round(time()) - round(t))),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "        if epoch % 20 == 0:\n",
    "            model_path = \"../outputs/models/{}-model-{}epochs.pt\".format(today, epoch)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "        early = early_stopping(list_loss_val, loss_val.item(), window=10)\n",
    "        if early:\n",
    "            break\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7f63b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train_model in module __main__:\n",
      "\n",
      "train_model(model, learning_rate, features, authors, adj, indices, y, val_indices, y_val, epochs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(train_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c7fbd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 0.6136 loss_val: 0.5655 acc_train: 0.6929 acc_val: 0.7684 time: 48 s total_time: 2 min\n",
      "Epoch: 006 loss_train: 0.6042 loss_val: 0.5659 acc_train: 0.7058 acc_val: 0.7769 time: 43 s total_time: 6 min\n",
      "Epoch: 011 loss_train: 0.6006 loss_val: 0.5576 acc_train: 0.7100 acc_val: 0.7800 time: 47 s total_time: 10 min\n",
      "Epoch: 016 loss_train: 0.5955 loss_val: 0.5485 acc_train: 0.7176 acc_val: 0.7826 time: 46 s total_time: 14 min\n",
      "Epoch: 021 loss_train: 0.5928 loss_val: 0.5437 acc_train: 0.7206 acc_val: 0.7861 time: 47 s total_time: 18 min\n",
      "Epoch: 026 loss_train: 0.5902 loss_val: 0.5405 acc_train: 0.7217 acc_val: 0.7895 time: 45 s total_time: 22 min\n",
      "Epoch: 031 loss_train: 0.5874 loss_val: 0.5357 acc_train: 0.7239 acc_val: 0.7918 time: 45 s total_time: 25 min\n",
      "Epoch: 036 loss_train: 0.5837 loss_val: 0.5306 acc_train: 0.7276 acc_val: 0.7931 time: 46 s total_time: 29 min\n",
      "Epoch: 041 loss_train: 0.5811 loss_val: 0.5269 acc_train: 0.7295 acc_val: 0.7952 time: 43 s total_time: 33 min\n",
      "Epoch: 046 loss_train: 0.5792 loss_val: 0.5235 acc_train: 0.7299 acc_val: 0.7971 time: 45 s total_time: 37 min\n",
      "Epoch: 051 loss_train: 0.5760 loss_val: 0.5193 acc_train: 0.7325 acc_val: 0.7989 time: 47 s total_time: 41 min\n",
      "Epoch: 056 loss_train: 0.5739 loss_val: 0.5153 acc_train: 0.7345 acc_val: 0.8009 time: 45 s total_time: 44 min\n",
      "Epoch: 061 loss_train: 0.5706 loss_val: 0.5120 acc_train: 0.7362 acc_val: 0.8027 time: 46 s total_time: 48 min\n",
      "Epoch: 066 loss_train: 0.5680 loss_val: 0.5088 acc_train: 0.7379 acc_val: 0.8046 time: 48 s total_time: 52 min\n",
      "Epoch: 071 loss_train: 0.5656 loss_val: 0.5056 acc_train: 0.7397 acc_val: 0.8060 time: 45 s total_time: 56 min\n",
      "Epoch: 076 loss_train: 0.5628 loss_val: 0.5029 acc_train: 0.7414 acc_val: 0.8073 time: 47 s total_time: 60 min\n",
      "Epoch: 081 loss_train: 0.5589 loss_val: 0.5003 acc_train: 0.7439 acc_val: 0.8087 time: 46 s total_time: 64 min\n",
      "Epoch: 086 loss_train: 0.5555 loss_val: 0.4977 acc_train: 0.7458 acc_val: 0.8096 time: 44 s total_time: 67 min\n",
      "Epoch: 091 loss_train: 0.5522 loss_val: 0.4952 acc_train: 0.7467 acc_val: 0.8106 time: 46 s total_time: 71 min\n",
      "Epoch: 096 loss_train: 0.5494 loss_val: 0.4927 acc_train: 0.7482 acc_val: 0.8116 time: 43 s total_time: 75 min\n",
      "Epoch: 101 loss_train: 0.5461 loss_val: 0.4901 acc_train: 0.7495 acc_val: 0.8126 time: 45 s total_time: 79 min\n",
      "Epoch: 106 loss_train: 0.5421 loss_val: 0.4876 acc_train: 0.7517 acc_val: 0.8137 time: 47 s total_time: 83 min\n",
      "Epoch: 111 loss_train: 0.5383 loss_val: 0.4854 acc_train: 0.7539 acc_val: 0.8145 time: 44 s total_time: 86 min\n",
      "Epoch: 116 loss_train: 0.5358 loss_val: 0.4834 acc_train: 0.7550 acc_val: 0.8152 time: 45 s total_time: 90 min\n",
      "Epoch: 121 loss_train: 0.5324 loss_val: 0.4814 acc_train: 0.7563 acc_val: 0.8162 time: 45 s total_time: 94 min\n",
      "Epoch: 126 loss_train: 0.5301 loss_val: 0.4796 acc_train: 0.7581 acc_val: 0.8173 time: 45 s total_time: 98 min\n",
      "Epoch: 131 loss_train: 0.5257 loss_val: 0.4779 acc_train: 0.7602 acc_val: 0.8182 time: 45 s total_time: 101 min\n",
      "Epoch: 136 loss_train: 0.5222 loss_val: 0.4763 acc_train: 0.7627 acc_val: 0.8194 time: 43 s total_time: 105 min\n",
      "Epoch: 141 loss_train: 0.5192 loss_val: 0.4733 acc_train: 0.7630 acc_val: 0.8203 time: 47 s total_time: 109 min\n",
      "Epoch: 146 loss_train: 0.5145 loss_val: 0.4683 acc_train: 0.7655 acc_val: 0.8215 time: 51 s total_time: 113 min\n",
      "Epoch: 151 loss_train: 0.5091 loss_val: 0.4598 acc_train: 0.7672 acc_val: 0.8226 time: 45 s total_time: 117 min\n",
      "Epoch: 156 loss_train: 0.5035 loss_val: 0.4505 acc_train: 0.7688 acc_val: 0.8234 time: 50 s total_time: 121 min\n",
      "Epoch: 161 loss_train: 0.4984 loss_val: 0.4401 acc_train: 0.7704 acc_val: 0.8244 time: 49 s total_time: 125 min\n",
      "Epoch: 166 loss_train: 0.4940 loss_val: 0.4330 acc_train: 0.7718 acc_val: 0.8252 time: 44 s total_time: 128 min\n",
      "Epoch: 171 loss_train: 0.4886 loss_val: 0.4281 acc_train: 0.7737 acc_val: 0.8260 time: 45 s total_time: 132 min\n",
      "Epoch: 176 loss_train: 0.4847 loss_val: 0.4245 acc_train: 0.7759 acc_val: 0.8273 time: 47 s total_time: 136 min\n",
      "Epoch: 181 loss_train: 0.4812 loss_val: 0.4215 acc_train: 0.7773 acc_val: 0.8283 time: 47 s total_time: 140 min\n",
      "Epoch: 186 loss_train: 0.4774 loss_val: 0.4186 acc_train: 0.7794 acc_val: 0.8292 time: 50 s total_time: 144 min\n",
      "Epoch: 191 loss_train: 0.4741 loss_val: 0.4159 acc_train: 0.7815 acc_val: 0.8298 time: 46 s total_time: 148 min\n",
      "Epoch: 196 loss_train: 0.4697 loss_val: 0.4132 acc_train: 0.7836 acc_val: 0.8308 time: 46 s total_time: 152 min\n",
      "Epoch: 201 loss_train: 0.4676 loss_val: 0.4105 acc_train: 0.7846 acc_val: 0.8317 time: 46 s total_time: 155 min\n",
      "Epoch: 206 loss_train: 0.4652 loss_val: 0.4078 acc_train: 0.7857 acc_val: 0.8324 time: 44 s total_time: 159 min\n",
      "Epoch: 211 loss_train: 0.4623 loss_val: 0.4053 acc_train: 0.7871 acc_val: 0.8328 time: 45 s total_time: 163 min\n",
      "Epoch: 216 loss_train: 0.4590 loss_val: 0.4030 acc_train: 0.7893 acc_val: 0.8334 time: 45 s total_time: 167 min\n",
      "Epoch: 221 loss_train: 0.4560 loss_val: 0.4010 acc_train: 0.7908 acc_val: 0.8340 time: 48 s total_time: 171 min\n",
      "Epoch: 226 loss_train: 0.4550 loss_val: 0.3992 acc_train: 0.7918 acc_val: 0.8345 time: 44 s total_time: 175 min\n",
      "Epoch: 231 loss_train: 0.4520 loss_val: 0.3976 acc_train: 0.7930 acc_val: 0.8350 time: 48 s total_time: 179 min\n",
      "Epoch: 236 loss_train: 0.4506 loss_val: 0.3961 acc_train: 0.7941 acc_val: 0.8355 time: 43 s total_time: 183 min\n",
      "Epoch: 241 loss_train: 0.4468 loss_val: 0.3947 acc_train: 0.7964 acc_val: 0.8360 time: 48 s total_time: 187 min\n",
      "Epoch: 246 loss_train: 0.4459 loss_val: 0.3934 acc_train: 0.7969 acc_val: 0.8363 time: 49 s total_time: 191 min\n",
      "Epoch: 251 loss_train: 0.4442 loss_val: 0.3922 acc_train: 0.7980 acc_val: 0.8368 time: 46 s total_time: 194 min\n",
      "Epoch: 256 loss_train: 0.4416 loss_val: 0.3911 acc_train: 0.8000 acc_val: 0.8374 time: 43 s total_time: 198 min\n",
      "Epoch: 261 loss_train: 0.4404 loss_val: 0.3901 acc_train: 0.8006 acc_val: 0.8376 time: 45 s total_time: 202 min\n",
      "Epoch: 266 loss_train: 0.4382 loss_val: 0.3891 acc_train: 0.8022 acc_val: 0.8379 time: 47 s total_time: 206 min\n",
      "Epoch: 271 loss_train: 0.4370 loss_val: 0.3882 acc_train: 0.8031 acc_val: 0.8383 time: 52 s total_time: 210 min\n",
      "Epoch: 276 loss_train: 0.4364 loss_val: 0.3874 acc_train: 0.8040 acc_val: 0.8386 time: 43 s total_time: 213 min\n",
      "Epoch: 281 loss_train: 0.4340 loss_val: 0.3865 acc_train: 0.8058 acc_val: 0.8390 time: 47 s total_time: 217 min\n",
      "Epoch: 286 loss_train: 0.4324 loss_val: 0.3858 acc_train: 0.8068 acc_val: 0.8393 time: 45 s total_time: 221 min\n",
      "Epoch: 291 loss_train: 0.4313 loss_val: 0.3849 acc_train: 0.8081 acc_val: 0.8397 time: 44 s total_time: 225 min\n",
      "Epoch: 296 loss_train: 0.4297 loss_val: 0.3842 acc_train: 0.8098 acc_val: 0.8400 time: 51 s total_time: 229 min\n",
      "Epoch: 301 loss_train: 0.4289 loss_val: 0.3835 acc_train: 0.8101 acc_val: 0.8402 time: 49 s total_time: 233 min\n",
      "Epoch: 306 loss_train: 0.4269 loss_val: 0.3829 acc_train: 0.8118 acc_val: 0.8404 time: 45 s total_time: 237 min\n",
      "Epoch: 311 loss_train: 0.4280 loss_val: 0.3823 acc_train: 0.8110 acc_val: 0.8406 time: 50 s total_time: 241 min\n",
      "Epoch: 316 loss_train: 0.4257 loss_val: 0.3816 acc_train: 0.8133 acc_val: 0.8409 time: 46 s total_time: 244 min\n",
      "Epoch: 321 loss_train: 0.4241 loss_val: 0.3811 acc_train: 0.8143 acc_val: 0.8414 time: 44 s total_time: 248 min\n",
      "Epoch: 326 loss_train: 0.4215 loss_val: 0.3806 acc_train: 0.8190 acc_val: 0.8416 time: 45 s total_time: 252 min\n",
      "Epoch: 331 loss_train: 0.4211 loss_val: 0.3800 acc_train: 0.8195 acc_val: 0.8419 time: 46 s total_time: 256 min\n",
      "Epoch: 336 loss_train: 0.4199 loss_val: 0.3794 acc_train: 0.8208 acc_val: 0.8420 time: 47 s total_time: 260 min\n",
      "Epoch: 341 loss_train: 0.4178 loss_val: 0.3789 acc_train: 0.8226 acc_val: 0.8422 time: 44 s total_time: 263 min\n",
      "Epoch: 346 loss_train: 0.4170 loss_val: 0.3777 acc_train: 0.8233 acc_val: 0.8425 time: 43 s total_time: 267 min\n",
      "Epoch: 351 loss_train: 0.4154 loss_val: 0.3765 acc_train: 0.8246 acc_val: 0.8426 time: 52 s total_time: 271 min\n",
      "Epoch: 356 loss_train: 0.4139 loss_val: 0.3761 acc_train: 0.8267 acc_val: 0.8426 time: 44 s total_time: 275 min\n",
      "Epoch: 361 loss_train: 0.4135 loss_val: 0.3758 acc_train: 0.8272 acc_val: 0.8427 time: 46 s total_time: 279 min\n",
      "Epoch: 366 loss_train: 0.4116 loss_val: 0.3755 acc_train: 0.8290 acc_val: 0.8429 time: 45 s total_time: 282 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 371 loss_train: 0.4107 loss_val: 0.3749 acc_train: 0.8298 acc_val: 0.8430 time: 45 s total_time: 286 min\n",
      "Epoch: 376 loss_train: 0.4095 loss_val: 0.3742 acc_train: 0.8306 acc_val: 0.8434 time: 57 s total_time: 290 min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 36\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, learning_rate, features, authors, adj, indices, y, val_indices, y_val, epochs)\u001b[0m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 36\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# we run the model that gives the output.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output, y) \u001b[38;5;66;03m# we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m acc_train \u001b[38;5;241m=\u001b[39m accuracy_score(torch\u001b[38;5;241m.\u001b[39margmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), y\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\u001b[38;5;66;03m# just to show it in the out put message of the training\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[7], line 29\u001b[0m, in \u001b[0;36mGNN.forward\u001b[0;34m(self, x_in, adj, pairs)\u001b[0m\n\u001b[1;32m     27\u001b[0m         x1 \u001b[38;5;241m=\u001b[39m z2[pairs[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m     28\u001b[0m         x2 \u001b[38;5;241m=\u001b[39m z2[pairs[\u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m---> 29\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#         x_auth = pairs[2].reshape([len(pairs[2]), 1])\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#         x_nb_auth = pairs[3].reshape([len(pairs[3]), 1]) \u001b[39;00m\n\u001b[1;32m     33\u001b[0m         \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m               \n\u001b[1;32m     40\u001b[0m         \u001b[38;5;66;03m#x = torch.cat((x1, x2), dim=1)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdouble_fc3(x))        \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, 0.001, features, authors, adj, indices, y, val_indices, y_val, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ac9ed8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 6, 9]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [1, 3, 5, 6, 9]\n",
    "\n",
    "l[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06637d6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (668683560.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7048f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('../test_data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "pairs = add_authors_to_pairs(pairs, authors)\n",
    "output = model(features, adj, pairs)\n",
    "y_pred = torch.exp(output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"../submissions_files/{}-submission-{}epochs-{}.csv\".format(today, epochs, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819ad57f",
   "metadata": {},
   "source": [
    "Essayer de remplacer les tags des auteurs par des 1 ones pour voir si l'autheur améliore vraiment le résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf86a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, features, adj, indices, y, val_indices, y_val, epochs):\n",
    "    # Train model\n",
    "    model.train()\n",
    "    start_time = time()\n",
    "    val_indices = add_authors_to_pairs(val_indices) #we add the authors to val_pairs\n",
    "    indices = add_authors_to_pairs(indices) #we add the authors to indices\n",
    "    rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)# We take random indices each time we run an epoch\n",
    "    rand_indices = add_authors_to_pairs(rand_indices)\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "        optimizer.zero_grad()\n",
    "        pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices.   \n",
    "        output = model(features, adj, pairs) # we run the model that gives the output.\n",
    "        loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "        acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "        loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "        optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, adj, val_indices)\n",
    "        y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {:.4f} s'.format(round(time() - t)),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "        if epochs % 50 == 0:\n",
    "            model_path = \"../outputs/models/{}-model-{}epochs.pt\".format(today, epoch)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model\n",
    "\n",
    "epochs = 100\n",
    "trained_model = train_model(model, optimizer, features, adj, indices, y, val_indices, y_val, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "break\n",
    "#y_val = torch.FloatTensor(y_val).to(device)\n",
    "trained_model = train_model(model, optimizer, features, adj, indices, y, val_indices, y_val, epochs)\n",
    "model_nb = randint(0, 1000)\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "model_path = \"../submissions_files/{}-model-{}epochs-{}.pt\".format(today, epochs, model_nb)\n",
    "torch.save(trained_model.state_dict(), model_path)\n",
    "print('Model saved in', model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c0b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot representation using Spark\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "def multi_label_binarizer(df, labels_col='labels', output_col='new_labels'):\n",
    "    \"\"\"\n",
    "    Function that takes as input:\n",
    "    - `df`, pyspark.sql.dataframe \n",
    "    - `labels_col`, string that indicates an array column containing labels\n",
    "    - `output_col`, string that indicates the name of the new labels column\n",
    "    \n",
    "    and returns a multi-label binarized column.\n",
    "    \"\"\"\n",
    "    \n",
    "    # get set of unique labels and sort them\n",
    "    labels_set = df\\\n",
    "        .withColumn('exploded', F.explode('labels'))\\\n",
    "        .agg(F.collect_set('exploded'))\\\n",
    "        .collect()[0][0]\n",
    "    labels_set = sorted(labels_set)\n",
    "    \n",
    "    # dynamically create columns for each value in `labels_set`\n",
    "    for i in labels_set:\n",
    "        df = df.withColumn(i, F.when(F.array_contains(labels_col, i), 1).otherwise(0))\n",
    "        \n",
    "    # create new, multi-label binarized array column\n",
    "    df = df.withColumn(output_col, F.array(*labels_set))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "output = model(features, adj, val_indices)\n",
    "y_val = torch.LongTensor(y_val).to(device)\n",
    "loss_val = F.nll_loss(output, y_val)\n",
    "acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "print(loss_val.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b650a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "trained_model.eval()\n",
    "eval_pairs = np.array(np.transpose(val_edges))\n",
    "#print(eval_pairs.shape)\n",
    "eval_pairs = torch.LongTensor(eval_pairs).to(device)\n",
    "#print(eval_pairs.shape)\n",
    "eval_output = trained_model(features, adj, eval_pairs)\n",
    "#print(eval_output.shape)\n",
    "y_pred = torch.exp(eval_output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "#y_val_pred_true = list()\n",
    "\n",
    "y_val_pred_true = y_pred[:, 1]\n",
    "\n",
    "    \n",
    "print('Log loss:', log_loss(y_val, y_val_pred_true))\n",
    "\n",
    "#y_val = torch.tensor(y_val).to(device)\n",
    "#y_val_pred_true = torch.tensor(y_val_pred_true).to(device)\n",
    "#print(y_val, y_val_pred_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('../test_data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "output = model(features, adj, pairs)\n",
    "y_pred = torch.exp(output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"../submissions_files/{}-submission-{}-{}.csv\".format(today, model_nb, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f130ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bc8b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = y_val.detach().cpu().numpy()\n",
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80600b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred)\n",
    "df['y_val'] = list(y_val)\n",
    "df.to_csv('../submissions_files/comparison_file.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae66fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(y_val, y_pred[:, 1]), mean_absolute_error(y_val, y_pred[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1822541",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_val, y_pred[:, 1]), log_loss(y_val, y_pred[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7e912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5717e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff03bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a828859c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e3c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c29d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b974905",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = np.array([[5, 3, 0], [2, 1, 0], [4, 0, 1], [0, 3, 2]])\n",
    "test_features = torch.LongTensor(test_features).to(device)\n",
    "test_pairs = [[0, 0, 1, 2, 3, 2, 0, 1, 2, 3], [1, 2, 0, 0, 2, 3, 0, 1, 2, 3]]\n",
    "test_pairs = torch.LongTensor(test_pairs).to(device)\n",
    "print(test_features.shape, test_pairs.shape)\n",
    "test_features[test_pairs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0639bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699d53ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape, np.shape(y_val), np.shape(y_pred_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf150c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ccd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(output.shape)\n",
    "print(features.shape, adj.shape, eval_pairs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4446f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "z2[pairs[1,:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bdb8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e2dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff0f64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425d6600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8607dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0728cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ad5476",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71737f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class labels\n",
    "y = np.zeros(4*G_train.number_of_edges())\n",
    "y[:2*G_train.number_of_edges()] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
    "train_edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede8a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "X_train = np.zeros((4*m, 2*features_np.shape[1]))\n",
    "\n",
    "for i, edge in enumerate(train_edges):\n",
    "    X_train[i] = np.concatenate((features_np[train_edges[i][0]], features_np[train_edges[i][1]]), axis=0)\n",
    "    X_train[m+i] = np.concatenate((features_np[train_edges[i][0]], features_np[train_edges[i][1]]), axis=0)\n",
    "    X_train[2*m+i] = np.concatenate((features_np[randint(0,n-1)], features_np[randint(0,n-1)]), axis=0)\n",
    "    X_train[3*m+i] = np.concatenate((features_np[randint(0,n-1)], features_np[randint(0,n-1)]), axis=0)\n",
    "    \n",
    "X_val = np.zeros((len(val_edges), 2*features_np.shape[1]))\n",
    "for i, edge in enumerate(val_edges):\n",
    "    X_val[i] = np.concatenate((features_np[val_edges[i][0]], features_np[val_edges[i][1]]), axis=0)\n",
    "    \n",
    "print('X_train and X_val created in {} s!'.format(round(time()-t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf44a3c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m (torchtext\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "print (torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6fa791",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y)\n",
    "y_pred = clf.predict_proba(X_val)\n",
    "y_pred = y_pred[:,1]\n",
    "print('Logistic regression performed in {} s!'.format(round(time()-t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c60b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Log loss:', log_loss(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b1ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac02a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Read test data. Each sample is a pair of nodes\n",
    "test_edges = list()\n",
    "with open('../test_data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        test_edges.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "X_test = np.zeros((len(test_edges), 2*features_np.shape[1]))\n",
    "for i, edge in enumerate(test_edges):\n",
    "    X_test[i] = np.concatenate((features_np[test_edges[i][0]], features_np[test_edges[i][1]]), axis=0)\n",
    "\n",
    "t = time()\n",
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "y_pred = y_pred[:,1]\n",
    "print('Logistic regression performed in {} s!'.format(round(time()-t)))\n",
    "\n",
    "\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 100000)\n",
    "\n",
    "pd.DataFrame(y_pred, columns={'predicted'}).to_csv(\n",
    "\"../submissions_files/{}-submission-{}.csv\".format(today, random_nb), header=True, index=True, index_label='id'\n",
    ")\n",
    "    \n",
    "    \n",
    "# # Testing\n",
    "# node_pairs = np.array(np.transpose(node_pairs))\n",
    "# pairs = torch.LongTensor(node_pairs).to(device)\n",
    "# output = model(features, adj, pairs)\n",
    "# y_pred = torch.exp(output)\n",
    "# y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "# y_pred_true = list()\n",
    "# for element in y_pred:\n",
    "#     y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "# today = datetime.today().strftime('%Y-%m-%d')\n",
    "# random_nb = randint(0, 100000)\n",
    "\n",
    "# pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "# \"../submissions_files/{}-submission-{}.csv\".format(today, random_nb), header=True, index=True, index_label='id'\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
