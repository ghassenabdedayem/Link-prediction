{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46a7427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from random import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from random import choice\n",
    "from gensim.models import Word2Vec\n",
    "import keras\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "from scipy.sparse import identity, diags\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcf6da2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adjacency(A):\n",
    "    n = A.shape[0]\n",
    "    A = A + 2*identity(n)\n",
    "    degs = A.dot(np.ones(n))\n",
    "    inv_degs = np.power(degs, -1)\n",
    "    D_inv = diags(inv_degs)\n",
    "    A_hat = D_inv.dot(A)\n",
    "    return A_hat\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    print(type(sparse_mx))\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def random_walk(G, node, walk_length):\n",
    "    walk = [node]\n",
    "  \n",
    "    for i in range(walk_length-1):\n",
    "        neibor_nodes = list(G.neighbors(walk[-1]))\n",
    "        if len(neibor_nodes) > 0:\n",
    "            next_node = choice(neibor_nodes)\n",
    "            walk.append(next_node)\n",
    "    walk = [str(node) for node in walk] # in case the nodes are in string format, we don't need to cast into string, but if the nodes are in numeric or integer, we need this line to cast into string\n",
    "    return walk\n",
    "\n",
    "def generate_walks(G, num_walks, walk_length):\n",
    "  # Runs \"num_walks\" random walks from each node, and returns a list of all random walk\n",
    "    walks = list()  \n",
    "    for i in range(num_walks):\n",
    "        for node in G.nodes():\n",
    "            walk = random_walk(G, node, walk_length)\n",
    "            walks.append(walk)\n",
    "        #print('walks : ', walks)\n",
    "    return walks\n",
    "\n",
    "def text_to_list(text):\n",
    "    return unidecode(text).split(',')\n",
    "\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    \n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52293e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_feat, n_hidden, n_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc4 = nn.Linear(n_hidden, n_class)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x_in, adj, pairs):\n",
    "        \n",
    "        h1 = self.fc1(x_in)\n",
    "        z1 = self.relu(torch.mm(adj, h1))\n",
    "        z1 = self.dropout(z1)\n",
    "\n",
    "        h2 = self.fc2(z1)\n",
    "        z2 = self.relu(torch.mm(adj, h2))\n",
    "        \n",
    "        x = z2[pairs[0,:],:] - z2[pairs[1,:],:] # embedded features (z2) of node 0 - embedded features of node 1\n",
    "        # could we add a new dimension to pairs to specify if same author(s)? and then what could we do?\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8681342c",
   "metadata": {},
   "source": [
    "### could we add a new dimension to pairs to specify if same author(s)? and then what could we do?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "fbac1450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3931636, 128])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(features[pairs[0,:],:] - features[pairs[1,:],:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b5ed76cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3931636"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "68a97119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d22582c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes of total set: 138499\n",
      "Number of edges of total set: 1091955\n",
      "Number of nodes of training set: 138499\n",
      "Number of edges of training set: 982849\n"
     ]
    }
   ],
   "source": [
    "G = nx.read_edgelist('../input_data/edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "nodes = list(G.nodes())\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "edges = list(G.edges())\n",
    "\n",
    "print('Number of nodes of total set:', n)\n",
    "print('Number of edges of total set:', m)\n",
    "\n",
    "node_to_idx = dict()\n",
    "for i, node in enumerate(nodes):\n",
    "    node_to_idx[node] = i\n",
    "\n",
    "val_edges = list()\n",
    "G_train = G\n",
    "\n",
    "for edge in edges:\n",
    "    if random() < 0.1:\n",
    "        val_edges.append(edge)\n",
    "\n",
    "# We remove the val edges from the graph G\n",
    "for edge in val_edges:\n",
    "    G_train.remove_edge(edge[0], edge[1])\n",
    "\n",
    "n = G_train.number_of_nodes()\n",
    "m = G_train.number_of_edges()\n",
    "train_edges = list(G_train.edges())\n",
    "    \n",
    "print('Number of nodes of training set:', n)\n",
    "print('Number of edges of training set:', m)\n",
    "\n",
    "y_val = [1]*len(val_edges)\n",
    "\n",
    "n_val_edges = len(val_edges)\n",
    "\n",
    "# Create random pairs of nodes (testing negative edges)\n",
    "for i in range(n_val_edges):\n",
    "    n1 = nodes[randint(0, n-1)]\n",
    "    n2 = nodes[randint(0, n-1)]\n",
    "    (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "    val_edges.append((n1, n2))\n",
    "    \n",
    "# Remove from val_edges edges that exist in both train and val\n",
    "# for edge in list(set(val_edges) & set(train_edges)):\n",
    "#     val_edges.remove(edge)\n",
    "    \n",
    "# n_val_edges = len(val_edges) # - len(y_val) #because we removed from val_edges edges that exist in both\n",
    "y_val.extend([0]*(n_val_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ea370eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[James H. Niblock, Jian-Xun Peng, Karen R. McM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Jian-Xun Peng, Kang Li, De-Shuang Huang]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[J. Heikkila]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[Long Zhang, Kang Li, Er-Wei Bai, George W. Ir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id                                            authors\n",
       "0         0  [James H. Niblock, Jian-Xun Peng, Karen R. McM...\n",
       "1         1          [Jian-Xun Peng, Kang Li, De-Shuang Huang]\n",
       "2         2                                      [J. Heikkila]\n",
       "3         3    [L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]\n",
       "4         4  [Long Zhang, Kang Li, Er-Wei Bai, George W. Ir..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors = pd.read_csv('../input_data/authors.txt', sep = '|', header=None)\n",
    "authors = authors.rename(columns={0: \"paper_id\", 2: \"authors\"})\n",
    "authors['authors'] = authors['authors'].apply(text_to_list)\n",
    "authors.drop(columns={1}, inplace=True)\n",
    "authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "435ac650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0,\n",
       "        list(['James H. Niblock', 'Jian-Xun Peng', 'Karen R. McMenemy', 'George W. Irwin'])],\n",
       "       [1, list(['Jian-Xun Peng', 'Kang Li', 'De-Shuang Huang'])],\n",
       "       [2, list(['J. Heikkila'])],\n",
       "       ...,\n",
       "       [138496,\n",
       "        list(['Hongge Chen', 'Huan Zhang', 'Pin-Yu Chen', 'Jinfeng Yi', 'Cho-Jui Hsieh'])],\n",
       "       [138497, list(['Sanjeev Arora', 'Andrej Risteski', 'Yi Zhang'])],\n",
       "       [138498,\n",
       "        list(['Pietro Morerio', 'Jacopo Cavazza', 'Vittorio Murino'])]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_np = np.array(authors)\n",
    "authors_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8bcb1db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['two', 'one'], dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_pd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5b945004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>two</th>\n",
       "      <th>one</th>\n",
       "      <th>one_authors</th>\n",
       "      <th>two_authors</th>\n",
       "      <th>intersection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[Jian-Xun Peng, Kang Li, De-Shuang Huang]</td>\n",
       "      <td>[James H. Niblock, Jian-Xun Peng, Karen R. McM...</td>\n",
       "      <td>[Jian-Xun Peng]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[J. Heikkila]</td>\n",
       "      <td>[James H. Niblock, Jian-Xun Peng, Karen R. McM...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[Jian-Xun Peng, Kang Li, De-Shuang Huang]</td>\n",
       "      <td>[L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>[Heriberto Cruz-Hernandez, Luis Gerardo de la ...</td>\n",
       "      <td>[L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>[Evgeniy Martyushev]</td>\n",
       "      <td>[L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   two  one                                        one_authors  \\\n",
       "0    0    1          [Jian-Xun Peng, Kang Li, De-Shuang Huang]   \n",
       "1    0    2                                      [J. Heikkila]   \n",
       "2    3    1          [Jian-Xun Peng, Kang Li, De-Shuang Huang]   \n",
       "3    3   60  [Heriberto Cruz-Hernandez, Luis Gerardo de la ...   \n",
       "4    3   61                               [Evgeniy Martyushev]   \n",
       "\n",
       "                                         two_authors     intersection  \n",
       "0  [James H. Niblock, Jian-Xun Peng, Karen R. McM...  [Jian-Xun Peng]  \n",
       "1  [James H. Niblock, Jian-Xun Peng, Karen R. McM...               []  \n",
       "2    [L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]               []  \n",
       "3    [L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]               []  \n",
       "4    [L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]               []  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_pd = pd.DataFrame(np.transpose(indices), columns={'one', 'two'})\n",
    "indices_pd = pd.merge(indices_pd, authors, left_on='one', right_on='paper_id').drop(columns='paper_id').rename(columns={'authors':'one_authors'})\n",
    "indices_pd = pd.merge(indices_pd, authors, left_on='two', right_on='paper_id').drop(columns='paper_id').rename(columns={'authors':'two_authors'})\n",
    "indices_pd['intersection'] = indices_pd.apply(lambda x: intersection(x['one_authors'], x['two_authors']), axis=1)\n",
    "\n",
    "#indices_pd['intersection'] = indices_pd['one_authors'] & indices_pd['two_authors']\n",
    "indices_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe3e518f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pd/glcnl2497w5b6xn3p94tnwlr0000gn/T/ipykernel_60385/4237042410.py:1: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G_train) # Obtains the adjacency matrix of the training graph\n"
     ]
    }
   ],
   "source": [
    "adj = nx.adjacency_matrix(G_train) # Obtains the adjacency matrix of the training graph\n",
    "indices = np.array(adj.nonzero()) # Gets the positions of non zeros of adj into indices (without or with diags ??)\n",
    "adj = normalize_adjacency(adj) # Normalizes the adjacency matrix only by adding ones to diag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a62de",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors[authors.index==indices[0, 1]]['authors']\n",
    "authors[authors.index==indices[1, 1]]['authors']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17705226",
   "metadata": {},
   "source": [
    "# With Giannis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aedd179",
   "metadata": {},
   "source": [
    "How to do if the edge\n",
    "\n",
    "To learn some low dimension of the authors and then concatenate them to the features with word2vec.\n",
    "features of pairs of nodes. treat them and then concatenate.\n",
    "\n",
    "NLP: directly the CNN and classify the nodes and train the model to predict if they are connected. And then combine with the GNN. From this model, once it's trained, we can use the embedded representation. Or combine the results.\n",
    "\n",
    "The second thing to do: in a single model CNN to produce some embedding of the abstract. Annotate based on the features.\n",
    "If you plan to do, he can take a \n",
    "\n",
    "\n",
    "It takes a pair of abstract and then predict if they site each other.\n",
    "\n",
    "Keep seperated abstracts.\n",
    "Element wide multiplication product or the difference of the two vectors. Multiplication of elements of vectors. Or sum or the absolute difference of the vectors. Once we have this vector, we apply a fc to predict the result.\n",
    "\n",
    "We can give the model the same abstract twice and it will give the same vector twice.\n",
    "\n",
    "We can use the word embedding of Google to have a matrix of the abstract. We need to use the embedding of the words and\n",
    "Or we can use a random embdedding of the words to have an intial embedding. And then we can fine tune the embedding during the training.\n",
    "\n",
    "\n",
    "For the seperation of validation and training, we need to make sure that there is no isolated nodes.\n",
    "\n",
    "\n",
    "Message passing then attention.\n",
    "\n",
    "\n",
    "\n",
    "We can create a graph of authors linked by the apers. And then make an embedding of the authors. Deep walk.\n",
    "\n",
    "We can have one graph of papers and papers. authors is connected to a paper. edges = the ones already exist from G. Then add extra nodes and edges. new nodes = authors and new edges = author -> paper.\n",
    "\n",
    "\n",
    "We can seperate the graph: unsupervised algorithm clustering. --> louvain python package. We can apply it to some graph. We can have one hot vector and each dimension correspond to one community.\n",
    "\n",
    "\n",
    "To take the last "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e30355",
   "metadata": {},
   "source": [
    "## Amin\n",
    "Get red of the authors that occured below a certain threshold and keep authors occuring a minimum number of times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec47747",
   "metadata": {},
   "source": [
    "### Is it fine to create walks only from the G_train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0858eadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "walks = generate_walks(G=G_train, num_walks=10, walk_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4b75c226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102892850, 102892850)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_model = Word2Vec(vector_size=128, window=5, min_count=0, sg=1, workers=8)\n",
    "wv_model.build_vocab(walks)\n",
    "wv_model.train(walks, total_examples=wv_model.corpus_count, epochs=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0e097cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features initializaed from deepwalk wv embedding\n",
    "features_np = []\n",
    "for node in G_train.nodes():\n",
    "    features_np.append(wv_model.wv[str(node)])\n",
    "features_np = np.array(features_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d6458fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Create class labels\n",
    "y = np.zeros(4*G_train.number_of_edges())\n",
    "y[:2*G_train.number_of_edges()] = 1 # Concatenated ones for edges indices and zeros for random indices.\n",
    "\n",
    "# Transforms the numpy matrices/vectors to torch tensors.\n",
    "features = torch.FloatTensor(features_np).to(device)\n",
    "y = torch.LongTensor(y).to(device)\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "indices = torch.LongTensor(indices).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9094f884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([138499, 138499])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "14a08291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 30\n",
    "n_hidden = 128\n",
    "dropout_rate = 0.2\n",
    "n_class = 2\n",
    "n_features = features.shape[1]\n",
    "\n",
    "# Creates the model and specifies the optimizer\n",
    "model = GNN(n_features, n_hidden, n_class, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "daac32ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 0.6932 acc_train: 0.5271 time: 38.7843s total_time: 1min\n",
      "Epoch: 006 loss_train: 0.5129 acc_train: 0.6261 time: 18.3107s total_time: 2min\n",
      "Epoch: 011 loss_train: 0.4141 acc_train: 0.8407 time: 15.7362s total_time: 4min\n",
      "Epoch: 016 loss_train: 0.3081 acc_train: 0.9000 time: 18.0251s total_time: 5min\n",
      "Epoch: 021 loss_train: 0.2308 acc_train: 0.9194 time: 15.7148s total_time: 7min\n",
      "Epoch: 026 loss_train: 0.1880 acc_train: 0.9290 time: 21.7787s total_time: 8min\n",
      "Optimization Finished in 10 min!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Train model\n",
    "model.train()\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    t = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)# We take random indices each time we run an epoch\n",
    "    pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices.   \n",
    "    output = model(features, adj, pairs) # we run the model that gives the output.\n",
    "    loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "    #print(type(loss_train), '\\n', loss_train.shape)\n",
    "    acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "    loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "    optimizer.step() # Performs a single optimization step (parameter update).\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "              'time: {:.4f}s'.format(time.time() - t),\n",
    "             'total_time: {}min'.format(round((time.time() - start_time)/60)))\n",
    "\n",
    "print(\"Optimization Finished in {} min!\".format(round((time.time() - start_time)/60)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b5bda6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_val)-len(val_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "26270d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 2.2495268918639555\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(val_edges))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "output = model(features, adj, pairs)\n",
    "y_pred = torch.exp(output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "    \n",
    "print('Log loss:', log_loss(y_val, y_pred_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ef65edfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1216"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(np.transpose(y_val)).shape[0] - 219308"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e74c8f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_pairs = np.array(np.transpose(val_edges))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "#features of the nodes\n",
    "#pairs of the val edges\n",
    "output = model(features, adj, pairs)\n",
    "y_pred_val = torch.exp(output)\n",
    "y_pred_val = y_pred_val.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "932f2680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97891575, 0.63894755, 0.9650434 , ..., 0.03416799, 0.8449153 ,\n",
       "       0.00570035], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_val[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358d7e38",
   "metadata": {},
   "source": [
    "### Why is it too big compared to loss obtained in Kaggle?! And that the log loss > 1 even if values are between 0 and 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b278d11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 2.2495268918639555\n"
     ]
    }
   ],
   "source": [
    "print('Log loss:', log_loss(y_val, y_pred_val[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0a9d9fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('../test_data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "output = model(features, adj, pairs)\n",
    "y_pred = torch.exp(output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 100000)\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"../submissions_files/{}-submission-{}.csv\".format(today, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22a327",
   "metadata": {},
   "source": [
    "CNN with Sigmoid to predict if they connected based on the abstract."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cad23c",
   "metadata": {},
   "source": [
    "A good way to aggregate the text is to calculate the average (mean)\n",
    "\n",
    "Another approach is to use directly a GNN. Take the abstract, compute the embeddng of the words. Take the mean of the node.\n",
    "Then you can take the features.\n",
    "\n",
    "For thr GN, we have only the \n",
    "\n",
    "pairs is a tensor, contains a pair of nodes that contains all the positive samples and some of the negative samples. y: half of them are equal to one, and half of them are connected. rand_indices are random pairs that are considered as not connected.\n",
    "\n",
    "As we have a non directed. We can take twice every edge (2*m instead of 2*m for y. Or we can take the edges only once.\n",
    "\n",
    "One vector for the abstract using the word2vec embedding or any other similar approach.\n",
    "\n",
    "Or we can directly use a CNN.We can take a CNN and feed pais of abstracts in the CNN, the CNN will produce one vector for the first abstract and one vector for the second. We can combine these two vectors.\n",
    "\n",
    "Then we can use an MLP to produce a vector, and then we can concatenate the two vectors from CNN and MLP.\n",
    "\n",
    "There is a pretrained word embedding (Google provided a pretrained embedding).\n",
    "\n",
    "Embedding of each word. Then the CNN will provide one vector for the abstract.\n",
    "\n",
    "Each abstract has a different number of words. the representation of the CNN will have a fixed size of the embedding vector.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af9219e",
   "metadata": {},
   "source": [
    "### Embedding of abstract? Unsupervised? Or get it inside the global MLP?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
