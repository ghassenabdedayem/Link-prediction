{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787a2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import random_walk, generate_walks, sparse_mx_to_torch_sparse_tensor \n",
    "from utils import text_to_list, intersection, read_train_val_graph, save_subgraph_in_file\n",
    "from utils import apply_word2vec_on_features, create_and_normalize_adjacency, train_model, add_authors_to_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d25112fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "667086c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499 number of edges: 1091955 in the Complete the set\n",
      "Number of nodes: 138499 number of edges: 982684 in the Training set\n",
      "len(nodes) 138499\n",
      "Creating random val_edges...\n",
      "Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects\n",
      "Loaded from edgelist.txt and with a training validation split ratio = 0.1\n",
      "Start generating walks....\n",
      "Random walks generated in in 54s!\n",
      "Start applying Word2Vec...\n",
      "Word2vec model trained on features in 2 min!\n",
      "(138499, 64) features numpy array created in 2 min!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghassenabdedayem/Documents/Data/Polytechnique/5- Data Challenge/data_challenge_2022/draft_nb/utils.py:157: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix of the training graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a normalized adjancency matrix of shape (138499, 138499)\n",
      "Created indices (2, 2103867) with the positions of non zeros in adj matrix\n"
     ]
    }
   ],
   "source": [
    "path = '../input_data/edgelist.txt'\n",
    "G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx = read_train_val_graph(val_ratio=0.1, path=path)\n",
    "walks = generate_walks(G=G_train, num_walks=10, walk_length=15)\n",
    "walks_wv = apply_word2vec_on_features(features=walks, nodes=nodes, vector_size=64)\n",
    "adj, indices = create_and_normalize_adjacency(G_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69776784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_np = np.concatenate([walks_wv, authors_wv], axis=1)\n",
    "features_np = walks_wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d6138a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_csv('../input_data/authors.txt', sep = '|', header=None)\n",
    "authors = authors.rename(columns={0: \"paper_id\", 2: \"authors\"})\n",
    "authors['authors'] = authors['authors'].apply(text_to_list)\n",
    "authors = authors[[\"paper_id\", \"authors\"]]\n",
    "authors = authors[authors['paper_id'] <= max(G.nodes())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bf540ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class labels\n",
    "y = np.zeros(2*indices.shape[1])\n",
    "y[:indices.shape[1]] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
    "\n",
    "# Transforms the numpy matrices/vectors to torch tensors.\n",
    "features = torch.FloatTensor(features_np).to(device)\n",
    "y = torch.LongTensor(y).to(device)\n",
    "if type(adj) != torch.Tensor:\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "indices = torch.LongTensor(indices).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a434e135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c48af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_feat, n_hidden, n_class, sub_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.double_fc3 = nn.Linear((3*n_hidden), n_hidden)\n",
    "        self.fc4 = nn.Linear(n_hidden, sub_class)\n",
    "        self.fc5 = nn.Linear(sub_class, n_class)        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x_in, adj, pairs):\n",
    "        \n",
    "        h1 = self.fc1(x_in)\n",
    "        z1 = self.relu(torch.mm(adj, h1))\n",
    "        z1 = self.dropout(z1)\n",
    "\n",
    "        h2 = self.fc2(z1)\n",
    "        z2 = self.relu(torch.mm(adj, h2))\n",
    "        z2 = self.dropout(z2)\n",
    "\n",
    "        x = z2[pairs[0]] - z2[pairs[1]] # embedded features (z2) of node 0 - embedded features of node 1\n",
    "        x = pairs[3][:, None] * x\n",
    "        x1 = z2[pairs[0]]\n",
    "        x2 = z2[pairs[1]]\n",
    "        x = torch.cat((x, x1, x2), dim=1)\n",
    "        \n",
    "#         x_auth = pairs[2].reshape([len(pairs[2]), 1])\n",
    "#         x_nb_auth = pairs[3].reshape([len(pairs[3]), 1]) \n",
    "        \n",
    "        \n",
    "        \n",
    "        #x1 = z2[pairs[0]]\n",
    "        #x2 = z2[pairs[1]]\n",
    "        # could we add a new dimension to pairs to specify if same author(s)? and then what could we do?\n",
    "              \n",
    "        #x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.relu(self.double_fc3(x))        \n",
    "        #x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        #x = torch.cat((x, x_auth, x_nb_auth), dim=1) #pairs[3] : number of same authors or 1 if same author\n",
    "        x = self.fc5(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7eb8d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "n_hidden = 64\n",
    "dropout_rate = 0.2\n",
    "sub_class = 8\n",
    "n_class = 2\n",
    "n_features = features.shape[1]\n",
    "\n",
    "# Creates the model and specifies the optimizer\n",
    "model = GNN(n_features, n_hidden, n_class, sub_class, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0c97d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, features, authors, adj, indices, y, val_indices, y_val, epochs):\n",
    "    # Train model\n",
    "    model.train()\n",
    "    start_time = time()\n",
    "    val_indices = add_authors_to_pairs(val_indices, authors) #we add the authors to val_pairs\n",
    "    indices = add_authors_to_pairs(indices, authors) #we add the authors to indices\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "        optimizer.zero_grad()\n",
    "        rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)# We take random indices each time we run an epoch\n",
    "        rand_indices = add_authors_to_pairs(rand_indices, authors)\n",
    "        pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices.   \n",
    "        #pairs = add_authors_to_pairs(pairs) #we add the authors to the pairs\n",
    "        output = model(features, adj, pairs) # we run the model that gives the output.\n",
    "        loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "        acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "        loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "        optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, adj, val_indices)\n",
    "        y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {} s'.format(int(round(time()) - round(t))),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "        if epoch % 20 == 0:\n",
    "            model_path = \"../outputs/models/{}-model-{}epochs-slow.pt\".format(today, epoch)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c7fbd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 0.6935 loss_val: 0.6920 acc_train: 0.5000 acc_val: 0.5000 time: 111 s total_time: 4 min\n",
      "Epoch: 006 loss_train: 0.6819 loss_val: 0.6715 acc_train: 0.6218 acc_val: 0.6613 time: 96 s total_time: 13 min\n",
      "Epoch: 011 loss_train: 0.6621 loss_val: 0.6536 acc_train: 0.6778 acc_val: 0.6852 time: 110 s total_time: 22 min\n",
      "Epoch: 016 loss_train: 0.6355 loss_val: 0.6244 acc_train: 0.6963 acc_val: 0.7024 time: 102 s total_time: 31 min\n",
      "Epoch: 021 loss_train: 0.5360 loss_val: 0.5513 acc_train: 0.7458 acc_val: 0.7419 time: 97 s total_time: 39 min\n",
      "Epoch: 026 loss_train: 0.4847 loss_val: 0.5502 acc_train: 0.7881 acc_val: 0.7655 time: 114 s total_time: 47 min\n",
      "Epoch: 031 loss_train: 0.4599 loss_val: 0.5531 acc_train: 0.8038 acc_val: 0.7851 time: 103 s total_time: 56 min\n",
      "Epoch: 036 loss_train: 0.4296 loss_val: 0.5106 acc_train: 0.8233 acc_val: 0.7945 time: 95 s total_time: 65 min\n",
      "Epoch: 041 loss_train: 0.3918 loss_val: 0.5053 acc_train: 0.8294 acc_val: 0.8047 time: 114 s total_time: 74 min\n",
      "Epoch: 046 loss_train: 0.4124 loss_val: 0.4925 acc_train: 0.8166 acc_val: 0.8069 time: 133 s total_time: 83 min\n",
      "Epoch: 051 loss_train: 0.3696 loss_val: 0.4708 acc_train: 0.8363 acc_val: 0.8042 time: 99 s total_time: 92 min\n",
      "Epoch: 056 loss_train: 0.3695 loss_val: 0.4775 acc_train: 0.8449 acc_val: 0.8148 time: 124 s total_time: 101 min\n",
      "Epoch: 061 loss_train: 0.3555 loss_val: 0.4760 acc_train: 0.8455 acc_val: 0.8170 time: 111 s total_time: 110 min\n",
      "Epoch: 066 loss_train: 0.3526 loss_val: 0.4524 acc_train: 0.8449 acc_val: 0.8198 time: 119 s total_time: 119 min\n",
      "Epoch: 071 loss_train: 0.3444 loss_val: 0.4487 acc_train: 0.8506 acc_val: 0.8219 time: 98 s total_time: 128 min\n",
      "Epoch: 076 loss_train: 0.3395 loss_val: 0.4483 acc_train: 0.8541 acc_val: 0.8241 time: 95 s total_time: 136 min\n",
      "Epoch: 081 loss_train: 0.3355 loss_val: 0.4424 acc_train: 0.8566 acc_val: 0.8281 time: 103 s total_time: 144 min\n",
      "Epoch: 086 loss_train: 0.3373 loss_val: 0.4339 acc_train: 0.8595 acc_val: 0.8317 time: 101 s total_time: 153 min\n",
      "Epoch: 091 loss_train: 0.3217 loss_val: 0.4198 acc_train: 0.8650 acc_val: 0.8425 time: 103 s total_time: 162 min\n",
      "Epoch: 096 loss_train: 0.3197 loss_val: 0.3986 acc_train: 0.8760 acc_val: 0.8504 time: 96 s total_time: 170 min\n",
      "Epoch: 101 loss_train: 0.2883 loss_val: 0.3806 acc_train: 0.8870 acc_val: 0.8595 time: 101 s total_time: 179 min\n",
      "Epoch: 106 loss_train: 0.2706 loss_val: 0.3942 acc_train: 0.8931 acc_val: 0.8601 time: 121 s total_time: 188 min\n",
      "Epoch: 111 loss_train: 0.2618 loss_val: 0.3812 acc_train: 0.8973 acc_val: 0.8658 time: 93 s total_time: 197 min\n",
      "Epoch: 116 loss_train: 0.2521 loss_val: 0.4155 acc_train: 0.8998 acc_val: 0.8615 time: 93 s total_time: 205 min\n",
      "Epoch: 121 loss_train: 0.2492 loss_val: 0.3722 acc_train: 0.9031 acc_val: 0.8706 time: 96 s total_time: 213 min\n",
      "Epoch: 126 loss_train: 0.2470 loss_val: 0.3618 acc_train: 0.9009 acc_val: 0.8720 time: 109 s total_time: 222 min\n",
      "Epoch: 131 loss_train: 0.2416 loss_val: 0.3683 acc_train: 0.9040 acc_val: 0.8725 time: 102 s total_time: 230 min\n",
      "Epoch: 136 loss_train: 0.2378 loss_val: 0.3701 acc_train: 0.9064 acc_val: 0.8729 time: 104 s total_time: 239 min\n",
      "Epoch: 141 loss_train: 0.2343 loss_val: 0.3618 acc_train: 0.9075 acc_val: 0.8755 time: 122 s total_time: 248 min\n",
      "Epoch: 146 loss_train: 0.2318 loss_val: 0.3731 acc_train: 0.9081 acc_val: 0.8751 time: 111 s total_time: 257 min\n",
      "Epoch: 151 loss_train: 0.2283 loss_val: 0.3720 acc_train: 0.9096 acc_val: 0.8779 time: 98 s total_time: 267 min\n",
      "Epoch: 156 loss_train: 0.2257 loss_val: 0.3731 acc_train: 0.9107 acc_val: 0.8788 time: 99 s total_time: 276 min\n",
      "Epoch: 161 loss_train: 0.2240 loss_val: 0.3770 acc_train: 0.9114 acc_val: 0.8793 time: 100 s total_time: 284 min\n",
      "Epoch: 166 loss_train: 0.2230 loss_val: 0.3862 acc_train: 0.9117 acc_val: 0.8784 time: 118 s total_time: 293 min\n",
      "Epoch: 171 loss_train: 0.2209 loss_val: 0.3825 acc_train: 0.9134 acc_val: 0.8808 time: 108 s total_time: 302 min\n",
      "Epoch: 176 loss_train: 0.2191 loss_val: 0.3816 acc_train: 0.9140 acc_val: 0.8814 time: 115 s total_time: 312 min\n",
      "Epoch: 181 loss_train: 0.2177 loss_val: 0.3757 acc_train: 0.9149 acc_val: 0.8830 time: 126 s total_time: 321 min\n",
      "Epoch: 186 loss_train: 0.2144 loss_val: 0.3699 acc_train: 0.9160 acc_val: 0.8840 time: 104 s total_time: 330 min\n",
      "Epoch: 191 loss_train: 0.2202 loss_val: 0.3731 acc_train: 0.9126 acc_val: 0.8835 time: 98 s total_time: 338 min\n",
      "Epoch: 196 loss_train: 0.2134 loss_val: 0.3513 acc_train: 0.9175 acc_val: 0.8852 time: 105 s total_time: 347 min\n",
      "Epoch: 201 loss_train: 0.2077 loss_val: 0.3880 acc_train: 0.9184 acc_val: 0.8783 time: 53 s total_time: 352 min\n",
      "Epoch: 206 loss_train: 0.2075 loss_val: 0.3329 acc_train: 0.9183 acc_val: 0.8876 time: 53 s total_time: 357 min\n",
      "Epoch: 211 loss_train: 0.2058 loss_val: 0.3647 acc_train: 0.9193 acc_val: 0.8849 time: 53 s total_time: 361 min\n",
      "Epoch: 216 loss_train: 0.2021 loss_val: 0.3352 acc_train: 0.9225 acc_val: 0.8911 time: 55 s total_time: 366 min\n",
      "Epoch: 221 loss_train: 0.1954 loss_val: 0.3369 acc_train: 0.9235 acc_val: 0.8913 time: 53 s total_time: 370 min\n",
      "Epoch: 226 loss_train: 0.1880 loss_val: 0.3234 acc_train: 0.9274 acc_val: 0.8954 time: 54 s total_time: 375 min\n",
      "Epoch: 231 loss_train: 0.1826 loss_val: 0.3217 acc_train: 0.9288 acc_val: 0.8974 time: 54 s total_time: 379 min\n",
      "Epoch: 236 loss_train: 0.1773 loss_val: 0.3159 acc_train: 0.9315 acc_val: 0.8991 time: 53 s total_time: 384 min\n",
      "Epoch: 241 loss_train: 0.1735 loss_val: 0.3226 acc_train: 0.9327 acc_val: 0.8991 time: 54 s total_time: 388 min\n",
      "Epoch: 246 loss_train: 0.2100 loss_val: 0.3245 acc_train: 0.9179 acc_val: 0.8972 time: 58 s total_time: 393 min\n",
      "Epoch: 251 loss_train: 0.1974 loss_val: 0.3249 acc_train: 0.9252 acc_val: 0.8965 time: 60 s total_time: 397 min\n",
      "Epoch: 256 loss_train: 0.1786 loss_val: 0.3065 acc_train: 0.9322 acc_val: 0.9003 time: 55 s total_time: 402 min\n",
      "Epoch: 261 loss_train: 0.1701 loss_val: 0.3040 acc_train: 0.9342 acc_val: 0.9015 time: 60 s total_time: 411 min\n",
      "Epoch: 266 loss_train: 0.1693 loss_val: 0.3238 acc_train: 0.9345 acc_val: 0.8983 time: 57 s total_time: 416 min\n",
      "Epoch: 271 loss_train: 0.1664 loss_val: 0.3057 acc_train: 0.9361 acc_val: 0.9028 time: 55 s total_time: 421 min\n",
      "Epoch: 276 loss_train: 0.1599 loss_val: 0.3244 acc_train: 0.9384 acc_val: 0.8989 time: 59 s total_time: 425 min\n",
      "Epoch: 281 loss_train: 0.1605 loss_val: 0.3251 acc_train: 0.9377 acc_val: 0.9004 time: 57 s total_time: 430 min\n",
      "Epoch: 286 loss_train: 0.1564 loss_val: 0.3310 acc_train: 0.9396 acc_val: 0.8995 time: 62 s total_time: 435 min\n",
      "Epoch: 291 loss_train: 0.1621 loss_val: 0.3034 acc_train: 0.9387 acc_val: 0.9060 time: 55 s total_time: 440 min\n",
      "Epoch: 296 loss_train: 0.1513 loss_val: 0.3323 acc_train: 0.9421 acc_val: 0.8992 time: 56 s total_time: 445 min\n",
      "Epoch: 301 loss_train: 0.1614 loss_val: 0.3152 acc_train: 0.9375 acc_val: 0.9034 time: 59 s total_time: 451 min\n",
      "Epoch: 306 loss_train: 0.1512 loss_val: 0.2923 acc_train: 0.9426 acc_val: 0.9087 time: 57 s total_time: 456 min\n",
      "Epoch: 311 loss_train: 0.1477 loss_val: 0.3225 acc_train: 0.9437 acc_val: 0.9019 time: 62 s total_time: 461 min\n",
      "Epoch: 316 loss_train: 0.1534 loss_val: 0.3183 acc_train: 0.9408 acc_val: 0.9036 time: 57 s total_time: 466 min\n",
      "Epoch: 321 loss_train: 0.1438 loss_val: 0.2983 acc_train: 0.9449 acc_val: 0.9091 time: 54 s total_time: 471 min\n",
      "Epoch: 326 loss_train: 0.1422 loss_val: 0.3016 acc_train: 0.9456 acc_val: 0.9093 time: 71 s total_time: 476 min\n",
      "Epoch: 331 loss_train: 0.1534 loss_val: 0.3492 acc_train: 0.9409 acc_val: 0.8968 time: 55 s total_time: 481 min\n",
      "Epoch: 336 loss_train: 0.1445 loss_val: 0.3315 acc_train: 0.9444 acc_val: 0.9021 time: 58 s total_time: 485 min\n",
      "Epoch: 341 loss_train: 0.1438 loss_val: 0.3103 acc_train: 0.9446 acc_val: 0.9079 time: 56 s total_time: 490 min\n",
      "Epoch: 346 loss_train: 0.1370 loss_val: 0.3006 acc_train: 0.9476 acc_val: 0.9113 time: 52 s total_time: 495 min\n",
      "Epoch: 351 loss_train: 0.1376 loss_val: 0.3151 acc_train: 0.9473 acc_val: 0.9086 time: 55 s total_time: 499 min\n",
      "Epoch: 356 loss_train: 0.1347 loss_val: 0.3282 acc_train: 0.9485 acc_val: 0.9061 time: 55 s total_time: 504 min\n",
      "Epoch: 361 loss_train: 0.1820 loss_val: 0.3582 acc_train: 0.9309 acc_val: 0.8949 time: 55 s total_time: 508 min\n",
      "Epoch: 366 loss_train: 0.1754 loss_val: 0.3019 acc_train: 0.9347 acc_val: 0.9066 time: 54 s total_time: 513 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 371 loss_train: 0.1584 loss_val: 0.3239 acc_train: 0.9416 acc_val: 0.8985 time: 55 s total_time: 517 min\n",
      "Epoch: 376 loss_train: 0.1450 loss_val: 0.3223 acc_train: 0.9453 acc_val: 0.9021 time: 54 s total_time: 522 min\n",
      "Epoch: 381 loss_train: 0.1411 loss_val: 0.3162 acc_train: 0.9461 acc_val: 0.9064 time: 53 s total_time: 526 min\n",
      "Epoch: 386 loss_train: 0.1365 loss_val: 0.3169 acc_train: 0.9480 acc_val: 0.9059 time: 53 s total_time: 531 min\n",
      "Epoch: 391 loss_train: 0.1355 loss_val: 0.3044 acc_train: 0.9486 acc_val: 0.9093 time: 53 s total_time: 535 min\n",
      "Epoch: 396 loss_train: 0.1315 loss_val: 0.2884 acc_train: 0.9501 acc_val: 0.9141 time: 52 s total_time: 540 min\n",
      "Epoch: 401 loss_train: 0.1298 loss_val: 0.3109 acc_train: 0.9507 acc_val: 0.9090 time: 53 s total_time: 544 min\n",
      "Epoch: 406 loss_train: 0.1280 loss_val: 0.2935 acc_train: 0.9514 acc_val: 0.9142 time: 53 s total_time: 549 min\n",
      "Epoch: 411 loss_train: 0.1274 loss_val: 0.2929 acc_train: 0.9518 acc_val: 0.9146 time: 53 s total_time: 553 min\n",
      "Epoch: 416 loss_train: 0.1256 loss_val: 0.2994 acc_train: 0.9525 acc_val: 0.9137 time: 56 s total_time: 558 min\n",
      "Epoch: 421 loss_train: 0.1246 loss_val: 0.2992 acc_train: 0.9529 acc_val: 0.9141 time: 54 s total_time: 562 min\n",
      "Epoch: 426 loss_train: 0.1236 loss_val: 0.2955 acc_train: 0.9532 acc_val: 0.9157 time: 52 s total_time: 567 min\n",
      "Epoch: 431 loss_train: 0.1226 loss_val: 0.3096 acc_train: 0.9536 acc_val: 0.9129 time: 73 s total_time: 572 min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 12\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, features, authors, adj, indices, y, val_indices, y_val, epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m rand_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, features\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), (indices\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m),indices\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)), device\u001b[38;5;241m=\u001b[39madj\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;66;03m# We take random indices each time we run an epoch\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m rand_indices \u001b[38;5;241m=\u001b[39m \u001b[43madd_authors_to_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrand_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m pairs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((indices, rand_indices), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Concatenate the edges indices and random indices.   \u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#pairs = add_authors_to_pairs(pairs) #we add the authors to the pairs\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Data/Polytechnique/5- Data Challenge/data_challenge_2022/draft_nb/utils.py:181\u001b[0m, in \u001b[0;36madd_authors_to_pairs\u001b[0;34m(pairs, authors)\u001b[0m\n\u001b[1;32m    178\u001b[0m pairs_df\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaper_id_x\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaper_id_y\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    180\u001b[0m pairs_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnb_common_author\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pairs_df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: intersection(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthors_1\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthors_2\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;241m0\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 181\u001b[0m pairs_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_common_author\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpairs_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mintersection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauthors_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauthors_2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m pairs_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(np\u001b[38;5;241m.\u001b[39mtranspose(pairs_df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaper_1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaper_2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_common_author\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnb_common_author\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()))\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pairs_tensor\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:8848\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   8837\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m   8839\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m   8840\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   8841\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8846\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   8847\u001b[0m )\n\u001b[0;32m-> 8848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py:733\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 733\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py:857\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 857\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py:873\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    874\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    875\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    876\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    877\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/Data/Polytechnique/5- Data Challenge/data_challenge_2022/draft_nb/utils.py:181\u001b[0m, in \u001b[0;36madd_authors_to_pairs.<locals>.<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m    178\u001b[0m pairs_df\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaper_id_x\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaper_id_y\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    180\u001b[0m pairs_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnb_common_author\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pairs_df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: intersection(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthors_1\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthors_2\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;241m0\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 181\u001b[0m pairs_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_common_author\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pairs_df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: intersection(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthors_1\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauthors_2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)[\u001b[38;5;241m1\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    183\u001b[0m pairs_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(np\u001b[38;5;241m.\u001b[39mtranspose(pairs_df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaper_1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaper_2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_common_author\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnb_common_author\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()))\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pairs_tensor\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py:958\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py:1069\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1069\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py:3627\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tolerance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3624\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtolerance argument only valid if using pad, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3625\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackfill or nearest lookups\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3626\u001b[0m     )\n\u001b[0;32m-> 3627\u001b[0m casted_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_cast_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3628\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py:6304\u001b[0m, in \u001b[0;36mIndex._maybe_cast_indexer\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   6299\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6300\u001b[0m \u001b[38;5;124;03mIf we have a float key and are not a floating index, then try to cast\u001b[39;00m\n\u001b[1;32m   6301\u001b[0m \u001b[38;5;124;03mto an int if equivalent.\u001b[39;00m\n\u001b[1;32m   6302\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_floating():\n\u001b[0;32m-> 6304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast_scalar_indexer\u001b[49m(key)\n\u001b[1;32m   6305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m key\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, optimizer, features, authors, adj, indices, y, val_indices, y_val, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81ac05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819ad57f",
   "metadata": {},
   "source": [
    "Essayer de remplacer les tags des auteurs par des 1 ones pour voir si l'autheur améliore vraiment le résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf86a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, features, adj, indices, y, val_indices, y_val, epochs):\n",
    "    # Train model\n",
    "    model.train()\n",
    "    start_time = time()\n",
    "    val_indices = add_authors_to_pairs(val_indices) #we add the authors to val_pairs\n",
    "    indices = add_authors_to_pairs(indices) #we add the authors to indices\n",
    "    rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)# We take random indices each time we run an epoch\n",
    "    rand_indices = add_authors_to_pairs(rand_indices)\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "        optimizer.zero_grad()\n",
    "        pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices.   \n",
    "        output = model(features, adj, pairs) # we run the model that gives the output.\n",
    "        loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "        acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "        loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "        optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, adj, val_indices)\n",
    "        y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {:.4f} s'.format(round(time() - t)),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "        if epochs % 50 == 0:\n",
    "            model_path = \"../outputs/models/{}-model-{}epochs.pt\".format(today, epoch)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model\n",
    "\n",
    "epochs = 100\n",
    "trained_model = train_model(model, optimizer, features, adj, indices, y, val_indices, y_val, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "break\n",
    "#y_val = torch.FloatTensor(y_val).to(device)\n",
    "trained_model = train_model(model, optimizer, features, adj, indices, y, val_indices, y_val, epochs)\n",
    "model_nb = randint(0, 1000)\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "model_path = \"../submissions_files/{}-model-{}epochs-{}.pt\".format(today, epochs, model_nb)\n",
    "torch.save(trained_model.state_dict(), model_path)\n",
    "print('Model saved in', model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c0b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot representation using Spark\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "def multi_label_binarizer(df, labels_col='labels', output_col='new_labels'):\n",
    "    \"\"\"\n",
    "    Function that takes as input:\n",
    "    - `df`, pyspark.sql.dataframe \n",
    "    - `labels_col`, string that indicates an array column containing labels\n",
    "    - `output_col`, string that indicates the name of the new labels column\n",
    "    \n",
    "    and returns a multi-label binarized column.\n",
    "    \"\"\"\n",
    "    \n",
    "    # get set of unique labels and sort them\n",
    "    labels_set = df\\\n",
    "        .withColumn('exploded', F.explode('labels'))\\\n",
    "        .agg(F.collect_set('exploded'))\\\n",
    "        .collect()[0][0]\n",
    "    labels_set = sorted(labels_set)\n",
    "    \n",
    "    # dynamically create columns for each value in `labels_set`\n",
    "    for i in labels_set:\n",
    "        df = df.withColumn(i, F.when(F.array_contains(labels_col, i), 1).otherwise(0))\n",
    "        \n",
    "    # create new, multi-label binarized array column\n",
    "    df = df.withColumn(output_col, F.array(*labels_set))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "output = model(features, adj, val_indices)\n",
    "y_val = torch.LongTensor(y_val).to(device)\n",
    "loss_val = F.nll_loss(output, y_val)\n",
    "acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "print(loss_val.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b650a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "trained_model.eval()\n",
    "eval_pairs = np.array(np.transpose(val_edges))\n",
    "#print(eval_pairs.shape)\n",
    "eval_pairs = torch.LongTensor(eval_pairs).to(device)\n",
    "#print(eval_pairs.shape)\n",
    "eval_output = trained_model(features, adj, eval_pairs)\n",
    "#print(eval_output.shape)\n",
    "y_pred = torch.exp(eval_output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "#y_val_pred_true = list()\n",
    "\n",
    "y_val_pred_true = y_pred[:, 1]\n",
    "\n",
    "    \n",
    "print('Log loss:', log_loss(y_val, y_val_pred_true))\n",
    "\n",
    "#y_val = torch.tensor(y_val).to(device)\n",
    "#y_val_pred_true = torch.tensor(y_val_pred_true).to(device)\n",
    "#print(y_val, y_val_pred_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('../test_data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "output = model(features, adj, pairs)\n",
    "y_pred = torch.exp(output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"../submissions_files/{}-submission-{}-{}.csv\".format(today, model_nb, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f130ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bc8b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = y_val.detach().cpu().numpy()\n",
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80600b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred)\n",
    "df['y_val'] = list(y_val)\n",
    "df.to_csv('../submissions_files/comparison_file.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae66fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(y_val, y_pred[:, 1]), mean_absolute_error(y_val, y_pred[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1822541",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_val, y_pred[:, 1]), log_loss(y_val, y_pred[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7e912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5717e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff03bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a828859c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e3c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c29d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b974905",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = np.array([[5, 3, 0], [2, 1, 0], [4, 0, 1], [0, 3, 2]])\n",
    "test_features = torch.LongTensor(test_features).to(device)\n",
    "test_pairs = [[0, 0, 1, 2, 3, 2, 0, 1, 2, 3], [1, 2, 0, 0, 2, 3, 0, 1, 2, 3]]\n",
    "test_pairs = torch.LongTensor(test_pairs).to(device)\n",
    "print(test_features.shape, test_pairs.shape)\n",
    "test_features[test_pairs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0639bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699d53ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape, np.shape(y_val), np.shape(y_pred_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf150c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ccd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(output.shape)\n",
    "print(features.shape, adj.shape, eval_pairs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4446f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "z2[pairs[1,:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bdb8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e2dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff0f64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425d6600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8607dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0728cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ad5476",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71737f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class labels\n",
    "y = np.zeros(4*G_train.number_of_edges())\n",
    "y[:2*G_train.number_of_edges()] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
    "train_edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede8a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "X_train = np.zeros((4*m, 2*features_np.shape[1]))\n",
    "\n",
    "for i, edge in enumerate(train_edges):\n",
    "    X_train[i] = np.concatenate((features_np[train_edges[i][0]], features_np[train_edges[i][1]]), axis=0)\n",
    "    X_train[m+i] = np.concatenate((features_np[train_edges[i][0]], features_np[train_edges[i][1]]), axis=0)\n",
    "    X_train[2*m+i] = np.concatenate((features_np[randint(0,n-1)], features_np[randint(0,n-1)]), axis=0)\n",
    "    X_train[3*m+i] = np.concatenate((features_np[randint(0,n-1)], features_np[randint(0,n-1)]), axis=0)\n",
    "    \n",
    "X_val = np.zeros((len(val_edges), 2*features_np.shape[1]))\n",
    "for i, edge in enumerate(val_edges):\n",
    "    X_val[i] = np.concatenate((features_np[val_edges[i][0]], features_np[val_edges[i][1]]), axis=0)\n",
    "    \n",
    "print('X_train and X_val created in {} s!'.format(round(time()-t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44a3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6fa791",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y)\n",
    "y_pred = clf.predict_proba(X_val)\n",
    "y_pred = y_pred[:,1]\n",
    "print('Logistic regression performed in {} s!'.format(round(time()-t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c60b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Log loss:', log_loss(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b1ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac02a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Read test data. Each sample is a pair of nodes\n",
    "test_edges = list()\n",
    "with open('../test_data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        test_edges.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "X_test = np.zeros((len(test_edges), 2*features_np.shape[1]))\n",
    "for i, edge in enumerate(test_edges):\n",
    "    X_test[i] = np.concatenate((features_np[test_edges[i][0]], features_np[test_edges[i][1]]), axis=0)\n",
    "\n",
    "t = time()\n",
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "y_pred = y_pred[:,1]\n",
    "print('Logistic regression performed in {} s!'.format(round(time()-t)))\n",
    "\n",
    "\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 100000)\n",
    "\n",
    "pd.DataFrame(y_pred, columns={'predicted'}).to_csv(\n",
    "\"../submissions_files/{}-submission-{}.csv\".format(today, random_nb), header=True, index=True, index_label='id'\n",
    ")\n",
    "    \n",
    "    \n",
    "# # Testing\n",
    "# node_pairs = np.array(np.transpose(node_pairs))\n",
    "# pairs = torch.LongTensor(node_pairs).to(device)\n",
    "# output = model(features, adj, pairs)\n",
    "# y_pred = torch.exp(output)\n",
    "# y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "# y_pred_true = list()\n",
    "# for element in y_pred:\n",
    "#     y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "# today = datetime.today().strftime('%Y-%m-%d')\n",
    "# random_nb = randint(0, 100000)\n",
    "\n",
    "# pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "# \"../submissions_files/{}-submission-{}.csv\".format(today, random_nb), header=True, index=True, index_label='id'\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
