{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787a2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import random_walk, generate_walks, read_train_val_graph, sparse_mx_to_torch_sparse_tensor\n",
    "from utils import apply_word2vec_on_features, create_and_normalize_adjacency, save_subgraph_in_file, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d30a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d25112fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "057c89f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 nodes, 25591 edges Graph extracted from edgelist.txt\n",
      "4964 nodes, 25591 edges Graph saved in small_edgelist.txt\n",
      "4999\n"
     ]
    }
   ],
   "source": [
    "save_subgraph_in_file(nbr_nodes=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a94ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499 number of edges: 1091955 in All the set\n",
      "Number of nodes: 138499 number of edges: 982325 in the Training set\n",
      "len(nodes) 138499\n",
      "Creating random val_edges...\n",
      "Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects\n",
      "Loaded from edgelist.txt and with a training validation split ratio = 0.1\n",
      "Start generating walks....\n",
      "Random walks generated in in 56s!\n",
      "Start applying Word2Vec...\n",
      "Word2vec model trained on features in 3 min!\n",
      "(138499, 128) features numpy array created in 3 min!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghassenabdedayem/Documents/Data/Polytechnique/5- Data Challenge/data_challenge_2022/draft_nb/utils.py:146: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix of the training graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a normalized adjancency matrix of shape (138499, 138499)\n",
      "Created indices (2, 2103149) with the positions of non zeros in adj matrix\n"
     ]
    }
   ],
   "source": [
    "path = '../input_data/edgelist.txt'\n",
    "\n",
    "G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx = read_train_val_graph(val_ratio=0.1, path=path)\n",
    "walks = generate_walks(G=G_train, num_walks=10, walk_length=15)\n",
    "features_np = apply_word2vec_on_features(features=walks, nodes=nodes, vector_size=128)\n",
    "adj, indices = create_and_normalize_adjacency(G_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ff07f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_edges in G 109639\n",
      "val_edges not in G 109621\n",
      "val_edges in G_train 8\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "non_counter = 0\n",
    "for val_edge in val_edges:\n",
    "    if val_edge in G.edges():\n",
    "        counter += 1\n",
    "    else:\n",
    "        non_counter += 1\n",
    "print('val_edges in G', counter)\n",
    "print('val_edges not in G', non_counter)\n",
    "\n",
    "counter = 0\n",
    "for val_edge in val_edges:\n",
    "    if val_edge in G_train.edges():\n",
    "        counter += 1\n",
    "print('val_edges in G_train', counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8312de1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138498"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(np.transpose(val_edges)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bf540ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class labels\n",
    "y = np.zeros(2*indices.shape[1])\n",
    "y[:indices.shape[1]] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c20a3980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms the numpy matrices/vectors to torch tensors.\n",
    "features = torch.FloatTensor(features_np).to(device)\n",
    "y = torch.LongTensor(y).to(device)\n",
    "if type(adj) != torch.Tensor:\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "indices = torch.LongTensor(indices).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3549512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "def text_to_list(text):\n",
    "    return unidecode(text).split(',')\n",
    "\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    is_common = 1 if len(lst3)>0 else 0\n",
    "    return len(lst3), is_common\n",
    "\n",
    "#def columns_intersection(df):\n",
    "    \n",
    "\n",
    "def add_authors_to_pairs (pairs, authors):\n",
    "    np_pairs = np.array(pairs)\n",
    "\n",
    "    pairs_df = pd.DataFrame(np.transpose(pairs)).rename(columns={0: \"paper_1\", 1: \"paper_2\"})\n",
    "    pairs_df = pairs_df.merge(authors, left_on='paper_1', right_on='paper_id', how='left').rename(columns={'authors': \"authors_1\"})\n",
    "    pairs_df = pairs_df.merge(authors, left_on='paper_2', right_on='paper_id', how='left').rename(columns={'authors': \"authors_2\"})\n",
    "    pairs_df.drop(['paper_id_x', 'paper_id_y'], axis=1, inplace=True)\n",
    "\n",
    "    pairs_df['nb_common_author'] = pairs_df.apply(lambda row: intersection(row['authors_1'], row['authors_2'])[0], axis=1)\n",
    "    pairs_df['is_common_author'] = pairs_df.apply(lambda row: intersection(row['authors_1'], row['authors_2'])[1], axis=1)\n",
    "\n",
    "    pairs_tensor = torch.LongTensor(np.transpose(pairs_df[[\"paper_1\", \"paper_2\", 'is_common_author', 'nb_common_author']].values.tolist()))\n",
    "    \n",
    "    return pairs_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5538749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c48af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_feat, n_hidden, n_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.double_fc3 = nn.Linear((2*n_hidden), n_hidden)\n",
    "        self.fc4 = nn.Linear(n_hidden, n_class)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x_in, adj, pairs):\n",
    "        \n",
    "        h1 = self.fc1(x_in)\n",
    "        z1 = self.relu(torch.mm(adj, h1))\n",
    "        z1 = self.dropout(z1)\n",
    "\n",
    "        h2 = self.fc2(z1)\n",
    "        z2 = self.relu(torch.mm(adj, h2))\n",
    "        z2 = self.dropout(z2)\n",
    "\n",
    "        x = z2[pairs[0]] - z2[pairs[1]] # embedded features (z2) of node 0 - embedded features of node 1\n",
    "        \n",
    "        #x_auth = 2*pairs[2].reshape([len(pairs[2]), 1])\n",
    "        #x_nb_auth = 2*pairs[3].reshape([len(pairs[3]), 1]) \n",
    "        \n",
    "        #x = torch.cat((x, x_auth, x_nb_auth), dim=1) #pairs[3] : number of same authors or 1 if same author\n",
    "        \n",
    "        #x1 = z2[pairs[0]]\n",
    "        #x2 = z2[pairs[1]]\n",
    "        # could we add a new dimension to pairs to specify if same author(s)? and then what could we do?\n",
    "              \n",
    "        #x = torch.cat((x1, x2), dim=1)        \n",
    "        #x = self.relu(self.double_fc3(x))        \n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eb8d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "n_hidden = 128\n",
    "dropout_rate = 0.2\n",
    "n_class = 2\n",
    "n_features = features.shape[1]\n",
    "\n",
    "# Creates the model and specifies the optimizer\n",
    "model = GNN(n_features, n_hidden, n_class, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033efbf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6aaf1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train_model in module utils:\n",
      "\n",
      "train_model(model, optimizer, features, adj, indices, y, val_indices, y_val, epochs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(train_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b1004e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, features, adj, indices, y, val_indices, y_val, epochs):\n",
    "    # Train model\n",
    "    model.train()\n",
    "    start_time = time()\n",
    "    #val_indices = add_authors_to_pairs(val_indices) #we add the authors to val_pairs\n",
    "    #indices = add_authors_to_pairs(indices)\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "        optimizer.zero_grad()\n",
    "        rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)# We take random indices each time we run an epoch\n",
    "        #rand_indices = add_authors_to_pairs(rand_indices)\n",
    "        pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices.   \n",
    "        #pairs = add_authors_to_pairs(pairs) #we add the authors to the pairs\n",
    "        output = model(features, adj, pairs) # we run the model that gives the output.\n",
    "        loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "        acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "        loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "        optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, adj, val_indices)\n",
    "        y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {:.4f} s'.format(time() - t),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "        if epochs % 50 == 0:\n",
    "            model_path = \"../submissions_files/{}-model-{}epochs.pt\".format(today, epoch)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c8c7eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2103149])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "081d446a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 0.6946 loss_val: 0.6816 acc_train: 0.5000 acc_val: 0.6919 time: 26.8777 s total_time: 0 min\n",
      "Epoch: 006 loss_train: 0.5066 loss_val: 0.5251 acc_train: 0.6755 acc_val: 0.7525 time: 8.2545 s total_time: 1 min\n",
      "Epoch: 011 loss_train: 0.3882 loss_val: 0.4235 acc_train: 0.8688 acc_val: 0.8475 time: 8.0930 s total_time: 2 min\n",
      "Epoch: 016 loss_train: 0.2847 loss_val: 0.3515 acc_train: 0.9128 acc_val: 0.8800 time: 8.8492 s total_time: 3 min\n",
      "Epoch: 021 loss_train: 0.2052 loss_val: 0.3135 acc_train: 0.9297 acc_val: 0.8966 time: 8.6157 s total_time: 3 min\n",
      "Epoch: 026 loss_train: 0.1580 loss_val: 0.3046 acc_train: 0.9415 acc_val: 0.9067 time: 8.4362 s total_time: 4 min\n",
      "Epoch: 031 loss_train: 0.1360 loss_val: 0.3018 acc_train: 0.9474 acc_val: 0.9118 time: 8.9058 s total_time: 5 min\n",
      "Epoch: 036 loss_train: 0.1283 loss_val: 0.3155 acc_train: 0.9503 acc_val: 0.9131 time: 8.5398 s total_time: 5 min\n",
      "Epoch: 041 loss_train: 0.1231 loss_val: 0.3201 acc_train: 0.9526 acc_val: 0.9153 time: 9.0056 s total_time: 6 min\n",
      "Epoch: 046 loss_train: 0.1204 loss_val: 0.3348 acc_train: 0.9538 acc_val: 0.9155 time: 9.0298 s total_time: 7 min\n",
      "Epoch: 051 loss_train: 0.1175 loss_val: 0.3429 acc_train: 0.9551 acc_val: 0.9160 time: 9.5586 s total_time: 8 min\n",
      "Epoch: 056 loss_train: 0.1162 loss_val: 0.3653 acc_train: 0.9554 acc_val: 0.9157 time: 9.2430 s total_time: 8 min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#y_val = torch.FloatTensor(y_val).to(device)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model_nb \u001b[38;5;241m=\u001b[39m randint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m      4\u001b[0m today \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mtoday()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, features, adj, indices, y, val_indices, y_val, epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output, y) \u001b[38;5;66;03m# we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m acc_train \u001b[38;5;241m=\u001b[39m accuracy_score(torch\u001b[38;5;241m.\u001b[39margmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), y\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\u001b[38;5;66;03m# just to show it in the out put message of the training\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mloss_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\u001b[39;00m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# Performs a single optimization step (parameter update).\u001b[39;00m\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#y_val = torch.FloatTensor(y_val).to(device)\n",
    "trained_model = train_model(model, optimizer, features, adj, indices, y, val_indices, y_val, epochs)\n",
    "model_nb = randint(0, 1000)\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "model_path = \"../submissions_files/{}-model-{}epochs-{}.pt\".format(today, epochs, model_nb)\n",
    "torch.save(trained_model.state_dict(), model_path)\n",
    "print('Model saved in', model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d4ddaad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNN(\n",
       "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (double_fc3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc4): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"../submissions_files/2023-03-23-model-51epochs.pt\"\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "799c0b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12730279564857483\n"
     ]
    }
   ],
   "source": [
    "rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)# We take random indices each time we run an epoch\n",
    "pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices.   \n",
    "output = model(features, adj, pairs) # we run the model that gives the output.\n",
    "loss_train = F.nll_loss(output, y)\n",
    "print(loss_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "output = model(features, adj, val_indices)\n",
    "y_val = torch.LongTensor(y_val).to(device)\n",
    "loss_val = F.nll_loss(output, y_val)\n",
    "acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "print(loss_val.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b650a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (double_fc3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc4): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Log loss: 2.554288012771918\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "print(model.eval())\n",
    "eval_pairs = np.array(np.transpose(val_edges))\n",
    "#print(eval_pairs.shape)\n",
    "eval_pairs = torch.LongTensor(eval_pairs).to(device)\n",
    "#print(eval_pairs.shape)\n",
    "eval_output = model(features, adj, eval_pairs)\n",
    "#print(eval_output.shape)\n",
    "y_pred = torch.exp(eval_output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "#y_val_pred_true = list()\n",
    "\n",
    "y_val_pred_true = y_pred[:, 1]\n",
    "\n",
    "    \n",
    "print('Log loss:', log_loss(y_val, y_val_pred_true))\n",
    "\n",
    "#y_val = torch.tensor(y_val).to(device)\n",
    "#y_val_pred_true = torch.tensor(y_val_pred_true).to(device)\n",
    "#print(y_val, y_val_pred_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bcd7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('../test_data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "output = model(features, adj, pairs)\n",
    "y_pred = torch.exp(output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"../submissions_files/{}-submission-{}-{}.csv\".format(today, 2, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f130ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bc8b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = y_val.detach().cpu().numpy()\n",
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80600b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred)\n",
    "df['y_val'] = list(y_val)\n",
    "df.to_csv('../submissions_files/comparison_file.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae66fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(y_val, y_pred[:, 1]), mean_absolute_error(y_val, y_pred[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1822541",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_val, y_pred[:, 1]), log_loss(y_val, y_pred[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7e912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5717e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff03bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a828859c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e3c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c29d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b974905",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = np.array([[5, 3, 0], [2, 1, 0], [4, 0, 1], [0, 3, 2]])\n",
    "test_features = torch.LongTensor(test_features).to(device)\n",
    "test_pairs = [[0, 0, 1, 2, 3, 2, 0, 1, 2, 3], [1, 2, 0, 0, 2, 3, 0, 1, 2, 3]]\n",
    "test_pairs = torch.LongTensor(test_pairs).to(device)\n",
    "print(test_features.shape, test_pairs.shape)\n",
    "test_features[test_pairs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0639bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699d53ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape, np.shape(y_val), np.shape(y_pred_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf150c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ccd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(output.shape)\n",
    "print(features.shape, adj.shape, eval_pairs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4446f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "z2[pairs[1,:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bdb8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e2dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff0f64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425d6600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8607dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0728cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ad5476",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71737f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class labels\n",
    "y = np.zeros(4*G_train.number_of_edges())\n",
    "y[:2*G_train.number_of_edges()] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
    "train_edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede8a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "X_train = np.zeros((4*m, 2*features_np.shape[1]))\n",
    "\n",
    "for i, edge in enumerate(train_edges):\n",
    "    X_train[i] = np.concatenate((features_np[train_edges[i][0]], features_np[train_edges[i][1]]), axis=0)\n",
    "    X_train[m+i] = np.concatenate((features_np[train_edges[i][0]], features_np[train_edges[i][1]]), axis=0)\n",
    "    X_train[2*m+i] = np.concatenate((features_np[randint(0,n-1)], features_np[randint(0,n-1)]), axis=0)\n",
    "    X_train[3*m+i] = np.concatenate((features_np[randint(0,n-1)], features_np[randint(0,n-1)]), axis=0)\n",
    "    \n",
    "X_val = np.zeros((len(val_edges), 2*features_np.shape[1]))\n",
    "for i, edge in enumerate(val_edges):\n",
    "    X_val[i] = np.concatenate((features_np[val_edges[i][0]], features_np[val_edges[i][1]]), axis=0)\n",
    "    \n",
    "print('X_train and X_val created in {} s!'.format(round(time()-t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44a3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6fa791",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y)\n",
    "y_pred = clf.predict_proba(X_val)\n",
    "y_pred = y_pred[:,1]\n",
    "print('Logistic regression performed in {} s!'.format(round(time()-t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c60b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Log loss:', log_loss(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b1ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac02a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Read test data. Each sample is a pair of nodes\n",
    "test_edges = list()\n",
    "with open('../test_data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        test_edges.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "X_test = np.zeros((len(test_edges), 2*features_np.shape[1]))\n",
    "for i, edge in enumerate(test_edges):\n",
    "    X_test[i] = np.concatenate((features_np[test_edges[i][0]], features_np[test_edges[i][1]]), axis=0)\n",
    "\n",
    "t = time()\n",
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "y_pred = y_pred[:,1]\n",
    "print('Logistic regression performed in {} s!'.format(round(time()-t)))\n",
    "\n",
    "\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 100000)\n",
    "\n",
    "pd.DataFrame(y_pred, columns={'predicted'}).to_csv(\n",
    "\"../submissions_files/{}-submission-{}.csv\".format(today, random_nb), header=True, index=True, index_label='id'\n",
    ")\n",
    "    \n",
    "    \n",
    "# # Testing\n",
    "# node_pairs = np.array(np.transpose(node_pairs))\n",
    "# pairs = torch.LongTensor(node_pairs).to(device)\n",
    "# output = model(features, adj, pairs)\n",
    "# y_pred = torch.exp(output)\n",
    "# y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "# y_pred_true = list()\n",
    "# for element in y_pred:\n",
    "#     y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "# today = datetime.today().strftime('%Y-%m-%d')\n",
    "# random_nb = randint(0, 100000)\n",
    "\n",
    "# pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "# \"../submissions_files/{}-submission-{}.csv\".format(today, random_nb), header=True, index=True, index_label='id'\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
