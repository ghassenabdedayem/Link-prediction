{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2845bce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from random import random\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "from scipy.sparse import identity, diags, csr_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b75016b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_list(text):\n",
    "    return unidecode(text).split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55f76d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef3163f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[James H. Niblock, Jian-Xun Peng, Karen R. McM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Jian-Xun Peng, Kang Li, De-Shuang Huang]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[J. Heikkila]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[Long Zhang, Kang Li, Er-Wei Bai, George W. Ir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id                                            authors\n",
       "0         0  [James H. Niblock, Jian-Xun Peng, Karen R. McM...\n",
       "1         1          [Jian-Xun Peng, Kang Li, De-Shuang Huang]\n",
       "2         2                                      [J. Heikkila]\n",
       "3         3    [L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]\n",
       "4         4  [Long Zhang, Kang Li, Er-Wei Bai, George W. Ir..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors = pd.read_csv('../input_data/authors.txt', sep = '|', header=None)\n",
    "authors = authors.rename(columns={0: \"paper_id\", 2: \"authors\"})\n",
    "authors['authors'] = authors['authors'].apply(text_to_list)\n",
    "authors = authors[[\"paper_id\", \"authors\"]]\n",
    "authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3bcf3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0983d229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_normalize_adjacency(A):\n",
    "    n = A.shape[0] \n",
    "    A = A + identity(n)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c66b61d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b5543f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessagePassing(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MessagePassing, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \n",
    "        x = self.cf(x)\n",
    "        out = torch.mm(adj, x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e5655a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_feat, n_hidden, n_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "#         self.fc11 = nn.Linear(n_feat, round(n_feat/10))\n",
    "#         self.fc12 = nn.Linear(round(n_feat/10), round(n_feat/100))\n",
    "#         self.fc13 = nn.Linear(round(n_feat/100), round(n_feat/200))\n",
    "#         self.fc14 = nn.Linear(round(n_feat/200), n_hidden)\n",
    "        self.fc11 = nn.Linear(n_feat, n_hidden)\n",
    "        self.fc12 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc13 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc14 = nn.Linear(n_hidden, n_hidden)\n",
    "                              \n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc4 = nn.Linear(n_hidden, n_class)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x_in, adj, pairs):\n",
    "        \n",
    "        # Authors embedding using 4 levels MLP to densify representation from sparse 147950 features to n_hidden embedded features\n",
    "        h11 = self.fc11(x_in)\n",
    "        z11 = self.relu(torch.mm(adj, h11)) # remove the multiplication with adj each time ?\n",
    "#         h12 = self.fc12(z11)\n",
    "#         z12 = self.relu(torch.mm(adj, h12))\n",
    "#         h13 = self.fc13(z12)\n",
    "#         z13 = self.relu(torch.mm(adj, h13))\n",
    "#         h14 = self.fc13(z13)\n",
    "#         z14 = self.relu(torch.mm(adj, h14))\n",
    "        \n",
    "        z1 = self.dropout(z11)\n",
    "        #print('h1.shape=', z1.shape, ' adj.shape=', adj.shape)\n",
    "        h2 = self.fc2(z1)\n",
    "        \n",
    "        z2 = self.relu(torch.mm(adj, h2))\n",
    "        \n",
    "        x = z2[pairs[0,:],:] - z2[pairs[1,:],:]\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18418dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes of training set: 138499\n",
      "Number of edges of training set: 982674\n"
     ]
    }
   ],
   "source": [
    "G = nx.read_edgelist('../input_data/edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "nodes = list(G.nodes())\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "edges = list(G.edges())\n",
    "\n",
    "val_edges = list()\n",
    "G_train = G\n",
    "\n",
    "for edge in edges:\n",
    "    if random() < 0.1:\n",
    "        val_edges.append(edge)\n",
    "\n",
    "# We remove the val edges from the graph G\n",
    "for edge in val_edges:\n",
    "    G_train.remove_edge(edge[0], edge[1])\n",
    "\n",
    "n = G_train.number_of_nodes()\n",
    "m = G_train.number_of_edges()\n",
    "train_edges = list(G_train.edges())\n",
    "    \n",
    "print('Number of nodes of training set:', n)\n",
    "print('Number of edges of training set:', m)\n",
    "\n",
    "y_val = [1]*len(val_edges)\n",
    "\n",
    "n_val_edges = len(val_edges)\n",
    "\n",
    "# Create random pairs of nodes\n",
    "for i in range(n_val_edges):\n",
    "    n1 = nodes[randint(0, n-1)]\n",
    "    n2 = nodes[randint(0, n-1)]\n",
    "    (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "    val_edges.append((n1, n2))\n",
    "    \n",
    "# Remove from val_edges edges that exist in both train and val\n",
    "\n",
    "for edge in list(set(val_edges) & set(train_edges)):\n",
    "    val_edges.remove(edge)\n",
    "    \n",
    "n_val_edges = len(val_edges) - len(y_val) #because we removed from val_edges edges that exist in both\n",
    "y_val.extend([0]*n_val_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2fc40b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cf6ab59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[James H. Niblock, Jian-Xun Peng, Karen R. McM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Jian-Xun Peng, Kang Li, De-Shuang Huang]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[J. Heikkila]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[Long Zhang, Kang Li, Er-Wei Bai, George W. Ir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id                                            authors\n",
       "0         0  [James H. Niblock, Jian-Xun Peng, Karen R. McM...\n",
       "1         1          [Jian-Xun Peng, Kang Li, De-Shuang Huang]\n",
       "2         2                                      [J. Heikkila]\n",
       "3         3    [L. Teslic, B. Hartmann, O. Nelles, I. Skrjanc]\n",
       "4         4  [Long Zhang, Kang Li, Er-Wei Bai, George W. Ir..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors = pd.read_csv('../input_data/authors.txt', sep = '|', header=None)\n",
    "authors = authors.rename(columns={0: \"paper_id\", 2: \"authors\"})\n",
    "authors['authors'] = authors['authors'].apply(text_to_list)\n",
    "authors = authors[[\"paper_id\", \"authors\"]]\n",
    "authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a7f359e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(138499, 147950)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features initializaed with a sparce representation of authors of the papers\n",
    "\n",
    "authors = pd.read_csv('../input_data/authors.txt', sep = '|', header=None)\n",
    "authors = authors.rename(columns={0: \"paper_id\", 2: \"authors\"})\n",
    "authors['authors'] = authors['authors'].apply(text_to_list)\n",
    "mlb = MultiLabelBinarizer()\n",
    "df = pd.DataFrame(mlb.fit_transform(authors['authors']),columns=mlb.classes_, index=authors.index)\n",
    "features_np = df.values\n",
    "\n",
    "#features_np = np.random.randn(G_train.number_of_nodes(), 32) # Generates node features randomly\n",
    "print(type(features_np))\n",
    "features_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc34a155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import apply_word2vec_on_features, read_train_val_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f15cf5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499 number of edges: 1091955 in All the set\n",
      "Number of nodes: 138499 number of edges: 982604 in the Training set\n",
      "len(nodes) 138499\n",
      "Creating random val_edges...\n",
      "Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects\n",
      "Loaded from edgelist.txt and with a training validation split ratio = 0.1\n"
     ]
    }
   ],
   "source": [
    "G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx = read_train_val_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fde463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start applying Word2Vec...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m features_wv \u001b[38;5;241m=\u001b[39m \u001b[43mapply_word2vec_on_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Data/Polytechnique/5- Data Challenge/data_challenge_2022/draft_nb/utils.py:125\u001b[0m, in \u001b[0;36mapply_word2vec_on_features\u001b[0;34m(features, nodes, vector_size, window, min_count, sg, workers)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart applying Word2Vec...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    124\u001b[0m wv_model \u001b[38;5;241m=\u001b[39m Word2Vec(vector_size\u001b[38;5;241m=\u001b[39mvector_size, window\u001b[38;5;241m=\u001b[39mwindow, min_count\u001b[38;5;241m=\u001b[39mmin_count, sg\u001b[38;5;241m=\u001b[39msg, workers\u001b[38;5;241m=\u001b[39mworkers)\n\u001b[0;32m--> 125\u001b[0m \u001b[43mwv_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m wv_model\u001b[38;5;241m.\u001b[39mtrain(features, total_examples\u001b[38;5;241m=\u001b[39mwv_model\u001b[38;5;241m.\u001b[39mcorpus_count, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m) \n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord2vec model trained on features in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m min!\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mround\u001b[39m((time()\u001b[38;5;241m-\u001b[39mt)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m)))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/models/word2vec.py:487\u001b[0m, in \u001b[0;36mWord2Vec.build_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m\"\"\"Build vocabulary from a sequence of sentences (can be a once-only generator stream).\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \n\u001b[1;32m    451\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    484\u001b[0m \n\u001b[1;32m    485\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 487\u001b[0m total_words, corpus_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_vocab\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_per\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_per\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrim_rule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_count \u001b[38;5;241m=\u001b[39m corpus_count\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_total_words \u001b[38;5;241m=\u001b[39m total_words\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/models/word2vec.py:582\u001b[0m, in \u001b[0;36mWord2Vec.scan_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_file:\n\u001b[1;32m    580\u001b[0m     corpus_iterable \u001b[38;5;241m=\u001b[39m LineSentence(corpus_file)\n\u001b[0;32m--> 582\u001b[0m total_words, corpus_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scan_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_per\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollected \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m word types from a corpus of \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m raw words and \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m sentences\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_vocab), total_words, corpus_count\n\u001b[1;32m    587\u001b[0m )\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_words, corpus_count\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/models/word2vec.py:566\u001b[0m, in \u001b[0;36mWord2Vec._scan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m    561\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROGRESS: at sentence #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m, processed \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m words, keeping \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m word types\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    563\u001b[0m         sentence_no, total_words, \u001b[38;5;28mlen\u001b[39m(vocab),\n\u001b[1;32m    564\u001b[0m     )\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sentence:\n\u001b[0;32m--> 566\u001b[0m     vocab[word] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    567\u001b[0m total_words \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sentence)\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_vocab_size \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(vocab) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_vocab_size:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "features_wv = apply_word2vec_on_features(features_np, nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaafb63e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'authors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m nb_same_author \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, edge \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(edges):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(intersection(\u001b[43mauthors\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthors\u001b[39m\u001b[38;5;124m'\u001b[39m][edge[\u001b[38;5;241m0\u001b[39m]], authors[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthors\u001b[39m\u001b[38;5;124m'\u001b[39m][edge[\u001b[38;5;241m1\u001b[39m]]))\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      5\u001b[0m         same_authors_edges\u001b[38;5;241m.\u001b[39mappend((edge[\u001b[38;5;241m0\u001b[39m], edge[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      6\u001b[0m         nb_same_author \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'authors' is not defined"
     ]
    }
   ],
   "source": [
    "same_authors_edges = []\n",
    "nb_same_author = 0\n",
    "for i, edge in enumerate(edges):\n",
    "    if len(intersection(authors['authors'][edge[0]], authors['authors'][edge[1]]))>0:\n",
    "        same_authors_edges.append((edge[0], edge[1], 1))\n",
    "        nb_same_author += 1\n",
    "    else:\n",
    "        same_authors_edges.append((edge[0], edge[1], 0))\n",
    "print(nb_same_author, len(edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083a013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors[authors['paper_id'] == 58351]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c740e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors[authors['paper_id'] == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02058af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "\n",
    "# model = Word2Vec(vector_size=1000, window=5, min_count=0, sg=1, workers=8)\n",
    "# model.build_vocab(features_np)\n",
    "# model.train(features_np, total_examples=model.corpus_count, epochs=5) \n",
    "# model.wv['32098']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bacbb98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33457238",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = nx.adjacency_matrix(G_train)# Obtains the adjacency matrix of the training graph\n",
    "print(type(adj))\n",
    "print(adj.shape)\n",
    "adj = new_normalize_adjacency(adj) # Normalizes the adjacency matrix only by adding ones to diag\n",
    "print(type(adj), '\\n')\n",
    "\n",
    "indices = np.array(adj.nonzero())\n",
    "\n",
    "# Create class labels\n",
    "y = np.zeros(2*len(indices[0]))\n",
    "y[:len(indices[0])] = 1 # Concatenated ones for edges indices and zeros for random indices.\n",
    "\n",
    "# Transforms the numpy matrices/vectors to torch tensors.\n",
    "print(type(features_np))\n",
    "#features_np = csr_matrix(features_np)\n",
    "#print(type(features_np))\n",
    "#features = sparse_mx_to_torch_sparse_tensor(features_np).to(device)\n",
    "#print(type(features_np))\n",
    "features = torch.FloatTensor(features_np).to(device)\n",
    "y = torch.LongTensor(y).to(device)\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "indices = torch.LongTensor(indices).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d93df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 20\n",
    "n_hidden = 128\n",
    "dropout_rate = 0.2\n",
    "n_class = 2\n",
    "n_features = features.shape[1]\n",
    "\n",
    "# Creates the model and specifies the optimizer\n",
    "model = GNN(n_features, n_hidden, n_class, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98554ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)\n",
    "print(adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7567b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Train model\n",
    "model.train()\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    t = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    rand_indices = torch.randint(0, features.shape[0], (indices.shape[0],indices.shape[1]), device=adj.device)# We take random indices each time we run an epoch\n",
    "    pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices.   \n",
    "    \n",
    "    output = model(features, adj, pairs) # we run the model that gives the output.\n",
    "    loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "    #print(type(loss_train), '\\n', loss_train.shape)\n",
    "    acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "    loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "    optimizer.step() # Performs a single optimization step (parameter update).\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "              'time: {:.4f}s'.format(time.time() - t),\n",
    "             'total_time: {}min'.format(round((time.time() - start_time)/60)))\n",
    "\n",
    "print(\"Optimization Finished in {} min!\".format(round((time.time() - start_time)/60)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb56093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation on val subset then calculate loss between prediction (y_pred) and valid y (y_val)\n",
    "node_pairs = np.array(np.transpose(val_edges))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "pred_output = model(features, adj, pairs)\n",
    "y_pred = torch.exp(pred_output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "print('Log loss:', log_loss(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4e5c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pairs[0]), len(indices[0]), adj.shape, len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973d7433",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220a4bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_val\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dd9074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c115a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features initializaed randomly because not yet ready\n",
    "features_np = np.random.randn(G_train.number_of_nodes(), 32) # Generates node features randomly\n",
    "features_np.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf743944",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(indices.shape)\n",
    "print(min(features_np[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ba2946",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_train.number_of_edges()*2 # = 1965036 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
