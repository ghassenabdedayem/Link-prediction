{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "787a2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import random_walk, generate_walks, sparse_mx_to_torch_sparse_tensor \n",
    "from utils import text_to_list, intersection, read_train_val_graph, save_subgraph_in_file\n",
    "from utils import apply_word2vec_on_features, create_and_normalize_adjacency, train_model, add_authors_to_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d25112fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "667086c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499 number of edges: 1091955 in the Complete the set\n",
      "Number of nodes: 138499 number of edges: 982960 in the Training set\n",
      "len(nodes) 138499\n",
      "Creating random val_edges...\n",
      "Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects\n",
      "Loaded from edgelist.txt and with a training validation split ratio = 0.1\n"
     ]
    }
   ],
   "source": [
    "path = '../input_data/edgelist.txt'\n",
    "G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx = read_train_val_graph(val_ratio=0.1, path=path)\n",
    "walks = generate_walks(G=G_train, num_walks=10, walk_length=15)\n",
    "walks_wv = apply_word2vec_on_features(features=walks, nodes=nodes, vector_size=64)\n",
    "adj, indices = create_and_normalize_adjacency(G_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69776784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_np = np.concatenate([walks_wv, authors_wv], axis=1)\n",
    "features_np = walks_wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02015432",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_csv('../input_data/authors.txt', sep = '|', header=None)\n",
    "authors = authors.rename(columns={0: \"paper_id\", 2: \"authors\"})\n",
    "authors['authors'] = authors['authors'].apply(text_to_list)\n",
    "authors = authors[[\"paper_id\", \"authors\"]]\n",
    "authors = authors[authors['paper_id'] <= max(G.nodes())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bf540ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class labels\n",
    "y = np.zeros(2*indices.shape[1])\n",
    "y[:indices.shape[1]] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
    "\n",
    "# Transforms the numpy matrices/vectors to torch tensors.\n",
    "features = torch.FloatTensor(features_np).to(device)\n",
    "y = torch.LongTensor(y).to(device)\n",
    "if type(adj) != torch.Tensor:\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "indices = torch.LongTensor(indices).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc36c754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c48af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_feat, n_hidden, n_class, sub_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.double_fc3 = nn.Linear((3*n_hidden), n_hidden)\n",
    "        self.fc4 = nn.Linear(n_hidden, sub_class)\n",
    "        self.fc5 = nn.Linear(sub_class, n_class)        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x_in, adj, pairs):\n",
    "        \n",
    "        h1 = self.fc1(x_in)\n",
    "        z1 = self.relu(torch.mm(adj, h1))\n",
    "        z1 = self.dropout(z1)\n",
    "\n",
    "        h2 = self.fc2(z1)\n",
    "        z2 = self.relu(torch.mm(adj, h2))\n",
    "        z2 = self.dropout(z2)\n",
    "\n",
    "        x = z2[pairs[0]] - z2[pairs[1]] # embedded features (z2) of node 0 - embedded features of node 1\n",
    "        x = pairs[3][:, None] * x\n",
    "        x1 = z2[pairs[0]]\n",
    "        x2 = z2[pairs[1]]\n",
    "        x = torch.cat((x, x1, x2), dim=1)\n",
    "        \n",
    "#         x_auth = pairs[2].reshape([len(pairs[2]), 1])\n",
    "#         x_nb_auth = pairs[3].reshape([len(pairs[3]), 1]) \n",
    "        \n",
    "        \n",
    "        \n",
    "        #x1 = z2[pairs[0]]\n",
    "        #x2 = z2[pairs[1]]\n",
    "        # could we add a new dimension to pairs to specify if same author(s)? and then what could we do?\n",
    "              \n",
    "        #x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.relu(self.double_fc3(x))        \n",
    "        #x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        #x = torch.cat((x, x_auth, x_nb_auth), dim=1) #pairs[3] : number of same authors or 1 if same author\n",
    "        x = self.fc5(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7eb8d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "n_hidden = 96\n",
    "dropout_rate = 0.2\n",
    "sub_class = 8\n",
    "n_class = 2\n",
    "n_features = features.shape[1]\n",
    "\n",
    "# Creates the model and specifies the optimizer\n",
    "model = GNN(n_features, n_hidden, n_class, sub_class, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0c97d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, features, authors, adj, indices, y, val_indices, y_val, epochs):\n",
    "    # Train model\n",
    "    start_time = time()\n",
    "    model.train()\n",
    "    \n",
    "    val_indices = add_authors_to_pairs(val_indices, authors) #we add the authors to val_pairs\n",
    "    indices = add_authors_to_pairs(indices, authors) #we add the authors to indices\n",
    "    rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)# We take random indices each time we run an epoch\n",
    "    rand_indices = add_authors_to_pairs(rand_indices, authors)\n",
    "    pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices. \n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    list_val_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features, adj, pairs) # we run the model that gives the output.\n",
    "        loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "        acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "        loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "        optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, adj, val_indices)\n",
    "        y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "        if epoch % 5 == 0:\n",
    "            print\n",
    "            ('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {} s'.format(int(round(time()) - round(t))),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "        if epoch % 20 == 0:\n",
    "            model_path = \"../outputs/models/{}-model-{}epochs.pt\".format(today, epoch)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "#         overfitting = []\n",
    "        \n",
    "#         for last_loss in list_val_loss[-10:]:\n",
    "#             if len(list_val_loss) >=10 and loss_val > last_loss:\n",
    "#                 overfitting.append(1)\n",
    "#         if len(overfitting)>=9: \n",
    "#             break\n",
    "        \n",
    "#         list_val_loss.append(float(loss_val))\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c7fbd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 0.6339 loss_val: 0.5535 acc_train: 0.7632 acc_val: 0.8150 time: 39 s total_time: 2 min\n",
      "Epoch: 006 loss_train: 0.3353 loss_val: 0.3271 acc_train: 0.8692 acc_val: 0.8680 time: 20 s total_time: 4 min\n",
      "Epoch: 011 loss_train: 0.2971 loss_val: 0.3060 acc_train: 0.8830 acc_val: 0.8794 time: 21 s total_time: 6 min\n",
      "Epoch: 016 loss_train: 0.2781 loss_val: 0.2870 acc_train: 0.8909 acc_val: 0.8870 time: 17 s total_time: 9 min\n",
      "Epoch: 021 loss_train: 0.2569 loss_val: 0.2710 acc_train: 0.8972 acc_val: 0.8915 time: 18 s total_time: 11 min\n",
      "Epoch: 026 loss_train: 0.2428 loss_val: 0.2585 acc_train: 0.9024 acc_val: 0.8970 time: 18 s total_time: 12 min\n",
      "Epoch: 031 loss_train: 0.2351 loss_val: 0.2503 acc_train: 0.9057 acc_val: 0.8998 time: 17 s total_time: 14 min\n",
      "Epoch: 036 loss_train: 0.2311 loss_val: 0.2441 acc_train: 0.9074 acc_val: 0.9030 time: 19 s total_time: 16 min\n",
      "Epoch: 041 loss_train: 0.2268 loss_val: 0.2410 acc_train: 0.9094 acc_val: 0.9052 time: 15 s total_time: 18 min\n",
      "Epoch: 046 loss_train: 0.2256 loss_val: 0.2427 acc_train: 0.9110 acc_val: 0.9037 time: 17 s total_time: 19 min\n",
      "Epoch: 051 loss_train: 0.2341 loss_val: 0.2374 acc_train: 0.9081 acc_val: 0.9062 time: 18 s total_time: 21 min\n",
      "Epoch: 056 loss_train: 0.2228 loss_val: 0.2349 acc_train: 0.9115 acc_val: 0.9075 time: 19 s total_time: 23 min\n",
      "Epoch: 061 loss_train: 0.2206 loss_val: 0.2315 acc_train: 0.9125 acc_val: 0.9090 time: 16 s total_time: 25 min\n",
      "Epoch: 066 loss_train: 0.2146 loss_val: 0.2319 acc_train: 0.9152 acc_val: 0.9094 time: 18 s total_time: 27 min\n",
      "Epoch: 071 loss_train: 0.2128 loss_val: 0.2261 acc_train: 0.9162 acc_val: 0.9115 time: 16 s total_time: 29 min\n",
      "Epoch: 076 loss_train: 0.2108 loss_val: 0.2248 acc_train: 0.9170 acc_val: 0.9125 time: 17 s total_time: 30 min\n",
      "Epoch: 081 loss_train: 0.2081 loss_val: 0.2217 acc_train: 0.9182 acc_val: 0.9139 time: 16 s total_time: 32 min\n",
      "Epoch: 086 loss_train: 0.2052 loss_val: 0.2207 acc_train: 0.9195 acc_val: 0.9143 time: 17 s total_time: 34 min\n",
      "Epoch: 091 loss_train: 0.2033 loss_val: 0.2196 acc_train: 0.9206 acc_val: 0.9152 time: 15 s total_time: 35 min\n",
      "Epoch: 096 loss_train: 0.2007 loss_val: 0.2170 acc_train: 0.9218 acc_val: 0.9166 time: 16 s total_time: 37 min\n",
      "Epoch: 101 loss_train: 0.2077 loss_val: 0.2268 acc_train: 0.9188 acc_val: 0.9132 time: 16 s total_time: 39 min\n",
      "Epoch: 106 loss_train: 0.2009 loss_val: 0.2129 acc_train: 0.9219 acc_val: 0.9183 time: 16 s total_time: 40 min\n",
      "Epoch: 111 loss_train: 0.1960 loss_val: 0.2150 acc_train: 0.9239 acc_val: 0.9178 time: 15 s total_time: 42 min\n",
      "Epoch: 116 loss_train: 0.1949 loss_val: 0.2132 acc_train: 0.9244 acc_val: 0.9187 time: 18 s total_time: 43 min\n",
      "Epoch: 121 loss_train: 0.1913 loss_val: 0.2073 acc_train: 0.9260 acc_val: 0.9215 time: 15 s total_time: 45 min\n",
      "Epoch: 126 loss_train: 0.1955 loss_val: 0.2338 acc_train: 0.9243 acc_val: 0.9117 time: 16 s total_time: 47 min\n",
      "Epoch: 131 loss_train: 0.2507 loss_val: 0.2197 acc_train: 0.8995 acc_val: 0.9167 time: 15 s total_time: 48 min\n",
      "Epoch: 136 loss_train: 0.2124 loss_val: 0.2386 acc_train: 0.9180 acc_val: 0.9077 time: 18 s total_time: 50 min\n",
      "Epoch: 141 loss_train: 0.2035 loss_val: 0.2165 acc_train: 0.9214 acc_val: 0.9174 time: 18 s total_time: 52 min\n",
      "Epoch: 146 loss_train: 0.2018 loss_val: 0.2131 acc_train: 0.9219 acc_val: 0.9190 time: 19 s total_time: 53 min\n",
      "Epoch: 151 loss_train: 0.1943 loss_val: 0.2094 acc_train: 0.9248 acc_val: 0.9205 time: 19 s total_time: 55 min\n",
      "Epoch: 156 loss_train: 0.1887 loss_val: 0.2047 acc_train: 0.9274 acc_val: 0.9230 time: 18 s total_time: 57 min\n",
      "Epoch: 161 loss_train: 0.1849 loss_val: 0.2006 acc_train: 0.9288 acc_val: 0.9249 time: 18 s total_time: 58 min\n",
      "Epoch: 166 loss_train: 0.1791 loss_val: 0.1984 acc_train: 0.9310 acc_val: 0.9259 time: 17 s total_time: 60 min\n",
      "Epoch: 171 loss_train: 0.1750 loss_val: 0.1911 acc_train: 0.9323 acc_val: 0.9287 time: 16 s total_time: 62 min\n",
      "Epoch: 176 loss_train: 0.1693 loss_val: 0.1872 acc_train: 0.9345 acc_val: 0.9303 time: 15 s total_time: 64 min\n",
      "Epoch: 181 loss_train: 0.1633 loss_val: 0.1979 acc_train: 0.9373 acc_val: 0.9260 time: 16 s total_time: 66 min\n",
      "Epoch: 186 loss_train: 0.1815 loss_val: 0.1942 acc_train: 0.9306 acc_val: 0.9270 time: 16 s total_time: 67 min\n",
      "Epoch: 191 loss_train: 0.1713 loss_val: 0.1920 acc_train: 0.9349 acc_val: 0.9287 time: 17 s total_time: 69 min\n",
      "Epoch: 196 loss_train: 0.1662 loss_val: 0.1871 acc_train: 0.9364 acc_val: 0.9299 time: 18 s total_time: 71 min\n",
      "Epoch: 201 loss_train: 0.1604 loss_val: 0.1870 acc_train: 0.9385 acc_val: 0.9301 time: 20 s total_time: 73 min\n",
      "Epoch: 206 loss_train: 0.1605 loss_val: 0.1767 acc_train: 0.9376 acc_val: 0.9346 time: 18 s total_time: 74 min\n",
      "Epoch: 211 loss_train: 0.1556 loss_val: 0.1749 acc_train: 0.9404 acc_val: 0.9359 time: 19 s total_time: 76 min\n",
      "Epoch: 216 loss_train: 0.1535 loss_val: 0.1742 acc_train: 0.9409 acc_val: 0.9364 time: 17 s total_time: 78 min\n",
      "Epoch: 221 loss_train: 0.1517 loss_val: 0.1744 acc_train: 0.9419 acc_val: 0.9363 time: 13 s total_time: 80 min\n",
      "Epoch: 226 loss_train: 0.1501 loss_val: 0.1726 acc_train: 0.9425 acc_val: 0.9369 time: 17 s total_time: 81 min\n",
      "Epoch: 231 loss_train: 0.1480 loss_val: 0.1709 acc_train: 0.9434 acc_val: 0.9378 time: 18 s total_time: 83 min\n",
      "Epoch: 236 loss_train: 0.1541 loss_val: 0.1827 acc_train: 0.9405 acc_val: 0.9325 time: 18 s total_time: 85 min\n",
      "Epoch: 241 loss_train: 0.1524 loss_val: 0.1741 acc_train: 0.9412 acc_val: 0.9359 time: 17 s total_time: 87 min\n",
      "Epoch: 246 loss_train: 0.1457 loss_val: 0.1886 acc_train: 0.9443 acc_val: 0.9293 time: 17 s total_time: 89 min\n",
      "Epoch: 251 loss_train: 0.1532 loss_val: 0.1717 acc_train: 0.9412 acc_val: 0.9370 time: 16 s total_time: 91 min\n",
      "Epoch: 256 loss_train: 0.1447 loss_val: 0.1794 acc_train: 0.9447 acc_val: 0.9331 time: 15 s total_time: 92 min\n",
      "Epoch: 261 loss_train: 0.1443 loss_val: 0.1662 acc_train: 0.9443 acc_val: 0.9397 time: 14 s total_time: 94 min\n",
      "Epoch: 266 loss_train: 0.1409 loss_val: 0.1741 acc_train: 0.9461 acc_val: 0.9362 time: 18 s total_time: 96 min\n",
      "Epoch: 271 loss_train: 0.1518 loss_val: 0.1604 acc_train: 0.9411 acc_val: 0.9422 time: 19 s total_time: 98 min\n",
      "Epoch: 276 loss_train: 0.1685 loss_val: 0.1805 acc_train: 0.9337 acc_val: 0.9328 time: 18 s total_time: 99 min\n",
      "Epoch: 281 loss_train: 0.1530 loss_val: 0.1635 acc_train: 0.9411 acc_val: 0.9399 time: 19 s total_time: 101 min\n",
      "Epoch: 286 loss_train: 0.1465 loss_val: 0.1599 acc_train: 0.9442 acc_val: 0.9417 time: 19 s total_time: 103 min\n",
      "Epoch: 291 loss_train: 0.1350 loss_val: 0.1681 acc_train: 0.9482 acc_val: 0.9385 time: 18 s total_time: 105 min\n",
      "Epoch: 296 loss_train: 0.1343 loss_val: 0.1551 acc_train: 0.9484 acc_val: 0.9439 time: 20 s total_time: 107 min\n",
      "Epoch: 301 loss_train: 0.1300 loss_val: 0.1649 acc_train: 0.9503 acc_val: 0.9406 time: 32 s total_time: 109 min\n",
      "Epoch: 306 loss_train: 0.1367 loss_val: 0.1502 acc_train: 0.9474 acc_val: 0.9462 time: 32 s total_time: 110 min\n",
      "Epoch: 311 loss_train: 0.1458 loss_val: 0.1520 acc_train: 0.9433 acc_val: 0.9453 time: 41 s total_time: 112 min\n",
      "Epoch: 316 loss_train: 0.1335 loss_val: 0.1481 acc_train: 0.9489 acc_val: 0.9467 time: 24 s total_time: 113 min\n",
      "Epoch: 321 loss_train: 0.1269 loss_val: 0.1517 acc_train: 0.9513 acc_val: 0.9455 time: 30 s total_time: 115 min\n",
      "Epoch: 326 loss_train: 0.1244 loss_val: 0.1449 acc_train: 0.9525 acc_val: 0.9483 time: 32 s total_time: 117 min\n",
      "Epoch: 331 loss_train: 0.1254 loss_val: 0.1450 acc_train: 0.9519 acc_val: 0.9484 time: 45 s total_time: 119 min\n",
      "Epoch: 336 loss_train: 0.1194 loss_val: 0.1479 acc_train: 0.9544 acc_val: 0.9477 time: 50 s total_time: 121 min\n",
      "Epoch: 341 loss_train: 0.1243 loss_val: 0.1448 acc_train: 0.9521 acc_val: 0.9493 time: 33 s total_time: 123 min\n",
      "Epoch: 346 loss_train: 0.1163 loss_val: 0.1388 acc_train: 0.9556 acc_val: 0.9512 time: 34 s total_time: 124 min\n",
      "Epoch: 351 loss_train: 0.1144 loss_val: 0.1378 acc_train: 0.9564 acc_val: 0.9521 time: 33 s total_time: 126 min\n",
      "Epoch: 356 loss_train: 0.1235 loss_val: 0.1533 acc_train: 0.9524 acc_val: 0.9456 time: 32 s total_time: 128 min\n",
      "Epoch: 361 loss_train: 0.1165 loss_val: 0.1431 acc_train: 0.9556 acc_val: 0.9497 time: 22 s total_time: 130 min\n",
      "Epoch: 366 loss_train: 0.1142 loss_val: 0.1360 acc_train: 0.9566 acc_val: 0.9529 time: 20 s total_time: 131 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 371 loss_train: 0.1103 loss_val: 0.1342 acc_train: 0.9581 acc_val: 0.9538 time: 20 s total_time: 133 min\n",
      "Epoch: 376 loss_train: 0.1202 loss_val: 0.1475 acc_train: 0.9537 acc_val: 0.9482 time: 19 s total_time: 135 min\n",
      "Epoch: 381 loss_train: 0.1121 loss_val: 0.1396 acc_train: 0.9574 acc_val: 0.9515 time: 21 s total_time: 136 min\n",
      "Epoch: 386 loss_train: 0.1116 loss_val: 0.1367 acc_train: 0.9575 acc_val: 0.9528 time: 20 s total_time: 138 min\n",
      "Epoch: 391 loss_train: 0.1106 loss_val: 0.1404 acc_train: 0.9579 acc_val: 0.9515 time: 21 s total_time: 140 min\n",
      "Epoch: 396 loss_train: 0.1124 loss_val: 0.1313 acc_train: 0.9570 acc_val: 0.9547 time: 20 s total_time: 141 min\n",
      "Epoch: 401 loss_train: 0.1072 loss_val: 0.1323 acc_train: 0.9594 acc_val: 0.9547 time: 30 s total_time: 143 min\n",
      "Epoch: 406 loss_train: 0.1052 loss_val: 0.1332 acc_train: 0.9603 acc_val: 0.9544 time: 20 s total_time: 145 min\n",
      "Epoch: 411 loss_train: 0.1079 loss_val: 0.1313 acc_train: 0.9590 acc_val: 0.9546 time: 20 s total_time: 146 min\n",
      "Epoch: 416 loss_train: 0.1068 loss_val: 0.1336 acc_train: 0.9595 acc_val: 0.9547 time: 19 s total_time: 148 min\n",
      "Epoch: 421 loss_train: 0.1034 loss_val: 0.1311 acc_train: 0.9610 acc_val: 0.9557 time: 20 s total_time: 150 min\n",
      "Epoch: 426 loss_train: 0.1203 loss_val: 0.1806 acc_train: 0.9538 acc_val: 0.9334 time: 22 s total_time: 151 min\n",
      "Epoch: 431 loss_train: 0.1226 loss_val: 0.1309 acc_train: 0.9523 acc_val: 0.9546 time: 30 s total_time: 153 min\n",
      "Epoch: 436 loss_train: 0.1088 loss_val: 0.1344 acc_train: 0.9590 acc_val: 0.9526 time: 30 s total_time: 155 min\n",
      "Epoch: 441 loss_train: 0.1102 loss_val: 0.1320 acc_train: 0.9586 acc_val: 0.9544 time: 52 s total_time: 157 min\n",
      "Epoch: 446 loss_train: 0.1077 loss_val: 0.1287 acc_train: 0.9590 acc_val: 0.9561 time: 44 s total_time: 159 min\n",
      "Epoch: 451 loss_train: 0.1044 loss_val: 0.1301 acc_train: 0.9605 acc_val: 0.9561 time: 45 s total_time: 160 min\n",
      "Epoch: 456 loss_train: 0.1013 loss_val: 0.1271 acc_train: 0.9618 acc_val: 0.9571 time: 36 s total_time: 162 min\n",
      "Epoch: 461 loss_train: 0.1005 loss_val: 0.1267 acc_train: 0.9622 acc_val: 0.9573 time: 31 s total_time: 164 min\n",
      "Epoch: 466 loss_train: 0.1026 loss_val: 0.1326 acc_train: 0.9611 acc_val: 0.9554 time: 29 s total_time: 166 min\n",
      "Epoch: 471 loss_train: 0.1046 loss_val: 0.1273 acc_train: 0.9604 acc_val: 0.9566 time: 24 s total_time: 167 min\n",
      "Epoch: 476 loss_train: 0.1012 loss_val: 0.1255 acc_train: 0.9618 acc_val: 0.9576 time: 22 s total_time: 169 min\n",
      "Epoch: 481 loss_train: 0.0999 loss_val: 0.1253 acc_train: 0.9623 acc_val: 0.9578 time: 24 s total_time: 170 min\n",
      "Epoch: 486 loss_train: 0.0989 loss_val: 0.1272 acc_train: 0.9628 acc_val: 0.9574 time: 25 s total_time: 172 min\n",
      "Epoch: 491 loss_train: 0.0987 loss_val: 0.1249 acc_train: 0.9629 acc_val: 0.9578 time: 15 s total_time: 173 min\n",
      "Epoch: 496 loss_train: 0.1775 loss_val: 0.1444 acc_train: 0.9307 acc_val: 0.9509 time: 20 s total_time: 175 min\n",
      "Epoch: 501 loss_train: 0.1300 loss_val: 0.1274 acc_train: 0.9487 acc_val: 0.9561 time: 20 s total_time: 177 min\n",
      "Epoch: 506 loss_train: 0.1066 loss_val: 0.1307 acc_train: 0.9595 acc_val: 0.9547 time: 21 s total_time: 179 min\n",
      "Epoch: 511 loss_train: 0.1035 loss_val: 0.1294 acc_train: 0.9612 acc_val: 0.9550 time: 21 s total_time: 180 min\n",
      "Epoch: 516 loss_train: 0.1049 loss_val: 0.1255 acc_train: 0.9606 acc_val: 0.9574 time: 20 s total_time: 182 min\n",
      "Epoch: 521 loss_train: 0.0997 loss_val: 0.1278 acc_train: 0.9624 acc_val: 0.9572 time: 20 s total_time: 184 min\n",
      "Epoch: 526 loss_train: 0.0988 loss_val: 0.1234 acc_train: 0.9626 acc_val: 0.9589 time: 17 s total_time: 185 min\n",
      "Epoch: 531 loss_train: 0.0995 loss_val: 0.1241 acc_train: 0.9626 acc_val: 0.9579 time: 17 s total_time: 187 min\n",
      "Epoch: 536 loss_train: 0.0960 loss_val: 0.1278 acc_train: 0.9639 acc_val: 0.9576 time: 17 s total_time: 189 min\n",
      "Epoch: 541 loss_train: 0.0988 loss_val: 0.1272 acc_train: 0.9626 acc_val: 0.9579 time: 18 s total_time: 191 min\n",
      "Epoch: 546 loss_train: 0.0956 loss_val: 0.1225 acc_train: 0.9640 acc_val: 0.9597 time: 16 s total_time: 193 min\n",
      "Epoch: 551 loss_train: 0.0940 loss_val: 0.1215 acc_train: 0.9647 acc_val: 0.9599 time: 17 s total_time: 195 min\n",
      "Epoch: 556 loss_train: 0.0938 loss_val: 0.1220 acc_train: 0.9648 acc_val: 0.9599 time: 17 s total_time: 196 min\n",
      "Epoch: 561 loss_train: 0.0945 loss_val: 0.1252 acc_train: 0.9645 acc_val: 0.9594 time: 16 s total_time: 198 min\n",
      "Epoch: 566 loss_train: 0.0939 loss_val: 0.1207 acc_train: 0.9648 acc_val: 0.9601 time: 13 s total_time: 199 min\n",
      "Epoch: 571 loss_train: 0.0952 loss_val: 0.1271 acc_train: 0.9641 acc_val: 0.9586 time: 18 s total_time: 201 min\n",
      "Epoch: 576 loss_train: 0.0949 loss_val: 0.1203 acc_train: 0.9644 acc_val: 0.9604 time: 16 s total_time: 203 min\n",
      "Epoch: 581 loss_train: 0.0930 loss_val: 0.1202 acc_train: 0.9651 acc_val: 0.9603 time: 14 s total_time: 204 min\n",
      "Epoch: 586 loss_train: 0.0930 loss_val: 0.1236 acc_train: 0.9651 acc_val: 0.9600 time: 16 s total_time: 206 min\n",
      "Epoch: 591 loss_train: 0.0922 loss_val: 0.1198 acc_train: 0.9655 acc_val: 0.9607 time: 16 s total_time: 208 min\n",
      "Epoch: 596 loss_train: 0.1145 loss_val: 0.1481 acc_train: 0.9558 acc_val: 0.9507 time: 16 s total_time: 209 min\n",
      "Epoch: 601 loss_train: 0.1196 loss_val: 0.1200 acc_train: 0.9538 acc_val: 0.9604 time: 19 s total_time: 211 min\n",
      "Epoch: 606 loss_train: 0.1178 loss_val: 0.1272 acc_train: 0.9540 acc_val: 0.9568 time: 15 s total_time: 213 min\n",
      "Epoch: 611 loss_train: 0.1019 loss_val: 0.1233 acc_train: 0.9610 acc_val: 0.9580 time: 16 s total_time: 214 min\n",
      "Epoch: 616 loss_train: 0.0964 loss_val: 0.1238 acc_train: 0.9640 acc_val: 0.9581 time: 17 s total_time: 216 min\n",
      "Epoch: 621 loss_train: 0.0986 loss_val: 0.1227 acc_train: 0.9630 acc_val: 0.9593 time: 17 s total_time: 218 min\n",
      "Epoch: 626 loss_train: 0.0955 loss_val: 0.1207 acc_train: 0.9641 acc_val: 0.9602 time: 13 s total_time: 219 min\n",
      "Epoch: 631 loss_train: 0.0934 loss_val: 0.1196 acc_train: 0.9648 acc_val: 0.9609 time: 14 s total_time: 221 min\n",
      "Epoch: 636 loss_train: 0.0937 loss_val: 0.1204 acc_train: 0.9649 acc_val: 0.9601 time: 14 s total_time: 223 min\n",
      "Epoch: 641 loss_train: 0.0905 loss_val: 0.1253 acc_train: 0.9661 acc_val: 0.9595 time: 18 s total_time: 224 min\n",
      "Epoch: 646 loss_train: 0.0968 loss_val: 0.1220 acc_train: 0.9633 acc_val: 0.9606 time: 18 s total_time: 226 min\n",
      "Epoch: 651 loss_train: 0.0906 loss_val: 0.1195 acc_train: 0.9661 acc_val: 0.9610 time: 18 s total_time: 228 min\n",
      "Epoch: 656 loss_train: 0.0902 loss_val: 0.1196 acc_train: 0.9662 acc_val: 0.9615 time: 18 s total_time: 229 min\n",
      "Epoch: 661 loss_train: 0.0890 loss_val: 0.1216 acc_train: 0.9667 acc_val: 0.9608 time: 17 s total_time: 231 min\n",
      "Epoch: 666 loss_train: 0.0890 loss_val: 0.1215 acc_train: 0.9666 acc_val: 0.9610 time: 18 s total_time: 233 min\n",
      "Epoch: 671 loss_train: 0.0883 loss_val: 0.1201 acc_train: 0.9670 acc_val: 0.9616 time: 19 s total_time: 234 min\n",
      "Epoch: 676 loss_train: 0.0892 loss_val: 0.1179 acc_train: 0.9667 acc_val: 0.9621 time: 18 s total_time: 236 min\n",
      "Epoch: 681 loss_train: 0.0898 loss_val: 0.1227 acc_train: 0.9663 acc_val: 0.9608 time: 18 s total_time: 238 min\n",
      "Epoch: 686 loss_train: 0.0882 loss_val: 0.1175 acc_train: 0.9670 acc_val: 0.9626 time: 17 s total_time: 240 min\n",
      "Epoch: 691 loss_train: 0.0873 loss_val: 0.1176 acc_train: 0.9674 acc_val: 0.9627 time: 17 s total_time: 241 min\n",
      "Epoch: 696 loss_train: 0.0919 loss_val: 0.1325 acc_train: 0.9654 acc_val: 0.9574 time: 18 s total_time: 243 min\n",
      "Epoch: 701 loss_train: 0.0894 loss_val: 0.1495 acc_train: 0.9667 acc_val: 0.9498 time: 18 s total_time: 245 min\n",
      "Epoch: 706 loss_train: 0.1078 loss_val: 0.1460 acc_train: 0.9595 acc_val: 0.9483 time: 18 s total_time: 247 min\n",
      "Epoch: 711 loss_train: 0.1201 loss_val: 0.1256 acc_train: 0.9555 acc_val: 0.9573 time: 17 s total_time: 249 min\n",
      "Epoch: 716 loss_train: 0.1089 loss_val: 0.1240 acc_train: 0.9599 acc_val: 0.9580 time: 18 s total_time: 251 min\n",
      "Epoch: 721 loss_train: 0.1027 loss_val: 0.1204 acc_train: 0.9620 acc_val: 0.9595 time: 17 s total_time: 252 min\n",
      "Epoch: 726 loss_train: 0.0980 loss_val: 0.1195 acc_train: 0.9633 acc_val: 0.9604 time: 17 s total_time: 254 min\n",
      "Epoch: 731 loss_train: 0.0905 loss_val: 0.1258 acc_train: 0.9662 acc_val: 0.9586 time: 16 s total_time: 256 min\n",
      "Epoch: 736 loss_train: 0.0915 loss_val: 0.1170 acc_train: 0.9658 acc_val: 0.9621 time: 17 s total_time: 258 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 741 loss_train: 0.0895 loss_val: 0.1184 acc_train: 0.9664 acc_val: 0.9621 time: 19 s total_time: 259 min\n",
      "Epoch: 746 loss_train: 0.0887 loss_val: 0.1166 acc_train: 0.9670 acc_val: 0.9624 time: 18 s total_time: 262 min\n",
      "Epoch: 751 loss_train: 0.0866 loss_val: 0.1210 acc_train: 0.9677 acc_val: 0.9616 time: 18 s total_time: 264 min\n",
      "Epoch: 756 loss_train: 0.0885 loss_val: 0.1181 acc_train: 0.9668 acc_val: 0.9623 time: 19 s total_time: 266 min\n",
      "Epoch: 761 loss_train: 0.0862 loss_val: 0.1159 acc_train: 0.9678 acc_val: 0.9629 time: 16 s total_time: 267 min\n",
      "Epoch: 766 loss_train: 0.0863 loss_val: 0.1158 acc_train: 0.9677 acc_val: 0.9634 time: 19 s total_time: 269 min\n",
      "Epoch: 771 loss_train: 0.0854 loss_val: 0.1168 acc_train: 0.9682 acc_val: 0.9631 time: 15 s total_time: 271 min\n",
      "Epoch: 776 loss_train: 0.0850 loss_val: 0.1168 acc_train: 0.9683 acc_val: 0.9632 time: 15 s total_time: 272 min\n",
      "Epoch: 781 loss_train: 0.0849 loss_val: 0.1160 acc_train: 0.9684 acc_val: 0.9634 time: 18 s total_time: 274 min\n",
      "Epoch: 786 loss_train: 0.0846 loss_val: 0.1157 acc_train: 0.9685 acc_val: 0.9636 time: 17 s total_time: 276 min\n",
      "Epoch: 791 loss_train: 0.0854 loss_val: 0.1203 acc_train: 0.9681 acc_val: 0.9621 time: 17 s total_time: 278 min\n",
      "Epoch: 796 loss_train: 0.0879 loss_val: 0.1170 acc_train: 0.9671 acc_val: 0.9625 time: 17 s total_time: 280 min\n",
      "Epoch: 801 loss_train: 0.0847 loss_val: 0.1152 acc_train: 0.9685 acc_val: 0.9634 time: 18 s total_time: 281 min\n",
      "Epoch: 806 loss_train: 0.0837 loss_val: 0.1149 acc_train: 0.9688 acc_val: 0.9638 time: 15 s total_time: 283 min\n",
      "Epoch: 811 loss_train: 0.0856 loss_val: 0.1200 acc_train: 0.9679 acc_val: 0.9623 time: 15 s total_time: 285 min\n",
      "Epoch: 816 loss_train: 0.0839 loss_val: 0.1150 acc_train: 0.9688 acc_val: 0.9640 time: 18 s total_time: 286 min\n",
      "Epoch: 821 loss_train: 0.0837 loss_val: 0.1191 acc_train: 0.9687 acc_val: 0.9629 time: 15 s total_time: 288 min\n",
      "Epoch: 826 loss_train: 0.0991 loss_val: 0.1462 acc_train: 0.9625 acc_val: 0.9493 time: 17 s total_time: 290 min\n",
      "Epoch: 831 loss_train: 0.0853 loss_val: 0.1341 acc_train: 0.9684 acc_val: 0.9563 time: 15 s total_time: 291 min\n",
      "Epoch: 836 loss_train: 0.0890 loss_val: 0.1189 acc_train: 0.9670 acc_val: 0.9608 time: 19 s total_time: 293 min\n",
      "Epoch: 841 loss_train: 0.0876 loss_val: 0.1278 acc_train: 0.9675 acc_val: 0.9583 time: 19 s total_time: 295 min\n",
      "Epoch: 846 loss_train: 0.0875 loss_val: 0.1154 acc_train: 0.9672 acc_val: 0.9629 time: 21 s total_time: 297 min\n",
      "Epoch: 851 loss_train: 0.0849 loss_val: 0.1244 acc_train: 0.9683 acc_val: 0.9605 time: 20 s total_time: 299 min\n",
      "Epoch: 856 loss_train: 0.0870 loss_val: 0.1145 acc_train: 0.9673 acc_val: 0.9641 time: 20 s total_time: 301 min\n",
      "Epoch: 861 loss_train: 0.0906 loss_val: 0.1243 acc_train: 0.9662 acc_val: 0.9584 time: 20 s total_time: 303 min\n",
      "Epoch: 866 loss_train: 0.0851 loss_val: 0.1790 acc_train: 0.9681 acc_val: 0.9398 time: 37 s total_time: 304 min\n",
      "Epoch: 871 loss_train: 0.0983 loss_val: 0.1270 acc_train: 0.9628 acc_val: 0.9572 time: 35 s total_time: 306 min\n",
      "Epoch: 876 loss_train: 0.0950 loss_val: 0.1222 acc_train: 0.9642 acc_val: 0.9589 time: 37 s total_time: 308 min\n",
      "Epoch: 881 loss_train: 0.0941 loss_val: 0.1176 acc_train: 0.9639 acc_val: 0.9608 time: 37 s total_time: 310 min\n",
      "Epoch: 886 loss_train: 0.0908 loss_val: 0.1162 acc_train: 0.9658 acc_val: 0.9615 time: 33 s total_time: 312 min\n",
      "Epoch: 891 loss_train: 0.0866 loss_val: 0.1159 acc_train: 0.9678 acc_val: 0.9621 time: 47 s total_time: 314 min\n",
      "Epoch: 896 loss_train: 0.0870 loss_val: 0.1141 acc_train: 0.9675 acc_val: 0.9633 time: 30 s total_time: 315 min\n",
      "Epoch: 901 loss_train: 0.0835 loss_val: 0.1188 acc_train: 0.9690 acc_val: 0.9628 time: 39 s total_time: 317 min\n",
      "Epoch: 906 loss_train: 0.0827 loss_val: 0.1137 acc_train: 0.9693 acc_val: 0.9642 time: 33 s total_time: 319 min\n",
      "Epoch: 911 loss_train: 0.0821 loss_val: 0.1159 acc_train: 0.9695 acc_val: 0.9641 time: 34 s total_time: 320 min\n",
      "Epoch: 916 loss_train: 0.0821 loss_val: 0.1148 acc_train: 0.9694 acc_val: 0.9645 time: 19 s total_time: 322 min\n",
      "Epoch: 921 loss_train: 0.0811 loss_val: 0.1131 acc_train: 0.9699 acc_val: 0.9648 time: 14 s total_time: 324 min\n",
      "Epoch: 926 loss_train: 0.0809 loss_val: 0.1131 acc_train: 0.9700 acc_val: 0.9649 time: 14 s total_time: 325 min\n",
      "Epoch: 931 loss_train: 0.0804 loss_val: 0.1134 acc_train: 0.9702 acc_val: 0.9650 time: 18 s total_time: 327 min\n",
      "Epoch: 936 loss_train: 0.0801 loss_val: 0.1133 acc_train: 0.9703 acc_val: 0.9652 time: 17 s total_time: 329 min\n",
      "Epoch: 941 loss_train: 0.0798 loss_val: 0.1128 acc_train: 0.9704 acc_val: 0.9653 time: 17 s total_time: 330 min\n",
      "Epoch: 946 loss_train: 0.0795 loss_val: 0.1143 acc_train: 0.9705 acc_val: 0.9651 time: 18 s total_time: 332 min\n",
      "Epoch: 951 loss_train: 0.0806 loss_val: 0.1133 acc_train: 0.9701 acc_val: 0.9644 time: 18 s total_time: 334 min\n",
      "Epoch: 956 loss_train: 0.0900 loss_val: 0.1143 acc_train: 0.9658 acc_val: 0.9647 time: 18 s total_time: 335 min\n",
      "Epoch: 961 loss_train: 0.0915 loss_val: 0.1261 acc_train: 0.9654 acc_val: 0.9578 time: 18 s total_time: 337 min\n",
      "Epoch: 966 loss_train: 0.0982 loss_val: 0.1551 acc_train: 0.9626 acc_val: 0.9480 time: 19 s total_time: 339 min\n",
      "Epoch: 971 loss_train: 0.0965 loss_val: 0.1163 acc_train: 0.9627 acc_val: 0.9626 time: 19 s total_time: 341 min\n",
      "Epoch: 976 loss_train: 0.0874 loss_val: 0.1126 acc_train: 0.9671 acc_val: 0.9636 time: 18 s total_time: 342 min\n",
      "Epoch: 981 loss_train: 0.0831 loss_val: 0.1132 acc_train: 0.9692 acc_val: 0.9635 time: 17 s total_time: 344 min\n",
      "Epoch: 986 loss_train: 0.0827 loss_val: 0.1173 acc_train: 0.9692 acc_val: 0.9631 time: 18 s total_time: 346 min\n",
      "Epoch: 991 loss_train: 0.0820 loss_val: 0.1117 acc_train: 0.9697 acc_val: 0.9649 time: 14 s total_time: 348 min\n",
      "Epoch: 996 loss_train: 0.0795 loss_val: 0.1185 acc_train: 0.9704 acc_val: 0.9638 time: 16 s total_time: 349 min\n",
      "Optimization Finished in 351 min!\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, optimizer, features, authors, adj, indices, y, val_indices, y_val, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d47e83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 6, 9]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [1, 3, 5, 6, 9]\n",
    "\n",
    "l[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06637d6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (668683560.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[18], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eea64541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('../test_data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "pairs = add_authors_to_pairs(pairs, authors)\n",
    "output = model(features, adj, pairs)\n",
    "y_pred = torch.exp(output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"../submissions_files/{}-submission-{}epochs-{}.csv\".format(today, epochs, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819ad57f",
   "metadata": {},
   "source": [
    "Essayer de remplacer les tags des auteurs par des 1 ones pour voir si l'autheur améliore vraiment le résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf86a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, features, adj, indices, y, val_indices, y_val, epochs):\n",
    "    # Train model\n",
    "    model.train()\n",
    "    start_time = time()\n",
    "    val_indices = add_authors_to_pairs(val_indices) #we add the authors to val_pairs\n",
    "    indices = add_authors_to_pairs(indices) #we add the authors to indices\n",
    "    rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)# We take random indices each time we run an epoch\n",
    "    rand_indices = add_authors_to_pairs(rand_indices)\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "        optimizer.zero_grad()\n",
    "        pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices.   \n",
    "        output = model(features, adj, pairs) # we run the model that gives the output.\n",
    "        loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "        acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "        loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "        optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, adj, val_indices)\n",
    "        y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {:.4f} s'.format(round(time() - t)),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "        if epochs % 50 == 0:\n",
    "            model_path = \"../outputs/models/{}-model-{}epochs.pt\".format(today, epoch)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model\n",
    "\n",
    "epochs = 100\n",
    "trained_model = train_model(model, optimizer, features, adj, indices, y, val_indices, y_val, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "break\n",
    "#y_val = torch.FloatTensor(y_val).to(device)\n",
    "trained_model = train_model(model, optimizer, features, adj, indices, y, val_indices, y_val, epochs)\n",
    "model_nb = randint(0, 1000)\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "model_path = \"../submissions_files/{}-model-{}epochs-{}.pt\".format(today, epochs, model_nb)\n",
    "torch.save(trained_model.state_dict(), model_path)\n",
    "print('Model saved in', model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c0b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot representation using Spark\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "def multi_label_binarizer(df, labels_col='labels', output_col='new_labels'):\n",
    "    \"\"\"\n",
    "    Function that takes as input:\n",
    "    - `df`, pyspark.sql.dataframe \n",
    "    - `labels_col`, string that indicates an array column containing labels\n",
    "    - `output_col`, string that indicates the name of the new labels column\n",
    "    \n",
    "    and returns a multi-label binarized column.\n",
    "    \"\"\"\n",
    "    \n",
    "    # get set of unique labels and sort them\n",
    "    labels_set = df\\\n",
    "        .withColumn('exploded', F.explode('labels'))\\\n",
    "        .agg(F.collect_set('exploded'))\\\n",
    "        .collect()[0][0]\n",
    "    labels_set = sorted(labels_set)\n",
    "    \n",
    "    # dynamically create columns for each value in `labels_set`\n",
    "    for i in labels_set:\n",
    "        df = df.withColumn(i, F.when(F.array_contains(labels_col, i), 1).otherwise(0))\n",
    "        \n",
    "    # create new, multi-label binarized array column\n",
    "    df = df.withColumn(output_col, F.array(*labels_set))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "output = model(features, adj, val_indices)\n",
    "y_val = torch.LongTensor(y_val).to(device)\n",
    "loss_val = F.nll_loss(output, y_val)\n",
    "acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "print(loss_val.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b650a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "trained_model.eval()\n",
    "eval_pairs = np.array(np.transpose(val_edges))\n",
    "#print(eval_pairs.shape)\n",
    "eval_pairs = torch.LongTensor(eval_pairs).to(device)\n",
    "#print(eval_pairs.shape)\n",
    "eval_output = trained_model(features, adj, eval_pairs)\n",
    "#print(eval_output.shape)\n",
    "y_pred = torch.exp(eval_output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "#y_val_pred_true = list()\n",
    "\n",
    "y_val_pred_true = y_pred[:, 1]\n",
    "\n",
    "    \n",
    "print('Log loss:', log_loss(y_val, y_val_pred_true))\n",
    "\n",
    "#y_val = torch.tensor(y_val).to(device)\n",
    "#y_val_pred_true = torch.tensor(y_val_pred_true).to(device)\n",
    "#print(y_val, y_val_pred_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bcd7b39",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for dimension 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m node_pairs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(np\u001b[38;5;241m.\u001b[39mtranspose(node_pairs))\n\u001b[1;32m     16\u001b[0m pairs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(node_pairs)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 17\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(output)\n\u001b[1;32m     19\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[14], line 26\u001b[0m, in \u001b[0;36mGNN.forward\u001b[0;34m(self, x_in, adj, pairs)\u001b[0m\n\u001b[1;32m     23\u001b[0m z2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(z2)\n\u001b[1;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m z2[pairs[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m-\u001b[39m z2[pairs[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;66;03m# embedded features (z2) of node 0 - embedded features of node 1\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mpairs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m[:, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m x\n\u001b[1;32m     27\u001b[0m x1 \u001b[38;5;241m=\u001b[39m z2[pairs[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m     28\u001b[0m x2 \u001b[38;5;241m=\u001b[39m z2[pairs[\u001b[38;5;241m1\u001b[39m]]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for dimension 0 with size 2"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('../test_data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "output = model(features, adj, pairs)\n",
    "y_pred = torch.exp(output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"../submissions_files/{}-submission-{}-{}.csv\".format(today, model_nb, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f130ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bc8b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = y_val.detach().cpu().numpy()\n",
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80600b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred)\n",
    "df['y_val'] = list(y_val)\n",
    "df.to_csv('../submissions_files/comparison_file.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae66fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(y_val, y_pred[:, 1]), mean_absolute_error(y_val, y_pred[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1822541",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_val, y_pred[:, 1]), log_loss(y_val, y_pred[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7e912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5717e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff03bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a828859c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e3c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c29d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b974905",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = np.array([[5, 3, 0], [2, 1, 0], [4, 0, 1], [0, 3, 2]])\n",
    "test_features = torch.LongTensor(test_features).to(device)\n",
    "test_pairs = [[0, 0, 1, 2, 3, 2, 0, 1, 2, 3], [1, 2, 0, 0, 2, 3, 0, 1, 2, 3]]\n",
    "test_pairs = torch.LongTensor(test_pairs).to(device)\n",
    "print(test_features.shape, test_pairs.shape)\n",
    "test_features[test_pairs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0639bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699d53ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape, np.shape(y_val), np.shape(y_pred_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf150c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ccd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(output.shape)\n",
    "print(features.shape, adj.shape, eval_pairs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4446f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "z2[pairs[1,:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bdb8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e2dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff0f64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425d6600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8607dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0728cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ad5476",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71737f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class labels\n",
    "y = np.zeros(4*G_train.number_of_edges())\n",
    "y[:2*G_train.number_of_edges()] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
    "train_edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede8a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "X_train = np.zeros((4*m, 2*features_np.shape[1]))\n",
    "\n",
    "for i, edge in enumerate(train_edges):\n",
    "    X_train[i] = np.concatenate((features_np[train_edges[i][0]], features_np[train_edges[i][1]]), axis=0)\n",
    "    X_train[m+i] = np.concatenate((features_np[train_edges[i][0]], features_np[train_edges[i][1]]), axis=0)\n",
    "    X_train[2*m+i] = np.concatenate((features_np[randint(0,n-1)], features_np[randint(0,n-1)]), axis=0)\n",
    "    X_train[3*m+i] = np.concatenate((features_np[randint(0,n-1)], features_np[randint(0,n-1)]), axis=0)\n",
    "    \n",
    "X_val = np.zeros((len(val_edges), 2*features_np.shape[1]))\n",
    "for i, edge in enumerate(val_edges):\n",
    "    X_val[i] = np.concatenate((features_np[val_edges[i][0]], features_np[val_edges[i][1]]), axis=0)\n",
    "    \n",
    "print('X_train and X_val created in {} s!'.format(round(time()-t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44a3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6fa791",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y)\n",
    "y_pred = clf.predict_proba(X_val)\n",
    "y_pred = y_pred[:,1]\n",
    "print('Logistic regression performed in {} s!'.format(round(time()-t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c60b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Log loss:', log_loss(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b1ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac02a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Read test data. Each sample is a pair of nodes\n",
    "test_edges = list()\n",
    "with open('../test_data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        test_edges.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "X_test = np.zeros((len(test_edges), 2*features_np.shape[1]))\n",
    "for i, edge in enumerate(test_edges):\n",
    "    X_test[i] = np.concatenate((features_np[test_edges[i][0]], features_np[test_edges[i][1]]), axis=0)\n",
    "\n",
    "t = time()\n",
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "y_pred = y_pred[:,1]\n",
    "print('Logistic regression performed in {} s!'.format(round(time()-t)))\n",
    "\n",
    "\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 100000)\n",
    "\n",
    "pd.DataFrame(y_pred, columns={'predicted'}).to_csv(\n",
    "\"../submissions_files/{}-submission-{}.csv\".format(today, random_nb), header=True, index=True, index_label='id'\n",
    ")\n",
    "    \n",
    "    \n",
    "# # Testing\n",
    "# node_pairs = np.array(np.transpose(node_pairs))\n",
    "# pairs = torch.LongTensor(node_pairs).to(device)\n",
    "# output = model(features, adj, pairs)\n",
    "# y_pred = torch.exp(output)\n",
    "# y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "# y_pred_true = list()\n",
    "# for element in y_pred:\n",
    "#     y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "# today = datetime.today().strftime('%Y-%m-%d')\n",
    "# random_nb = randint(0, 100000)\n",
    "\n",
    "# pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "# \"../submissions_files/{}-submission-{}.csv\".format(today, random_nb), header=True, index=True, index_label='id'\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
