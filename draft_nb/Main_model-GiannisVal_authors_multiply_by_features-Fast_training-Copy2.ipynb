{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787a2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import random_walk, generate_walks, sparse_mx_to_torch_sparse_tensor \n",
    "from utils import text_to_list, intersection, read_train_val_graph, save_subgraph_in_file\n",
    "from utils import apply_word2vec_on_features, create_and_normalize_adjacency, train_model, add_authors_to_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d25112fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "667086c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 980 number of edges: 2405 in the Complete the set\n",
      "Number of nodes: 980 number of edges: 2162 in the Training set\n",
      "len(nodes) 980\n",
      "Creating random val_edges...\n",
      "Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects\n",
      "Loaded from small_edgelist.txt and with a training validation split ratio = 0.1\n",
      "Start generating walks....\n",
      "Random walks generated in in 0s!\n",
      "Start applying Word2Vec...\n",
      "Word2vec model trained on features in 0 min!\n",
      "(980, 64) features numpy array created in 0 min!\n",
      "Created a normalized adjancency matrix of shape (980, 980)\n",
      "Created indices (2, 5304) with the positions of non zeros in adj matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghassenabdedayem/Documents/Data/Polytechnique/5- Data Challenge/data_challenge_2022/draft_nb/utils.py:157: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix of the training graph\n"
     ]
    }
   ],
   "source": [
    "path = '../input_data/small_edgelist.txt'\n",
    "G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx = read_train_val_graph(val_ratio=0.1, path=path)\n",
    "walks = generate_walks(G=G_train, num_walks=10, walk_length=15)\n",
    "walks_wv = apply_word2vec_on_features(features=walks, nodes=nodes, vector_size=64)\n",
    "adj, indices = create_and_normalize_adjacency(G_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69776784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_np = np.concatenate([walks_wv, authors_wv], axis=1)\n",
    "features_np = walks_wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02015432",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_csv('../input_data/authors.txt', sep = '|', header=None)\n",
    "authors = authors.rename(columns={0: \"paper_id\", 2: \"authors\"})\n",
    "authors['authors'] = authors['authors'].apply(text_to_list)\n",
    "authors = authors[[\"paper_id\", \"authors\"]]\n",
    "authors = authors[authors['paper_id'] <= max(G.nodes())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bf540ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class labels\n",
    "y = np.zeros(2*indices.shape[1])\n",
    "y[:indices.shape[1]] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
    "\n",
    "# Transforms the numpy matrices/vectors to torch tensors.\n",
    "features = torch.FloatTensor(features_np).to(device)\n",
    "y = torch.LongTensor(y).to(device)\n",
    "if type(adj) != torch.Tensor:\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "indices = torch.LongTensor(indices).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc36c754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c48af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_feat, n_hidden, n_class, sub_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.double_fc3 = nn.Linear((3*n_hidden), n_hidden)\n",
    "        self.fc4 = nn.Linear(n_hidden, sub_class)\n",
    "        self.fc5 = nn.Linear(sub_class, n_class)        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x_in, adj, pairs):\n",
    "        \n",
    "        h1 = self.fc1(x_in)\n",
    "        z1 = self.relu(torch.mm(adj, h1))\n",
    "        z1 = self.dropout(z1)\n",
    "\n",
    "        h2 = self.fc2(z1)\n",
    "        z2 = self.relu(torch.mm(adj, h2))\n",
    "        z2 = self.dropout(z2)\n",
    "\n",
    "        x = z2[pairs[0]] - z2[pairs[1]] # embedded features (z2) of node 0 - embedded features of node 1\n",
    "        x = pairs[3][:, None] * x\n",
    "        x1 = z2[pairs[0]]\n",
    "        x2 = z2[pairs[1]]\n",
    "        x = torch.cat((x, x1, x2), dim=1)\n",
    "        \n",
    "#         x_auth = pairs[2].reshape([len(pairs[2]), 1])\n",
    "#         x_nb_auth = pairs[3].reshape([len(pairs[3]), 1]) \n",
    "        \n",
    "        \n",
    "        \n",
    "        #x1 = z2[pairs[0]]\n",
    "        #x2 = z2[pairs[1]]\n",
    "        # could we add a new dimension to pairs to specify if same author(s)? and then what could we do?\n",
    "              \n",
    "        #x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.relu(self.double_fc3(x))        \n",
    "        #x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        #x = torch.cat((x, x_auth, x_nb_auth), dim=1) #pairs[3] : number of same authors or 1 if same author\n",
    "        x = self.fc5(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eb8d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "n_hidden = 96\n",
    "dropout_rate = 0.2\n",
    "sub_class = 8\n",
    "n_class = 2\n",
    "n_features = features.shape[1]\n",
    "\n",
    "# Creates the model and specifies the optimizer\n",
    "model = GNN(n_features, n_hidden, n_class, sub_class, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0c97d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, features, authors, adj, indices, y, val_indices, y_val, epochs):\n",
    "    # Train model\n",
    "    start_time = time()\n",
    "    model.train()\n",
    "    \n",
    "    val_indices = add_authors_to_pairs(val_indices, authors) #we add the authors to val_pairs\n",
    "    indices = add_authors_to_pairs(indices, authors) #we add the authors to indices\n",
    "    rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)# We take random indices each time we run an epoch\n",
    "    rand_indices = add_authors_to_pairs(rand_indices, authors)\n",
    "    pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices. \n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    list_val_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features, adj, pairs) # we run the model that gives the output.\n",
    "        loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "        acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "        loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "        optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, adj, val_indices)\n",
    "        y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print ('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {} s'.format(int(round(time()) - round(t))),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "        if epoch % 20 == 0:\n",
    "            model_path = \"../outputs/models/{}-model-{}epochs.pt\".format(today, epoch)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "#         overfitting = []\n",
    "        \n",
    "#         for last_loss in list_val_loss[-10:]:\n",
    "#             if len(list_val_loss) >=10 and loss_val > last_loss:\n",
    "#                 overfitting.append(1)\n",
    "#         if len(overfitting)>=9: \n",
    "#             break\n",
    "        \n",
    "#         list_val_loss.append(float(loss_val))\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7fbd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 0.6930 loss_val: 0.6931 acc_train: 0.5002 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 006 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 011 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 016 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 021 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 026 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 031 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 036 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 041 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 1 s total_time: 0 min\n",
      "Epoch: 046 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 051 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 056 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 061 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 066 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 1 s total_time: 0 min\n",
      "Epoch: 071 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 076 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 081 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 086 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 091 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 096 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 101 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 106 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 111 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 116 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 121 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 126 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 131 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 136 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 141 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 146 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 151 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 156 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 161 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 166 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 171 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 176 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 181 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 186 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 191 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 196 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 201 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 206 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 211 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 216 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 221 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 226 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 231 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 236 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 241 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 246 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 251 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 256 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 261 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 266 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 271 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 276 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 281 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 286 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 291 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 296 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 301 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 306 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 311 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 316 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 321 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 326 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 331 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 336 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 341 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 346 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 351 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 356 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 361 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 366 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 371 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 376 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 381 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 386 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 391 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 396 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 401 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 406 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 411 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 416 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 421 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 426 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 431 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 436 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 441 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 446 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 451 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 456 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 461 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 1 s total_time: 0 min\n",
      "Epoch: 466 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 471 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 476 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 481 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 486 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 491 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 496 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 501 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 506 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 511 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 516 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 521 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 526 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 531 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 536 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 1 s total_time: 0 min\n",
      "Epoch: 541 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 546 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 551 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 556 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 561 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 566 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 571 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 576 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 581 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 586 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 591 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 596 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 601 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 606 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 611 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 616 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 621 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 626 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 631 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 636 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 641 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 646 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 651 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 656 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 661 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 666 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 671 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 676 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 681 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 686 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 1 s total_time: 0 min\n",
      "Epoch: 691 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 696 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 701 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 706 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 711 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 716 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 0 min\n",
      "Epoch: 721 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 726 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 731 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 736 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 741 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 746 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 751 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 756 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 761 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 766 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 771 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 776 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 781 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 786 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 1 s total_time: 1 min\n",
      "Epoch: 791 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 796 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 801 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 806 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 811 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 816 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 821 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 826 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 831 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 836 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 841 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 846 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 851 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 856 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n",
      "Epoch: 861 loss_train: 0.6931 loss_val: 0.6931 acc_train: 0.5001 acc_val: 0.5000 time: 0 s total_time: 1 min\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, optimizer, features, authors, adj, indices, y, val_indices, y_val, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ac9ed8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 6, 9]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06637d6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (668683560.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7048f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('../test_data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "pairs = add_authors_to_pairs(pairs, authors)\n",
    "output = model(features, adj, pairs)\n",
    "y_pred = torch.exp(output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"../submissions_files/{}-submission-{}epochs-{}.csv\".format(today, epochs, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819ad57f",
   "metadata": {},
   "source": [
    "Essayer de remplacer les tags des auteurs par des 1 ones pour voir si l'autheur améliore vraiment le résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf86a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, features, adj, indices, y, val_indices, y_val, epochs):\n",
    "    # Train model\n",
    "    model.train()\n",
    "    start_time = time()\n",
    "    val_indices = add_authors_to_pairs(val_indices) #we add the authors to val_pairs\n",
    "    indices = add_authors_to_pairs(indices) #we add the authors to indices\n",
    "    rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)# We take random indices each time we run an epoch\n",
    "    rand_indices = add_authors_to_pairs(rand_indices)\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "        optimizer.zero_grad()\n",
    "        pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices.   \n",
    "        output = model(features, adj, pairs) # we run the model that gives the output.\n",
    "        loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "        acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "        loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "        optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, adj, val_indices)\n",
    "        y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {:.4f} s'.format(round(time() - t)),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "        if epochs % 50 == 0:\n",
    "            model_path = \"../outputs/models/{}-model-{}epochs.pt\".format(today, epoch)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model\n",
    "\n",
    "epochs = 100\n",
    "trained_model = train_model(model, optimizer, features, adj, indices, y, val_indices, y_val, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "break\n",
    "#y_val = torch.FloatTensor(y_val).to(device)\n",
    "trained_model = train_model(model, optimizer, features, adj, indices, y, val_indices, y_val, epochs)\n",
    "model_nb = randint(0, 1000)\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "model_path = \"../submissions_files/{}-model-{}epochs-{}.pt\".format(today, epochs, model_nb)\n",
    "torch.save(trained_model.state_dict(), model_path)\n",
    "print('Model saved in', model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c0b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot representation using Spark\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "def multi_label_binarizer(df, labels_col='labels', output_col='new_labels'):\n",
    "    \"\"\"\n",
    "    Function that takes as input:\n",
    "    - `df`, pyspark.sql.dataframe \n",
    "    - `labels_col`, string that indicates an array column containing labels\n",
    "    - `output_col`, string that indicates the name of the new labels column\n",
    "    \n",
    "    and returns a multi-label binarized column.\n",
    "    \"\"\"\n",
    "    \n",
    "    # get set of unique labels and sort them\n",
    "    labels_set = df\\\n",
    "        .withColumn('exploded', F.explode('labels'))\\\n",
    "        .agg(F.collect_set('exploded'))\\\n",
    "        .collect()[0][0]\n",
    "    labels_set = sorted(labels_set)\n",
    "    \n",
    "    # dynamically create columns for each value in `labels_set`\n",
    "    for i in labels_set:\n",
    "        df = df.withColumn(i, F.when(F.array_contains(labels_col, i), 1).otherwise(0))\n",
    "        \n",
    "    # create new, multi-label binarized array column\n",
    "    df = df.withColumn(output_col, F.array(*labels_set))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "output = model(features, adj, val_indices)\n",
    "y_val = torch.LongTensor(y_val).to(device)\n",
    "loss_val = F.nll_loss(output, y_val)\n",
    "acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "print(loss_val.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b650a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "trained_model.eval()\n",
    "eval_pairs = np.array(np.transpose(val_edges))\n",
    "#print(eval_pairs.shape)\n",
    "eval_pairs = torch.LongTensor(eval_pairs).to(device)\n",
    "#print(eval_pairs.shape)\n",
    "eval_output = trained_model(features, adj, eval_pairs)\n",
    "#print(eval_output.shape)\n",
    "y_pred = torch.exp(eval_output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "#y_val_pred_true = list()\n",
    "\n",
    "y_val_pred_true = y_pred[:, 1]\n",
    "\n",
    "    \n",
    "print('Log loss:', log_loss(y_val, y_val_pred_true))\n",
    "\n",
    "#y_val = torch.tensor(y_val).to(device)\n",
    "#y_val_pred_true = torch.tensor(y_val_pred_true).to(device)\n",
    "#print(y_val, y_val_pred_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('../test_data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "output = model(features, adj, pairs)\n",
    "y_pred = torch.exp(output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"../submissions_files/{}-submission-{}-{}.csv\".format(today, model_nb, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f130ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bc8b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = y_val.detach().cpu().numpy()\n",
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80600b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred)\n",
    "df['y_val'] = list(y_val)\n",
    "df.to_csv('../submissions_files/comparison_file.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae66fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(y_val, y_pred[:, 1]), mean_absolute_error(y_val, y_pred[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1822541",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_val, y_pred[:, 1]), log_loss(y_val, y_pred[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7e912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5717e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff03bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a828859c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e3c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c29d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b974905",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = np.array([[5, 3, 0], [2, 1, 0], [4, 0, 1], [0, 3, 2]])\n",
    "test_features = torch.LongTensor(test_features).to(device)\n",
    "test_pairs = [[0, 0, 1, 2, 3, 2, 0, 1, 2, 3], [1, 2, 0, 0, 2, 3, 0, 1, 2, 3]]\n",
    "test_pairs = torch.LongTensor(test_pairs).to(device)\n",
    "print(test_features.shape, test_pairs.shape)\n",
    "test_features[test_pairs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0639bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699d53ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape, np.shape(y_val), np.shape(y_pred_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf150c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ccd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(output.shape)\n",
    "print(features.shape, adj.shape, eval_pairs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4446f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "z2[pairs[1,:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bdb8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e2dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff0f64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425d6600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8607dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0728cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ad5476",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71737f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class labels\n",
    "y = np.zeros(4*G_train.number_of_edges())\n",
    "y[:2*G_train.number_of_edges()] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
    "train_edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede8a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "X_train = np.zeros((4*m, 2*features_np.shape[1]))\n",
    "\n",
    "for i, edge in enumerate(train_edges):\n",
    "    X_train[i] = np.concatenate((features_np[train_edges[i][0]], features_np[train_edges[i][1]]), axis=0)\n",
    "    X_train[m+i] = np.concatenate((features_np[train_edges[i][0]], features_np[train_edges[i][1]]), axis=0)\n",
    "    X_train[2*m+i] = np.concatenate((features_np[randint(0,n-1)], features_np[randint(0,n-1)]), axis=0)\n",
    "    X_train[3*m+i] = np.concatenate((features_np[randint(0,n-1)], features_np[randint(0,n-1)]), axis=0)\n",
    "    \n",
    "X_val = np.zeros((len(val_edges), 2*features_np.shape[1]))\n",
    "for i, edge in enumerate(val_edges):\n",
    "    X_val[i] = np.concatenate((features_np[val_edges[i][0]], features_np[val_edges[i][1]]), axis=0)\n",
    "    \n",
    "print('X_train and X_val created in {} s!'.format(round(time()-t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44a3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6fa791",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y)\n",
    "y_pred = clf.predict_proba(X_val)\n",
    "y_pred = y_pred[:,1]\n",
    "print('Logistic regression performed in {} s!'.format(round(time()-t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c60b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Log loss:', log_loss(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b1ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac02a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Read test data. Each sample is a pair of nodes\n",
    "test_edges = list()\n",
    "with open('../test_data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        test_edges.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "X_test = np.zeros((len(test_edges), 2*features_np.shape[1]))\n",
    "for i, edge in enumerate(test_edges):\n",
    "    X_test[i] = np.concatenate((features_np[test_edges[i][0]], features_np[test_edges[i][1]]), axis=0)\n",
    "\n",
    "t = time()\n",
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "y_pred = y_pred[:,1]\n",
    "print('Logistic regression performed in {} s!'.format(round(time()-t)))\n",
    "\n",
    "\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 100000)\n",
    "\n",
    "pd.DataFrame(y_pred, columns={'predicted'}).to_csv(\n",
    "\"../submissions_files/{}-submission-{}.csv\".format(today, random_nb), header=True, index=True, index_label='id'\n",
    ")\n",
    "    \n",
    "    \n",
    "# # Testing\n",
    "# node_pairs = np.array(np.transpose(node_pairs))\n",
    "# pairs = torch.LongTensor(node_pairs).to(device)\n",
    "# output = model(features, adj, pairs)\n",
    "# y_pred = torch.exp(output)\n",
    "# y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "# y_pred_true = list()\n",
    "# for element in y_pred:\n",
    "#     y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "# today = datetime.today().strftime('%Y-%m-%d')\n",
    "# random_nb = randint(0, 100000)\n",
    "\n",
    "# pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "# \"../submissions_files/{}-submission-{}.csv\".format(today, random_nb), header=True, index=True, index_label='id'\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
