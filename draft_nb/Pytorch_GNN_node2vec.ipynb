{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "46a7427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from random import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from random import choice\n",
    "from gensim.models import Word2Vec\n",
    "import keras\n",
    "\n",
    "from scipy.sparse import identity, diags\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "dcf6da2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adjacency(A):\n",
    "    n = A.shape[0]\n",
    "    A = A + identity(n)\n",
    "    degs = A.dot(np.ones(n))\n",
    "    inv_degs = np.power(degs, -1)\n",
    "    D_inv = diags(inv_degs)\n",
    "    A_hat = D_inv.dot(A)\n",
    "    return A_hat\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    print(type(sparse_mx))\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def random_walk(G, node, walk_length):\n",
    "    walk = [node]\n",
    "  \n",
    "    for i in range(walk_length-1):\n",
    "        neibor_nodes = list(G.neighbors(walk[-1]))\n",
    "        if len(neibor_nodes) > 0:\n",
    "            next_node = choice(neibor_nodes)\n",
    "            walk.append(next_node)\n",
    "    walk = [str(node) for node in walk] # in case the nodes are in string format, we don't need to cast into string, but if the nodes are in numeric or integer, we need this line to cast into string\n",
    "    return walk\n",
    "\n",
    "def generate_walks(G, num_walks, walk_length):\n",
    "  # Runs \"num_walks\" random walks from each node, and returns a list of all random walk\n",
    "    walks = list()  \n",
    "    for i in range(num_walks):\n",
    "        for node in G.nodes():\n",
    "            walk = random_walk(G, node, walk_length)\n",
    "            walks.append(walk)\n",
    "        #print('walks : ', walks)\n",
    "    return walks\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_feat, n_hidden, n_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc4 = nn.Linear(n_hidden, n_class)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x_in, adj, pairs):\n",
    "        \n",
    "        h1 = self.fc1(x_in)\n",
    "        z1 = self.relu(torch.mm(adj, h1))\n",
    "        z1 = self.dropout(z1)\n",
    "\n",
    "        h2 = self.fc2(z1)\n",
    "        z2 = self.relu(torch.mm(adj, h2))\n",
    "        \n",
    "        x = z2[pairs[0,:],:] - z2[pairs[1,:],:]\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d22582c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes of total set: 138499\n",
      "Number of edges of total set: 1091955\n",
      "Number of nodes of training set: 138499\n",
      "Number of edges of training set: 982328\n"
     ]
    }
   ],
   "source": [
    "G = nx.read_edgelist('../input_data/edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "nodes = list(G.nodes())\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "edges = list(G.edges())\n",
    "\n",
    "print('Number of nodes of total set:', n)\n",
    "print('Number of edges of total set:', m)\n",
    "\n",
    "node_to_idx = dict()\n",
    "for i, node in enumerate(nodes):\n",
    "    node_to_idx[node] = i\n",
    "\n",
    "val_edges = list()\n",
    "G_train = G\n",
    "\n",
    "for edge in edges:\n",
    "    if random() < 0.1:\n",
    "        val_edges.append(edge)\n",
    "\n",
    "# We remove the val edges from the graph G\n",
    "for edge in val_edges:\n",
    "    G_train.remove_edge(edge[0], edge[1])\n",
    "\n",
    "n = G_train.number_of_nodes()\n",
    "m = G_train.number_of_edges()\n",
    "train_edges = list(G_train.edges())\n",
    "    \n",
    "print('Number of nodes of training set:', n)\n",
    "print('Number of edges of training set:', m)\n",
    "\n",
    "y_val = [1]*len(val_edges)\n",
    "\n",
    "n_val_edges = len(val_edges)\n",
    "\n",
    "# Create random pairs of nodes\n",
    "for i in range(n_val_edges):\n",
    "    n1 = nodes[randint(0, n-1)]\n",
    "    n2 = nodes[randint(0, n-1)]\n",
    "    (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "    val_edges.append((n1, n2))\n",
    "    \n",
    "# Remove from val_edges edges that exist in both train and val\n",
    "\n",
    "for edge in list(set(val_edges) & set(train_edges)):\n",
    "    val_edges.remove(edge)\n",
    "    \n",
    "n_val_edges = len(val_edges) - len(y_val) #because we removed from val_edges edges that exist in both\n",
    "y_val.extend([0]*n_val_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec47747",
   "metadata": {},
   "source": [
    "### Is it fine to create walks only from the G_train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0858eadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "walks = generate_walks(G=G_train, num_walks=10, walk_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "4b75c226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.39484984, -0.46734977,  0.26839542, -0.26394665,  0.19405559,\n",
       "       -0.6791535 ,  0.8497028 ,  0.22450827, -0.8088502 , -0.13068233,\n",
       "        0.6681871 , -0.2724623 , -0.40058526, -0.5342294 , -0.14299789,\n",
       "        0.532179  , -0.3065716 , -0.08885144,  0.22567934, -1.0684742 ,\n",
       "        0.44301444,  0.31018835, -0.22123612,  0.09345564,  0.24089716,\n",
       "       -0.47015134, -0.20757306, -0.27669477, -0.09022675, -0.44280678,\n",
       "        0.07159458, -0.52626   ,  0.9142592 , -0.46609974,  0.07174298,\n",
       "       -0.2335478 , -0.03954452, -0.81300277,  0.07897256, -0.5355212 ,\n",
       "        0.01435002,  0.00409464, -0.25541407, -0.24264942,  0.27601284,\n",
       "       -0.22861034, -0.2940636 ,  0.5951595 , -0.12762967, -0.4103339 ,\n",
       "        0.26178753,  0.8978525 , -0.18115993,  0.02725913, -0.5648883 ,\n",
       "        0.21274579,  0.22102082, -0.6636808 ,  0.13150162,  0.8125767 ,\n",
       "       -0.6556572 ,  0.25510177, -0.26354107, -0.11755897, -0.0264527 ,\n",
       "        0.36873448, -0.24239857,  0.44601217,  0.24678412, -0.08253359,\n",
       "        0.08374815,  0.07531892,  0.03931298, -0.6413774 ,  0.3493401 ,\n",
       "        0.4140399 , -0.04159411,  0.04411974, -0.4405932 , -0.6234672 ,\n",
       "       -0.17989555,  0.2899456 ,  0.74230105,  0.02788679, -0.49576008,\n",
       "        0.30109626,  0.5335078 , -0.33140558,  0.19425553,  0.7976276 ,\n",
       "        0.2572879 ,  0.01188355, -0.06678827, -0.33684838, -0.1128367 ,\n",
       "       -0.08644569, -0.08846389, -0.0991912 , -0.35133716, -0.02589074,\n",
       "       -0.62622076,  0.42765957, -0.32713625,  0.12223818,  0.07323682,\n",
       "       -0.1054469 , -0.17270619, -0.08328862,  0.11856075, -0.4293744 ,\n",
       "        0.04115481, -0.09786023, -0.00209821,  0.32910427, -0.22224617,\n",
       "        0.28804946,  0.59030217, -0.06512782, -0.3380256 ,  0.2329077 ,\n",
       "       -0.30463868, -0.07596052,  0.23817967, -0.21966244,  0.39391354,\n",
       "        0.45771962, -0.4400651 ,  0.0644887 ], dtype=float32)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(vector_size=128, window=5, min_count=0, sg=1, workers=8)\n",
    "model.build_vocab(walks)\n",
    "model.train(walks, total_examples=model.corpus_count, epochs=5) \n",
    "model.wv['32098']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "fe3e518f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pd/glcnl2497w5b6xn3p94tnwlr0000gn/T/ipykernel_44001/2621795019.py:1: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G_train) # Obtains the adjacency matrix of the training graph\n"
     ]
    }
   ],
   "source": [
    "adj = nx.adjacency_matrix(G_train) # Obtains the adjacency matrix of the training graph\n",
    "indices = np.array(adj.nonzero()) # Gets the positions of non zeros of adj into indices\n",
    "adj = normalize_adjacency(adj) # Normalizes the adjacency matrix only by adding ones to diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "0e097cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features initializaed randomly because not yet ready\n",
    "features_np = []\n",
    "\n",
    "for node in G_train.nodes():\n",
    "    features_np.append(model.wv[str(node)])\n",
    "\n",
    "features_np = np.array(features_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d6458fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Create class labels\n",
    "y = np.zeros(4*G_train.number_of_edges())\n",
    "y[:2*G_train.number_of_edges()] = 1 # Concatenated ones for edges indices and zeros for random indices.\n",
    "\n",
    "# Transforms the numpy matrices/vectors to torch tensors.\n",
    "features = torch.FloatTensor(features_np).to(device)\n",
    "y = torch.LongTensor(y).to(device)\n",
    "print(type(adj))\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "print(type(adj))\n",
    "indices = torch.LongTensor(indices).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "14a08291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 40\n",
    "n_hidden = 128\n",
    "dropout_rate = 0.2\n",
    "n_class = 2\n",
    "n_features = features.shape[1]\n",
    "\n",
    "# Creates the model and specifies the optimizer\n",
    "model = GNN(n_features, n_hidden, n_class, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "daac32ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 22.6118 acc_train: 0.4894 time: 35.8183s total_time: 1min\n",
      "Epoch: 006 loss_train: 8.9000 acc_train: 0.5023 time: 24.3090s total_time: 2min\n",
      "Epoch: 011 loss_train: 3.8519 acc_train: 0.5182 time: 14.4293s total_time: 4min\n",
      "Epoch: 016 loss_train: 1.6082 acc_train: 0.5588 time: 20.0659s total_time: 6min\n",
      "Epoch: 021 loss_train: 0.8859 acc_train: 0.5880 time: 19.3813s total_time: 7min\n",
      "Epoch: 026 loss_train: 0.6487 acc_train: 0.6367 time: 19.1522s total_time: 9min\n",
      "Epoch: 031 loss_train: 0.6059 acc_train: 0.6787 time: 12.8576s total_time: 10min\n",
      "Epoch: 036 loss_train: 0.6023 acc_train: 0.6791 time: 15.9874s total_time: 12min\n",
      "Optimization Finished in 13 min!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Train model\n",
    "model.train()\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    t = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)# We take random indices each time we run an epoch\n",
    "    pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices.   \n",
    "    output = model(features, adj, pairs) # we run the model that gives the output.\n",
    "    loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "    #print(type(loss_train), '\\n', loss_train.shape)\n",
    "    acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "    loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "    optimizer.step() # Performs a single optimization step (parameter update).\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "              'time: {:.4f}s'.format(time.time() - t),\n",
    "             'total_time: {}min'.format(round((time.time() - start_time)/60)))\n",
    "\n",
    "print(\"Optimization Finished in {} min!\".format(round((time.time() - start_time)/60)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e74c8f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_pairs = np.array(np.transpose(val_edges))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "output = model(features, adj, pairs)\n",
    "y_pred_val = torch.exp(output)\n",
    "y_pred_val = y_pred_val.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "932f2680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.38938335, 0.3865336 , 0.38906032, ..., 0.37584653, 0.3831838 ,\n",
       "       0.33335236], dtype=float32)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_val[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358d7e38",
   "metadata": {},
   "source": [
    "### Why is it too big compared to loss obtained in Kaggle?! And that the log loss > 1 even if values are between 0 and 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b278d11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghassenabdedayem/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2465: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/Users/ghassenabdedayem/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2465: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    }
   ],
   "source": [
    "print('Log loss:', log_loss(y_val, y_pred_val[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0a9d9fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 100000)\n",
    "\n",
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('../test_data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "output = model(features, adj, pairs)\n",
    "y_pred = torch.exp(output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"../submissions_files/{}-submission-{}.csv\".format(today, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22a327",
   "metadata": {},
   "source": [
    "CNN with Sigmoid to predict if they connected based on the abstract."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cad23c",
   "metadata": {},
   "source": [
    "A good way to aggregate the text is to calculate the average (mean)\n",
    "\n",
    "Another approach is to use directly a GNN. Take the abstract, compute the embeddng of the words. Take the mean of the node.\n",
    "Then you can take the features.\n",
    "\n",
    "For thr GN, we have only the \n",
    "\n",
    "pairs is a tensor, contains a pair of nodes that contains all the positive samples and some of the negative samples. y: half of them are equal to one, and half of them are connected. rand_indices are random pairs that are considered as not connected.\n",
    "\n",
    "As we have a non directed. We can take twice every edge (2*m instead of 2*m for y. Or we can take the edges only once.\n",
    "\n",
    "One vector for the abstract using the word2vec embedding or any other similar approach.\n",
    "\n",
    "Or we can directly use a CNN.We can take a CNN and feed pais of abstracts in the CNN, the CNN will produce one vector for the first abstract and one vector for the second. We can combine these two vectors.\n",
    "\n",
    "Then we can use an MLP to produce a vector, and then we can concatenate the two vectors from CNN and MLP.\n",
    "\n",
    "There is a pretrained word embedding (Google provided a pretrained embedding).\n",
    "\n",
    "Embedding of each word. Then the CNN will provide one vector for the abstract.\n",
    "\n",
    "Each abstract has a different number of words. the representation of the CNN will have a fixed size of the embedding vector.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
