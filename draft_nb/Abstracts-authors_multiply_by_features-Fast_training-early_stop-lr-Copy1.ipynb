{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "787a2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import random_walk, generate_walks, sparse_mx_to_torch_sparse_tensor \n",
    "from utils import text_to_list, intersection, read_train_val_graph, save_subgraph_in_file\n",
    "from utils import apply_word2vec_on_features, create_and_normalize_adjacency \n",
    "from utils import add_authors_to_pairs#, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d25112fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings #This module ignores the various types of warnings generated\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c8bed83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499 number of edges: 1091955 in the Complete the set\n",
      "Number of nodes: 138499 number of edges: 982943 in the Training set\n",
      "len(nodes) 138499\n",
      "Creating random val_edges...\n",
      "Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects\n",
      "Loaded from edgelist.txt and with a training validation split ratio = 0.1\n"
     ]
    }
   ],
   "source": [
    "path = '../input_data/edgelist.txt'\n",
    "G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx = read_train_val_graph(val_ratio=0.1, path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "667086c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499 number of edges: 1091955 in the Complete the set\n",
      "Number of nodes: 138499 number of edges: 982230 in the Training set\n",
      "len(nodes) 138499\n",
      "Creating random val_edges...\n",
      "Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects\n",
      "Loaded from edgelist.txt and with a training validation split ratio = 0.1\n",
      "Start generating walks....\n",
      "Random walks generated in in 86s!\n",
      "Start applying Word2Vec...\n",
      "Word2vec model trained on features in 4 min!\n",
      "(138499, 64) features numpy array created in 4 min!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghassenabdedayem/Documents/Data/Polytechnique/5- Data Challenge/data_challenge_2022/draft_nb/utils.py:157: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix of the training graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a normalized adjancency matrix of shape (138499, 138499)\n",
      "Created indices (2, 2102959) with the positions of non zeros in adj matrix\n"
     ]
    }
   ],
   "source": [
    "walks = generate_walks(G=G_train, num_walks=10, walk_length=15)\n",
    "walks_wv = apply_word2vec_on_features(features=walks, nodes=nodes, vector_size=64)\n",
    "adj, indices = create_and_normalize_adjacency(G_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf3e7ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A unified approach is proposed for data modelling that includes supervised regression and classification applications as well as unsupervised probability density function estimation. The orthogonal-least-squares regression based on the leave-one-out test criteria is formulated within this unified data-modelling framework to construct sparse kernel models that generalise well. Examples from regression, classification and density estimation applications are used to illustrate the effectiveness of this generic data-modelling approach for constructing parsimonious kernel models with excellent generalisation capability.\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts_path = '../input_data/abstracts.txt'\n",
    "abstracts = dict()\n",
    "with open(abstracts_path, 'r') as f:\n",
    "    for line in f:\n",
    "        node, abstract = line.split('|--|')\n",
    "        abstracts[int(node)] = abstract\n",
    "abstracts[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3af2870d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model at ../pretrained_nlp/GoogleNews-vectors-negative300.bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "import psutil #This module helps in retrieving information on running processes and system resource utilization\n",
    "process = psutil.Process(os.getpid())\n",
    "from psutil import virtual_memory\n",
    "mem = virtual_memory()\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "pretrainedpath = gn_vec_path\n",
    "\n",
    "import time #This module is used to calculate the time  \n",
    "\n",
    "gn_vec_path = \"../pretrained_nlp/GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "print(f\"Model at {gn_vec_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ee6d78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used in GB before Loading the Model: 2.51\n",
      "----------\n",
      "22.41 seconds taken to load\n",
      "----------\n",
      "Finished loading Word2Vec\n",
      "----------\n",
      "Memory used in GB after Loading the Model: 5.82\n",
      "----------\n",
      "Percentage increase in memory usage: 232.12% \n",
      "----------\n",
      "Numver of words in vocablulary:  3000000\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "pretrainedpath = gn_vec_path\n",
    "\n",
    "#Load W2V model. This will take some time, but it is a one time effort! \n",
    "pre = process.memory_info().rss\n",
    "print(\"Memory used in GB before Loading the Model: %0.2f\"%float(pre/(10**9))) #Check memory usage before loading the model\n",
    "print('-'*10)\n",
    "\n",
    "start_time = time.time() #Start the timer\n",
    "ttl = mem.total #Toal memory available\n",
    "\n",
    "w2v_model = KeyedVectors.load_word2vec_format(pretrainedpath, binary=True) #load the model\n",
    "print(\"%0.2f seconds taken to load\"%float(time.time() - start_time)) #Calculate the total time elapsed since starting the timer\n",
    "print('-'*10)\n",
    "\n",
    "print('Finished loading Word2Vec')\n",
    "print('-'*10)\n",
    "\n",
    "post = process.memory_info().rss\n",
    "print(\"Memory used in GB after Loading the Model: {:.2f}\".format(float(post/(10**9)))) #Calculate the memory used after loading the model\n",
    "print('-'*10)\n",
    "\n",
    "print(\"Percentage increase in memory usage: {:.2f}% \".format(float((post/pre)*100))) #Percentage increase in memory after loading the model\n",
    "print('-'*10)\n",
    "\n",
    "print(\"Numver of words in vocablulary: \",len(w2v_model.index_to_key)) #Number of words in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "03e5c24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05664062, -0.17382812,  0.01953125,  0.13085938, -0.24414062,\n",
       "        0.08496094, -0.07275391, -0.11621094,  0.15429688,  0.13671875,\n",
       "        0.21484375, -0.20703125,  0.09814453, -0.09082031, -0.17285156,\n",
       "        0.07470703,  0.26367188,  0.07666016,  0.05297852, -0.04541016,\n",
       "        0.3515625 , -0.18261719,  0.00288391,  0.4140625 , -0.16796875,\n",
       "       -0.1640625 ,  0.2109375 ,  0.24804688, -0.10449219, -0.25585938,\n",
       "        0.06884766, -0.3515625 , -0.13476562,  0.10498047, -0.09570312,\n",
       "       -0.49804688, -0.29882812, -0.3046875 , -0.02209473,  0.25585938,\n",
       "       -0.09179688,  0.05395508,  0.21386719,  0.01165771,  0.01202393,\n",
       "       -0.00689697,  0.09619141,  0.16894531, -0.25195312, -0.25      ,\n",
       "        0.04614258,  0.04785156, -0.00646973,  0.25390625,  0.00189209,\n",
       "       -0.05761719,  0.390625  ,  0.01586914, -0.2890625 , -0.04492188,\n",
       "       -0.20019531,  0.13671875,  0.04174805, -0.12890625, -0.08154297,\n",
       "       -0.00656128, -0.27929688,  0.07421875, -0.19628906, -0.18457031,\n",
       "        0.07910156,  0.14550781,  0.06347656,  0.14550781,  0.15332031,\n",
       "        0.20410156,  0.03491211, -0.18066406,  0.20996094,  0.29101562,\n",
       "       -0.18847656, -0.0402832 , -0.05883789,  0.13867188, -0.19726562,\n",
       "        0.13476562,  0.30664062, -0.01434326,  0.125     , -0.09960938,\n",
       "       -0.12695312, -0.2578125 ,  0.19140625, -0.31640625,  0.35351562,\n",
       "       -0.3203125 ,  0.390625  ,  0.03393555, -0.16503906, -0.11523438,\n",
       "        0.25585938, -0.10546875, -0.11425781,  0.08251953,  0.15917969,\n",
       "       -0.49414062,  0.20410156, -0.37695312, -0.25390625,  0.03442383,\n",
       "        0.15820312,  0.21386719, -0.14355469, -0.51953125, -0.03955078,\n",
       "       -0.09472656,  0.26367188, -0.21289062, -0.16503906, -0.02404785,\n",
       "       -0.34179688,  0.13183594,  0.09765625,  0.45507812,  0.07666016,\n",
       "       -0.39257812, -0.06835938,  0.07373047,  0.1875    ,  0.00747681,\n",
       "       -0.22070312, -0.14550781, -0.1328125 ,  0.11230469, -0.09765625,\n",
       "       -0.07714844,  0.11572266,  0.140625  ,  0.20507812,  0.19628906,\n",
       "        0.13378906, -0.19921875, -0.03955078,  0.28710938,  0.09082031,\n",
       "        0.3203125 , -0.13574219,  0.32617188, -0.15332031, -0.13769531,\n",
       "        0.53515625, -0.21289062,  0.13671875,  0.06884766, -0.23632812,\n",
       "        0.28320312, -0.17773438, -0.17480469,  0.03442383,  0.06787109,\n",
       "        0.1953125 ,  0.21191406, -0.15917969,  0.015625  ,  0.06445312,\n",
       "        0.16601562,  0.08105469,  0.27734375, -0.00094604, -0.171875  ,\n",
       "       -0.07666016, -0.03979492, -0.00448608, -0.01190186, -0.25976562,\n",
       "       -0.05224609,  0.22265625, -0.18652344, -0.328125  ,  0.3515625 ,\n",
       "       -0.01904297,  0.3359375 ,  0.05004883,  0.08496094,  0.15429688,\n",
       "        0.06054688,  0.25      , -0.15820312, -0.03808594,  0.13671875,\n",
       "       -0.09570312, -0.25      , -0.16796875, -0.09960938,  0.09863281,\n",
       "       -0.02612305, -0.12988281, -0.3671875 , -0.24609375, -0.140625  ,\n",
       "        0.20800781,  0.06689453,  0.10400391, -0.56640625,  0.26171875,\n",
       "       -0.015625  ,  0.15429688, -0.01843262, -0.3125    , -0.3671875 ,\n",
       "        0.18261719, -0.27539062,  0.25195312, -0.23339844,  0.01953125,\n",
       "       -0.18359375, -0.05688477,  0.13769531, -0.16796875, -0.30664062,\n",
       "       -0.04345703, -0.12207031,  0.01208496, -0.15234375,  0.19628906,\n",
       "       -0.15039062,  0.41796875,  0.14550781, -0.08691406, -0.16601562,\n",
       "        0.2734375 , -0.21582031,  0.19042969, -0.34570312,  0.03417969,\n",
       "       -0.05810547,  0.47070312, -0.02966309,  0.13867188,  0.33203125,\n",
       "       -0.18457031, -0.19921875, -0.04199219, -0.47070312, -0.01831055,\n",
       "        0.1953125 , -0.28320312, -0.07568359,  0.25390625, -0.35546875,\n",
       "        0.12207031, -0.18261719,  0.10595703,  0.2578125 ,  0.26953125,\n",
       "       -0.30273438,  0.0625    , -0.01531982, -0.16308594,  0.2421875 ,\n",
       "       -0.34765625, -0.24023438,  0.00680542,  0.08886719, -0.23046875,\n",
       "       -0.15332031, -0.18847656, -0.03271484, -0.32617188,  0.09716797,\n",
       "       -0.06738281,  0.36523438, -0.01190186,  0.33203125,  0.00098419,\n",
       "       -0.19140625,  0.21582031,  0.11669922,  0.38085938, -0.24609375,\n",
       "        0.02453613,  0.30273438,  0.12402344,  0.33203125, -0.12011719,\n",
       "       -0.07910156,  0.11035156,  0.39453125, -0.00418091,  0.08203125,\n",
       "       -0.28515625,  0.29492188,  0.01104736,  0.00817871, -0.09326172,\n",
       "       -0.10888672,  0.0534668 , -0.28320312, -0.15625   , -0.12792969],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model['unsupervised']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a66eeb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-01 18:26:17.971096: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/ghassenabdedayem/opt/anaconda3/bin/python: No module named pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4b303942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 5.72 µs\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men_core_web_md\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# process a sentence using the model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m mydoc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCanada is a large country\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     31\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     37\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/util.py:427\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 427\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "#import spacy\n",
    "\n",
    "%time \n",
    "nlp = spacy.load('en_core_web_md')\n",
    "# process a sentence using the model\n",
    "mydoc = nlp(\"Canada is a large country\")\n",
    "#Get a vector for individual words\n",
    "#print(doc[0].vector) #vector for 'Canada', the first word in the text \n",
    "print(mydoc.vector) #Averaged vector for the entire sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02015432",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_csv('../input_data/authors.txt', sep = '|', header=None)\n",
    "authors = authors.rename(columns={0: \"paper_id\", 2: \"authors\"})\n",
    "authors['authors'] = authors['authors'].apply(text_to_list)\n",
    "authors = authors[[\"paper_id\", \"authors\"]]\n",
    "authors = authors[authors['paper_id'] <= max(G.nodes())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69776784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_np = np.concatenate([walks_wv, authors_wv], axis=1)\n",
    "features_np = walks_wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bf540ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class labels\n",
    "y = np.zeros(2*indices.shape[1])\n",
    "y[:indices.shape[1]] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
    "\n",
    "# Transforms the numpy matrices/vectors to torch tensors.\n",
    "features = torch.FloatTensor(features_np).to(device)\n",
    "y = torch.LongTensor(y).to(device)\n",
    "if type(adj) != torch.Tensor:\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "indices = torch.LongTensor(indices).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc36c754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c48af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_feat, n_hidden, n_class, sub_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.double_fc3 = nn.Linear((3*n_hidden), n_hidden)\n",
    "        self.fc4 = nn.Linear(n_hidden, sub_class)\n",
    "        self.fc5 = nn.Linear(sub_class, n_class)        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x_in, adj, pairs):\n",
    "        \n",
    "        h1 = self.fc1(x_in)\n",
    "        z1 = self.relu(torch.mm(adj, h1))\n",
    "        z1 = self.dropout(z1)\n",
    "\n",
    "        h2 = self.fc2(z1)\n",
    "        z2 = self.relu(torch.mm(adj, h2))\n",
    "        z2 = self.dropout(z2)\n",
    "\n",
    "        x = z2[pairs[0]] - z2[pairs[1]] # embedded features (z2) of node 0 - embedded features of node 1\n",
    "        x = pairs[3][:, None] * x#torch.ones(x.shape[0], x.shape[1])\n",
    "        x1 = z2[pairs[0]]\n",
    "        x2 = z2[pairs[1]]\n",
    "        x = torch.cat((x, x1, x2), dim=1)\n",
    "        #x = torch.cat((x1, x2), dim=1)\n",
    "        \n",
    "\n",
    "        x = self.relu(self.double_fc3(x))        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7eb8d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 64\n",
    "dropout_rate = 0.2\n",
    "sub_class = 8\n",
    "n_class = 2\n",
    "n_features = features.shape[1]\n",
    "\n",
    "# Creates the model\n",
    "model = GNN(n_features, n_hidden, n_class, sub_class, dropout_rate).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0c97d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping(list_loss_val, loss_val, window=10):\n",
    "    lst = list(list_loss_val)[-window:]\n",
    "    if len(lst) == window and loss_val > (sum(lst)/len(lst)):\n",
    "        print('mean: {:.5f} val: {:.5f}'.format((sum(lst)/len(lst)),loss_val))\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "    \n",
    "def train_model(model, learning_rate, tolerence, features, authors, adj, indices, y, val_indices, y_val, epochs):\n",
    "    # Train model\n",
    "    start_time = time()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) #optimizer with halving learning rate in training\n",
    "    \n",
    "    val_indices = add_authors_to_pairs(val_indices, authors) #we add the authors to val_pairs\n",
    "    indices = add_authors_to_pairs(indices, authors) #we add the authors to indices\n",
    "    rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)# We take random indices each time we run an epoch\n",
    "    rand_indices = add_authors_to_pairs(rand_indices, authors)\n",
    "    pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices. \n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    list_loss_val = []\n",
    "    \n",
    "    halving_lr = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model.train()\n",
    "        output = model(features, adj, pairs) # we run the model that gives the output.\n",
    "        loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "        acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "        loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "        optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, adj, val_indices)\n",
    "        y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        list_loss_val.append(loss_val.item())\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {} s'.format(int(round(time()) - round(t))),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "        if epoch % 20 == 0:\n",
    "            model_path = \"../outputs/models/{}-model-{}epochs-lr.pt\".format(today, epoch)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "        early = early_stopping(list_loss_val, loss_val.item(), window=tolerence)        \n",
    "        if early:\n",
    "            halving_lr += 1\n",
    "            if halving_lr > 7:\n",
    "                break\n",
    "            list_loss_val=[]\n",
    "            learning_rate = learning_rate/2\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            print('Deviding the learning rate by 2. New learning rate: {:.5f}'.format(learning_rate))\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab35d89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c7fbd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 0.7207 loss_val: 0.7083 acc_train: 0.5000 acc_val: 0.5000 time: 13 s total_time: 2 min\n",
      "Epoch: 006 loss_train: 0.6888 loss_val: 0.6854 acc_train: 0.5348 acc_val: 0.5448 time: 9 s total_time: 3 min\n",
      "Epoch: 011 loss_train: 0.6764 loss_val: 0.6675 acc_train: 0.6232 acc_val: 0.6668 time: 10 s total_time: 4 min\n",
      "Epoch: 016 loss_train: 0.6711 loss_val: 0.6628 acc_train: 0.6283 acc_val: 0.6545 time: 9 s total_time: 4 min\n",
      "Epoch: 021 loss_train: 0.6650 loss_val: 0.6568 acc_train: 0.6552 acc_val: 0.6579 time: 11 s total_time: 5 min\n",
      "Epoch: 026 loss_train: 0.6603 loss_val: 0.6491 acc_train: 0.6630 acc_val: 0.6715 time: 10 s total_time: 6 min\n",
      "Epoch: 031 loss_train: 0.6525 loss_val: 0.6432 acc_train: 0.6649 acc_val: 0.6746 time: 9 s total_time: 7 min\n",
      "Epoch: 036 loss_train: 0.6353 loss_val: 0.6262 acc_train: 0.6778 acc_val: 0.6986 time: 9 s total_time: 8 min\n",
      "Epoch: 041 loss_train: 0.5843 loss_val: 0.5572 acc_train: 0.7112 acc_val: 0.7448 time: 10 s total_time: 8 min\n",
      "Epoch: 046 loss_train: 0.5255 loss_val: 0.5397 acc_train: 0.7585 acc_val: 0.7605 time: 9 s total_time: 9 min\n",
      "Epoch: 051 loss_train: 0.4956 loss_val: 0.5317 acc_train: 0.7751 acc_val: 0.7762 time: 9 s total_time: 10 min\n",
      "Epoch: 056 loss_train: 0.4750 loss_val: 0.5063 acc_train: 0.7881 acc_val: 0.7888 time: 10 s total_time: 11 min\n",
      "Epoch: 061 loss_train: 0.4571 loss_val: 0.4913 acc_train: 0.7974 acc_val: 0.7980 time: 9 s total_time: 12 min\n",
      "Epoch: 066 loss_train: 0.4396 loss_val: 0.4786 acc_train: 0.8047 acc_val: 0.8058 time: 9 s total_time: 12 min\n",
      "Epoch: 071 loss_train: 0.4196 loss_val: 0.4558 acc_train: 0.8136 acc_val: 0.8135 time: 9 s total_time: 13 min\n",
      "Epoch: 076 loss_train: 0.4090 loss_val: 0.4509 acc_train: 0.8197 acc_val: 0.8202 time: 16 s total_time: 14 min\n",
      "Epoch: 081 loss_train: 0.4009 loss_val: 0.4326 acc_train: 0.8232 acc_val: 0.8241 time: 9 s total_time: 15 min\n",
      "Epoch: 086 loss_train: 0.3839 loss_val: 0.4240 acc_train: 0.8323 acc_val: 0.8252 time: 9 s total_time: 16 min\n",
      "Epoch: 091 loss_train: 0.3695 loss_val: 0.3958 acc_train: 0.8409 acc_val: 0.8418 time: 9 s total_time: 17 min\n",
      "Epoch: 096 loss_train: 0.3425 loss_val: 0.3744 acc_train: 0.8591 acc_val: 0.8570 time: 10 s total_time: 17 min\n",
      "Epoch: 101 loss_train: 0.3223 loss_val: 0.3599 acc_train: 0.8698 acc_val: 0.8631 time: 9 s total_time: 18 min\n",
      "Epoch: 106 loss_train: 0.3065 loss_val: 0.3519 acc_train: 0.8766 acc_val: 0.8690 time: 9 s total_time: 19 min\n",
      "Epoch: 111 loss_train: 0.2935 loss_val: 0.3480 acc_train: 0.8831 acc_val: 0.8734 time: 8 s total_time: 20 min\n",
      "Epoch: 116 loss_train: 0.2839 loss_val: 0.3443 acc_train: 0.8884 acc_val: 0.8774 time: 9 s total_time: 20 min\n",
      "Epoch: 121 loss_train: 0.2748 loss_val: 0.3451 acc_train: 0.8920 acc_val: 0.8804 time: 9 s total_time: 21 min\n",
      "Epoch: 126 loss_train: 0.2675 loss_val: 0.3479 acc_train: 0.8951 acc_val: 0.8834 time: 9 s total_time: 22 min\n",
      "mean: 0.34502 val: 0.34793\n",
      "Deviding the learning rate by 2. New learning rate: 0.00500\n",
      "Epoch: 131 loss_train: 0.2824 loss_val: 0.3635 acc_train: 0.8919 acc_val: 0.8815 time: 9 s total_time: 23 min\n",
      "Epoch: 136 loss_train: 0.2709 loss_val: 0.3386 acc_train: 0.8923 acc_val: 0.8816 time: 9 s total_time: 23 min\n",
      "Epoch: 141 loss_train: 0.2654 loss_val: 0.3507 acc_train: 0.8973 acc_val: 0.8840 time: 8 s total_time: 24 min\n",
      "mean: 0.35057 val: 0.35066\n",
      "Deviding the learning rate by 2. New learning rate: 0.00250\n",
      "Epoch: 146 loss_train: 0.2636 loss_val: 0.3476 acc_train: 0.8980 acc_val: 0.8849 time: 9 s total_time: 25 min\n",
      "Epoch: 151 loss_train: 0.2599 loss_val: 0.3449 acc_train: 0.8985 acc_val: 0.8853 time: 9 s total_time: 26 min\n",
      "Epoch: 156 loss_train: 0.2582 loss_val: 0.3419 acc_train: 0.8994 acc_val: 0.8858 time: 10 s total_time: 26 min\n",
      "mean: 0.34411 val: 0.34556\n",
      "Deviding the learning rate by 2. New learning rate: 0.00125\n",
      "Epoch: 161 loss_train: 0.2568 loss_val: 0.3405 acc_train: 0.9004 acc_val: 0.8854 time: 8 s total_time: 27 min\n",
      "Epoch: 166 loss_train: 0.2552 loss_val: 0.3435 acc_train: 0.9012 acc_val: 0.8870 time: 8 s total_time: 28 min\n",
      "Epoch: 171 loss_train: 0.2543 loss_val: 0.3458 acc_train: 0.9015 acc_val: 0.8875 time: 8 s total_time: 29 min\n",
      "Epoch: 176 loss_train: 0.2524 loss_val: 0.3432 acc_train: 0.9022 acc_val: 0.8876 time: 9 s total_time: 29 min\n",
      "mean: 0.34413 val: 0.34440\n",
      "Deviding the learning rate by 2. New learning rate: 0.00063\n",
      "Epoch: 181 loss_train: 0.2514 loss_val: 0.3449 acc_train: 0.9027 acc_val: 0.8882 time: 9 s total_time: 30 min\n",
      "Epoch: 186 loss_train: 0.2502 loss_val: 0.3447 acc_train: 0.9031 acc_val: 0.8887 time: 8 s total_time: 31 min\n",
      "Epoch: 191 loss_train: 0.2490 loss_val: 0.3451 acc_train: 0.9038 acc_val: 0.8891 time: 8 s total_time: 31 min\n",
      "Epoch: 196 loss_train: 0.2479 loss_val: 0.3446 acc_train: 0.9043 acc_val: 0.8895 time: 8 s total_time: 32 min\n",
      "Epoch: 201 loss_train: 0.2472 loss_val: 0.3441 acc_train: 0.9046 acc_val: 0.8900 time: 8 s total_time: 33 min\n",
      "Epoch: 206 loss_train: 0.2458 loss_val: 0.3437 acc_train: 0.9054 acc_val: 0.8905 time: 8 s total_time: 34 min\n",
      "Epoch: 211 loss_train: 0.2444 loss_val: 0.3427 acc_train: 0.9060 acc_val: 0.8912 time: 9 s total_time: 34 min\n",
      "Epoch: 216 loss_train: 0.2434 loss_val: 0.3423 acc_train: 0.9064 acc_val: 0.8917 time: 8 s total_time: 35 min\n",
      "Epoch: 221 loss_train: 0.2420 loss_val: 0.3418 acc_train: 0.9071 acc_val: 0.8923 time: 8 s total_time: 36 min\n",
      "Epoch: 226 loss_train: 0.2404 loss_val: 0.3410 acc_train: 0.9077 acc_val: 0.8929 time: 9 s total_time: 36 min\n",
      "Epoch: 231 loss_train: 0.2390 loss_val: 0.3401 acc_train: 0.9083 acc_val: 0.8938 time: 8 s total_time: 37 min\n",
      "Epoch: 236 loss_train: 0.2375 loss_val: 0.3387 acc_train: 0.9090 acc_val: 0.8946 time: 8 s total_time: 38 min\n",
      "Epoch: 241 loss_train: 0.2363 loss_val: 0.3385 acc_train: 0.9097 acc_val: 0.8955 time: 8 s total_time: 39 min\n",
      "Epoch: 246 loss_train: 0.2350 loss_val: 0.3379 acc_train: 0.9103 acc_val: 0.8960 time: 9 s total_time: 39 min\n",
      "Epoch: 251 loss_train: 0.2339 loss_val: 0.3371 acc_train: 0.9109 acc_val: 0.8967 time: 8 s total_time: 40 min\n",
      "Epoch: 256 loss_train: 0.2315 loss_val: 0.3372 acc_train: 0.9118 acc_val: 0.8974 time: 8 s total_time: 41 min\n",
      "Epoch: 261 loss_train: 0.2304 loss_val: 0.3367 acc_train: 0.9124 acc_val: 0.8983 time: 9 s total_time: 41 min\n",
      "mean: 0.33703 val: 0.33704\n",
      "Deviding the learning rate by 2. New learning rate: 0.00031\n",
      "Epoch: 266 loss_train: 0.2292 loss_val: 0.3356 acc_train: 0.9129 acc_val: 0.8987 time: 10 s total_time: 42 min\n",
      "Epoch: 271 loss_train: 0.2285 loss_val: 0.3369 acc_train: 0.9134 acc_val: 0.8990 time: 9 s total_time: 43 min\n",
      "Epoch: 276 loss_train: 0.2276 loss_val: 0.3362 acc_train: 0.9136 acc_val: 0.8991 time: 8 s total_time: 44 min\n",
      "mean: 0.33662 val: 0.33708\n",
      "Deviding the learning rate by 2. New learning rate: 0.00016\n",
      "Epoch: 281 loss_train: 0.2277 loss_val: 0.3363 acc_train: 0.9136 acc_val: 0.8992 time: 8 s total_time: 44 min\n",
      "Epoch: 286 loss_train: 0.2267 loss_val: 0.3369 acc_train: 0.9139 acc_val: 0.8994 time: 8 s total_time: 45 min\n",
      "Epoch: 291 loss_train: 0.2269 loss_val: 0.3368 acc_train: 0.9138 acc_val: 0.8995 time: 8 s total_time: 46 min\n",
      "Epoch: 296 loss_train: 0.2268 loss_val: 0.3366 acc_train: 0.9140 acc_val: 0.8996 time: 9 s total_time: 46 min\n",
      "mean: 0.33672 val: 0.33678\n",
      "Deviding the learning rate by 2. New learning rate: 0.00008\n",
      "Epoch: 301 loss_train: 0.2261 loss_val: 0.3372 acc_train: 0.9143 acc_val: 0.8996 time: 8 s total_time: 47 min\n",
      "Epoch: 306 loss_train: 0.2265 loss_val: 0.3367 acc_train: 0.9143 acc_val: 0.8997 time: 9 s total_time: 48 min\n",
      "Epoch: 311 loss_train: 0.2259 loss_val: 0.3371 acc_train: 0.9144 acc_val: 0.8998 time: 9 s total_time: 49 min\n",
      "mean: 0.33698 val: 0.33713\n",
      "Optimization Finished in 49 min!\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "trained_model = train_model(model, 0.01, 15, features, authors, adj, indices, y, val_indices, y_val, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ac9ed8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2102959"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_authors_to_pairs(indices, authors)[3].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06637d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7048f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('../test_data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "pairs = add_authors_to_pairs(pairs, authors)\n",
    "output = model(features, adj, pairs)\n",
    "y_pred = torch.exp(output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"../submissions_files/{}-submission-{}epochs-{}.csv\".format(today, epochs, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819ad57f",
   "metadata": {},
   "source": [
    "Essayer de remplacer les tags des auteurs par des 1 ones pour voir si l'autheur améliore vraiment le résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf86a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, features, adj, indices, y, val_indices, y_val, epochs):\n",
    "    # Train model\n",
    "    model.train()\n",
    "    start_time = time()\n",
    "    val_indices = add_authors_to_pairs(val_indices) #we add the authors to val_pairs\n",
    "    indices = add_authors_to_pairs(indices) #we add the authors to indices\n",
    "    rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)# We take random indices each time we run an epoch\n",
    "    rand_indices = add_authors_to_pairs(rand_indices)\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "        optimizer.zero_grad()\n",
    "        pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices.   \n",
    "        output = model(features, adj, pairs) # we run the model that gives the output.\n",
    "        loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "        acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "        loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "        optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, adj, val_indices)\n",
    "        y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {:.4f} s'.format(round(time() - t)),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "        if epochs % 50 == 0:\n",
    "            model_path = \"../outputs/models/{}-model-{}epochs.pt\".format(today, epoch)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model\n",
    "\n",
    "epochs = 100\n",
    "trained_model = train_model(model, optimizer, features, adj, indices, y, val_indices, y_val, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "break\n",
    "#y_val = torch.FloatTensor(y_val).to(device)\n",
    "trained_model = train_model(model, optimizer, features, adj, indices, y, val_indices, y_val, epochs)\n",
    "model_nb = randint(0, 1000)\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "model_path = \"../submissions_files/{}-model-{}epochs-{}.pt\".format(today, epochs, model_nb)\n",
    "torch.save(trained_model.state_dict(), model_path)\n",
    "print('Model saved in', model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c0b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot representation using Spark\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "def multi_label_binarizer(df, labels_col='labels', output_col='new_labels'):\n",
    "    \"\"\"\n",
    "    Function that takes as input:\n",
    "    - `df`, pyspark.sql.dataframe \n",
    "    - `labels_col`, string that indicates an array column containing labels\n",
    "    - `output_col`, string that indicates the name of the new labels column\n",
    "    \n",
    "    and returns a multi-label binarized column.\n",
    "    \"\"\"\n",
    "    \n",
    "    # get set of unique labels and sort them\n",
    "    labels_set = df\\\n",
    "        .withColumn('exploded', F.explode('labels'))\\\n",
    "        .agg(F.collect_set('exploded'))\\\n",
    "        .collect()[0][0]\n",
    "    labels_set = sorted(labels_set)\n",
    "    \n",
    "    # dynamically create columns for each value in `labels_set`\n",
    "    for i in labels_set:\n",
    "        df = df.withColumn(i, F.when(F.array_contains(labels_col, i), 1).otherwise(0))\n",
    "        \n",
    "    # create new, multi-label binarized array column\n",
    "    df = df.withColumn(output_col, F.array(*labels_set))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "output = model(features, adj, val_indices)\n",
    "y_val = torch.LongTensor(y_val).to(device)\n",
    "loss_val = F.nll_loss(output, y_val)\n",
    "acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "print(loss_val.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b650a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "trained_model.eval()\n",
    "eval_pairs = np.array(np.transpose(val_edges))\n",
    "#print(eval_pairs.shape)\n",
    "eval_pairs = torch.LongTensor(eval_pairs).to(device)\n",
    "#print(eval_pairs.shape)\n",
    "eval_output = trained_model(features, adj, eval_pairs)\n",
    "#print(eval_output.shape)\n",
    "y_pred = torch.exp(eval_output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "#y_val_pred_true = list()\n",
    "\n",
    "y_val_pred_true = y_pred[:, 1]\n",
    "\n",
    "    \n",
    "print('Log loss:', log_loss(y_val, y_val_pred_true))\n",
    "\n",
    "#y_val = torch.tensor(y_val).to(device)\n",
    "#y_val_pred_true = torch.tensor(y_val_pred_true).to(device)\n",
    "#print(y_val, y_val_pred_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('../test_data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "output = model(features, adj, pairs)\n",
    "y_pred = torch.exp(output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"../submissions_files/{}-submission-{}-{}.csv\".format(today, model_nb, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f130ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bc8b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = y_val.detach().cpu().numpy()\n",
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80600b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred)\n",
    "df['y_val'] = list(y_val)\n",
    "df.to_csv('../submissions_files/comparison_file.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae66fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(y_val, y_pred[:, 1]), mean_absolute_error(y_val, y_pred[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1822541",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_val, y_pred[:, 1]), log_loss(y_val, y_pred[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7e912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5717e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff03bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a828859c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e3c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c29d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b974905",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = np.array([[5, 3, 0], [2, 1, 0], [4, 0, 1], [0, 3, 2]])\n",
    "test_features = torch.LongTensor(test_features).to(device)\n",
    "test_pairs = [[0, 0, 1, 2, 3, 2, 0, 1, 2, 3], [1, 2, 0, 0, 2, 3, 0, 1, 2, 3]]\n",
    "test_pairs = torch.LongTensor(test_pairs).to(device)\n",
    "print(test_features.shape, test_pairs.shape)\n",
    "test_features[test_pairs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0639bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699d53ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape, np.shape(y_val), np.shape(y_pred_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf150c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ccd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(output.shape)\n",
    "print(features.shape, adj.shape, eval_pairs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4446f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "z2[pairs[1,:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bdb8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e2dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff0f64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425d6600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8607dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0728cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ad5476",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71737f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class labels\n",
    "y = np.zeros(4*G_train.number_of_edges())\n",
    "y[:2*G_train.number_of_edges()] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
    "train_edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede8a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "X_train = np.zeros((4*m, 2*features_np.shape[1]))\n",
    "\n",
    "for i, edge in enumerate(train_edges):\n",
    "    X_train[i] = np.concatenate((features_np[train_edges[i][0]], features_np[train_edges[i][1]]), axis=0)\n",
    "    X_train[m+i] = np.concatenate((features_np[train_edges[i][0]], features_np[train_edges[i][1]]), axis=0)\n",
    "    X_train[2*m+i] = np.concatenate((features_np[randint(0,n-1)], features_np[randint(0,n-1)]), axis=0)\n",
    "    X_train[3*m+i] = np.concatenate((features_np[randint(0,n-1)], features_np[randint(0,n-1)]), axis=0)\n",
    "    \n",
    "X_val = np.zeros((len(val_edges), 2*features_np.shape[1]))\n",
    "for i, edge in enumerate(val_edges):\n",
    "    X_val[i] = np.concatenate((features_np[val_edges[i][0]], features_np[val_edges[i][1]]), axis=0)\n",
    "    \n",
    "print('X_train and X_val created in {} s!'.format(round(time()-t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44a3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6fa791",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y)\n",
    "y_pred = clf.predict_proba(X_val)\n",
    "y_pred = y_pred[:,1]\n",
    "print('Logistic regression performed in {} s!'.format(round(time()-t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c60b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Log loss:', log_loss(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b1ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac02a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Read test data. Each sample is a pair of nodes\n",
    "test_edges = list()\n",
    "with open('../test_data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        test_edges.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "X_test = np.zeros((len(test_edges), 2*features_np.shape[1]))\n",
    "for i, edge in enumerate(test_edges):\n",
    "    X_test[i] = np.concatenate((features_np[test_edges[i][0]], features_np[test_edges[i][1]]), axis=0)\n",
    "\n",
    "t = time()\n",
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "y_pred = y_pred[:,1]\n",
    "print('Logistic regression performed in {} s!'.format(round(time()-t)))\n",
    "\n",
    "\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 100000)\n",
    "\n",
    "pd.DataFrame(y_pred, columns={'predicted'}).to_csv(\n",
    "\"../submissions_files/{}-submission-{}.csv\".format(today, random_nb), header=True, index=True, index_label='id'\n",
    ")\n",
    "    \n",
    "    \n",
    "# # Testing\n",
    "# node_pairs = np.array(np.transpose(node_pairs))\n",
    "# pairs = torch.LongTensor(node_pairs).to(device)\n",
    "# output = model(features, adj, pairs)\n",
    "# y_pred = torch.exp(output)\n",
    "# y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "# y_pred_true = list()\n",
    "# for element in y_pred:\n",
    "#     y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "# today = datetime.today().strftime('%Y-%m-%d')\n",
    "# random_nb = randint(0, 100000)\n",
    "\n",
    "# pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "# \"../submissions_files/{}-submission-{}.csv\".format(today, random_nb), header=True, index=True, index_label='id'\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
