{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787a2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import random_walk, generate_walks, read_train_val_graph, sparse_mx_to_torch_sparse_tensor\n",
    "from utils import apply_word2vec_on_features, create_and_normalize_adjacency, save_subgraph_in_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5d30a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d25112fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "057c89f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 nodes, 2405 edges Graph extracted from edgelist.txt\n",
      "999\n",
      "980 nodes, 2405 edges Graph saved in small_edgelist.txt\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "save_subgraph_in_file(nbr_nodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "635cf81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "def read_train_val_graph(path='../input_data/edgelist.txt', val_ratio=0.1):\n",
    "    G = nx.read_edgelist(path, delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "    nodes = list(G.nodes())\n",
    "    n = G.number_of_nodes()\n",
    "    m = G.number_of_edges()\n",
    "    edges = list(G.edges())\n",
    "\n",
    "    print('Number of nodes:', n, 'number of edges:', m,'in All the set')\n",
    "\n",
    "    node_to_idx = dict()\n",
    "    for i, node in enumerate(nodes):\n",
    "        node_to_idx[node] = i\n",
    "\n",
    "    val_edges = list()\n",
    "    G_train = G.copy()\n",
    "\n",
    "    for edge in edges:\n",
    "        if random() < val_ratio and edge[0] < n and edge[1] < n:\n",
    "            val_edges.append(edge)\n",
    "            G_train.remove_edge(edge[0], edge[1]) # We remove the val edges from the graph G\n",
    "\n",
    "   \n",
    "    #for edge in val_edges:\n",
    "        \n",
    "\n",
    "    n = G_train.number_of_nodes()\n",
    "    m = G_train.number_of_edges()\n",
    "    train_edges = list(G_train.edges())\n",
    "\n",
    "    print('Number of nodes:', n, 'number of edges:', m, 'in the Training set')\n",
    "    print('len(nodes)', len(nodes))\n",
    "\n",
    "    y_val = [1]*len(val_edges)\n",
    "\n",
    "    n_val_edges = len(val_edges)\n",
    "    \n",
    "    print('Creating random val_edges...')\n",
    "    for i in range(n_val_edges):\n",
    "        n1 = nodes[randint(0, n-1)]\n",
    "        n2 = nodes[randint(0, n-1)]\n",
    "        (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "        while n2 > n: #or (n1, n2) in train_edges:\n",
    "            if (n1, n2) in train_edges:\n",
    "                print((n1, n2), 'in train_edges:')\n",
    "            else: print((n1, n2))\n",
    "            n1 = nodes[randint(0, n-1)]\n",
    "            n2 = nodes[randint(0, n-1)]\n",
    "            (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "        val_edges.append((n1, n2))\n",
    "    \n",
    "#     print('Cleaning random val_edges...')\n",
    "#     l = len(intersection(val_edges,train_edges))\n",
    "#     print('len(intersection(val_edges, train_edges))', len(intersection(val_edges, train_edges)))\n",
    "#     for i, val_edge in enumerate(intersection(val_edges, train_edges)):\n",
    "#         print(i+1, '/', l)\n",
    "#         n1 = nodes[randint(0, n-1)]\n",
    "#         n2 = nodes[randint(0, n-1)]\n",
    "#         (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "#         while (n1, n2) in train_edges or (n1, n2) in val_edges or n2 > n:\n",
    "#             n1 = nodes[randint(0, n-1)]\n",
    "#             n2 = nodes[randint(0, n-1)]\n",
    "#             (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "#         val_edges = [(n1, n2) if item == val_edge else item for item in val_edges]\n",
    "\n",
    "# another way to clean::\n",
    "# for edge in list(set(val_edges) & set(train_edges)):\n",
    "#     val_edges.remove(edge)\n",
    "\n",
    "\n",
    "    y_val.extend([0]*(n_val_edges))\n",
    "    \n",
    "    print('Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects')\n",
    "    print('Loaded from', path[path.rfind('/')+1:], 'and with a training validation split ratio =', val_ratio)\n",
    "    \n",
    "    return G, G_train, train_edges, val_edges, y_val, nodes, node_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52e43ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "18b1bc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499 number of edges: 1091955 in All the set\n",
      "Number of nodes: 138499 number of edges: 983250 in the Training set\n",
      "len(nodes) 138499\n",
      "Creating random val_edges...\n",
      "Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects\n",
      "Loaded from edgelist.txt and with a training validation split ratio = 0.1\n"
     ]
    }
   ],
   "source": [
    "path = '../input_data/edgelist.txt'\n",
    "\n",
    "G, G_train, train_edges, val_edges, y_val, nodes, node_to_idx = read_train_val_graph(val_ratio=0.1, path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0934e605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1091955, 983250)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_edges(), G_train.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a337489b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108725\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for val_edge in val_edges:\n",
    "    if val_edge in G.edges():\n",
    "        counter += 1\n",
    "print(counter)\n",
    "\n",
    "counter = 0\n",
    "for val_edge in val_edges:\n",
    "    if val_edge in G_train.edges():\n",
    "        counter += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e894f370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start generating walks....\n",
      "Random walks generated in in 64s!\n",
      "Start applying Word2Vec...\n",
      "Word2vec model trained on features in 3 min!\n",
      "(138499, 128) features numpy array created in 3 min!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghassenabdedayem/Documents/Data/Polytechnique/5- Data Challenge/data_challenge_2022/draft_nb/utils.py:147: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix of the training graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a normalized adjancency matrix of shape (138499, 138499)\n",
      "Created indices (2, 2104999) with the positions of non zeros in adj matrix\n"
     ]
    }
   ],
   "source": [
    "\n",
    "walks = generate_walks(G=G_train, num_walks=10, walk_length=15)\n",
    "features_np = apply_word2vec_on_features(features=walks, nodes=nodes)\n",
    "adj, indices = create_and_normalize_adjacency(G_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a369dbec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "74ace461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Graph.number_of_edges of <networkx.classes.graph.Graph object at 0x7f91b8167fd0>>\n",
      "[(2, 0), (2, 25), (2, 26), (2, 27), (2, 28), (2, 29), (2, 30), (2, 32), (2, 34), (2, 35), (2, 36), (2, 37), (2, 38), (2, 40), (2, 41), (2, 42), (2, 43), (2, 44), (2, 46), (2, 48), (2, 49), (2, 50), (2, 51), (2, 53), (2, 54), (2, 55), (2, 56), (2, 57), (2, 58), (2, 59), (2, 60), (2, 62), (2, 63)]\n",
      "[(2, 0), (2, 25), (2, 26), (2, 27), (2, 28), (2, 29), (2, 30), (2, 31), (2, 32), (2, 33), (2, 34), (2, 35), (2, 36), (2, 37), (2, 38), (2, 39), (2, 40), (2, 41), (2, 42), (2, 43), (2, 44), (2, 45), (2, 46), (2, 48), (2, 49), (2, 50), (2, 51), (2, 52), (2, 53), (2, 54), (2, 55), (2, 56), (2, 57), (2, 58), (2, 59), (2, 60), (2, 61), (2, 62), (2, 63)]\n",
      "False\n",
      "True\n",
      "(2, 31)\n",
      "(2, 33)\n",
      "(2, 39)\n",
      "(2, 45)\n",
      "(2, 52)\n",
      "(2, 61)\n"
     ]
    }
   ],
   "source": [
    "G = nx.read_edgelist('../input_data/edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "print(G.number_of_edges)\n",
    "print(G_train.edges(2))\n",
    "print(G.edges(2))\n",
    "print(element in G_train.edges(2))\n",
    "print(element in G.edges(2))\n",
    "for element in G.edges(2):\n",
    "    if element not in G_train.edges(2):\n",
    "        print(element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e8e137bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 10),\n",
       " (2, 31),\n",
       " (2, 33),\n",
       " (2, 39),\n",
       " (2, 45),\n",
       " (2, 52),\n",
       " (2, 61),\n",
       " (5, 71762),\n",
       " (7, 55111),\n",
       " (7, 55118),\n",
       " (7, 55335),\n",
       " (10, 111357),\n",
       " (10, 105975),\n",
       " (13, 95564),\n",
       " (15, 121201),\n",
       " (15, 120800),\n",
       " (15, 118059),\n",
       " (16, 54943),\n",
       " (16, 101847),\n",
       " (16, 101814),\n",
       " (16, 81307),\n",
       " (16, 101855),\n",
       " (17, 103529),\n",
       " (17, 105330),\n",
       " (17, 105026),\n",
       " (19, 101822),\n",
       " (20, 111127),\n",
       " (20, 101829),\n",
       " (20, 86130),\n",
       " (20, 123441),\n",
       " (22, 7287),\n",
       " (22, 44193),\n",
       " (22, 64198),\n",
       " (22, 49125),\n",
       " (23, 120871),\n",
       " (23, 107491),\n",
       " (25, 58),\n",
       " (26, 49831),\n",
       " (31, 721),\n",
       " (32, 407),\n",
       " (37, 96455),\n",
       " (38, 729),\n",
       " (39, 43),\n",
       " (39, 115087),\n",
       " (40, 407),\n",
       " (40, 59304),\n",
       " (42, 127462),\n",
       " (43, 429),\n",
       " (43, 58),\n",
       " (44, 407),\n",
       " (48, 107414),\n",
       " (48, 14104),\n",
       " (52, 113584),\n",
       " (53, 96317),\n",
       " (53, 58),\n",
       " (54, 112488),\n",
       " (56, 105474),\n",
       " (62, 698),\n",
       " (48051, 16970),\n",
       " (48051, 16999),\n",
       " (48051, 28054),\n",
       " (48051, 38936),\n",
       " (48051, 39016),\n",
       " (48051, 107793),\n",
       " (48051, 66342),\n",
       " (48051, 54140),\n",
       " (101494, 101487),\n",
       " (111705, 124645),\n",
       " (107826, 119178),\n",
       " (111339, 123649),\n",
       " (110133, 92549),\n",
       " (110133, 128707),\n",
       " (4, 124266),\n",
       " (35522, 3493),\n",
       " (35522, 4874),\n",
       " (35522, 7208),\n",
       " (35522, 15299),\n",
       " (35522, 24201),\n",
       " (35522, 45933),\n",
       " (55135, 4611),\n",
       " (55135, 36758),\n",
       " (55135, 42232),\n",
       " (55135, 54916),\n",
       " (55135, 54936),\n",
       " (55135, 54938),\n",
       " (55135, 54961),\n",
       " (55135, 54972),\n",
       " (55135, 54997),\n",
       " (55135, 55001),\n",
       " (55135, 55027),\n",
       " (55135, 55067),\n",
       " (55135, 55068),\n",
       " (55135, 55084),\n",
       " (55135, 55100),\n",
       " (55135, 55111),\n",
       " (55135, 101822),\n",
       " (55135, 105983),\n",
       " (55135, 55168),\n",
       " (55135, 55192),\n",
       " (55135, 89341),\n",
       " (55135, 55251),\n",
       " (55135, 55279),\n",
       " (55135, 55282),\n",
       " (55135, 55288),\n",
       " (55135, 55291),\n",
       " (55135, 55300),\n",
       " (55135, 55332),\n",
       " (55135, 55333),\n",
       " (55135, 89318),\n",
       " (42285, 18263),\n",
       " (42285, 111846),\n",
       " (100292, 54998),\n",
       " (100292, 89339),\n",
       " (100292, 105999),\n",
       " (100292, 107026),\n",
       " (100292, 113194),\n",
       " (105999, 57128),\n",
       " (105999, 57366),\n",
       " (105999, 77272),\n",
       " (105999, 123987),\n",
       " (105999, 123989),\n",
       " (105999, 123992),\n",
       " (105999, 123267),\n",
       " (111287, 62953),\n",
       " (111212, 106707),\n",
       " (101838, 95337),\n",
       " (5202, 4051),\n",
       " (5202, 44189),\n",
       " (5202, 100124),\n",
       " (5202, 37678),\n",
       " (5202, 109856),\n",
       " (110796, 101826),\n",
       " (110796, 106707),\n",
       " (110796, 115169),\n",
       " (110796, 126948),\n",
       " (110796, 111547),\n",
       " (42299, 17191),\n",
       " (42299, 18221),\n",
       " (42299, 26196),\n",
       " (42299, 100124),\n",
       " (42299, 100125),\n",
       " (124266, 72619),\n",
       " (77474, 54453),\n",
       " (77474, 60787),\n",
       " (34604, 4672),\n",
       " (34604, 7207),\n",
       " (34604, 22818),\n",
       " (34604, 34180),\n",
       " (34604, 34593),\n",
       " (34604, 34602),\n",
       " (34604, 44641),\n",
       " (34604, 43782),\n",
       " (34604, 58314),\n",
       " (34604, 58322),\n",
       " (34604, 47506),\n",
       " (34604, 58335),\n",
       " (34604, 58352),\n",
       " (34604, 58353),\n",
       " (34604, 46421),\n",
       " (34604, 55826),\n",
       " (34604, 37993),\n",
       " (34604, 58372),\n",
       " (34604, 58378),\n",
       " (34604, 58382),\n",
       " (34604, 47635),\n",
       " (34604, 36828),\n",
       " (34604, 44796),\n",
       " (34604, 36840),\n",
       " (34604, 58442),\n",
       " (34604, 55845),\n",
       " (34604, 39636),\n",
       " (34604, 58461),\n",
       " (34604, 58474),\n",
       " (17314, 1969),\n",
       " (17314, 4240),\n",
       " (17314, 9549),\n",
       " (17314, 11245),\n",
       " (17314, 11782),\n",
       " (17314, 13977),\n",
       " (17314, 14024),\n",
       " (17314, 14204),\n",
       " (17314, 14433),\n",
       " (17314, 15232),\n",
       " (17314, 15347),\n",
       " (17314, 17225),\n",
       " (17314, 17255),\n",
       " (17314, 17297),\n",
       " (17314, 42092),\n",
       " (17314, 32564),\n",
       " (17314, 54151),\n",
       " (17314, 32539),\n",
       " (17314, 26690),\n",
       " (17314, 17546),\n",
       " (17314, 33514),\n",
       " (17314, 31343),\n",
       " (17314, 39163),\n",
       " (17314, 62902),\n",
       " (17314, 32547),\n",
       " (17314, 62908),\n",
       " (17314, 62909),\n",
       " (17314, 31730),\n",
       " (17314, 46417),\n",
       " (17314, 32550),\n",
       " (17314, 58350),\n",
       " (17314, 59176),\n",
       " (17314, 57864),\n",
       " (17314, 62915),\n",
       " (17314, 32528),\n",
       " (17314, 55954),\n",
       " (17314, 31821),\n",
       " (17314, 38900),\n",
       " (17314, 29150),\n",
       " (17314, 32569),\n",
       " (17314, 62767),\n",
       " (17314, 26860),\n",
       " (17314, 22937),\n",
       " (17314, 31589),\n",
       " (17314, 62661),\n",
       " (17314, 62944),\n",
       " (47062, 2576),\n",
       " (47062, 14095),\n",
       " (47062, 36000),\n",
       " (47062, 36730),\n",
       " (47062, 36785),\n",
       " (47062, 36920),\n",
       " (47062, 37573),\n",
       " (47062, 39567),\n",
       " (47062, 45678),\n",
       " (47062, 45692),\n",
       " (47062, 45862),\n",
       " (47062, 45980),\n",
       " (47062, 46023),\n",
       " (47062, 46028),\n",
       " (47062, 46097),\n",
       " (47062, 46275),\n",
       " (47062, 50556),\n",
       " (47062, 60789),\n",
       " (47062, 58479),\n",
       " (47062, 62969),\n",
       " (47062, 58357),\n",
       " (47062, 62634),\n",
       " (47062, 62760),\n",
       " (47062, 62978),\n",
       " (47062, 62638),\n",
       " (47062, 62729),\n",
       " (47062, 63002),\n",
       " (17446, 58294),\n",
       " (17446, 36712),\n",
       " (17446, 72612),\n",
       " (17446, 72615),\n",
       " (17446, 29075),\n",
       " (17446, 56827),\n",
       " (17446, 72617),\n",
       " (17446, 19238),\n",
       " (17446, 61022),\n",
       " (17446, 52804),\n",
       " (17446, 72626),\n",
       " (17446, 55937),\n",
       " (17446, 72628),\n",
       " (17446, 45684),\n",
       " (17446, 28971),\n",
       " (17446, 29119),\n",
       " (17446, 51340),\n",
       " (17446, 71938),\n",
       " (17446, 32558),\n",
       " (17446, 59894),\n",
       " (17446, 72647),\n",
       " (71762, 58351),\n",
       " (71762, 62945),\n",
       " (71762, 92516),\n",
       " (71762, 79499),\n",
       " (71762, 72619),\n",
       " (71762, 73098),\n",
       " (60676, 45787),\n",
       " (60676, 79229),\n",
       " (60676, 62975),\n",
       " (60676, 68332),\n",
       " (60676, 60764),\n",
       " (48338, 38043),\n",
       " (48338, 44775),\n",
       " (48338, 56813),\n",
       " (48338, 102394),\n",
       " (55141, 55128),\n",
       " (55141, 89340),\n",
       " (55141, 55213),\n",
       " (58351, 58361),\n",
       " (58351, 58359),\n",
       " (58361, 104828),\n",
       " (46368, 1205),\n",
       " (46368, 22809),\n",
       " (46368, 39658),\n",
       " (68395, 100282),\n",
       " (68327, 68504),\n",
       " (90583, 60683),\n",
       " (68398, 124155),\n",
       " (52644, 44091),\n",
       " (52644, 124155),\n",
       " (89340, 55276),\n",
       " (89340, 55314),\n",
       " (89340, 103370),\n",
       " (107028, 107527),\n",
       " (45970, 38901),\n",
       " (121977, 107280),\n",
       " (111432, 122828),\n",
       " (121978, 111318),\n",
       " (121978, 111320),\n",
       " (101856, 54989),\n",
       " (107026, 32035),\n",
       " (107026, 106706),\n",
       " (54897, 5453),\n",
       " (54897, 5461),\n",
       " (54897, 6005),\n",
       " (54897, 16967),\n",
       " (54897, 18142),\n",
       " (54897, 29043),\n",
       " (54897, 35928),\n",
       " (54897, 36507),\n",
       " (54897, 37280),\n",
       " (54897, 37313),\n",
       " (54897, 37386),\n",
       " (54897, 37426),\n",
       " (54897, 38989),\n",
       " (54897, 49794),\n",
       " (54897, 51199),\n",
       " (54897, 54910),\n",
       " (54897, 54911),\n",
       " (54897, 54913),\n",
       " (54897, 54921),\n",
       " (54897, 54950),\n",
       " (54897, 54952),\n",
       " (54897, 54966),\n",
       " (54897, 54968),\n",
       " (54897, 54971),\n",
       " (54897, 54986),\n",
       " (54897, 54992),\n",
       " (54897, 54994),\n",
       " (54897, 55010),\n",
       " (54897, 55017),\n",
       " (54897, 55023),\n",
       " (54897, 55025),\n",
       " (54897, 55026),\n",
       " (54897, 55035),\n",
       " (54897, 55042),\n",
       " (54897, 55046),\n",
       " (54897, 55056),\n",
       " (54897, 55067),\n",
       " (54897, 55078),\n",
       " (54897, 55085),\n",
       " (54897, 55089),\n",
       " (54897, 55093),\n",
       " (54897, 55099),\n",
       " (54897, 55109),\n",
       " (54897, 55124),\n",
       " (54897, 55126),\n",
       " (54897, 55140),\n",
       " (54897, 55145),\n",
       " (54897, 55147),\n",
       " (54897, 55152),\n",
       " (54897, 55156),\n",
       " (54897, 55159),\n",
       " (54897, 55195),\n",
       " (54897, 55197),\n",
       " (54897, 55200),\n",
       " (54897, 55203),\n",
       " (54897, 55209),\n",
       " (54897, 55225),\n",
       " (54897, 55231),\n",
       " (54897, 55232),\n",
       " (54897, 55238),\n",
       " (54897, 55241),\n",
       " (54897, 55247),\n",
       " (54897, 55265),\n",
       " (54897, 55268),\n",
       " (54897, 55278),\n",
       " (54897, 55292),\n",
       " (54897, 55303),\n",
       " (54897, 55304),\n",
       " (54897, 55323),\n",
       " (55343, 28993),\n",
       " (55343, 32466),\n",
       " (55343, 36763),\n",
       " (55343, 36775),\n",
       " (55343, 54945),\n",
       " (55343, 54978),\n",
       " (55343, 55009),\n",
       " (55343, 55015),\n",
       " (55343, 55024),\n",
       " (55343, 55039),\n",
       " (55343, 55048),\n",
       " (55343, 55054),\n",
       " (55343, 55057),\n",
       " (55343, 55120),\n",
       " (55343, 55143),\n",
       " (55343, 55151),\n",
       " (55343, 55170),\n",
       " (55343, 55193),\n",
       " (55343, 55286),\n",
       " (55343, 105974),\n",
       " (55343, 101822),\n",
       " (55343, 68355),\n",
       " (55343, 68020),\n",
       " (54959, 4611),\n",
       " (54959, 16966),\n",
       " (54959, 19975),\n",
       " (54959, 36501),\n",
       " (54959, 36763),\n",
       " (54959, 46797),\n",
       " (54959, 54899),\n",
       " (54959, 54978),\n",
       " (54959, 54983),\n",
       " (54959, 54987),\n",
       " (54959, 55039),\n",
       " (54959, 55123),\n",
       " (54959, 55143),\n",
       " (54959, 68020),\n",
       " (54959, 55347),\n",
       " (55059, 28993),\n",
       " (55059, 37360),\n",
       " (55059, 54956),\n",
       " (55059, 55340),\n",
       " (55059, 55178),\n",
       " (54902, 46484),\n",
       " (54902, 55139),\n",
       " (54978, 11963),\n",
       " (54978, 36907),\n",
       " (54978, 54942),\n",
       " (54978, 55091),\n",
       " (54978, 55154),\n",
       " (54909, 55140),\n",
       " (54909, 55025),\n",
       " (54909, 55111),\n",
       " (54944, 55023),\n",
       " (54948, 55040),\n",
       " (54948, 55181),\n",
       " (54963, 55140),\n",
       " (55111, 11963),\n",
       " (55111, 15473),\n",
       " (55111, 35918),\n",
       " (55111, 46008),\n",
       " (55111, 54970),\n",
       " (55111, 55001),\n",
       " (55111, 55012),\n",
       " (55111, 55030),\n",
       " (55111, 55077),\n",
       " (55111, 55089),\n",
       " (55111, 55140),\n",
       " (55111, 55154),\n",
       " (55111, 55177),\n",
       " (55111, 55222),\n",
       " (55117, 54924),\n",
       " (55117, 55094),\n",
       " (55117, 74630),\n",
       " (55117, 55221),\n",
       " (55118, 55021),\n",
       " (55142, 39614),\n",
       " (55142, 54912),\n",
       " (55142, 54992),\n",
       " (55142, 55091),\n",
       " (55142, 55194),\n",
       " (55142, 55305),\n",
       " (55119, 55044),\n",
       " (55119, 67484),\n",
       " (55119, 113192),\n",
       " (55119, 113194),\n",
       " (55119, 89344),\n",
       " (8, 54989),\n",
       " (55104, 98648),\n",
       " (55167, 55025),\n",
       " (55167, 105981),\n",
       " (54987, 5226),\n",
       " (54987, 54931),\n",
       " (54987, 105974),\n",
       " (54987, 55032),\n",
       " (28993, 55056),\n",
       " (107092, 107149),\n",
       " (107153, 88314),\n",
       " (107153, 104891),\n",
       " (107153, 120865),\n",
       " (104741, 104699),\n",
       " (104741, 110635),\n",
       " (108449, 103738),\n",
       " (108449, 103745),\n",
       " (108449, 104715),\n",
       " (18, 123761),\n",
       " (101850, 124268),\n",
       " (101850, 120415),\n",
       " (101850, 112088),\n",
       " (107079, 106549),\n",
       " (110705, 49724),\n",
       " (110705, 124274),\n",
       " (103570, 99304),\n",
       " (103570, 101822),\n",
       " (103570, 103481),\n",
       " (111357, 37600),\n",
       " (111357, 86319),\n",
       " (111357, 110873),\n",
       " (111357, 121202),\n",
       " (106417, 84711),\n",
       " (107576, 107549),\n",
       " (109597, 95564),\n",
       " (109597, 99737),\n",
       " (109597, 105991),\n",
       " (109597, 125078),\n",
       " (123244, 112313),\n",
       " (123244, 129948),\n",
       " (123613, 102741),\n",
       " (93425, 127256),\n",
       " (93425, 101847),\n",
       " (84711, 88654),\n",
       " (101863, 89318),\n",
       " (101863, 101822),\n",
       " (101992, 55089),\n",
       " (101992, 102463),\n",
       " (118029, 101813),\n",
       " (114980, 98817),\n",
       " (7277, 80125),\n",
       " (124274, 128130),\n",
       " (58381, 55130),\n",
       " (45582, 38901),\n",
       " (109991, 60700),\n",
       " (109991, 80959),\n",
       " (109991, 104743),\n",
       " (109991, 109978),\n",
       " (95564, 123083),\n",
       " (104759, 108468),\n",
       " (125200, 64723),\n",
       " (106549, 101851),\n",
       " (106549, 106706),\n",
       " (106549, 107082),\n",
       " (115508, 115514),\n",
       " (118052, 55938),\n",
       " (97534, 111482),\n",
       " (86675, 54975),\n",
       " (119338, 61406),\n",
       " (115510, 103539),\n",
       " (110874, 86696),\n",
       " (110874, 108298),\n",
       " (89333, 123652),\n",
       " (89333, 124493),\n",
       " (110873, 16935),\n",
       " (110873, 44336),\n",
       " (110873, 92036),\n",
       " (110873, 105978),\n",
       " (56849, 31812),\n",
       " (56849, 51117),\n",
       " (56849, 86317),\n",
       " (120800, 37600),\n",
       " (97543, 97529),\n",
       " (97543, 104510),\n",
       " (97721, 94298),\n",
       " (66201, 35987),\n",
       " (66201, 68680),\n",
       " (121203, 111164),\n",
       " (71105, 4658),\n",
       " (109065, 55298),\n",
       " (113194, 44661),\n",
       " (56378, 41689),\n",
       " (56378, 110509),\n",
       " (56378, 65906),\n",
       " (97722, 32876),\n",
       " (74218, 35986),\n",
       " (55140, 5224),\n",
       " (55140, 22873),\n",
       " (55140, 29043),\n",
       " (55140, 37382),\n",
       " (55140, 41775),\n",
       " (55140, 54914),\n",
       " (55140, 54920),\n",
       " (55140, 54990),\n",
       " (55140, 54996),\n",
       " (55140, 55026),\n",
       " (55140, 55056),\n",
       " (55140, 60691),\n",
       " (55140, 56019),\n",
       " (55140, 86303),\n",
       " (55140, 55146),\n",
       " (55140, 55219),\n",
       " (55140, 55305),\n",
       " (55050, 55942),\n",
       " (55050, 101807),\n",
       " (55050, 56228),\n",
       " (101797, 101813),\n",
       " (101797, 101814),\n",
       " (89326, 107255),\n",
       " (89326, 107257),\n",
       " (101821, 109810),\n",
       " (54943, 42458),\n",
       " (54943, 125498),\n",
       " (54943, 114064),\n",
       " (101822, 70683),\n",
       " (101822, 88329),\n",
       " (101822, 89014),\n",
       " (101822, 102936),\n",
       " (54955, 36946),\n",
       " (54968, 96558),\n",
       " (101826, 105990),\n",
       " (101826, 111437),\n",
       " (101826, 111318),\n",
       " (101830, 126532),\n",
       " (101830, 101834),\n",
       " (54996, 55014),\n",
       " (55017, 89332),\n",
       " (101831, 101866),\n",
       " (101832, 68208),\n",
       " (101832, 89322),\n",
       " (55042, 107487),\n",
       " (101833, 123518),\n",
       " (97537, 97525),\n",
       " (89329, 86119),\n",
       " (89329, 93147),\n",
       " (89329, 100645),\n",
       " (55064, 106279),\n",
       " (68326, 36406),\n",
       " (68326, 44802),\n",
       " (68326, 60707),\n",
       " (68326, 78355),\n",
       " (101841, 101819),\n",
       " (101841, 106527),\n",
       " (101842, 101700),\n",
       " (101842, 124048),\n",
       " (89332, 54967),\n",
       " (89332, 105998),\n",
       " (89332, 101814),\n",
       " (101844, 101866),\n",
       " (60755, 101828),\n",
       " (60755, 120874),\n",
       " (101846, 111707),\n",
       " (101846, 107261),\n",
       " (101847, 97173),\n",
       " (101847, 126456),\n",
       " (101851, 107277),\n",
       " (101852, 96360),\n",
       " (101852, 105435),\n",
       " (101808, 123658),\n",
       " (101853, 128430),\n",
       " (81307, 60677),\n",
       " (89339, 54958),\n",
       " (89339, 88120),\n",
       " (89339, 124244),\n",
       " (100300, 100298),\n",
       " (100300, 120872),\n",
       " (100300, 107482),\n",
       " (100300, 108803),\n",
       " (101857, 103546),\n",
       " (101858, 109324),\n",
       " (101859, 55161),\n",
       " (101859, 104705),\n",
       " (101859, 111160),\n",
       " (101859, 106065),\n",
       " (101859, 103057),\n",
       " (101859, 106710),\n",
       " (56376, 41689),\n",
       " (101860, 107029),\n",
       " (101860, 121263),\n",
       " (101860, 107534),\n",
       " (101861, 131723),\n",
       " (101866, 129149),\n",
       " (103481, 103569),\n",
       " (103527, 103319),\n",
       " (103529, 111544),\n",
       " (105330, 105327),\n",
       " (105330, 105332),\n",
       " (105330, 108782),\n",
       " (105330, 120501),\n",
       " (103353, 103306),\n",
       " (103353, 103312),\n",
       " (103353, 108194),\n",
       " (103353, 120514),\n",
       " (103353, 111601),\n",
       " (105338, 102917),\n",
       " (105338, 103350),\n",
       " (105338, 103357),\n",
       " (105338, 103378),\n",
       " (105338, 103550),\n",
       " (105338, 103578),\n",
       " (103495, 123246),\n",
       " (105026, 103317),\n",
       " (105026, 105312),\n",
       " (105026, 108759),\n",
       " (103572, 103055),\n",
       " (103572, 103352),\n",
       " (103572, 106266),\n",
       " (105024, 102739),\n",
       " (105024, 105327),\n",
       " (103345, 103320),\n",
       " (103345, 128806),\n",
       " (106305, 109229),\n",
       " (106305, 110127),\n",
       " (110634, 108399),\n",
       " (103519, 103578),\n",
       " (106266, 103371),\n",
       " (109225, 103515),\n",
       " (108766, 104908),\n",
       " (108766, 105312),\n",
       " (108766, 106302),\n",
       " (109229, 103346),\n",
       " (109229, 109623),\n",
       " (106320, 102907),\n",
       " (118368, 83862),\n",
       " (118368, 107700),\n",
       " (6174, 49900),\n",
       " (108229, 123143),\n",
       " (107273, 99776),\n",
       " (107273, 101573),\n",
       " (107483, 123162),\n",
       " (109595, 103722),\n",
       " (60656, 60683),\n",
       " (36949, 4151),\n",
       " (36949, 4167),\n",
       " (36949, 5153),\n",
       " (36949, 6068),\n",
       " (36949, 7095),\n",
       " (36949, 10877),\n",
       " (36949, 11265),\n",
       " (36949, 12265),\n",
       " (36949, 14107),\n",
       " (36949, 14522),\n",
       " (36949, 17392),\n",
       " (36949, 22722),\n",
       " (36949, 29133),\n",
       " (36949, 36704),\n",
       " (36949, 36802),\n",
       " (36949, 36816),\n",
       " (36949, 36860),\n",
       " (36949, 86080),\n",
       " (36949, 64665),\n",
       " (36949, 86088),\n",
       " (36949, 79385),\n",
       " (36949, 86104),\n",
       " (36949, 86112),\n",
       " (36949, 54446),\n",
       " (36949, 68399),\n",
       " (36949, 73631),\n",
       " (36949, 86126),\n",
       " (36949, 45695),\n",
       " (36949, 66944),\n",
       " (36949, 62992),\n",
       " (36949, 36967),\n",
       " (36949, 60782),\n",
       " (36949, 44216),\n",
       " (36949, 78572),\n",
       " (36949, 37000),\n",
       " (62729, 45981),\n",
       " (62729, 62989),\n",
       " (44796, 41840),\n",
       " (44796, 54921),\n",
       " (111420, 60687),\n",
       " (111420, 108298),\n",
       " (111420, 124663),\n",
       " (110101, 26455),\n",
       " (111421, 102936),\n",
       " (111421, 124711),\n",
       " (22724, 11856),\n",
       " (22724, 12126),\n",
       " (22724, 72621),\n",
       " (22724, 62641),\n",
       " (22724, 62635),\n",
       " (22724, 26841),\n",
       " (22724, 128902),\n",
       " (22724, 86692),\n",
       " (58359, 95758),\n",
       " (73127, 26648),\n",
       " (86130, 121908),\n",
       " (89014, 2336),\n",
       " (89014, 68342),\n",
       " (99008, 35907),\n",
       " (99008, 129901),\n",
       " (46988, 17437),\n",
       " (46988, 39553),\n",
       " (46988, 66929),\n",
       " (107477, 107533),\n",
       " (97304, 55098),\n",
       " (97304, 114393),\n",
       " (92070, 88134),\n",
       " (37600, 73068),\n",
       " (79536, 130817),\n",
       " (92381, 133037),\n",
       " (7287, 4943),\n",
       " (7287, 5480),\n",
       " (7287, 44121),\n",
       " (7287, 10771),\n",
       " (7287, 31316),\n",
       " (7287, 44130),\n",
       " (7287, 44136),\n",
       " (7287, 44150),\n",
       " (7287, 44152),\n",
       " (7287, 44081),\n",
       " (7287, 44084),\n",
       " (7287, 44159),\n",
       " (7287, 44161),\n",
       " (7287, 44163),\n",
       " (7287, 18186),\n",
       " (7287, 16223),\n",
       " (7287, 44178),\n",
       " (7287, 44096),\n",
       " (7287, 18243),\n",
       " (38701, 4428),\n",
       " (38701, 4890),\n",
       " (38701, 5151),\n",
       " (38701, 5194),\n",
       " (38701, 6666),\n",
       " (38701, 13122),\n",
       " (38701, 18108),\n",
       " (38701, 18121),\n",
       " (38701, 20061),\n",
       " (38701, 25039),\n",
       " (38701, 44058),\n",
       " (38701, 42813),\n",
       " (38701, 44581),\n",
       " (38701, 64310),\n",
       " (38701, 79403),\n",
       " (38701, 44173),\n",
       " (38701, 68040),\n",
       " (38701, 44250),\n",
       " (38701, 83406),\n",
       " (38701, 44361),\n",
       " (38701, 44105),\n",
       " (38701, 42405),\n",
       " (44178, 15880),\n",
       " (44178, 19708),\n",
       " (44178, 42346),\n",
       " (44178, 44172),\n",
       " (44178, 94985),\n",
       " (44193, 14195),\n",
       " (44193, 18092),\n",
       " (44193, 19004),\n",
       " (44193, 19578),\n",
       " (44193, 44101),\n",
       " (44193, 44122),\n",
       " (44193, 44171),\n",
       " (44193, 61538),\n",
       " (44193, 62463),\n",
       " (44193, 94983),\n",
       " (19546, 5537),\n",
       " (19546, 17437),\n",
       " (19546, 95998),\n",
       " (19546, 44308),\n",
       " (67042, 14551),\n",
       " (67042, 18088),\n",
       " (67042, 67019),\n",
       " (67042, 97360),\n",
       " (67042, 97366),\n",
       " (44172, 7283),\n",
       " (44172, 31654),\n",
       " (44172, 32625),\n",
       " (44172, 42333),\n",
       " (44172, 44050),\n",
       " (44172, 44060),\n",
       " (44172, 44086),\n",
       " (44172, 44274),\n",
       " (44172, 64183),\n",
       " (44172, 95008),\n",
       " (44172, 44176),\n",
       " (44172, 98120),\n",
       " (61572, 36826),\n",
       " (61572, 64178),\n",
       " (44182, 86604),\n",
       " (44182, 44217),\n",
       " (31654, 18181),\n",
       " (31654, 28328),\n",
       " (31654, 55364),\n",
       " (31654, 96612),\n",
       " (96612, 5060),\n",
       " (96612, 18129),\n",
       " (96612, 18304),\n",
       " (96612, 22431),\n",
       " (96612, 31325),\n",
       " (96612, 44122),\n",
       " (96612, 67134),\n",
       " (96612, 79403),\n",
       " (96612, 114596),\n",
       " (36826, 36804),\n",
       " (36826, 55925),\n",
       " (36826, 56856),\n",
       " (36826, 119265),\n",
       " (64198, 39164),\n",
       " (64198, 120191),\n",
       " (47711, 6841),\n",
       " (47711, 12060),\n",
       " (47711, 14195),\n",
       " (47711, 32504),\n",
       " (47711, 39165),\n",
       " (47711, 44306),\n",
       " (47711, 46436),\n",
       " (47711, 62452),\n",
       " (6841, 4636),\n",
       " (6841, 7288),\n",
       " (6841, 47260),\n",
       " (6841, 78287),\n",
       " (5091, 5190),\n",
       " (64183, 7255),\n",
       " (64188, 44140),\n",
       " (15198, 15188),\n",
       " (15198, 74015),\n",
       " (15198, 15222),\n",
       " (19225, 44104),\n",
       " (72629, 26441),\n",
       " (72629, 80960),\n",
       " (72629, 94983),\n",
       " (49125, 16740),\n",
       " (49125, 38859),\n",
       " (51114, 4927),\n",
       " (51114, 7727),\n",
       " (51114, 10213),\n",
       " (51114, 10579),\n",
       " (51114, 11620),\n",
       " (51114, 12416),\n",
       " (51114, 17221),\n",
       " (51114, 17468),\n",
       " (51114, 21440),\n",
       " (51114, 23105),\n",
       " (51114, 26863),\n",
       " (51114, 36824),\n",
       " (51114, 36857),\n",
       " (51114, 36872),\n",
       " (51114, 36988),\n",
       " (51114, 36995),\n",
       " (51114, 37241),\n",
       " (51114, 37356),\n",
       " (51114, 37560),\n",
       " (51114, 37653),\n",
       " (51114, 37696),\n",
       " (51114, 40318),\n",
       " (51114, 42765),\n",
       " (51114, 44478),\n",
       " (51114, 45564),\n",
       " (51114, 46267),\n",
       " (51114, 46985),\n",
       " (51114, 78647),\n",
       " (51114, 86079),\n",
       " (51114, 88096),\n",
       " (51114, 54927),\n",
       " (51114, 64279),\n",
       " (51114, 60688),\n",
       " (51114, 71457),\n",
       " (51114, 73620),\n",
       " (51114, 63731),\n",
       " (51114, 56034),\n",
       " (51114, 63986),\n",
       " (51114, 51196),\n",
       " (51114, 88121),\n",
       " (51114, 88124),\n",
       " (51114, 55915),\n",
       " (51114, 64158),\n",
       " (51114, 61021),\n",
       " (51114, 88139),\n",
       " (51114, 88142),\n",
       " (51114, 60780),\n",
       " (51114, 73837),\n",
       " (51114, 88148),\n",
       " (51114, 58488),\n",
       " (51114, 52814),\n",
       " (51114, 69701),\n",
       " (51114, 88156),\n",
       " (51114, 88160),\n",
       " (51114, 88164),\n",
       " (51114, 56324),\n",
       " (51114, 62007),\n",
       " (51114, 60673),\n",
       " (51114, 52764),\n",
       " (51114, 56250),\n",
       " (51114, 67200),\n",
       " (51114, 60978),\n",
       " (51114, 80097),\n",
       " (51114, 70977),\n",
       " (51114, 75281),\n",
       " (51114, 88197),\n",
       " (51114, 62415),\n",
       " (51114, 80516),\n",
       " (51114, 63789),\n",
       " (51114, 63877),\n",
       " (51114, 88211),\n",
       " (51114, 88214),\n",
       " (51114, 88216),\n",
       " (107475, 107401),\n",
       " (37341, 14283),\n",
       " (37341, 37255),\n",
       " (37341, 37412),\n",
       " (95337, 35956),\n",
       " (95337, 55355),\n",
       " (95337, 123082),\n",
       " (120872, 72451),\n",
       " (120872, 86088),\n",
       " (115898, 85363),\n",
       " (4589, 4153),\n",
       " (4589, 4171),\n",
       " (4589, 4212),\n",
       " (4589, 14095),\n",
       " (4589, 46816),\n",
       " (4589, 48729),\n",
       " (41881, 22713),\n",
       " (41881, 65690),\n",
       " (107491, 86145),\n",
       " (798, 4005),\n",
       " (798, 4013),\n",
       " (798, 4019),\n",
       " (798, 4021),\n",
       " (798, 4039),\n",
       " (798, 4042),\n",
       " (798, 4060),\n",
       " ...]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8cd3e36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(EdgeDataView([(1, 0), (1, 3), (1, 5), (1, 6), (1, 7), (1, 9), (1, 11), (1, 12), (1, 13), (1, 14), (1, 15), (1, 16), (1, 17), (1, 19), (1, 20), (1, 21), (1, 22), (1, 23), (1, 24)]),\n",
       " EdgeDataView([(1, 0), (1, 3), (1, 5), (1, 6), (1, 7), (1, 9), (1, 10), (1, 11), (1, 12), (1, 13), (1, 14), (1, 15), (1, 16), (1, 17), (1, 19), (1, 20), (1, 21), (1, 22), (1, 23), (1, 24)]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_train.edges(1), G.edges(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3bf540ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class labels\n",
    "y = np.zeros(2*indices.shape[1])\n",
    "y[:indices.shape[1]] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c20a3980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms the numpy matrices/vectors to torch tensors.\n",
    "features = torch.FloatTensor(features_np).to(device)\n",
    "y = torch.LongTensor(y).to(device)\n",
    "if type(adj) != torch.Tensor:\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "indices = torch.LongTensor(indices).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2c48af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_feat, n_hidden, n_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.double_fc3 = nn.Linear((2*n_hidden), n_hidden)\n",
    "        self.fc4 = nn.Linear(n_hidden, n_class)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x_in, adj, pairs):\n",
    "        \n",
    "        h1 = self.fc1(x_in)\n",
    "        z1 = self.relu(torch.mm(adj, h1))\n",
    "        z1 = self.dropout(z1)\n",
    "\n",
    "        h2 = self.fc2(z1)\n",
    "        z2 = self.relu(torch.mm(adj, h2))\n",
    "\n",
    "        #x = z2[pairs[0,:],:] - z2[pairs[1,:],:] # embedded features (z2) of node 0 - embedded features of node 1\n",
    "        x1 = z2[pairs[0]]\n",
    "        x2 = z2[pairs[1]]\n",
    "        # could we add a new dimension to pairs to specify if same author(s)? and then what could we do?\n",
    "              \n",
    "        x = torch.cat((x1, x2), dim=1)        \n",
    "        x = self.relu(self.double_fc3(x))        \n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7eb8d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "n_hidden = 128\n",
    "dropout_rate = 0.2\n",
    "n_class = 2\n",
    "n_features = features.shape[1]\n",
    "\n",
    "# Creates the model and specifies the optimizer\n",
    "model = GNN(n_features, n_hidden, n_class, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "033efbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 0.6931 acc_train: 0.5013 time: 34.0421 s total_time: 1 min\n",
      "Epoch: 006 loss_train: 0.6638 acc_train: 0.6660 time: 25.2962 s total_time: 3 min\n",
      "Epoch: 011 loss_train: 0.4971 acc_train: 0.7480 time: 22.6470 s total_time: 5 min\n",
      "Epoch: 016 loss_train: 0.4738 acc_train: 0.7818 time: 22.8302 s total_time: 7 min\n",
      "Epoch: 021 loss_train: 0.4411 acc_train: 0.8108 time: 24.8623 s total_time: 9 min\n",
      "Epoch: 026 loss_train: 0.4052 acc_train: 0.8203 time: 25.3849 s total_time: 11 min\n",
      "Epoch: 031 loss_train: 0.3833 acc_train: 0.8319 time: 22.7382 s total_time: 13 min\n",
      "Epoch: 036 loss_train: 0.3653 acc_train: 0.8418 time: 23.5530 s total_time: 15 min\n",
      "Epoch: 041 loss_train: 0.3532 acc_train: 0.8458 time: 25.5614 s total_time: 17 min\n",
      "Epoch: 046 loss_train: 0.3411 acc_train: 0.8526 time: 22.1799 s total_time: 19 min\n",
      "Epoch: 051 loss_train: 0.3325 acc_train: 0.8587 time: 25.8525 s total_time: 21 min\n",
      "Epoch: 056 loss_train: 0.3204 acc_train: 0.8657 time: 25.7711 s total_time: 23 min\n",
      "Epoch: 061 loss_train: 0.2957 acc_train: 0.8821 time: 25.3322 s total_time: 26 min\n",
      "Epoch: 066 loss_train: 0.2664 acc_train: 0.8963 time: 27.4570 s total_time: 28 min\n",
      "Epoch: 071 loss_train: 0.2621 acc_train: 0.8974 time: 24.4239 s total_time: 30 min\n",
      "Epoch: 076 loss_train: 0.2471 acc_train: 0.9032 time: 24.8108 s total_time: 32 min\n",
      "Epoch: 081 loss_train: 0.2341 acc_train: 0.9082 time: 32.2422 s total_time: 34 min\n",
      "Epoch: 086 loss_train: 0.2333 acc_train: 0.9094 time: 24.6454 s total_time: 36 min\n",
      "Epoch: 091 loss_train: 0.2207 acc_train: 0.9135 time: 24.3481 s total_time: 38 min\n",
      "Epoch: 096 loss_train: 0.2179 acc_train: 0.9148 time: 27.8790 s total_time: 41 min\n",
      "Optimization Finished in 42 min!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Train model\n",
    "model.train()\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    t = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=adj.device)# We take random indices each time we run an epoch\n",
    "    pairs = torch.cat((indices, rand_indices), dim=1) # Concatenate the edges indices and random indices.   \n",
    "    output = model(features, adj, pairs) # we run the model that gives the output.\n",
    "    loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "    #print(type(loss_train), '\\n', loss_train.shape)\n",
    "    acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "    loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "    optimizer.step() # Performs a single optimization step (parameter update).\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "              'time: {:.4f} s'.format(time.time() - t),\n",
    "             'total_time: {} min'.format(round((time.time() - start_time)/60)))\n",
    "\n",
    "print(\"Optimization Finished in {} min!\".format(round((time.time() - start_time)/60)))\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a84336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7b650a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 217410)\n",
      "torch.Size([2, 217410])\n",
      "torch.Size([217410, 2])\n",
      "Log loss: 2.430132547449547\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "model.eval()\n",
    "eval_pairs = np.array(np.transpose(val_edges))\n",
    "print(eval_pairs.shape)\n",
    "eval_pairs = torch.LongTensor(eval_pairs).to(device)\n",
    "print(eval_pairs.shape)\n",
    "eval_output = model(features, adj, eval_pairs)\n",
    "print(eval_output.shape)\n",
    "y_pred = torch.exp(eval_output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_val_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_val_pred_true.append(element[1])\n",
    "    \n",
    "print('Log loss:', log_loss(y_val, y_pred_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "63488976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0068791644,\n",
       " 0.51461416,\n",
       " 0.8669021,\n",
       " 0.36618185,\n",
       " 0.020769881,\n",
       " 0.9832517,\n",
       " 0.5173123,\n",
       " 0.6228757,\n",
       " 0.75390375,\n",
       " 0.93110675,\n",
       " 2.0570105e-05,\n",
       " 0.98765576,\n",
       " 0.92761695,\n",
       " 0.00077021314,\n",
       " 0.7263025,\n",
       " 3.0105695e-05,\n",
       " 0.986623,\n",
       " 0.7775482,\n",
       " 0.8152208,\n",
       " 0.04375075,\n",
       " 4.2025167e-05,\n",
       " 0.086349435,\n",
       " 0.94377106,\n",
       " 0.73130643,\n",
       " 0.9754456,\n",
       " 0.0006405931,\n",
       " 0.95496994,\n",
       " 0.0011800342,\n",
       " 2.1838985e-06,\n",
       " 0.9563583,\n",
       " 0.9677939,\n",
       " 0.97538483,\n",
       " 0.76104665,\n",
       " 0.88995045,\n",
       " 0.9866052,\n",
       " 0.8217835,\n",
       " 0.9142653,\n",
       " 0.93872434,\n",
       " 0.07839595,\n",
       " 9.0892775e-07,\n",
       " 0.9366415,\n",
       " 0.98872584,\n",
       " 4.6704403e-07,\n",
       " 0.9268164,\n",
       " 0.99513865,\n",
       " 0.055306844,\n",
       " 4.53322e-06,\n",
       " 1.3047533e-06,\n",
       " 0.91571563,\n",
       " 0.98373574,\n",
       " 0.27989337,\n",
       " 0.82782567,\n",
       " 0.01180967,\n",
       " 0.6880035,\n",
       " 0.007686667,\n",
       " 0.0021729802,\n",
       " 0.00015756862,\n",
       " 0.076484,\n",
       " 0.87629396,\n",
       " 0.75086284,\n",
       " 0.9829269,\n",
       " 0.24153973,\n",
       " 0.00027305738,\n",
       " 0.7829623,\n",
       " 0.00022818931,\n",
       " 0.00013948919,\n",
       " 0.17347898,\n",
       " 0.10724894,\n",
       " 5.4507407e-05,\n",
       " 0.150281,\n",
       " 0.9491255,\n",
       " 0.7924724,\n",
       " 0.94902766,\n",
       " 0.003370582,\n",
       " 0.96380657,\n",
       " 1.30326525e-05,\n",
       " 0.9831503,\n",
       " 0.00013968648,\n",
       " 0.98807573,\n",
       " 0.94131315,\n",
       " 0.0001395717,\n",
       " 0.44034076,\n",
       " 0.7137058,\n",
       " 9.163392e-06,\n",
       " 5.773068e-07,\n",
       " 0.9896366,\n",
       " 0.005344725,\n",
       " 0.95201933,\n",
       " 0.005672389,\n",
       " 0.97579455,\n",
       " 0.728688,\n",
       " 0.00011650447,\n",
       " 9.099603e-06,\n",
       " 0.005191606,\n",
       " 0.9834819,\n",
       " 0.050907157,\n",
       " 0.95044297,\n",
       " 0.9410092,\n",
       " 2.4636525e-05,\n",
       " 0.8306502,\n",
       " 0.904616,\n",
       " 0.0003981387,\n",
       " 0.96845824,\n",
       " 0.0017258234,\n",
       " 0.97824436,\n",
       " 0.9138631,\n",
       " 0.94751006,\n",
       " 0.9357726,\n",
       " 0.0024465409,\n",
       " 9.3227216e-05,\n",
       " 0.0049060904,\n",
       " 0.055429045,\n",
       " 0.012673673,\n",
       " 0.712106,\n",
       " 0.0004067222,\n",
       " 0.029005077,\n",
       " 7.653673e-06,\n",
       " 0.880732,\n",
       " 0.014126873,\n",
       " 0.040109515,\n",
       " 0.94957745,\n",
       " 0.808538,\n",
       " 0.00013928182,\n",
       " 0.017232932,\n",
       " 0.9318427,\n",
       " 0.026622267,\n",
       " 1.99632e-05,\n",
       " 0.039522346,\n",
       " 0.00042634222,\n",
       " 0.9658063,\n",
       " 0.9302149,\n",
       " 0.95268387,\n",
       " 3.5372188e-06,\n",
       " 0.82513916,\n",
       " 3.864824e-06,\n",
       " 0.5900865,\n",
       " 0.009447173,\n",
       " 0.0017538624,\n",
       " 0.0020034397,\n",
       " 0.006096509,\n",
       " 0.86665666,\n",
       " 0.9494664,\n",
       " 0.034641195,\n",
       " 0.71548426,\n",
       " 0.4099122,\n",
       " 0.8053399,\n",
       " 0.049791664,\n",
       " 0.9880937,\n",
       " 0.05152598,\n",
       " 0.9514462,\n",
       " 0.8946648,\n",
       " 0.9681378,\n",
       " 0.9692332,\n",
       " 0.5566327,\n",
       " 0.97219354,\n",
       " 0.871628,\n",
       " 0.9214767,\n",
       " 0.81344813,\n",
       " 0.33186296,\n",
       " 0.22160688,\n",
       " 0.9657508,\n",
       " 0.96987736,\n",
       " 0.012581937,\n",
       " 0.0025531338,\n",
       " 0.00046897063,\n",
       " 0.05901967,\n",
       " 0.89495987,\n",
       " 0.9596228,\n",
       " 0.61613756,\n",
       " 0.90895754,\n",
       " 0.0045478432,\n",
       " 0.85806465,\n",
       " 0.00076577,\n",
       " 0.0003157532,\n",
       " 0.0062907315,\n",
       " 0.0011523729,\n",
       " 0.91693217,\n",
       " 0.93235743,\n",
       " 0.9444895,\n",
       " 0.08215278,\n",
       " 4.6536745e-05,\n",
       " 0.0072860625,\n",
       " 0.8601901,\n",
       " 0.0790763,\n",
       " 0.9714622,\n",
       " 0.97774124,\n",
       " 1.4560773e-07,\n",
       " 0.76307714,\n",
       " 0.010139025,\n",
       " 0.9632055,\n",
       " 1.0893728e-05,\n",
       " 0.28930694,\n",
       " 0.9613909,\n",
       " 0.5392216,\n",
       " 9.614739e-09,\n",
       " 1.9244899e-06,\n",
       " 0.98088443,\n",
       " 0.000101739475,\n",
       " 0.9767949,\n",
       " 0.7123122,\n",
       " 3.822266e-07,\n",
       " 0.931755,\n",
       " 0.00035870026,\n",
       " 0.85065204,\n",
       " 0.29692486,\n",
       " 0.010796678,\n",
       " 0.38243434,\n",
       " 0.96144736,\n",
       " 0.9840852,\n",
       " 0.04035507,\n",
       " 0.00797185,\n",
       " 0.9597043,\n",
       " 0.95967525,\n",
       " 0.020870604,\n",
       " 3.1522228e-05,\n",
       " 0.0027351282,\n",
       " 0.99228513,\n",
       " 0.94533503,\n",
       " 7.158805e-05,\n",
       " 0.001680663,\n",
       " 0.050311398,\n",
       " 0.124727964,\n",
       " 0.45357248,\n",
       " 0.93158674,\n",
       " 0.43500662,\n",
       " 0.9654044,\n",
       " 0.97051704,\n",
       " 0.48328573,\n",
       " 0.22740056,\n",
       " 0.01326253,\n",
       " 0.47790784,\n",
       " 0.9371368,\n",
       " 8.02892e-07,\n",
       " 0.4844096,\n",
       " 0.011212089,\n",
       " 0.9620835,\n",
       " 0.18916714,\n",
       " 0.0018750894,\n",
       " 0.8977856,\n",
       " 0.00046627727,\n",
       " 0.001401455,\n",
       " 0.038295012,\n",
       " 0.89223254,\n",
       " 0.9690634,\n",
       " 3.0682676e-07,\n",
       " 2.8739537e-06,\n",
       " 0.94186544,\n",
       " 0.9842264,\n",
       " 0.03993008,\n",
       " 0.098480135,\n",
       " 0.011652425,\n",
       " 0.6926388,\n",
       " 0.91285443,\n",
       " 0.9399365,\n",
       " 0.0003370839,\n",
       " 0.37817582,\n",
       " 0.00021418603,\n",
       " 0.83493996,\n",
       " 0.9855569,\n",
       " 2.9951112e-05,\n",
       " 8.158212e-09,\n",
       " 0.0017456358,\n",
       " 0.8740499,\n",
       " 0.9849017,\n",
       " 0.9750093,\n",
       " 0.0045192474,\n",
       " 0.011923807,\n",
       " 0.0024727115,\n",
       " 0.097297005,\n",
       " 0.91179746,\n",
       " 0.001151968,\n",
       " 0.040253542,\n",
       " 0.64761406,\n",
       " 0.9646958,\n",
       " 0.12395402,\n",
       " 2.6237376e-07,\n",
       " 0.9823918,\n",
       " 0.024689855,\n",
       " 0.9746661,\n",
       " 1.3423138e-05,\n",
       " 0.10352137,\n",
       " 0.013187851,\n",
       " 0.94209456,\n",
       " 3.5006227e-08,\n",
       " 0.86090726,\n",
       " 0.017358545,\n",
       " 0.97935367,\n",
       " 7.2781827e-06,\n",
       " 2.5479903e-05,\n",
       " 0.0031963396,\n",
       " 4.126184e-05,\n",
       " 0.93815553,\n",
       " 9.0350644e-05,\n",
       " 0.98816466,\n",
       " 0.928694,\n",
       " 7.532237e-05,\n",
       " 0.80632037,\n",
       " 0.06925356,\n",
       " 0.0002480023,\n",
       " 0.9309474,\n",
       " 0.004629228,\n",
       " 0.1867683,\n",
       " 0.0023649205,\n",
       " 0.90282905,\n",
       " 0.75170296,\n",
       " 0.96562487,\n",
       " 0.022566913,\n",
       " 0.7844023,\n",
       " 0.8247222,\n",
       " 0.52624625,\n",
       " 1.2532769e-07,\n",
       " 0.9617421,\n",
       " 0.95597476,\n",
       " 0.95938903,\n",
       " 0.00018549978,\n",
       " 0.00010861482,\n",
       " 0.37063387,\n",
       " 0.030306054,\n",
       " 0.9832545,\n",
       " 0.022202605,\n",
       " 0.97027135,\n",
       " 0.9439834,\n",
       " 0.9217238,\n",
       " 0.9563395,\n",
       " 0.9553085,\n",
       " 0.9448678,\n",
       " 0.9492301,\n",
       " 0.9781659,\n",
       " 0.89014214,\n",
       " 0.0016224126,\n",
       " 0.07271443,\n",
       " 0.27761817,\n",
       " 0.8880403,\n",
       " 7.367663e-08,\n",
       " 0.880177,\n",
       " 0.9327488,\n",
       " 0.81755245,\n",
       " 5.486342e-06,\n",
       " 0.12973763,\n",
       " 0.9783015,\n",
       " 0.0014873038,\n",
       " 0.0018600405,\n",
       " 3.2194228e-07,\n",
       " 0.56372404,\n",
       " 0.6512765,\n",
       " 0.30732912,\n",
       " 0.74793667,\n",
       " 0.0007736348,\n",
       " 0.9834367,\n",
       " 0.21163641,\n",
       " 0.03820265,\n",
       " 0.00013737915,\n",
       " 0.94274265,\n",
       " 0.2627115,\n",
       " 0.6678136,\n",
       " 0.9579867,\n",
       " 0.98196167,\n",
       " 0.8193198,\n",
       " 0.78614295,\n",
       " 0.98706645,\n",
       " 0.9792021,\n",
       " 0.91082865,\n",
       " 2.2673828e-06,\n",
       " 0.44726968,\n",
       " 0.9911562,\n",
       " 0.005086969,\n",
       " 0.014563067,\n",
       " 0.87997913,\n",
       " 0.9250876,\n",
       " 0.95249987,\n",
       " 0.9885445,\n",
       " 0.008753587,\n",
       " 5.055166e-06,\n",
       " 5.88673e-05,\n",
       " 0.0008664332,\n",
       " 0.9706458,\n",
       " 0.9478563,\n",
       " 0.9685422,\n",
       " 0.9371465,\n",
       " 0.9744942,\n",
       " 0.43876946,\n",
       " 0.11433116,\n",
       " 0.17395581,\n",
       " 0.08011908,\n",
       " 0.0042280843,\n",
       " 0.896193,\n",
       " 0.9832102,\n",
       " 0.8456437,\n",
       " 0.97331166,\n",
       " 0.5036404,\n",
       " 0.9873806,\n",
       " 0.00032459162,\n",
       " 0.35607812,\n",
       " 0.99464166,\n",
       " 0.086591,\n",
       " 0.000825547,\n",
       " 0.9397675,\n",
       " 0.36573085,\n",
       " 3.60429e-05,\n",
       " 0.98603314,\n",
       " 0.9682739,\n",
       " 0.0012978595,\n",
       " 1.8278581e-09,\n",
       " 0.924234,\n",
       " 0.7550998,\n",
       " 0.8009416,\n",
       " 0.9814773,\n",
       " 4.767683e-05,\n",
       " 0.010295059,\n",
       " 0.9625889,\n",
       " 0.08250848,\n",
       " 0.9175771,\n",
       " 0.9742562,\n",
       " 0.9727322,\n",
       " 0.9395334,\n",
       " 0.9422202,\n",
       " 0.90772784,\n",
       " 0.91093004,\n",
       " 0.061840758,\n",
       " 1.16733396e-07,\n",
       " 0.97057927,\n",
       " 0.47808564,\n",
       " 0.9895728,\n",
       " 0.93774843,\n",
       " 0.7459735,\n",
       " 0.9668756,\n",
       " 0.60122055,\n",
       " 0.9843833,\n",
       " 0.40457758,\n",
       " 5.7925856e-05,\n",
       " 0.975239,\n",
       " 0.00071219826,\n",
       " 1.7431985e-05,\n",
       " 0.97136176,\n",
       " 8.9707355e-06,\n",
       " 0.003417572,\n",
       " 0.23314849,\n",
       " 0.3922129,\n",
       " 0.22541723,\n",
       " 0.0002333099,\n",
       " 0.39237982,\n",
       " 0.9806835,\n",
       " 0.9510734,\n",
       " 0.9309347,\n",
       " 0.3802241,\n",
       " 0.18732993,\n",
       " 0.8709936,\n",
       " 0.9662573,\n",
       " 8.7563575e-07,\n",
       " 0.81402713,\n",
       " 0.9387286,\n",
       " 4.455335e-08,\n",
       " 0.00049159117,\n",
       " 0.9730674,\n",
       " 0.9075624,\n",
       " 0.9745281,\n",
       " 0.0003726403,\n",
       " 0.96796775,\n",
       " 9.4165125e-06,\n",
       " 0.9716225,\n",
       " 0.9710944,\n",
       " 0.7451168,\n",
       " 0.63216394,\n",
       " 0.69176394,\n",
       " 0.000642408,\n",
       " 0.016344817,\n",
       " 0.5886914,\n",
       " 0.9024119,\n",
       " 4.9482627e-05,\n",
       " 0.46700138,\n",
       " 0.0011696062,\n",
       " 4.384873e-06,\n",
       " 7.656363e-05,\n",
       " 0.0047971415,\n",
       " 0.9557651,\n",
       " 0.7513199,\n",
       " 0.9560941,\n",
       " 0.96668977,\n",
       " 1.7428145e-05,\n",
       " 0.17698154,\n",
       " 0.368272,\n",
       " 2.0493877e-05,\n",
       " 0.8742508,\n",
       " 0.45555106,\n",
       " 0.934395,\n",
       " 0.8937016,\n",
       " 0.43870878,\n",
       " 0.8665145,\n",
       " 4.886702e-05,\n",
       " 0.0006747345,\n",
       " 0.0019120639,\n",
       " 1.1339953e-05,\n",
       " 0.00050464674,\n",
       " 0.87481993,\n",
       " 0.974214,\n",
       " 0.9007969,\n",
       " 0.967545,\n",
       " 0.74252266,\n",
       " 8.8817316e-05,\n",
       " 0.98113203,\n",
       " 0.41075063,\n",
       " 0.017839715,\n",
       " 0.9850579,\n",
       " 0.0020718307,\n",
       " 0.8907058,\n",
       " 0.76008767,\n",
       " 1.4571289e-05,\n",
       " 0.86287963,\n",
       " 0.020907348,\n",
       " 0.00893451,\n",
       " 1.215074e-06,\n",
       " 0.0060998006,\n",
       " 0.9011445,\n",
       " 0.0031887554,\n",
       " 0.918071,\n",
       " 0.92693704,\n",
       " 0.103774115,\n",
       " 0.96367323,\n",
       " 0.0001226633,\n",
       " 0.9088223,\n",
       " 5.0866023e-05,\n",
       " 0.021818945,\n",
       " 7.120515e-06,\n",
       " 9.676962e-07,\n",
       " 0.19000417,\n",
       " 0.9288744,\n",
       " 2.590339e-06,\n",
       " 0.021750571,\n",
       " 0.9932348,\n",
       " 0.9514893,\n",
       " 0.8711526,\n",
       " 0.0013055119,\n",
       " 1.2809143e-05,\n",
       " 2.0012914e-05,\n",
       " 0.96012145,\n",
       " 0.0012418008,\n",
       " 0.9752192,\n",
       " 0.28734004,\n",
       " 0.9347183,\n",
       " 0.7896607,\n",
       " 2.473525e-06,\n",
       " 0.013950624,\n",
       " 0.9556262,\n",
       " 0.85971826,\n",
       " 0.07159963,\n",
       " 0.03694193,\n",
       " 0.92209035,\n",
       " 0.65830433,\n",
       " 4.7814497e-06,\n",
       " 0.7642246,\n",
       " 0.96983117,\n",
       " 0.015573749,\n",
       " 0.9893021,\n",
       " 0.9831534,\n",
       " 0.9345679,\n",
       " 0.9448751,\n",
       " 0.0017957605,\n",
       " 0.000634004,\n",
       " 0.9658492,\n",
       " 0.004449425,\n",
       " 0.003204226,\n",
       " 2.2788288e-06,\n",
       " 0.90365183,\n",
       " 3.892906e-05,\n",
       " 0.9880316,\n",
       " 0.9882111,\n",
       " 0.03960692,\n",
       " 0.9368445,\n",
       " 0.011011004,\n",
       " 3.4485092e-05,\n",
       " 0.84963363,\n",
       " 2.9051964e-05,\n",
       " 0.16260086,\n",
       " 0.22378199,\n",
       " 0.0035309182,\n",
       " 0.9512917,\n",
       " 2.4743615e-05,\n",
       " 0.913896,\n",
       " 0.81586784,\n",
       " 0.00408752,\n",
       " 0.0015013136,\n",
       " 0.94997585,\n",
       " 0.0040575094,\n",
       " 0.7944465,\n",
       " 1.9702445e-06,\n",
       " 0.6798468,\n",
       " 0.8635317,\n",
       " 0.3862394,\n",
       " 0.0072672213,\n",
       " 0.7435576,\n",
       " 0.89167565,\n",
       " 0.015654815,\n",
       " 0.25973082,\n",
       " 0.98270756,\n",
       " 0.98462635,\n",
       " 0.0007360662,\n",
       " 0.0057734265,\n",
       " 0.034070205,\n",
       " 0.8556132,\n",
       " 0.002360397,\n",
       " 0.928289,\n",
       " 0.51605254,\n",
       " 5.300407e-06,\n",
       " 0.9441123,\n",
       " 0.03824199,\n",
       " 0.016726585,\n",
       " 0.97326094,\n",
       " 0.008501418,\n",
       " 1.8766098e-06,\n",
       " 5.305656e-06,\n",
       " 5.359719e-07,\n",
       " 3.0447066e-07,\n",
       " 7.5022484e-07,\n",
       " 0.981574,\n",
       " 0.994507,\n",
       " 4.224815e-05,\n",
       " 0.95805675,\n",
       " 0.023184523,\n",
       " 1.682454e-05,\n",
       " 0.023652194,\n",
       " 0.25009385,\n",
       " 0.001692535,\n",
       " 0.0032807987,\n",
       " 0.0014873868,\n",
       " 0.96808225,\n",
       " 0.047513224,\n",
       " 0.99157166,\n",
       " 3.243241e-05,\n",
       " 0.9407869,\n",
       " 0.95410645,\n",
       " 0.69632363,\n",
       " 0.3126515,\n",
       " 0.23352821,\n",
       " 2.1869172e-07,\n",
       " 0.8848933,\n",
       " 4.4635453e-05,\n",
       " 0.019873979,\n",
       " 0.00014090684,\n",
       " 0.9038194,\n",
       " 0.9900629,\n",
       " 0.0007391934,\n",
       " 0.96013904,\n",
       " 0.010368118,\n",
       " 0.43024284,\n",
       " 1.549834e-05,\n",
       " 0.84054697,\n",
       " 4.8670656e-05,\n",
       " 0.9040244,\n",
       " 0.9938605,\n",
       " 0.5053432,\n",
       " 0.9488698,\n",
       " 5.6635616e-07,\n",
       " 0.9349606,\n",
       " 0.87619066,\n",
       " 0.32900637,\n",
       " 0.00026259295,\n",
       " 0.08297139,\n",
       " 0.96020544,\n",
       " 0.061323516,\n",
       " 0.93798214,\n",
       " 0.7426425,\n",
       " 0.9368002,\n",
       " 0.0033022035,\n",
       " 0.88475955,\n",
       " 0.7310686,\n",
       " 0.95452845,\n",
       " 0.31633416,\n",
       " 0.0005092371,\n",
       " 0.09205342,\n",
       " 2.8712936e-06,\n",
       " 0.7289862,\n",
       " 0.9166581,\n",
       " 0.0018026273,\n",
       " 0.90446323,\n",
       " 0.97344863,\n",
       " 0.060926743,\n",
       " 0.58278286,\n",
       " 0.018110802,\n",
       " 0.95911217,\n",
       " 0.08056421,\n",
       " 0.9235164,\n",
       " 0.97268933,\n",
       " 0.7247197,\n",
       " 0.0171366,\n",
       " 0.87349105,\n",
       " 0.00089492794,\n",
       " 0.02318622,\n",
       " 0.8151849,\n",
       " 4.222696e-07,\n",
       " 2.5661956e-08,\n",
       " 0.00080566603,\n",
       " 0.98171955,\n",
       " 0.90821886,\n",
       " 0.9388252,\n",
       " 2.9456054e-08,\n",
       " 9.988603e-08,\n",
       " 0.0004987536,\n",
       " 0.004260715,\n",
       " 0.89466345,\n",
       " 0.9555541,\n",
       " 0.01607432,\n",
       " 0.0013968009,\n",
       " 0.7571857,\n",
       " 0.001942796,\n",
       " 1.7245767e-10,\n",
       " 0.44582123,\n",
       " 0.69236463,\n",
       " 0.003282631,\n",
       " 0.30691552,\n",
       " 0.12284343,\n",
       " 0.0016378349,\n",
       " 0.94559747,\n",
       " 0.0012130265,\n",
       " 0.9512324,\n",
       " 2.5308236e-06,\n",
       " 0.94958806,\n",
       " 0.738856,\n",
       " 0.00860126,\n",
       " 0.48602307,\n",
       " 0.27264306,\n",
       " 0.011978795,\n",
       " 0.88464034,\n",
       " 0.00017315432,\n",
       " 0.0058297557,\n",
       " 0.55721104,\n",
       " 6.911297e-05,\n",
       " 8.093891e-05,\n",
       " 0.93956876,\n",
       " 0.005264202,\n",
       " 0.8100565,\n",
       " 0.97040546,\n",
       " 0.00030620606,\n",
       " 0.20973322,\n",
       " 0.2909011,\n",
       " 0.95508486,\n",
       " 0.0004702125,\n",
       " 0.82571095,\n",
       " 0.01254217,\n",
       " 0.9032959,\n",
       " 0.9416557,\n",
       " 0.97167575,\n",
       " 0.9826172,\n",
       " 0.0002536519,\n",
       " 0.00020953512,\n",
       " 0.98421395,\n",
       " 0.8482203,\n",
       " 0.028861098,\n",
       " 0.28251284,\n",
       " 0.96105355,\n",
       " 6.061122e-06,\n",
       " 9.387251e-05,\n",
       " 0.00027947463,\n",
       " 0.0006810396,\n",
       " 0.9696432,\n",
       " 0.0058365813,\n",
       " 0.00036739558,\n",
       " 0.71395206,\n",
       " 0.3955992,\n",
       " 0.003466288,\n",
       " 0.0105701955,\n",
       " 0.015160743,\n",
       " 2.0615282e-08,\n",
       " 0.004395288,\n",
       " 0.59904104,\n",
       " 0.26538625,\n",
       " 0.48949423,\n",
       " 6.873967e-05,\n",
       " 0.0038344811,\n",
       " 0.0008645253,\n",
       " 2.7242673e-05,\n",
       " 2.111041e-05,\n",
       " 0.109777406,\n",
       " 0.69914466,\n",
       " 4.124036e-05,\n",
       " 0.85513896,\n",
       " 0.9350296,\n",
       " 0.9664832,\n",
       " 0.98191434,\n",
       " 0.9753466,\n",
       " 0.0043924428,\n",
       " 0.94013673,\n",
       " 0.98071367,\n",
       " 0.9654542,\n",
       " 0.93237215,\n",
       " 0.40119416,\n",
       " 0.9544061,\n",
       " 0.93722636,\n",
       " 0.82089436,\n",
       " 0.0019902808,\n",
       " 0.9452059,\n",
       " 0.9281779,\n",
       " 0.50006026,\n",
       " 2.1546112e-08,\n",
       " 0.98756707,\n",
       " 0.49140528,\n",
       " 0.09248292,\n",
       " 0.9142092,\n",
       " 0.7719317,\n",
       " 0.94282097,\n",
       " 0.05934249,\n",
       " 0.8997917,\n",
       " 2.0228707e-07,\n",
       " 0.95413154,\n",
       " 5.7005887e-05,\n",
       " 0.8848677,\n",
       " 0.92715526,\n",
       " 0.00021250888,\n",
       " 0.9848268,\n",
       " 1.3466989e-07,\n",
       " 0.031066667,\n",
       " 0.24985461,\n",
       " 0.9956917,\n",
       " 0.8188412,\n",
       " 0.87775856,\n",
       " 0.97107005,\n",
       " 0.982366,\n",
       " 0.048570346,\n",
       " 0.9862255,\n",
       " 0.0003558362,\n",
       " 0.831409,\n",
       " 0.86813086,\n",
       " 0.00037667903,\n",
       " 0.017369598,\n",
       " 0.8387267,\n",
       " 0.00016231507,\n",
       " 9.233954e-05,\n",
       " 0.05286609,\n",
       " 0.20012854,\n",
       " 0.9432953,\n",
       " 0.0005970546,\n",
       " 0.51549214,\n",
       " 0.9150401,\n",
       " 0.75247824,\n",
       " 7.533185e-07,\n",
       " 0.9675175,\n",
       " 0.04845904,\n",
       " 0.7458271,\n",
       " 0.009904164,\n",
       " 0.9035922,\n",
       " 0.00041469443,\n",
       " 0.75087917,\n",
       " 0.014341437,\n",
       " 0.2525381,\n",
       " 0.0007417699,\n",
       " 0.88170505,\n",
       " 0.30935135,\n",
       " 0.95694906,\n",
       " 6.090793e-05,\n",
       " 0.9367011,\n",
       " 0.06333974,\n",
       " 0.9697518,\n",
       " 0.0116026085,\n",
       " 2.5980887e-07,\n",
       " 0.78959453,\n",
       " 0.12319827,\n",
       " 0.87148476,\n",
       " 0.56245464,\n",
       " 4.0047802e-05,\n",
       " 0.9450806,\n",
       " 0.00013410964,\n",
       " 0.0007234679,\n",
       " 0.0006149216,\n",
       " 0.80792046,\n",
       " 0.7873578,\n",
       " 0.00019656877,\n",
       " 0.9959115,\n",
       " 0.0002718901,\n",
       " 0.14491124,\n",
       " 0.3206653,\n",
       " 0.010868005,\n",
       " 0.9872143,\n",
       " 0.95168626,\n",
       " 0.024077117,\n",
       " 0.94342566,\n",
       " 0.10453779,\n",
       " 0.019023776,\n",
       " 0.044945303,\n",
       " 0.009133176,\n",
       " 0.008609779,\n",
       " 0.90031666,\n",
       " 0.9409669,\n",
       " 0.9629444,\n",
       " 0.9624704,\n",
       " 0.060322728,\n",
       " 0.3034269,\n",
       " 2.087207e-05,\n",
       " 5.614136e-07,\n",
       " 1.3075185e-06,\n",
       " 0.001794042,\n",
       " 0.98643655,\n",
       " 0.9936785,\n",
       " 0.29354173,\n",
       " 5.8174896e-06,\n",
       " 0.9553205,\n",
       " 0.00022020978,\n",
       " 0.06429383,\n",
       " 0.7032028,\n",
       " 0.98609066,\n",
       " 0.9787248,\n",
       " 5.166375e-05,\n",
       " 0.7874052,\n",
       " 0.13769515,\n",
       " 0.015128138,\n",
       " 0.007843395,\n",
       " 0.96228194,\n",
       " 0.9608422,\n",
       " 0.76368403,\n",
       " 2.8575709e-05,\n",
       " 0.00011072305,\n",
       " 0.9580897,\n",
       " 0.11394545,\n",
       " 0.9499083,\n",
       " 0.97139055,\n",
       " 0.0003322577,\n",
       " 0.89431477,\n",
       " 0.0027533746,\n",
       " 0.73551106,\n",
       " 0.0060173622,\n",
       " 0.00035423384,\n",
       " 0.16625634,\n",
       " 0.890636,\n",
       " 0.9645421,\n",
       " 0.97647846,\n",
       " 0.96643984,\n",
       " 9.646316e-05,\n",
       " 0.5626085,\n",
       " 2.5787617e-06,\n",
       " 0.75888264,\n",
       " 0.004089214,\n",
       " 0.90631044,\n",
       " 0.7231606,\n",
       " 0.00034308629,\n",
       " 0.9842852,\n",
       " 0.086940974,\n",
       " 0.40749052,\n",
       " 0.97739863,\n",
       " 0.012854668,\n",
       " 0.0076795486,\n",
       " 0.88669413,\n",
       " 0.40648708,\n",
       " 3.838926e-06,\n",
       " 0.99521613,\n",
       " 0.00011943427,\n",
       " 0.8703674,\n",
       " 0.8944949,\n",
       " 0.006074151,\n",
       " 0.19140668,\n",
       " 3.6857293e-07,\n",
       " 0.96058387,\n",
       " 0.96789336,\n",
       " 0.055293385,\n",
       " 0.95953673,\n",
       " 0.9405605,\n",
       " 0.081806555,\n",
       " 0.98842144,\n",
       " 0.96998423,\n",
       " 0.0012866172,\n",
       " 0.9596935,\n",
       " 2.298515e-05,\n",
       " 0.978315,\n",
       " 0.93722355,\n",
       " 0.6030684,\n",
       " 0.9000229,\n",
       " 0.76411134,\n",
       " 0.9468212,\n",
       " 0.96965706,\n",
       " 0.67776465,\n",
       " 0.00026745463,\n",
       " 0.55367476,\n",
       " 0.66541576,\n",
       " 0.6659157,\n",
       " 0.24912353,\n",
       " 0.81455505,\n",
       " 3.0973322e-05,\n",
       " 1.2740128e-05,\n",
       " 0.9569364,\n",
       " 0.020236427,\n",
       " 0.00015933448,\n",
       " 0.98385644,\n",
       " 0.0059289127,\n",
       " 0.66606754,\n",
       " 0.9714524,\n",
       " 0.048665162,\n",
       " 0.01710238,\n",
       " 0.97657096,\n",
       " 0.9247997,\n",
       " 0.017409641,\n",
       " 6.2016244e-07,\n",
       " 0.014191858,\n",
       " 0.0013636408,\n",
       " 0.00010598418,\n",
       " 0.06145659,\n",
       " 0.6369485,\n",
       " 1.155286e-06,\n",
       " 0.9386995,\n",
       " 0.036851052,\n",
       " 0.43813995,\n",
       " 0.9702968,\n",
       " 0.0014555791,\n",
       " 0.44356906,\n",
       " ...]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "03d52aba",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (668683560.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[132], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7bcd7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('../test_data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "pairs = torch.LongTensor(node_pairs).to(device)\n",
    "output = model(features, adj, pairs)\n",
    "y_pred = torch.exp(output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"../submissions_files/{}-submission-{}.csv\".format(today, random_nb), header=True, index=True, index_label='id'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7e912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5717e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff03bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a828859c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e3c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c29d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b974905",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = np.array([[5, 3, 0], [2, 1, 0], [4, 0, 1], [0, 3, 2]])\n",
    "test_features = torch.LongTensor(test_features).to(device)\n",
    "test_pairs = [[0, 0, 1, 2, 3, 2, 0, 1, 2, 3], [1, 2, 0, 0, 2, 3, 0, 1, 2, 3]]\n",
    "test_pairs = torch.LongTensor(test_pairs).to(device)\n",
    "print(test_features.shape, test_pairs.shape)\n",
    "test_features[test_pairs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0639bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699d53ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape, np.shape(y_val), np.shape(y_pred_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf150c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ccd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(output.shape)\n",
    "print(features.shape, adj.shape, eval_pairs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4446f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "z2[pairs[1,:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bdb8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e2dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff0f64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425d6600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8607dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0728cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ad5476",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71737f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class labels\n",
    "y = np.zeros(4*G_train.number_of_edges())\n",
    "y[:2*G_train.number_of_edges()] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
    "train_edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede8a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "X_train = np.zeros((4*m, 2*features_np.shape[1]))\n",
    "\n",
    "for i, edge in enumerate(train_edges):\n",
    "    X_train[i] = np.concatenate((features_np[train_edges[i][0]], features_np[train_edges[i][1]]), axis=0)\n",
    "    X_train[m+i] = np.concatenate((features_np[train_edges[i][0]], features_np[train_edges[i][1]]), axis=0)\n",
    "    X_train[2*m+i] = np.concatenate((features_np[randint(0,n-1)], features_np[randint(0,n-1)]), axis=0)\n",
    "    X_train[3*m+i] = np.concatenate((features_np[randint(0,n-1)], features_np[randint(0,n-1)]), axis=0)\n",
    "    \n",
    "X_val = np.zeros((len(val_edges), 2*features_np.shape[1]))\n",
    "for i, edge in enumerate(val_edges):\n",
    "    X_val[i] = np.concatenate((features_np[val_edges[i][0]], features_np[val_edges[i][1]]), axis=0)\n",
    "    \n",
    "print('X_train and X_val created in {} s!'.format(round(time()-t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44a3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6fa791",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y)\n",
    "y_pred = clf.predict_proba(X_val)\n",
    "y_pred = y_pred[:,1]\n",
    "print('Logistic regression performed in {} s!'.format(round(time()-t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c60b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Log loss:', log_loss(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b1ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac02a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Read test data. Each sample is a pair of nodes\n",
    "test_edges = list()\n",
    "with open('../test_data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        test_edges.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "X_test = np.zeros((len(test_edges), 2*features_np.shape[1]))\n",
    "for i, edge in enumerate(test_edges):\n",
    "    X_test[i] = np.concatenate((features_np[test_edges[i][0]], features_np[test_edges[i][1]]), axis=0)\n",
    "\n",
    "t = time()\n",
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "y_pred = y_pred[:,1]\n",
    "print('Logistic regression performed in {} s!'.format(round(time()-t)))\n",
    "\n",
    "\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 100000)\n",
    "\n",
    "pd.DataFrame(y_pred, columns={'predicted'}).to_csv(\n",
    "\"../submissions_files/{}-submission-{}.csv\".format(today, random_nb), header=True, index=True, index_label='id'\n",
    ")\n",
    "    \n",
    "    \n",
    "# # Testing\n",
    "# node_pairs = np.array(np.transpose(node_pairs))\n",
    "# pairs = torch.LongTensor(node_pairs).to(device)\n",
    "# output = model(features, adj, pairs)\n",
    "# y_pred = torch.exp(output)\n",
    "# y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "# y_pred_true = list()\n",
    "# for element in y_pred:\n",
    "#     y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "# today = datetime.today().strftime('%Y-%m-%d')\n",
    "# random_nb = randint(0, 100000)\n",
    "\n",
    "# pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "# \"../submissions_files/{}-submission-{}.csv\".format(today, random_nb), header=True, index=True, index_label='id'\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
