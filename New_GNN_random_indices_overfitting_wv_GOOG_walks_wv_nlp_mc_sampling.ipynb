{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbW5vRi3mWS0"
   },
   "source": [
    "# Packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5GfuP1hlTmg",
    "outputId": "a0d9605e-c7b7-4037-8d2a-51949cfb37c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ghassenabdedayem/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from scipy.sparse import identity, diags\n",
    "from urllib.request import urlopen\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import re\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from random import choice\n",
    "\n",
    "\n",
    "\n",
    "!pip install unidecode\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rgUHHNwD2fzO"
   },
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BJSlSwejljfp"
   },
   "outputs": [],
   "source": [
    "\n",
    "def text_to_list(text): # a function that split the text of the authors to a list of authors\n",
    "    return unidecode(text).split(',')\n",
    "\n",
    "def intersection(lst1, lst2): # a function that returns the number of common items of two lists and 1 or 0 if there are common. This function will be used in add_authors_to_pairs to add this features to the pairs.\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    is_common = 1 if len(lst3)>0 else 0\n",
    "    return len(lst3), is_common\n",
    "\n",
    "\n",
    "def save_subgraph_in_file(nbr_nodes, source_path='../input_data/edgelist.txt', destination_path='../input_data/small_edgelist.txt'):\n",
    "    G = nx.read_edgelist(source_path, delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "    G = G.subgraph(range(nbr_nodes))\n",
    "    nx.write_edgelist(G, path=destination_path, delimiter=',')\n",
    "    print(G.number_of_nodes(), 'nodes,', G.number_of_edges(), 'edges Graph extracted from', source_path[source_path.rfind('/')+1:])\n",
    "    G = nx.read_edgelist(destination_path, delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "    print(G.number_of_nodes(), 'nodes,', G.number_of_edges(), 'edges Graph saved in', destination_path[destination_path.rfind('/')+1:])\n",
    "    print(max(G.nodes))\n",
    "    return\n",
    "\n",
    "\n",
    "def read_train_val_graph(path='small_edgelist.txt', val_ratio=0.1):\n",
    "    #gets the data from the file on the distant server\n",
    "    G = nx.read_edgelist(urlopen('https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/edgelist.txt'), delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "    #G = nx.read_edgelist(path, delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "    nodes = list(G.nodes())\n",
    "    n = G.number_of_nodes()\n",
    "    m = G.number_of_edges()\n",
    "    edges = list(G.edges())\n",
    "\n",
    "    print('Number of nodes:', n, 'number of edges:', m,'in the Complete the set')\n",
    "\n",
    "    node_to_idx = dict()\n",
    "    for i, node in enumerate(nodes):\n",
    "        node_to_idx[node] = i\n",
    "\n",
    "    val_edges = list()\n",
    "    G_train = G.copy()\n",
    "\n",
    "    for edge in edges:\n",
    "        if random() < val_ratio and edge[0] < n and edge[1] < n:\n",
    "            val_edges.append(edge)\n",
    "            G_train.remove_edge(edge[0], edge[1]) # We remove the val edges from the graph G\n",
    "\n",
    "   \n",
    "    #for edge in val_edges:\n",
    "        \n",
    "\n",
    "    n = G_train.number_of_nodes()\n",
    "    m = G_train.number_of_edges()\n",
    "    train_edges = list(G_train.edges())\n",
    "\n",
    "    print('Number of nodes:', n, 'number of edges:', m, 'in the Training set')\n",
    "    print('len(nodes)', len(nodes))\n",
    "\n",
    "    y_val = [1]*len(val_edges)\n",
    "\n",
    "    n_val_edges = len(val_edges)\n",
    "    \n",
    "    print('Creating random val_edges...')\n",
    "    for i in range(n_val_edges):\n",
    "        n1 = nodes[randint(0, n-1)]\n",
    "        n2 = nodes[randint(0, n-1)]\n",
    "        (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "        while n2 >= n: #or (n1, n2) in train_edges:\n",
    "            if (n1, n2) in train_edges:\n",
    "                print((n1, n2), 'in train_edges:')\n",
    "            n1 = nodes[randint(0, n-1)]\n",
    "            n2 = nodes[randint(0, n-1)]\n",
    "            (n1, n2) = (min(n1, n2), max(n1, n2))\n",
    "        val_edges.append((n1, n2))\n",
    "\n",
    "    y_val.extend([0]*(n_val_edges))\n",
    "    \n",
    "    ### From Giannis /!\\\n",
    "    val_indices = np.zeros((2,len(val_edges)))\n",
    "    for i,edge in enumerate(val_edges):\n",
    "        val_indices[0,i] = node_to_idx[edge[0]]\n",
    "        val_indices[1,i] = node_to_idx[edge[1]]\n",
    "    \n",
    "    print('Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects')\n",
    "    print('Loaded from', path[path.rfind('/')+1:], 'and with a training validation split ratio =', val_ratio)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx\n",
    "\n",
    "def random_walk(G, node, walk_length):\n",
    "    walk = [node]\n",
    "  \n",
    "    for i in range(walk_length-1):\n",
    "        neibor_nodes = list(G.neighbors(walk[-1]))\n",
    "        if len(neibor_nodes) > 0:\n",
    "            next_node = choice(neibor_nodes)\n",
    "            walk.append(next_node)\n",
    "    walk = [node for node in walk] # in case the nodes are in string format, we don't need to cast into string, but if the nodes are in numeric or integer, we need this line to cast into string\n",
    "    return walk\n",
    "\n",
    "\n",
    "def generate_walks(G, num_walks, walk_length):\n",
    "  # Runs \"num_walks\" random walks from each node, and returns a list of all random walk\n",
    "    t = time()\n",
    "    print('Start generating walks....')\n",
    "    walks = list()  \n",
    "    for i in range(num_walks):\n",
    "        for node in G.nodes():\n",
    "            walk = random_walk(G, node, walk_length)\n",
    "            walks.append(walk)\n",
    "        #print('walks : ', walks)\n",
    "    print('Random walks generated in in {}s!'.format(round(time()-t)))\n",
    "    return walks\n",
    "\n",
    "def apply_word2vec_on_features(features, nodes, vector_size=128, window=5, min_count=0, sg=1, workers=8):\n",
    "    t = time()\n",
    "    print('Start applying Word2Vec...')\n",
    "    wv_model = Word2Vec(vector_size=vector_size, window=window, min_count=min_count, sg=sg, workers=workers)\n",
    "    wv_model.build_vocab(features)\n",
    "    wv_model.train(features, total_examples=wv_model.corpus_count, epochs=5) \n",
    "    print('Word2vec model trained on features in {} min!'.format(round((time()-t)/60)))\n",
    "    features_np = []\n",
    "    for node in nodes:\n",
    "        features_np.append(wv_model.wv[node])\n",
    "\n",
    "    features_np = np.array(features_np)\n",
    "    print(features_np.shape, 'features numpy array created in {} min!'.format(round((time()-t)/60)))\n",
    "    return features_np\n",
    "\n",
    "\n",
    "\n",
    "def normalize_adjacency(A):\n",
    "    n = A.shape[0]\n",
    "    A = A + identity(n)\n",
    "    degs = A.dot(np.ones(n))\n",
    "    inv_degs = np.power(degs, -1)\n",
    "    D_inv = diags(inv_degs)\n",
    "    A_hat = D_inv.dot(A)\n",
    "    return A_hat\n",
    "\n",
    "def create_and_normalize_adjacency(G):\n",
    "    adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix of the training graph\n",
    "    adj = normalize_adjacency(adj)\n",
    "    print('Created a normalized adjancency matrix of shape', adj.shape)\n",
    "    indices = np.array(adj.nonzero()) # Gets the positions of non zeros of adj into indices\n",
    "    print('Created indices', indices.shape, 'with the positions of non zeros in adj matrix')\n",
    "    return adj, indices\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    " \n",
    "\n",
    "def add_authors_to_pairs (pairs, authors):\n",
    "    #authors = pd.DataFrame(authors)\n",
    "    try: \n",
    "        pairs = pairs.detach().cpu().numpy()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    pairs_df = pd.DataFrame(np.transpose(pairs)).rename(columns={0: \"paper_1\", 1: \"paper_2\"})\n",
    "    #pairs = torch.tensor(pairs).to(device)\n",
    "    pairs_df = pairs_df.merge(authors, left_on='paper_1', right_on='paper_id', how='left').rename(columns={'authors': \"authors_1\"})\n",
    "    pairs_df = pairs_df.merge(authors, left_on='paper_2', right_on='paper_id', how='left').rename(columns={'authors': \"authors_2\"})\n",
    "    pairs_df.drop(['paper_id_x', 'paper_id_y'], axis=1, inplace=True)\n",
    "\n",
    "    pairs_df['nb_common_author'] = pairs_df.apply(lambda row: intersection(row['authors_1'], row['authors_2'])[0], axis=1)\n",
    "    pairs_df['is_common_author'] = pairs_df.apply(lambda row: intersection(row['authors_1'], row['authors_2'])[1], axis=1)\n",
    "\n",
    "    #pairs_tensor = torch.LongTensor(np.transpose(pairs_df[[\"paper_1\", \"paper_2\", 'is_common_author', 'nb_common_author']].values.tolist())).to(device)\n",
    "    \n",
    "    return np.transpose(pairs_df[[\"paper_1\", \"paper_2\", 'is_common_author', 'nb_common_author']].values.tolist())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5ef7pPp7lBeS"
   },
   "outputs": [],
   "source": [
    "def read_and_clean_abstracts (nodes, sample_length=-1, abstracts_path = 'https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/abstracts.txt'):\n",
    "    t = time()\n",
    "    abstracts = dict()\n",
    "    abstracts_list = list()\n",
    "    f = urlopen(abstracts_path)\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        if i == sample_length:\n",
    "            break\n",
    "        if i in nodes:\n",
    "            node, abstract = str(line).lower().split('|--|')\n",
    "            abstract = remove_stopwords(abstract)\n",
    "            abstract = re.sub(r\"[,.;@#?!&$()-]\", \" \", abstract)\n",
    "\n",
    "            for word in abstract.split()[:-1]:\n",
    "                #abstract = abstract.replace(word, stemmer.stem(word))\n",
    "                abstract = abstract.replace(word, lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word), pos='s'), pos='a'), pos='n'), pos='v'), pos='r'))\n",
    "            \n",
    "            node = re.sub(\"[^0-9]\", \"\", node)\n",
    "            if i != int(node):\n",
    "                print('i and node not the same', i, node)\n",
    "            abstracts[int(node)] = abstract\n",
    "            abstracts_list.append(abstract)\n",
    "        \n",
    "    print('Text loaded and cleaned in {:.0f} sec'.format(time()-t))\n",
    "    return abstracts\n",
    "\n",
    "def doc_counter (documents, word): #a function that return the number of documents containing a word\n",
    "    counter = 0\n",
    "    for i in documents:\n",
    "        if word in documents[i]:\n",
    "            counter += 1\n",
    "    return counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "UAOZ3MLP98UR"
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        self.word_occurrence = {}\n",
    "        self.words_list = []\n",
    "        self.sentences_list = []\n",
    "        self.sentences_list_words = []\n",
    "        self.num_words = 0\n",
    "        self.num_sentences = 0\n",
    "        self.longest_sentence = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            # First entry of word into vocabulary\n",
    "            self.words_list.append(word)\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "            self.word_occurrence[word] = 1\n",
    "        else:\n",
    "            # Word exists; increase word count\n",
    "            self.word2count[word] += 1\n",
    "            self.word_occurrence[word] += 1\n",
    "            \n",
    "    def add_sentence(self, sentence):\n",
    "        sentence_len = 0\n",
    "        for word in sentence.split()[:-1]:\n",
    "            sentence_len += 1\n",
    "            self.add_word(word)\n",
    "        if sentence_len > self.longest_sentence:\n",
    "            # This is the longest sentence\n",
    "            self.longest_sentence = sentence_len\n",
    "        # Count the number of sentences\n",
    "        self.num_sentences += 1\n",
    "        self.sentences_list.append(sentence)\n",
    "        self.sentences_list_words.append(sentence.split()[:-1])\n",
    "\n",
    "    def to_word(self, index):\n",
    "        return self.index2word[index]\n",
    "\n",
    "    def to_index(self, word):\n",
    "        return self.word2index[word]\n",
    "\n",
    "    def words(self):\n",
    "        return self.words_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QlXtTyCe4C_J"
   },
   "outputs": [],
   "source": [
    "def list_words_to_one_sentence_wv_vector(sentences_list_words, wv_model):\n",
    "    t = time()\n",
    "\n",
    "    embedded_abstracts = dict()\n",
    "    nb_errors = 0\n",
    "    for node, abstract in enumerate(sentences_list_words):\n",
    "        #some abstracts are null\n",
    "        if len(abstract) > 0:\n",
    "            try: wv_model[abstract]\n",
    "            except: nb_errors += 1\n",
    "        \n",
    "#         cleaned_abstract=[]\n",
    "#         for word in abstract:\n",
    "#             try: \n",
    "#                 wv_model[word] #we try to find the word in the Vocabulary\n",
    "#                 cleaned_abstract.append(word)\n",
    "#             except: pass\n",
    "#         if len(cleaned_abstract) > 0:\n",
    "#             wv_model[cleaned_abstract]\n",
    "#             embedded_abstracts[node] = np.mean(wv_model[cleaned_abstract], axis=0)\n",
    "#         # for quartile in np.percentile(wv_model.wv[abstract], [25, 50, 75], axis=0):\n",
    "#         #     embedded_abstracts[node] = np.concatenate((embedded_abstracts[node], quartile), axis=0)\n",
    "#         else: #if the abstract text is null, we fill the embedded text vector by random numbers (it could help to prevent overfittiing)\n",
    "#             embedded_abstracts[node] = np.random.uniform(wv_model.vectors.min(), wv_model.vectors.max(), size=embedded_abstracts[0].shape)\n",
    "    print('nodes embeddings generated based on words embeddings in {:.0f} sec in embedded_abstracts'.format(time()-t)) #206 sec\n",
    "    return embedded_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "iHDwUFfhUmJw"
   },
   "outputs": [],
   "source": [
    "def train_wv_on_vocab (voc, vector_size):\n",
    "    t = time()\n",
    "    wv_model = Word2Vec(vector_size=vector_size, window=5, min_count=1, sg=1, workers=8)\n",
    "    wv_model.build_vocab(voc.sentences_list_words)\n",
    "    wv_model.train(voc.sentences_list_words, total_examples=wv_model.corpus_count, epochs=5) \n",
    "    print('word2vec trained in {:.0f} sec'.format(time()-t)) #219 sec\n",
    "    return wv_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpFQg_jBsGoM"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ROBXxQaZnfQ3"
   },
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_feat, wv_walk_size, n_hidden, n_class, sub_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.double_fc3 = nn.Linear((2*n_hidden+wv_walk_size), n_hidden)\n",
    "        self.double_fc3 = nn.Linear((2*n_hidden), n_hidden)\n",
    "        self.fc4 = nn.Linear(n_hidden, sub_class)\n",
    "        self.fc5 = nn.Linear(sub_class, n_class)        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x_in, adj, pairs, wv_walk_size):\n",
    "        \n",
    "        h1 = self.fc1(x_in)\n",
    "        #adj = adj.float()\n",
    "        z1 = torch.mm(adj, h1)\n",
    "        del(h1)\n",
    "        z1 = self.relu(z1)\n",
    "        z1 = self.dropout(z1)\n",
    "        \n",
    "\n",
    "        h2 = self.fc2(z1)\n",
    "        del(z1)\n",
    "        z2 = torch.mm(adj, h2)\n",
    "        del(h2, adj)\n",
    "        z2 = self.relu(z2)\n",
    "        z2 = self.dropout(z2)\n",
    "        \n",
    "\n",
    "        x = z2[:, :wv_walk_size][pairs[0]] - z2[:, :wv_walk_size][pairs[1]] # embedded features (z2) of node 0 - embedded features of node 1 // x_in[:, :64].shape\n",
    "        #x = torch.ones(size=(pairs.shape[1], wv_walk_size)).to(device)\n",
    "        x = pairs[2][:, None] * x #we multiply by the number of common authors by pairs of nodes (papers) with pairs[3] || or yes/no if at least one same author (pairs[2])\n",
    "        #x = x.to(device)\n",
    "        x1 = z2[pairs[0]]#.to(device)\n",
    "        x2 = z2[pairs[1]]#.to(device)\n",
    "        del(pairs)\n",
    "        #x = torch.cat((x, x1, x2), dim=1)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "\n",
    "        #print(np.shape(x1), np.shape(x2))\n",
    "        del(x1, x2)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        x = self.relu(self.double_fc3(x))        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bnRNz5z4nh6x"
   },
   "outputs": [],
   "source": [
    "def early_stopping(loss_train, list_loss_train, loss_val, list_loss_val, \n",
    "                   tolerance=0.01, patience=15):\n",
    "    list_loss_val = list(list_loss_val)[-patience:]\n",
    "    list_loss_train = list(list_loss_train)[-patience:]\n",
    "    if (len(list_loss_val) == patience and loss_val > (sum(list_loss_val)/len(list_loss_val)) and loss_train + tolerance < loss_val) or (len(list_loss_train) == patience and loss_train > (sum(list_loss_train)/len(list_loss_train))):\n",
    "        #print('train: {:.5f} val: {:.5f} mean val: {:.5f}'.format(loss_train, loss_val, (sum(list_loss_val)/len(list_loss_val))))\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "    \n",
    "def train_model(model, learning_rate, features, adj, indices_mc, y, val_indices, \n",
    "                y_val, epochs, run_number, wv_walk_size, tolerence = 0.01, patience = 15):\n",
    "    # Train model\n",
    "    start_time = time()\n",
    "    print('Initializing the optimizer with learning rate:', learning_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) #optimizer with halving learning rate in training\n",
    "    try: os.mkdir('./outputs')\n",
    "    except: pass\n",
    "    print('Preparing the data for training...')        \n",
    "\n",
    "    today = datetime.today().strftime('%Y-%m-%d-%H:%M')\n",
    "    list_loss_val = []\n",
    "    list_loss_train = []\n",
    "\n",
    "    \n",
    "    halving_lr = 0 # counter of the number of halving lr\n",
    "    print('Start training...')\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "\n",
    "        # we create the rand indices corresponding to non edges (their y = 0)\n",
    "        optimizer.zero_grad()\n",
    "        rand_indices = np.random.randint(0, len(indices_mc), size=(indices_mc.shape[0], indices_mc.shape[1]))\n",
    "        rand_indices = add_authors_to_pairs(rand_indices, authors)        \n",
    "        pairs = np.concatenate((indices_mc, rand_indices), axis=1)\n",
    "        pairs = torch.LongTensor(pairs).to(device)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        output = model(features, adj, pairs, wv_walk_size).to(device) # we run the model that gives the output.\n",
    "        loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "        acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "        loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "        optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, adj, val_indices, wv_walk_size).to(device)\n",
    "        #y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        list_loss_val.append(loss_val.item())\n",
    "        list_loss_train.append(loss_train.item())\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {} s'.format(int(round(time()) - round(t))),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "        if epoch % 20 == 0:\n",
    "            model_path = \"outputs/{}-model-{}epochs-{}.pt\".format(today, epoch, run_number)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "        if int(loss_val.item()) > 5:\n",
    "            break\n",
    "            \n",
    "        # early = early_stopping(loss_train.item(), list_loss_train, loss_val.item(), list_loss_val, patience=15)        \n",
    "        # if early:\n",
    "        #     halving_lr += 1\n",
    "        #     if halving_lr > 5:\n",
    "        #         break\n",
    "        #     list_loss_val=[]\n",
    "        #     learning_rate = learning_rate/10\n",
    "        #     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        #     print('Deviding the learning rate by 2. New learning rate: {:.6f}'.format(learning_rate))\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MH6hAskFw5AU"
   },
   "source": [
    "# Features processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oHyYv78NlkXU",
    "outputId": "78b0a7d2-2876-4648-8d46-ae4b389c3ce6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499 number of edges: 1091955 in the Complete the set\n",
      "Number of nodes: 138499 number of edges: 982558 in the Training set\n",
      "len(nodes) 138499\n",
      "Creating random val_edges...\n",
      "Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects\n",
      "Loaded from small_edgelist.txt and with a training validation split ratio = 0.1\n",
      "Start generating walks....\n",
      "Random walks generated in in 56s!\n",
      "Start applying Word2Vec...\n",
      "Word2vec model trained on features in 2 min!\n",
      "(138499, 64) features numpy array created in 2 min!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pd/glcnl2497w5b6xn3p94tnwlr0000gn/T/ipykernel_70550/975224092.py:139: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix of the training graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a normalized adjancency matrix of shape (138499, 138499)\n",
      "Created indices (2, 2103615) with the positions of non zeros in adj matrix\n"
     ]
    }
   ],
   "source": [
    "path = 'input_data/small_edgelist.txt' #not used\n",
    "num_walks = 10\n",
    "walk_length=15\n",
    "wv_vector_size = 64\n",
    "\n",
    "G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx = read_train_val_graph(path=path, val_ratio=0.1)\n",
    "walks = generate_walks(G=G_train, num_walks=num_walks, walk_length=walk_length)\n",
    "walks_wv = apply_word2vec_on_features(features=walks, nodes=nodes, vector_size=wv_vector_size)\n",
    "adj, indices = create_and_normalize_adjacency(G_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sbhiufK3kLG7",
    "outputId": "4f39c2a0-1fba-4110-d757-45f5531150e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text loaded and cleaned in 437 sec\n",
      "Text cleaned and vocab built in 450 sec\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "n = -1 #length of the sample to develop and test the pipeline (-1 or negative values to take all the dataset)\n",
    "\n",
    "#takes 4 minutes to process all the abstracts\n",
    "abstracts = read_and_clean_abstracts(nodes, sample_length=n)  #149s #194s\n",
    "abstracts_dict_list_words = {i: abstracts[i].split()[:-1] for i in nodes}\n",
    "abstracts_list_sentences = [list(item)[1][:-3] for item in abstracts.items()]\n",
    "\n",
    "#we create a vacabulary of words and sentences (abstracts)\n",
    "#we take only a sample of 3 abstracts (i=2) to explore the approach\n",
    "voc = Vocabulary('abstracts') \n",
    "for i in nodes:\n",
    "    voc.add_sentence(abstracts[i])\n",
    "\n",
    "print('Text cleaned and vocab built in {:.0f} sec'.format(time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "DesyOWhh86uv"
   },
   "outputs": [],
   "source": [
    "# #apply wv to abstracts\n",
    "# vector_size = 192\n",
    "\n",
    "# wv_model = train_wv_on_vocab (voc, vector_size=vector_size)\n",
    "# embedded_abstracts = list_words_to_one_sentence_wv_vector(voc.sentences_list_words, wv_model)\n",
    "# embedded_abstracts = np.array(list(embedded_abstracts.values()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CVZG-PAh_PZc",
    "outputId": "9d56545f-d05c-4070-9f3b-803eded2657f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iA80kLyFBO8l",
    "outputId": "86ed2c5d-529b-443d-ed3c-5b07d952d64b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading word2vec google news 300...\n",
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
      "Model downloaded in 232 s\n"
     ]
    }
   ],
   "source": [
    "#### NEW with GOOG vocab --> to test\n",
    "import gensim.downloader as api\n",
    "print('downloading word2vec google news 300...')\n",
    "t = time()\n",
    "wv_model = api.load('word2vec-google-news-300')\n",
    "print('Model downloaded in {} s'.format(round(time()-t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_w2v (abstract, wv_model):\n",
    "    sum_vectors = np.zeros(300)\n",
    "    l = [len (abstract) if len (abstract)>0 else 1]\n",
    "    word_vect = np.zeros(300)\n",
    "\n",
    "    for i, word in enumerate(abstract):\n",
    "        try: \n",
    "            word_vect = wv_model[word]\n",
    "        except:\n",
    "            pass\n",
    "        sum_vectors = sum_vectors + word_vect\n",
    "    return sum_vectors / l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procssed at 0 % in 0 sec\n",
      "Procssed at 20 % in 52 sec\n",
      "Procssed at 40 % in 104 sec\n",
      "Procssed at 60 % in 153 sec\n",
      "Procssed at 80 % in 203 sec\n",
      "Procssed at 100 % in 249 sec\n",
      "mean embedding constructed in 249 sec\n"
     ]
    }
   ],
   "source": [
    "embedded_mean_abstracts = dict()\n",
    "t = time()\n",
    "\n",
    "for node in abstracts.keys():\n",
    "    embedded_mean_abstracts[node] = mean_w2v(abstracts[node], wv_model)\n",
    "    if (node % (len(abstracts) // 5) == 0):\n",
    "        print('Procssed at {:.0f} % in {:.0f} sec'.format((node / len(abstracts))*100, time()-t))\n",
    "print('mean embedding constructed in {:.0f} sec'.format(time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ICsvnq-w4J9-"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wv_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m##### NEW with GOOG vocab --> to test\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# import gensim.downloader as api\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# print('downloading word2vec google news 300...')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# t = time()\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# wv_model = api.load('word2vec-google-news-300')\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# print('Model downloaded in {} s'.format(round(time()-t)))\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m embedded_abstracts \u001b[38;5;241m=\u001b[39m list_words_to_one_sentence_wv_vector(voc\u001b[38;5;241m.\u001b[39msentences_list_words, \u001b[43mwv_model\u001b[49m)\n\u001b[1;32m      8\u001b[0m embedded_abstracts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(embedded_abstracts\u001b[38;5;241m.\u001b[39mvalues()))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wv_model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "embedded_abstracts = list_words_to_one_sentence_wv_vector(voc.sentences_list_words, wv_model)\n",
    "embedded_abstracts = np.array(list(embedded_abstracts.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFN6fF-Nstmm"
   },
   "outputs": [],
   "source": [
    "embedded_abstracts.shape, walks_wv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5RJqWqu8nz1"
   },
   "outputs": [],
   "source": [
    "# HERE WE CONCATENATE THE WALKS FEATURES AND THE ABSTRACTS FEATURES\n",
    "features_np = np.concatenate((walks_wv, embedded_abstracts), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWAzHqmxmYff"
   },
   "outputs": [],
   "source": [
    "authors = pd.read_csv(urlopen('https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/authors.txt'), sep = '|', header=None)\n",
    "authors = authors.rename(columns={0: \"paper_id\", 2: \"authors\"})\n",
    "#authors = authors[authors['paper_id'].isin(nodes) ]\n",
    "authors['authors'] = authors['authors'].apply(text_to_list)\n",
    "authors = authors[[\"paper_id\", \"authors\"]]\n",
    "authors = authors[authors['paper_id'] <= max(G.nodes())]\n",
    "authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCXRY-SPyJn6"
   },
   "outputs": [],
   "source": [
    "def random_subset_with_mc_sampling(lst, ratio):\n",
    "    weights = np.ones(np.shape(lst)[1]) / np.shape(lst)[1]  # Uniform distribution\n",
    "    indices = np.random.choice(np.shape(lst)[1], size=round(ratio*np.shape(lst)[1]), replace=False, p=weights)\n",
    "    return lst[:, indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHyXpfe_SVEb"
   },
   "outputs": [],
   "source": [
    "t = time()\n",
    "#indices_mc = random_subset_with_mc_sampling(indices, ratio = 0.3)\n",
    "indices_mc = indices # without mc sampling\n",
    "indices_mc = add_authors_to_pairs(indices_mc, authors)\n",
    "\n",
    "#### we put this part inside the training to avoid overfitting\n",
    "# rand_indices = np.random.randint(0, len(indices_mc), size=(indices_mc.shape[0], indices_mc.shape[1]))\n",
    "# rand_indices = add_authors_to_pairs(rand_indices, authors)\n",
    "\n",
    "# pairs = np.concatenate((indices_mc, rand_indices), axis=1)\n",
    "\n",
    "val_indices = add_authors_to_pairs(val_indices, authors)\n",
    "print ('Finished sampling and random pairs concatenation in {:.0f} sec'.format(time()-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-k313Z82z4w"
   },
   "source": [
    "# Transform variable into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ht8fo8cK2fzU"
   },
   "outputs": [],
   "source": [
    "### as we removed the creation of rand indices, we keep only indices_mc (positive edges)\n",
    "#pairs_torch = torch.LongTensor(pairs).to(device)\n",
    "#indices_mc_torch = torch.LongTensor(indices_mc).to(device)\n",
    "val_indices_torch = torch.LongTensor(val_indices).to(device)\n",
    "y_val_torch = torch.LongTensor(y_val).to(device)\n",
    "\n",
    "# Create class labels\n",
    "y = np.zeros(2*indices_mc.shape[1])\n",
    "y[:indices_mc.shape[1]] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
    "y = torch.LongTensor(y).to(device)\n",
    "\n",
    "features_torch = torch.FloatTensor(features_np).to(device)\n",
    "print('node features shape:', features_np.shape)\n",
    "\n",
    "adj_torch = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "print('adj converted into a sparce torch tensor in {:.0f} sec'.format(time()-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StYPzRnT2fzV"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhC2OuMhBMKD"
   },
   "outputs": [],
   "source": [
    "wv_vector_size, features_torch.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UiGcIYWvmXA9"
   },
   "outputs": [],
   "source": [
    "add_authors_to_pairs(indices, authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7i8UpGHuNhQp"
   },
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "wv_walk_size = wv_vector_size\n",
    "n_hidden = 224\n",
    "dropout_rate = 0.3\n",
    "sub_class = 20\n",
    "n_class = 2\n",
    "n_features = features_torch.shape[1]\n",
    "epochs = 100\n",
    "run_number = randint(0, 1000)\n",
    "learning_rate = 0.01\n",
    "patience = 30\n",
    "tolerence = 0.1\n",
    "\n",
    "# Creates the model\n",
    "model = GNN(n_features, wv_walk_size, n_hidden, n_class, sub_class, dropout_rate).to(device)\n",
    "trained_model = train_model(model, learning_rate, features_torch, adj_torch, indices_mc, y, \n",
    "                            val_indices_torch, y_val_torch, epochs, wv_walk_size=wv_walk_size, \n",
    "                            tolerence=tolerence, patience=patience, run_number=run_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDQ_DCCJavQZ"
   },
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEpkbzgYDswA"
   },
   "outputs": [],
   "source": [
    "rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1))).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V42H3MbEh6Ba"
   },
   "outputs": [],
   "source": [
    "features.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Von8ehIosDPa"
   },
   "outputs": [],
   "source": [
    "features.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZcN9lfNA6ly"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5VIVVzmMCaG"
   },
   "outputs": [],
   "source": [
    "del(adj, G, G_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "158xT56h4_QY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZezlQHa24_Xc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zev8mABQ4_fP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoayPq8E5AGd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okgeOyoZ5AJu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qe1G4eUBQLNM"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, filter_sizes, embedding_dim, dropout, n_class=2):\n",
    "        super(CNN, self).__init__()\n",
    "        #self.conv1d = nn.Conv1d(in_channels=vocab_size, out_channels=embedding_dim, kernel_size=3, stride=1, padding=0)\n",
    "        \n",
    "        # self.convs = nn.ModuleList([\n",
    "        #     nn.Conv1d(in_channels=vocab_size, out_channels=embedding_dim, \n",
    "        #               kernel_size=fs, stride=1, padding=0)\n",
    "        #     for fs in filter_sizes\n",
    "        # ])\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(in_channels=vocab_size, out_channels=embedding_dim*4, \n",
    "                      kernel_size=3, stride=1, padding=3)\n",
    "        self.maxpooling = nn.MaxPool1d(kernel_size=5)\n",
    "\n",
    "\n",
    "        self.fc2 = nn.Linear(embedding_dim*4, embedding_dim) \n",
    "        self.fc1 = nn.Linear(embedding_dim*2, n_class)             \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, features, pairs):\n",
    "        max_filter = max(filter_sizes)\n",
    "        \n",
    "        features = self.dropout(features)\n",
    "        features = features.unsqueeze(2)\n",
    "\n",
    "        features = self.conv1d(features)\n",
    "        features = self.maxpooling(features)\n",
    "        features = self.sigmoid(features)\n",
    "        features = self.dropout(features)        \n",
    "        features = features.view(features.shape[0], -1)\n",
    "        features = features.squeeze(-1)\n",
    "\n",
    "        features = self.fc2(features)\n",
    "        features = self.relu(features)\n",
    "        features = self.dropout(features)\n",
    "\n",
    "        x1 = features[pairs[0]]\n",
    "        x = features[pairs[1]]        \n",
    "        y = torch.cat((x, x1), 1)\n",
    "        del (features, x1)        \n",
    "        \n",
    "        y = self.fc1(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.dropout(y) ### we distinguish x = embedding and y = output \n",
    "        \n",
    "        return F.log_softmax(y, dim=1), x        \n",
    "         \n",
    "\n",
    "\n",
    "        #print(x.shape)\n",
    "        #print(features.shape)\n",
    "\n",
    "\n",
    "        #features = F.pad(features, (1, max_filter+1), 'constant', 0) #(1, 1): pad last dim in a 'constant' mode with the value 0\n",
    "        #features = features.unsqueeze(2)\n",
    "\n",
    "        #features_conv = self.conv1d(features)\n",
    "\n",
    "        # conv_outputs = []\n",
    "        # for conv in self.convs:\n",
    "            \n",
    "        #     features = conv(features)\n",
    "        #     features = nn.functional.max_pool1d(features, kernel_size=features.shape[2])\n",
    "        #     features = nn.functional.relu(features)\n",
    "        #     features = self.dropout(features)\n",
    "        #     conv_outputs.append(features)\n",
    "\n",
    "        # Concatenate and flatten output when filter sizes is longer than one element\n",
    "        #try: features = torch.cat(features, dim=1)\n",
    "        #except: pass\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        #print(features.shape)\n",
    "     \n",
    "\n",
    "        # x = self.fc2(x)\n",
    "        # x = self.sigmoid(x)\n",
    "        # y = self.dropout(x)       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evrXfCMq9MDn"
   },
   "outputs": [],
   "source": [
    "def early_stopping(loss_train, list_loss_train, loss_val, list_loss_val, wait=15, tolerance=0.05):\n",
    "    list_loss_val = list(list_loss_val)[-wait:]\n",
    "    list_loss_train = list(list_loss_train)[-wait:]\n",
    "    if (len(list_loss_val) == wait and loss_val >= (sum(list_loss_val)/len(list_loss_val)) and (loss_train + tolerance) < loss_val):\n",
    "        print('VAL early stop: train = {:.5f} val = {:.5f} mean val = {:.5f}'.format(loss_train, loss_val, (sum(list_loss_val)/len(list_loss_val))))\n",
    "        return True\n",
    "    elif  len(list_loss_train) == wait and loss_train >= (sum(list_loss_train)/len(list_loss_train)):\n",
    "        print('TRAIN early stop: train = {:.5f} val = {:.5f} mean train = {:.5f}'.format(loss_train, loss_val, (sum(list_loss_train)/len(list_loss_train))))\n",
    "        return True\n",
    "    elif (loss_train + tolerance) < loss_val and len(list_loss_val) >= wait:\n",
    "        print('VAL early stop: train = {:.5f} val = {:.5f} mean val = {:.5f}'.format(loss_train, loss_val, (sum(list_loss_val)/len(list_loss_val))))\n",
    "        return True \n",
    "    return False\n",
    "    \n",
    "\n",
    "    \n",
    "def train_NLP_model(model, learning_rate, features, indices, y, val_indices, y_val, epochs, batch_size, run_number):\n",
    "    # Train model\n",
    "    start_time = time()\n",
    "    print('Initializing the optimizer with learning rate:', learning_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) #optimizer with halving learning rate in training\n",
    "    try: os.mkdir('./outputs')\n",
    "    except: pass\n",
    "    print('Preparing the data for training...')\n",
    "    #indices must be a torch tensor to be able to apply size method\n",
    "    rand_indices = torch.randint(0, features.size(0), (indices.size(0),indices.size(1)), device=indices.device)# We take random indices each time we run an epoch\n",
    "    pairs = torch.cat((indices, rand_indices), dim=1).to(device) # Concatenate the edges indices and random indices. \n",
    "    del(indices, rand_indices)\n",
    "\n",
    "    today = datetime.today().strftime('%Y-%m-%d-%H:%M')\n",
    "    list_loss_val = []\n",
    "    list_loss_train = []\n",
    "    \n",
    "    #features = features.to(device)\n",
    "    #pairs = pairs.to(device)\n",
    "    \n",
    "    halving_lr = 0 # counter of the number of halving lr\n",
    "    print('Start training...')\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "        \n",
    "        permutation = torch.randperm(pairs.size()[1])\n",
    "\n",
    "        for i in range(0, pairs.size()[1], batch_size):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            elts_indices = permutation[i:i+batch_size]\n",
    "            batch_pairs = pairs[:, elts_indices]\n",
    "            batch_y = y[elts_indices]\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            output, embedding = model(features, batch_pairs) # we run the model that gives the output.\n",
    "            loss_train = F.nll_loss(output, batch_y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "            acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), batch_y.cpu().numpy())# just to show it in the out put message of the training\n",
    "            loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "            optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output, embedding = model(features, val_indices)\n",
    "        #y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        list_loss_val.append(loss_val.item())\n",
    "        list_loss_train.append(loss_train.item())\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {} s'.format(int(round(time()) - round(t))),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "        if epoch % 20 == 0:\n",
    "            model_path = \"outputs/{}-model-{}epochs-{}.pt\".format(today, epoch, run_number)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "        early = early_stopping(loss_train.item(), list_loss_train, loss_val.item(), list_loss_val, wait=30)        \n",
    "        if early:\n",
    "            halving_lr += 1\n",
    "            if halving_lr > 5:\n",
    "                break\n",
    "            list_loss_val=[]\n",
    "            list_loss_train=[]\n",
    "            learning_rate = learning_rate/10\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            print('Deviding the learning rate by 10. New learning rate: {:.6f}'.format(learning_rate))\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SD5KLOx86u1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UH0ytHBhGvAa"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "vocab_size = embedded_abstracts.shape[1]\n",
    "learning_rate = 0.1\n",
    "features = embedded_abstracts\n",
    "indices = indices\n",
    "epochs = 400\n",
    "batch_size = 2000 #2000 : 10min\n",
    "filter_sizes = [3]\n",
    "embedding_dim=64\n",
    "dropout = 0.3\n",
    "run_number = 1 #an arbitrary number to identify the run number (not really used)\n",
    "\n",
    "model_NLP = CNN(vocab_size=vocab_size, embedding_dim=embedding_dim, filter_sizes=filter_sizes, dropout=dropout).to(device)\n",
    "train_NLP_model(model_NLP, learning_rate=learning_rate, features=embedded_abstracts, indices=indices, y=y, val_indices=val_indices, y_val=y_val, epochs=epochs, batch_size=batch_size, run_number=run_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTTCoAFheRZ2"
   },
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MeDK2NXYDWmG"
   },
   "outputs": [],
   "source": [
    "# Generate tf-idf matrix\n",
    "corpus = [\"This is a sample sentence.\", \"Another sample sentence.\"]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert matrix to PyTorch tensor\n",
    "X = torch.tensor(X.toarray(), dtype=torch.float)\n",
    "print(X.shape)\n",
    "\n",
    "# Define Conv1D layer\n",
    "conv = nn.Conv1d(in_channels=X.shape[1], out_channels=32, kernel_size=1)\n",
    "\n",
    "# Apply Conv1D layer to tensor\n",
    "X = X.unsqueeze(2)\n",
    "print(X.shape)\n",
    "\n",
    "X_conv = conv(X)  # add extra dimension to tensor for batch size\n",
    "\n",
    "# Print output shape\n",
    "print(X_conv.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3NB6dirDWmH"
   },
   "outputs": [],
   "source": [
    "#tfidf_reduced[pairs[0][:5]]\n",
    "pairs[:, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekTGAyTrneWx"
   },
   "outputs": [],
   "source": [
    "features_np = features_abstracts_wv\n",
    "\n",
    "# Create class labels\n",
    "y = np.zeros(2*indices.shape[1])\n",
    "y[:indices.shape[1]] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
    "\n",
    "# Transforms the numpy matrices/vectors to torch tensors.\n",
    "features = torch.FloatTensor(features_np).to(device)\n",
    "y = torch.LongTensor(y).to(device)\n",
    "if type(adj) != torch.Tensor:\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "indices = torch.LongTensor(indices).to(device)\n",
    "val_indices = torch.LongTensor(val_indices).to(device)\n",
    "y_val = torch.LongTensor(y_val).to(device)\n",
    "\n",
    "#del (G, G_train, train_edges, val_edges, nodes, abstracts, embedded_mean_abstracts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dmob3xD4pd5R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfvPqy79pGEW"
   },
   "outputs": [],
   "source": [
    "np.transpose(node_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmFO4qI7_245"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "test_path = 'https://www.lix.polytechnique.fr/~nikolentzos/files/aai/challenge/test.txt'\n",
    "node_pairs = list()\n",
    "f = urlopen(test_path)\n",
    "\n",
    "for line in f:\n",
    "    t = str(line).split(',')\n",
    "    t[0] = int(re.sub(\"[^0-9]\", \"\", t[0]))\n",
    "    t[1] = int(re.sub(\"[^0-9]\", \"\", t[1]))\n",
    "    node_pairs.append((node_to_idx[int(t[0])], node_to_idx[int(t[1])]))\n",
    "\n",
    "node_pairs = np.transpose(node_pairs)\n",
    "node_pairs = add_authors_to_pairs(node_pairs, authors)\n",
    "node_pairs = torch.LongTensor(node_pairs).to(device)\n",
    "\n",
    "test_output = model(features, adj, node_pairs)\n",
    "y_pred = torch.exp(test_output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "model_nb = 1\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"{}-submission-{}-{}.csv\".format(today, model_nb, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2UWL4y-K1jd"
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "model.eval()\n",
    "node_pairs = np.array(np.transpose(node_pairs))\n",
    "node_pairs = torch.LongTensor(node_pairs).to(device)\n",
    "\n",
    "test_output = model(features, adj, node_pairs)\n",
    "y_pred = torch.exp(test_output)\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "y_pred_true = list()\n",
    "for element in y_pred:\n",
    "    y_pred_true.append(element[1])\n",
    "    \n",
    "\n",
    "    \n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "random_nb = randint(0, 1000)\n",
    "\n",
    "pd.DataFrame(y_pred_true, columns={'predicted'}).to_csv(\n",
    "\"{}-submission-{}-{}.csv\".format(today, model_nb, random_nb), header=True, index=True, index_label='id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwNIJmkGn6-G"
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "trained_model = train_model(model, 0.01, features, authors, adj, indices, y, torch.tensor(val_indices).to(device), torch.tensor(y_val).to(device), epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OSAu20O0zri"
   },
   "outputs": [],
   "source": [
    "print(type(features), type(adj), type(indices), type(y))\n",
    "print(features.get_device(), adj.get_device(), indices.get_device(), y.get_device())\n",
    "\n",
    "#torch.tensor(np.array(authors)).to(device).get_device()\n",
    "authors\n",
    "print(type(torch.tensor(val_indices)))\n",
    "print(torch.tensor(val_indices).to(device).get_device())\n",
    "\n",
    "type(y_val)\n",
    "\n",
    "####### randindicies to check on which device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_5ih-4EQQqW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print (torch.__version__)\n",
    "!pip install torchvision==0.14.0\n",
    "!pip install torchtext==0.14.0\n",
    "!pip install torchaudio==0.13.0\n",
    "!pip install torch==1.13.0\n",
    "import torch\n",
    "print (torch.__version__)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
