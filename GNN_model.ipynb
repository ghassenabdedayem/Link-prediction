{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7345aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unidecode\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from random import choice\n",
    "from urllib.request import urlopen\n",
    "import gzip\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "import io\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e5bade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import torch.nn as nn\n",
    "\n",
    "from read_data import read_train_val_graph\n",
    "from data_processing import create_and_normalize_adjacency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce851416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499 number of edges: 1091955 in the Complete set\n",
      "Number of nodes: 138499 number of edges: 982856 in the Training set\n",
      "len(nodes) 138499\n",
      "Returned G_train, train_edges, val_edges, y_val, nodes and node_to_idx objects\n",
      "Loaded from edgelist.txt and with a training validation split ratio = 0.1\n",
      "Graph loaded and seperated, val indices generated and node to index mapping returned in 10 s\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "G, G_train, train_edges, val_edges, val_indices, y_val, nodes, node_to_idx = read_train_val_graph()\n",
    "\n",
    "print('Graph loaded and seperated, val indices generated and node to index mapping returned in {:.0f} s'.format(time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a856425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghassenabdedayem/Documents/Data/Polytechnique/5- Data Challenge/data_challenge_2022/data_processing.py:19: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(G) # Obtains the adjacency matrix of the training graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a normalized adjancency matrix of shape (138499, 138499)\n",
      "Created indices (2, 2104211) with the positions of non zeros in adj matrix\n"
     ]
    }
   ],
   "source": [
    "adj, indices = create_and_normalize_adjacency(G_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704a427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_text, n_text_auth, n_auth, n_feat, n_hidden, n_class, sub_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        self.abstract_emb = nn.Linear(n_text_auth, n_hidden)\n",
    "        # self.abst_auth = nn.Linear(n_text_auth, n_hidden)\n",
    "        self.auth_emb = nn.Linear(n_auth, n_hidden)\n",
    "        self.fc1 = nn.Linear(n_feat+2*n_hidden, n_hidden)        \n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc21 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(2*n_hidden, n_hidden)\n",
    "        self.fc4 = nn.Linear(n_hidden, sub_class)\n",
    "        self.fc5 = nn.Linear(sub_class, n_class)        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.norm = nn.LayerNorm(n_auth+n_text+n_feat+n_text_auth)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x_in, abstract, abst_auth, auth, adj, pairs):\n",
    "\n",
    "        #y = torch.cat((abstract, abst_auth, auth, x_in), dim=1)\n",
    "        #y = torch.cat((abstract, auth, abst_auth, x_in), dim=1)\n",
    "        #y = self.norm(y)\n",
    "        y = self.abstract_emb(abstract)\n",
    "        y = self.relu(y)\n",
    "        y = self.dropout(y)\n",
    "        del(abstract)\n",
    "\n",
    "        # yz = self.abst_auth(abst_auth)\n",
    "        # yz = self.relu(yz)\n",
    "        # yz = self.dropout(yz)\n",
    "        del(abst_auth)\n",
    "\n",
    "        z = self.auth_emb(auth)\n",
    "        z = self.relu(z)\n",
    "        z = self.dropout(z)\n",
    "        del(auth)\n",
    "\n",
    "        x_in = torch.cat((x_in, y, z), dim=1)\n",
    "\n",
    "        \n",
    "        h1 = self.fc1(x_in)\n",
    "        z1 = self.relu(torch.spmm(adj, h1))\n",
    "        z1 = self.dropout(z1)\n",
    "        #del(y)\n",
    "\n",
    "        h2 = self.fc2(z1)\n",
    "        z2 = self.relu(torch.spmm(adj, h2))\n",
    "        z2 = self.dropout(z2)\n",
    "        del(h2, z1)\n",
    "\n",
    "        h2 = self.fc21(z2)\n",
    "        z2 = self.relu(torch.spmm(adj, h2))\n",
    "        z2 = self.dropout(z2)\n",
    "\n",
    "\n",
    "        z2 = torch.cat((z2, h1), dim=1)\n",
    "\n",
    "        x = z2[pairs[0]] - z2[pairs[1]]\n",
    "        # x = torch.cat((z2[pairs[0]] , z2[pairs[1]]), dim=1)\n",
    "        del(z2)\n",
    "\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "    \n",
    "        del(pairs)\n",
    "        \n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe3dd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, learning_rate, abstract, text_auth, auth, features, adj, indices, val_edges, y_val, epochs, run_number, window = 10):\n",
    "    # Train model\n",
    "    start_time = time()\n",
    "    \n",
    "    print('Initializing the optimizer with learning rate:', learning_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) #optimizer with halving learning rate in training\n",
    "    try: os.mkdir('./outputs')\n",
    "    except: pass\n",
    "\n",
    "    today = datetime.today().strftime('%Y-%m-%d-%H:%M')\n",
    "    list_loss_val = []\n",
    "    list_loss_train = []\n",
    "    list_epochs = []\n",
    "\n",
    "    \n",
    "    # Create class labels\n",
    "    y = np.zeros(2*indices.shape[1])\n",
    "    y[:indices.shape[1]] = 1 # Concatenated ones for edges indices and later in the model we add zeros for random indices.\n",
    "    y = torch.LongTensor(y).to(device)\n",
    "\n",
    "    # Create negative edges by taking a random pairs of nodes (there is 1/10,000 chance of getting a positive edge)\n",
    "    rand_indices = torch.randint(0, features.shape[0], size=(indices.shape[0],indices.shape[1])).to(device)\n",
    "    pairs = torch.cat((indices, rand_indices), dim=1)\n",
    "\n",
    "    # Creating negative val indices by taking random pairs of nodes\n",
    "    rand_indices = torch.randint(0, features.shape[0], size=(val_edges.shape[0],val_edges.shape[1])).to(device)\n",
    "    val_indices = torch.cat((val_edges, rand_indices), dim=1)\n",
    "    \n",
    "    print('Start training...')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        t = time()\n",
    "        optimizer.zero_grad()     \n",
    "\n",
    "        model.train()\n",
    "        output = model(features, abstract, text_auth, auth, adj, pairs).to(device) # we run the model that gives the output.\n",
    "        loss_train = F.nll_loss(output, y) # we are using nll_loss as loss to optimize, we store it in loss_train. We compare to y which is stable and contains the tag ones and zeros.\n",
    "        acc_train = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y.cpu().numpy())# just to show it in the out put message of the training\n",
    "        loss_train.backward() # The back propagation ? --> Computes the gradient of current tensor w.r.t. graph leaves\n",
    "        optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(features, abstract, text_auth, auth, adj, val_indices).to(device)\n",
    "        #y_val = torch.LongTensor(y_val).to(device)\n",
    "        loss_val = F.nll_loss(output, y_val)\n",
    "        list_loss_val.append(loss_val.item())\n",
    "        list_loss_train.append(loss_train.item())\n",
    "        list_epochs.append(epoch)\n",
    "        acc_val = accuracy_score(torch.argmax(output, dim=1).detach().cpu().numpy(), y_val.cpu().numpy())        \n",
    "\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {:03d}'.format(epoch+1),\n",
    "                  'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                  'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                  'time: {} s'.format(int(round(time()) - round(t))),\n",
    "                 'total_time: {} min'.format(round((time() - start_time)/60)))\n",
    "            \n",
    "\n",
    "\n",
    "    print(\"Optimization Finished in {} min!\".format(round((time() - start_time)/60)))\n",
    "    return model, list_loss_val, list_loss_train, list_epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ab639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_to_train (features, authors, adj, auth_matrix, indices, val_indices, y_val):\n",
    "    \n",
    "    print('Preparing the data for training...')\n",
    "    \n",
    "    t = time()\n",
    "    \n",
    "    y_val = torch.LongTensor(y_val).to(device)\n",
    "    \n",
    "    features = torch.FloatTensor(features).to(device)\n",
    "    \n",
    "    indices = torch.LongTensor(indices).to(device)\n",
    "    val_indices = torch.LongTensor(val_indices).to(device)\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "    auth_matrix = sparse_mx_to_torch_sparse_tensor(auth_matrix).to(device)\n",
    "    \n",
    "    print('Data converted into torch tensors and authors added to indices in {:.0f} min'.format((time()-t)/60))\n",
    "\n",
    "    return features, adj, auth_matrix, indices, val_indices, y_val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d1233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_torch, adj_torch, auth_torch, indices_torch, val_indices_torch, y_val_torch = prepare_data_to_train(walks_wv, authors, adj, auth_matrix, indices, val_indices, y_val)\n",
    "tfidf_matrix_torch = torch.FloatTensor(tfidf_reduced).to(device)\n",
    "authors_reduced_torch = torch.FloatTensor(authors_reduced).to(device)\n",
    "bert_abstract_torch = torch.FloatTensor(bert_abstract_embedding).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55637267",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "#Create the model\n",
    "n_hidden = 64\n",
    "dropout_rate = 0.2\n",
    "sub_class = 16\n",
    "n_class = 2\n",
    "text_embedding = bert_abstract_torch\n",
    "text_auth_emb = tfidf_matrix_torch\n",
    "n_text = text_embedding.shape[1]\n",
    "n_text_auth = text_auth_emb.shape[1]\n",
    "n_auth = authors_reduced_torch.shape[1] \n",
    "n_features = features_torch.shape[1]\n",
    "\n",
    "model = GNN(n_text, n_text_auth, n_auth, n_features, n_hidden, n_class, sub_class, dropout_rate).to(device)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "epochs = 220\n",
    "run_number = randint(0, 1000)\n",
    "learning_rate = 0.01\n",
    "\n",
    "trained_model, list_loss_val, list_loss_train, list_epochs = train_model(model, learning_rate, text_embedding, \n",
    "                            text_auth_emb, authors_reduced_torch, features_torch, adj_torch, indices_torch, \n",
    "                            val_indices_torch, y_val_torch, epochs, run_number)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
